- en: '**15'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PARALLEL PROCESSING**
  prefs: []
  type: TYPE_NORMAL
- en: '*If one ox could not do the job they did not try to grow a bigger ox, but used
    two oxen.*'
  prefs: []
  type: TYPE_NORMAL
- en: —Grace Hopper
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Parallel processing is a class of strategies for computation where we divide
    a problem into pieces and tackle each piece with a different computer or different
    processing units on a single computer—or a combination of both approaches. This
    chapter treats *true parallel processing*, where different computations occur
    simultaneously, and *concurrent processing*, where we ask the computer to do several
    things at once, but it may have to alternate among them.
  prefs: []
  type: TYPE_NORMAL
- en: While writing effective parallel programs can be tricky, Julia goes a long way
    toward making parallel and concurrent processing as easy as possible. The same
    program may run in parallel or merely concurrently, depending on machine resources,
    but Julia’s abstractions free us to write one version of the program that can
    take advantage of varying runtime environments.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will provide an overview of how to implement the major concurrency
    paradigms using facilities built into Julia and several convenient packages.
  prefs: []
  type: TYPE_NORMAL
- en: '**Concurrency Paradigms**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A natural distinction from the programmer’s point of view is between *multithreading*
    and *multiprocessing*, and that’s the major divide that organizes this chapter.
    This area suffers from some terminological inconsistency. We use multithreading
    to mean programming *aimed* at parallel execution on multiple CPU cores on a single
    machine. A *core* is a processing unit within a CPU chip. Each one is equipped
    with its own resources, such as caches and arithmetic logic units, and can execute
    instructions independently, although it may share some resources with other cores.
    If someone happens to run a multithreaded program on a computer with only one
    core, there won’t be any parallelism happening, but that need not concern us when
    we’re writing the program. The same code will run faster on a multicore machine
    if we’ve written it correctly.
  prefs: []
  type: TYPE_NORMAL
- en: We use multiprocessing to refer to a style of programming where we launch tasks
    that can be executed by different processes on a single machine or by multiple
    machines (or both).
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important distinction between the two styles of programming has to
    do with access to memory: all of the threads in a multithreaded program have access
    to the same pool of memory, while the processes in a multiprocessing program have
    separate memory areas.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multithreading**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section deals with speeding up the work within a single process by dividing
    it among a number of *tasks*. Since all these tasks exist within the same process,
    they all have access to the same memory space. The task is the basic concept upon
    which Julia’s parallel and concurrent processing is built. It’s a discrete unit
    of work, usually a function call, that’s assigned to a particular thread by the
    *scheduler*. Tasks are inherently asynchronous; once launched, they continue on
    their assigned thread until they’re done or suspend themselves by yielding to
    the scheduler. However, we can synchronize and orchestrate the tasks’ life cycles
    in various ways.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*You may have done parallel computing with Julia without knowing it. Many linear
    algebra routines, including the matrix multiplication dispatched by *, run multithreaded
    BLAS (Basic Linear Algebra Subprograms) routines that automatically take advantage
    of all CPU cores, transparently to the user. You can verify this by executing
    a matrix multiply in the REPL and keeping an eye on your CPU meters.*'
  prefs: []
  type: TYPE_NORMAL
- en: When we enter the Julia REPL or use the `julia` command to run a program stored
    on the disk, we have several available command line options. Unless we use the
    `-t` option, Julia uses exactly one thread (and, consequently, one CPU core),
    no matter the hardware configuration on which it’s running.
  prefs: []
  type: TYPE_NORMAL
- en: To allow Julia to use all the available threads, use the `-t auto` argument.
    In that case, all of the “available” threads will be all of the *logical* threads
    on the machine. This is often not optimal. A better choice can be `-t` n, where
    n is the number of *physical* cores. For example, the popular Intel Core processors
    provide two logical cores for each physical core using a technique called *hyperthreading*.
    Hyperthreading can yield anything from a modest speedup to an actual slowdown,
    depending on the type of calculation.
  prefs: []
  type: TYPE_NORMAL
- en: On Linux we can use the `lscpu` command at the system shell to get information
    about the CPU. For example, if the output contains the lines
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: then the machine has a total of two physical compute cores and four logical
    threads provided by hyperthreading. We usually need to experiment to discover
    whether `-t` n (in this case, `-t 2`) or `-t auto` leads to a better outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Within a program, or in the REPL, we can check for the number of available threads
    with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: which reports the total number in use and is blind to how many of them represent
    real cores.
  prefs: []
  type: TYPE_NORMAL
- en: With multiple threads, we can speed up our programs by assigning tasks to run
    on more than one CPU core simultaneously, either automatically or by applying
    various levels of control.
  prefs: []
  type: TYPE_NORMAL
- en: '***Easy Multithreading with Folds***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One automatic way of launching tasks is with the `Folds` package, which provides
    multithreaded versions of `map()`, `sum()`, `maximum()`, `minimum()`, `reduce()`,
    `collect()`, and a handful of other functions over collections. Its use is as
    easy as replacing, for example, `sum()` with `Folds.sum()`. The parallelized function
    takes care of dividing the work among all the available threads.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, [Listing 15-1](ch15.xhtml#ch15lis1) shows the parallelized map
    of an expensive function over an array.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 15-1: Easy parallelism with* Folds.jl'
  prefs: []
  type: TYPE_NORMAL
- en: The `@belapsed` macro is part of `BenchmarkTools`. Like the `@btime` macro that
    we’ve used before, it runs the job repeatedly and reports an average of resource
    utilizations. This version is convenient when we just want the CPU time consumed.
  prefs: []
  type: TYPE_NORMAL
- en: The parallelized version of `map()` gives each thread an approximately equal
    portion of the loop over 5,001 numbers. Ideally, the total compute time should
    be 1/*N*, where *N* is the number of threads. Behind the scenes, it’s creating
    tasks, each with some portion of the loop, and assigning them to available threads;
    it may use two tasks, or more. It also synchronizes the computation, waiting for
    all the tasks to complete before returning.
  prefs: []
  type: TYPE_NORMAL
- en: This REPL session was started using the `-t 2` flag. The results show that the
    parallel version used just slightly more than half the time of the serial computation.
    Since we are running on two (physical) threads, the result indicates an almost
    ideal parallel speedup.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we’re not always so lucky. Whether parallelizing a computation helps,
    hinders, or has no effect is the result of the trade-off between the overhead
    of setting up and managing a set of tasks and the benefits of dividing up the
    work. It’s sensitive to the cost of the calculation per array element, the size
    of the array, and the patterns of memory access. The same calculation on a smaller
    array has a better outcome using the serial `map()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, working on a single processor is actually faster than trying to parallelize
    the short computation. Successful parallel computing requires a good deal of testing.
    We need to ensure that we’re taking good advantage of the hardware and that the
    results running on multiple cores are identical to the results run serially, aside
    from small numerical differences that reordering of floating-point calculations
    can cause in some programs.
  prefs: []
  type: TYPE_NORMAL
- en: '***Manual Multithreading with @threads***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `Folds` package is a higher-level interface to the manual multithreading
    that’s the subject of this section. Going manual requires more care, but it can
    provide an extra degree of control that we sometimes need.
  prefs: []
  type: TYPE_NORMAL
- en: '**Threads.@threads**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The main facility for multithreading in Julia is the `Threads.@threads` macro,
    which is part of `Base`, so it’s always available. To run a loop in parallel,
    we preface it with the macro. As an introduction, [Listing 15-2](ch15.xhtml#ch15lis2)
    tackles the same problem as in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 15-2: Timing a threaded loop*'
  prefs: []
  type: TYPE_NORMAL
- en: Apparently, the `@threads` version performs similarly to the wrapper from the
    `Folds` package.
  prefs: []
  type: TYPE_NORMAL
- en: The `@threads` macro works by dividing the loop into *N* segments and assigning
    each segment to a separate task. The scheduler apportions these tasks among the
    available threads. Normally *N* is a small multiple of the number of threads,
    so if we have two cores and have used the `-t 2` flag, `@threads` will probably
    divide the loop over 5,001 elements into two or four loops of approximately equal
    length.
  prefs: []
  type: TYPE_NORMAL
- en: The `@threads` loop is synchronized in the sense that computation does not continue
    past the end of the loop until all tasks are complete. Different parts of the
    loop, hence different tasks, may take different amounts of time. If this difference
    is large, some threads will be idle waiting for the others to catch up. This is
    why, as mentioned previously, this style of multithreading works best when all
    iterations take roughly the same computing time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of throwing out the result, let’s try adding together all the `f(x)`s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The parallel results not only differ from the serial result, but it seems that
    we can get different answers for different runs of the parallel program. What
    did we do wrong?
  prefs: []
  type: TYPE_NORMAL
- en: '**Atomic Theory**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The problem arises when we update `s` within the parallel loop. Multiple independent
    threads trying to access and write to the same scalar variable creates a *race
    condition*, a conflict where the result depends on an order of operations which
    the program does not control. We can get different results from different runs
    because the timings will differ, based on unknown influences such as the other
    tasks that the operating system happens to be performing during the run. There’s
    no problem when updating array locations because in the threaded loop, arrays
    will be divided among the threads and no thread will step on another thread’s
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Julia provides several strategies for protecting a scalar during multithreaded
    execution. One way is to use *atomic variables*, as [Listing 15-3](ch15.xhtml#ch15lis3)
    shows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 15-3: Using an atomic variable*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve initialized `s` as an atomic variable using the built-in `Threads.Atomic`
    declaration. It allows only simple types: the various floats, integers, and the
    `Bool` type. We update atomic variables using a small collection of functions
    for the purpose, all namespaced with `Threads`. In addition to `Threads.atomic_add!()`,
    we have `atomic_sub!()` for subtraction, several logical operators, `atomic_xchg!()`
    for setting the variable to a new value, and a few more. We access the value of
    an atomic variable with the odd-looking syntax in the last line of the program.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is close to the serial result, so the atomic variable fixed the
    problem. The results are close, but not equal: they vary in the last few decimal
    places. The result of a series of floating-point operations can depend on their
    order, and the order varies between serial and parallel runs and among parallel
    runs with different numbers of threads. We’ll also get a minutely different result
    if we run the serial code counting backward in the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: These small variations in the least significant parts of the answers are normal
    and expected, and are something that the numericist must be alert to when comparing
    the results from a parallelized program when run on different computers with possibly
    different numbers of cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also get a correct summation using a different strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We’ve essentially given each thread its private copy of the summation variable
    and added all the copies together at the end. We use `Threads.nthreads()` to create
    a vector the same length as the number of threads. Within each thread, `Threads.threadid()`
    returns that thread’s unique integer identifier. We use this identifier to index
    into the array of summations ➊, ensuring that each thread updates only the element
    that belongs to it. The sum of sums in the last line should be the same as the
    scalar `s` in the other versions of the program.
  prefs: []
  type: TYPE_NORMAL
- en: The technique of using an array instead of an atomic variable can be faster,
    because before a thread is allowed to read or update an atomic variable, it must
    wait until it’s released by any other thread that’s using it. The use of arrays
    eliminates this *locking* and the consequent waiting time. However, it uses a
    bit more memory for the new arrays.
  prefs: []
  type: TYPE_NORMAL
- en: '***Spawning and Synchronizing Tasks***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The techniques we’ve described in the previous two sections implement parallelism
    by dividing the work among tasks and launching them behind the scenes. Here we’ll
    learn how to take control of spawning and synchronizing tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Launching Tasks with Threads.@spawn**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can also launch tasks manually with the `Threads.@spawn` macro, as shown
    in [Listing 15-4](ch15.xhtml#ch15lis4).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 15-4: Introducing task spawning*'
  prefs: []
  type: TYPE_NORMAL
- en: Since `@belapsed` and the other benchmarking tools in `BenchmarkTools` run code
    multiple times, we place the timed code within a function to force the atomic
    variable to be initialized in each trial run.
  prefs: []
  type: TYPE_NORMAL
- en: The `@sync` macro ➊ works for any block, not just `for` loops. It synchronizes
    all tasks launched within the lexical scope of the block, which means that the
    statement following its `end` statement will wait until they’re all done. In [Listing
    15-4](ch15.xhtml#ch15lis4), `@sync` ensures that when we access `s[]`, it will
    have its final value, and that the timings include the time for completion of
    all tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The block in [Listing 15-4](ch15.xhtml#ch15lis4) is a version of the function
    in [Listing 15-3](ch15.xhtml#ch15lis3), with manually spawned tasks. In general,
    the loop
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: is semantically equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: but their implementations are different, in that, as mentioned earlier, `Threads`
    `.@threads` is *coarse-grained*, dividing the loop into a small number of tasks.
    The manually spawned version creates a new task for every loop iteration.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the timings in these two examples are almost the same demonstrates
    that spawning a task in Julia has almost no overhead; we can spawn thousands of
    tasks with little performance penalty. If we move a program using tasks to a different
    machine with more cores, it should run faster with no changes required on our
    part.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*In this chapter we perform many timings on bare loops at the top level in
    order to compare the effects of different approaches to concurrency and parallelism
    in as few lines of code as possible. In developing a real program, all timing
    studies should be on functions, preferably in modules. Many compiler optimizations
    are available only for code in functions.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synchronizing**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Using `Folds.map()` or `@threads` synchronizes tasks for us. However, if we
    launch tasks with `Threads.@spawn` manually, we can’t know which have completed
    their work at any particular point in the program. That’s why the program in [Listing
    15-4](ch15.xhtml#ch15lis4) needs a `@sync` macro.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example illustrates what can happen if we neglect synchronization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this program, we’ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Each loop iteration launches a task that mutates the global array, writing to
    one of its locations. However, at the end of the loop, the array `W` doesn’t seem
    to have changed.
  prefs: []
  type: TYPE_NORMAL
- en: Each `@spawn` sends off a task to do its work, and the loop continues to the
    next iteration immediately. Although each spawned job has a built-in delay created
    by the `sleep()` call, the loop itself is complete almost instantaneously. We
    then execute the statement following the loop, printing the value of `W`, which
    hasn’t yet been written to.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to wait at the end of the loop for all tasks spawned within it to
    complete, so that `W` is up to date, we use the `@sync` macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this program, we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of synchronizing all the tasks within a block, we can wait for some
    of them to complete, letting the others run their courses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We initialize a `jobs` vector to hold the return values of each call to `@spawn`.
    These are `Task`s, a data type that holds information about an asynchronous task.
    The `wait()` function pauses execution until its argument is ready. We change
    the loop a bit to wait on each iteration for `i` seconds, so each task will take
    longer than the preceding one. As soon as the second job is complete, the next
    instruction, printing `W`, is run.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program produces this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can see that when the `println()` statement is reached, the first two elements
    of `W` are modified, but the remaining tasks are still running (sleeping).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful synchronization function is `fetch()`. Like `wait()`, it receives
    a `Task` as an argument and waits for the task to finish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'That function prints this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Since the result returned by an assignment is the value assigned, the task that
    executes `W[2] = 2` returns 2, and this gets assigned to `job2` by the call to
    `fetch()`. The condition of `W` at this point is its state immediately after the
    second task is complete.
  prefs: []
  type: TYPE_NORMAL
- en: '**Yielding**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: After the scheduler places tasks on all available threads, any remaining spawned
    tasks are on the *queue*, waiting for their turn to run. They will have to wait
    until one of the running tasks finishes or *yields* its place. This system is
    called *cooperative multitasking*, and it’s the model Julia usually applies to
    task scheduling. Some operations cause a task to yield automatically. The more
    important ones are waiting for I/O and sleeping. But if a program involves multiple
    tasks that perform long calculations, it’s our job to break up the calculations
    manually and insert `yield()`s in order to provide opportunities for other tasks
    to run, *unless* we don’t mind each thread waiting for each expensive task on
    it to finish (which may indeed be acceptable).
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 15-5](ch15.xhtml#ch15lis5) contains two functions that each do the
    identical piece of busywork, applying `f(x)`, which was defined in [Listing 15-1](ch15.xhtml#ch15lis1),
    to a range of numbers. The difference between the two functions is that the first
    does the job in one lump, while the second divides the range into two halves,
    calling `yield()` between them. The `yield()` function tells the scheduler that
    it may suspend the task and run the next task from the queue, if there is one
    waiting. After that task is complete, the suspended task will resume.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 15-5: Inserting an opportunity to yield*'
  prefs: []
  type: TYPE_NORMAL
- en: The functions assume the existence of a global array named `times`. They place
    the result of calling `time()`, within a tuple with the integer `n` identifying
    the task, onto the end of this array as soon as they begin and just before they
    return. The `time()` function returns the system time in seconds to approximately
    microsecond precision. Its value is uninteresting, but we can use the difference
    between two calls to `time()` to find out how much time passed between two code
    locations, which is a pretty accurate measure of how long the intervening calculation
    took.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 15-6](ch15.xhtml#ch15lis6) spawns three tasks using the first function,
    recording the saved times, and then does the same using the modified function
    with the `yield()` call.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 15-6: Testing the effects of yielding*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-1](ch15.xhtml#ch15fig1) plots the task numbers versus the *elapsed*
    times from the start of each thread-spawning loop, for experiments on a single
    thread.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch15fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-1: Timings for cooperative and selfish tasks*'
  prefs: []
  type: TYPE_NORMAL
- en: We can see from [Figure 15-1](ch15.xhtml#ch15fig1) that each complete loop takes
    about 5.5 seconds. The experiment with yielding (circles) shows that each task
    does its half loop and then allows the next task in the queue to run. It doesn’t
    resume until all subsequent tasks have completed their first halves and yielded.
    In the experiment without yielding (hexagons), each task monopolizes the thread
    until it’s finished.
  prefs: []
  type: TYPE_NORMAL
- en: With only one thread active, the order of task operations is predictable. Also,
    when using one thread it’s impossible for yielding or any rearrangement of tasks
    to decrease the time to completion of all the calculations; we can’t get something
    for nothing. However, in cases where lighter tasks are mixed with more time-consuming
    ones, allowing the latter to yield will get us access to the results of the lighter
    tasks sooner, which can be desirable in some programs. When multiple threads are
    available, yielding gives the scheduler a chance to migrate tasks among threads,
    keeping them all occupied and potentially increasing the total throughput. This
    rearrangement of tasks is called *load balancing*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiprocessing**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we decide to run with Grace Hopper’s metaphor that starts this chapter, we
    might say that the multithreading explored in the previous section amounts to
    yoking together a team of oxen to pull a big load, while we can compare the multiprocessing
    explored in this section to dividing the load into separate carts and letting
    each ox pull its cart at its own pace.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing and distributed computing are closely related concepts, and
    the two terms are often used interchangeably. This style of computation divides
    the work into multiple *processes* that have their own memory spaces. The processes
    may share resources on a single computer or on multiple networked computers. Julia’s
    abstractions make it possible to write the multiprocessed program once and run
    it in a variety of environments.
  prefs: []
  type: TYPE_NORMAL
- en: Because the various processes don’t have access to the same memory, any data
    that they need must be copied and sent to them, possibly over a network. Because
    of this, distributed computing is most suited to handling time-consuming tasks
    on small data, especially if computing resources are communicating over a slow
    network such as the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Running on a cluster uses multiprocessing to distribute the work to an array
    of processors usually communicating over a higher-bandwidth network, combined
    with the multithreading of the previous section to make the best use of each node.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing is based on the same concept of an asynchronous task that forms
    the basis of the multithreading described previously. It adds the concept of the
    process and the possibility of spawning tasks on more than one process. It allows
    us to do this automatically or with control of individual tasks, with program
    interfaces similar to the ones we explored with multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: '***Easy Multiprocessing with pmap***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To start the Julia REPL or runtime in multiprocessor mode, use the `-p` flag.
    As with the `-t` flag, it usually makes the most sense to ask for a number of
    processes equal to the number of hardware threads available. On a machine with
    two cores, start Julia using `julia -p2`. This creates two *worker processes*
    that can accept tasks. We’ll have (in this case) a total of three processes: the
    two workers and the executive process, in which the REPL runs if we’re working
    interactively. We can assign tasks to workers automatically or by specifying the
    process number.'
  prefs: []
  type: TYPE_NORMAL
- en: With the `-p2` flag, each process will be single-threaded, and each will run
    on its own thread on the two-core machine. We can also use the flags `-p2 -t2`,
    which creates two worker processes, each with access to two threads. Then we have
    the option of spawning tasks on either process, and, within each task, running
    multithreaded or multiprocessing loops. At this point it may seem as if we have
    too many options, and that it would be difficult to decide on a strategy. One
    rational approach that takes good advantage of all available computing resources
    is to launch one worker process on each remote machine, using the mechanism described
    in the next section, and use the `-t auto` flag. This strategy allows each networked
    machine to use all its available threads for shared-memory parallel computing
    and helps to avoid unnecessary data movement.
  prefs: []
  type: TYPE_NORMAL
- en: Starting Julia with the `-p` flag automatically performs the equivalent of `using
    Distributed`, loading the standard library package that provides utilities for
    multiprocessing. We can retrieve the number of available processes with `nworkers()`,
    provided by `Distributed`. One of the useful utilities from `Distributed` is `pmap()`,
    a distributed version of `map()`, as shown in [Listing 15-7](ch15.xhtml#ch15lis7).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 15-7: The distributed map*'
  prefs: []
  type: TYPE_NORMAL
- en: Since each process has its own memory, we have to give copies of all function
    definitions to the workers. That’s what the `@everywhere` macro does ➊. We also
    need to decorate module imports, constant definitions, and everything else that
    the workers need to use with `@everywhere`.
  prefs: []
  type: TYPE_NORMAL
- en: Once all the workers have copies of the `f()` function, we can repeat our timing
    tests from [Listing 15-1](ch15.xhtml#ch15lis1) using `pmap()`. This works similarly
    to `Folds.map()`, but instead of orchestrating a synchronized computation by spawning
    tasks to multiple threads in the current process, it spawns tasks in multiple
    processes. If we’ve launched Julia with the worker process number equal to the
    number of physical cores, as suggested earlier, normally each of the processes
    launched by `pmap()` will occupy its own hardware thread, and `pmap()` will assign
    tasks to processes, and hence to threads, in a way that attempts to balance the
    load.
  prefs: []
  type: TYPE_NORMAL
- en: '***Networking with Machine Files***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Julia makes multiprocessing on a collection of networked computers almost as
    easy as on a single computer. The first step is to create a text file that contains
    the network addresses of the machines that we want to enlist in helping with the
    calculation, along with some other details. The machines in question must have
    Julia installed, and should contain a directory path identical to the path from
    which we’re running the controlling program. We need to have passwordless `ssh`
    access to each machine. Leaving out some optional details, the *machine file*
    contains one line per machine, in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here n is the number of workers to start on the machine at host, which can be
    an IP address or a hostname that the controlling computer can resolve. The `:`port
    part is optional and only needed for nonstandard `ssh` ports (ports other than
    22).
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, I put two computers into a machine file named *machines*.
    Here’s the entire file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Both hostnames are resolved into their IP addresses by entries in my */etc/hosts*
    file. I could have used the IP numbers directly as well. The computer called `tc`
    is in my house, and `pluton`, a server I maintain mostly for serving Pluto notebooks
    that I drafted for this exercise, is about 1,200 miles away. It listens on port
    86 for `ssh` connections, whereas `tc` uses the standard port. The machine file
    specifies that each machine will use two worker processes.
  prefs: []
  type: TYPE_NORMAL
- en: To start a REPL that will use these remote resources as well as two worker processes
    on the machine where the REPL is running, we execute
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: omitting other options, such as specifying a project directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a modest delay, we get a REPL prompt. At this point the Julia workers
    on both remote computers are running and waiting to receive tasks. Let’s check
    that everyone’s listening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The host `sp3` is the laptop where the REPL is running. We use `pmap()` to launch
    six processes, asking each one to run the system command `hostname`. There’s no
    guarantee that they’ll be equally divided, as it turns out in this example, or
    that every machine receives a job—but in this case it turns out that six tasks
    was enough. Using `run()` provides a report identifying which worker ID is assigned
    to which machine. If we need merely the output from the shell command, we can
    use `readchomp()` instead of `run()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The worker numbers range from 2 to 7 because process 1 is the REPL process.
    We can get a list of workers anytime with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Let’s repeat the timing in [Listing 15-7](ch15.xhtml#ch15lis7) on our three-machine
    network, as shown in [Listing 15-8](ch15.xhtml#ch15lis8).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 15-8: A distributed map over a network of computers*'
  prefs: []
  type: TYPE_NORMAL
- en: The machines `pluton` and `tc` each have two CPU cores, so we have tripled the
    number of cores available for the calculation. We did observe a speedup, but only
    by about 50 percent over performing the calculation confined to the local machine.
    Computing over the internet incurs significant overhead. Monitoring the remote
    machine’s CPU usage shows that both of `tc`’s CPU cores were active during the
    calculation, at about 70 percent utilization, while `pluton`’s two cores were
    nearly quiescent. The ping time to `pluton` during the experiment was about 50
    times longer than to `tc`, as we might expect from their relative distances. Clearly
    Julia’s scheduler sent more units of work to the closer computer while waiting
    to receive responses from the distant machine.
  prefs: []
  type: TYPE_NORMAL
- en: '***Going Manual with @spawnat***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `@spawnat` macro spawns an asynchronous task, just as `@spawn` does, but
    on a worker process. We can leave the decision about which process is to receive
    the task by using `@spawnat :any`, or pick one with `@spawnat` n. The macro is
    part of `Distributed`, so it is always available if we’ve started Julia with the
    `-p` flag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check that the macro does what we expect by using it to ask each machine
    to report its hostname:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `myid()` function returns the process number of the process where it is
    called. The program prints this message when run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We observed a modest speedup when running a `pmap()` over a network of three
    computers in [Listing 15-8](ch15.xhtml#ch15lis8). [Listing 15-9](ch15.xhtml#ch15lis9)
    shows what happens if we try a version of the loop with manual spawning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 15-9: Spawning too many distributed processes*'
  prefs: []
  type: TYPE_NORMAL
- en: We would observe terrible performance, worse than performing the calculation
    on a single thread. This is because, unlike the coarse-grained concurrency that
    `pmap()` transforms the loop into, the manual multiprocessing in this loop launches
    thousands of tasks on a handful of processes. Each one requires interprocess communication
    to manage, which far outweighs any gains from concurrency. The situation is different
    from the version in [Listing 15-4](ch15.xhtml#ch15lis4), where the fine-grained
    loop performs as well as the coarse-grained loop, because in that case, all computation
    takes place on one process. Creating tasks within a process is very cheap, but
    interprocess communication is not; therefore, `@spawnat` is best used for small
    numbers of expensive tasks that don’t require massive copying of data.
  prefs: []
  type: TYPE_NORMAL
- en: '***Multiprocessing Threads with @distributed***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The multiprocessed analogy to the `Threads.@threads` macro is the `@distributed`
    macro. While the former divides a loop into a coarse-grained set of tasks on the
    available threads of the local machine or process, the latter divides a loop into
    a coarse-grained set of tasks spawned across processes, which may be across machines
    on a network.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 15-10](ch15.xhtml#ch15lis10) shows the `@distributed` version of the
    threaded loop in [Listing 15-2](ch15.xhtml#ch15lis2).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 15-10: Using* @distributed'
  prefs: []
  type: TYPE_NORMAL
- en: I performed this timing test on my little network of three machines, each with
    two CPU cores. It’s the best time we’ve achieved for this loop so far. We need
    to use the `@sync` macro with this use of `@distributed`, unlike with `Threads.@threads`,
    which always synchronizes. (Even though we’re not using the results of the calculations,
    leaving off the `@sync` renders the timing meaningless, as in that case the loop
    will return immediately after spawning its tasks.)
  prefs: []
  type: TYPE_NORMAL
- en: 'A common pattern is to combine the results from each iteration of a loop, as
    we did in [Listing 15-4](ch15.xhtml#ch15lis4), using an atomic variable. If we
    insert a function between the `@distributed` macro and the `for` keyword, the
    macro will gather the results from each iteration, reduce them using the function,
    and return the result of combining the reductions from each process. Since returning
    this final result implies synchronization, we can leave off the `@sync` when supplying
    a reduction function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The loop is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: which also automatically performs a reduction across multiple processes.
  prefs: []
  type: TYPE_NORMAL
- en: Why is the loop in [Listing 15-10](ch15.xhtml#ch15lis10) faster than the `pmap()`
    version shown in [Listing 15-8](ch15.xhtml#ch15lis8)? Both approaches perform
    the same calculation distributed over the same machines. As always, when setting
    out to increase performance through concurrency, we’re obligated to analyze the
    workloads in our programs. The loop in this case is over 5,001 function evaluations
    that are nontrivial, but also not very expensive (on the local machine `f(105_000)`
    takes 2.77 ms to evaluate). The `pmap()` function, by default, spawns a new task
    for each iteration of the loop. The scheduler will attempt load balancing by apportioning
    these tasks to various processes as they’re spawned. The speedup through concurrency
    is partially offset by the overhead of scheduling and interprocess communication.
    Due to these considerations, `pmap()`, with no additional tuning parameters, works
    best with a small number of expensive tasks, which doesn’t describe the situation
    in this example.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the coarse-grained concurrency of the `@distributed` loop works
    well in this case, with a large number of relatively light tasks. Far fewer tasks
    are spawned, and more computer time is devoted to calculation, with less interprocess
    communication and scheduling overhead.
  prefs: []
  type: TYPE_NORMAL
- en: In the multithreaded examples, there’s little difference in performance between
    the coarse-grained `Threads.@threads` version and the fine-grained `Folds.map()`
    version. This is because there’s no interprocess communication in that case and
    spawning tasks is very fast.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can tell `pmap()` to break up the loop into larger chunks using the `batch_size`
    keyword argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The default for `batch_size` is 1, meaning one task spawned for each iteration.
    A `batch_size` of n divides the loop into segments of length *up to* n, sending
    each loop segment off to a worker process as a separate task. The example shows
    that we can get performance similar to the `@distributed` loop from `pmap()` by
    dividing the work into halves.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary of Concurrency in Julia**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s likely that any program intended for large-scale, high-performance computing
    will take advantage of a combination of multiprocessing and multithreading. The
    former allows the program to distribute its work over the nodes of a supercomputing
    cluster, while the latter exploits multiple cores on each node. Therefore, Julia
    programs are often run using combinations of startup flags such as `-p`, `-t`,
    and a reference to a `--machine-file`.
  prefs: []
  type: TYPE_NORMAL
- en: Julia’s abstractions go far in allowing us to write one version of our program
    that will run fast on a single thread, faster on multicore hardware, and even
    faster on a network of computers. Nevertheless, for the best performance, we can’t
    escape the need to carefully consider the patterns of computation in our programs
    and make it possible for Julia’s scheduler and the operating system to take the
    best advantage of the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 15-1](ch15.xhtml#ch15tab1) is a highly simplified summary of the main
    utilities for parallel and distributed processing that we’ve explored throughout
    this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 15-1:** Multithreaded and Distributed Processing'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Threaded (shared memory)** | **Distributed (private memory)**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Startup** | `julia -t n` | `julia -p n` |'
  prefs: []
  type: TYPE_TB
- en: '| **Loops** | `Threads.@threads for` | `@distributed for` |'
  prefs: []
  type: TYPE_TB
- en: '| **Maps** | `Folds.map()` | `pmap()` |'
  prefs: []
  type: TYPE_TB
- en: '| **Launch task** | `Threads.@spawn` | `@spawnat (p` or `:any)` |'
  prefs: []
  type: TYPE_TB
- en: Before tuning the parallelization of a program, we should strive to achieve
    the best single-thread performance possible, by applying the optimization principles
    discussed in previous chapters. The most important of these are type stability,
    correct order of memory accesses, and caution around globals. However, even more
    important than these common pitfalls is the choice of an appropriate algorithm,
    a subject largely beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The subject of concurrency in Julia is large and complicated, and could consume
    a book of this size on its own. The next topics of interest after mastering the
    material in this chapter might be using *shared arrays*, which allow multiprocessing-style
    programming using shared memory; *GPU programming*, which uses a graphical processing
    unit as an array processor; and using the *message passing interface (MPI)* library,
    which is popular in Fortran programs for high-performance scientific computing,
    from within Julia. “Further Reading” contains links to starting points for all
    of these topics.
  prefs: []
  type: TYPE_NORMAL
- en: '**FURTHER READING**'
  prefs: []
  type: TYPE_NORMAL
- en: The `Folds` package resides at [*https://github.com/JuliaFolds/Folds.jl*](https://github.com/JuliaFolds/Folds.jl).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“A quick introduction to data parallelism in Julia” by Takafumi Arakaki, the
    author of `Folds.jl`, is especially welcome, as `Folds` has little documentation:
    [*https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/*](https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General Julia performance tips are available at [*https://docs.julialang.org/en/v1/manual/performance-tips/*](https://docs.julialang.org/en/v1/manual/performance-tips/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For documentation on shared arrays, visit [*https://docs.julialang.org/en/v1/stdlib/SharedArrays/*](https://docs.julialang.org/en/v1/stdlib/SharedArrays/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GitHub organization JuliaGPU ([*https://juliagpu.org*](https://juliagpu.org))
    serves as an umbrella for Julia packages that implement or can exploit graphics
    processing units for parallelization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of GPU programming with Julia are available at [*https://enccs.se/news/2022/07/julia-for-hpc*](https://enccs.se/news/2022/07/julia-for-hpc).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The JuliaParallel GitHub organization is home to a number of packages for parallel
    computing in Julia, including the `MPI` package ([*https://github.com/JuliaParallel/MPI.jl*](https://github.com/JuliaParallel/MPI.jl)
    and the `ClusterManagers` package ([*https://github.com/JuliaParallel/ClusterManagers.jl*](https://github.com/JuliaParallel/ClusterManagers.jl))
    for managing job schedulers like Slurm on high-performance computing clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
