- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: Autoencoders
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: 'This chapter is about a particular kind of learning architecture called an
    *autoencoder*. One way to think about a standard autoencoder is that it’s a mechanism
    for compressing input, so it takes up less disk space and can be communicated
    more quickly, much as an MP3 encoder compresses music, or a JPG encoder compresses
    an image. The autoencoder gets its name from the idea that it *automatically*
    learns, by virtue of training, how best to *encode*, or represent, the input data.
    In practice, we usually use autoencoders for two types of jobs: removing the noise
    from a dataset, and reducing the dimensionality of a dataset.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一种特定类型的学习架构，叫做*自编码器*。可以把标准的自编码器看作是一种压缩输入的机制，这样它占用更少的磁盘空间，并能更快地进行传输，就像MP3编码器压缩音乐，或者JPG编码器压缩图像一样。自编码器得名于它通过训练*自动*学习如何最佳地*编码*或表示输入数据的理念。实际上，我们通常使用自编码器来完成两种任务：从数据集中去除噪声和减少数据集的维度。
- en: We begin this chapter by looking at how to compress data while preserving the
    information we care about. Armed with this information, we look at a tiny autoencoder.
    We’ll use it to get our bearings and to discuss key ideas about how these systems
    work and how their version of representing the data lets us manipulate it in meaningful
    ways. Then we’ll make a bigger autoencoder and look more closely at its data representation.
    We’ll see that the encoded data has a surprising amount of natural structure.
    This enables us to use the second half of the autoencoder as a standalone *generator*.
    We can feed the generator random inputs and get back new data that looks like
    the training data but is, in fact, a wholly new piece of data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从研究如何在保留我们关心的信息的同时压缩数据开始。掌握了这些信息后，我们将查看一个小型自编码器。我们将使用它来了解基本概念，并讨论这些系统如何工作，以及它们如何通过数据表示让我们能够以有意义的方式操作数据。接下来，我们将构建一个更大的自编码器，进一步深入研究其数据表示。我们会发现，编码后的数据具有出乎意料的自然结构。这使得我们能够将自编码器的第二部分作为一个独立的*生成器*使用。我们可以为生成器输入随机数据，然后得到看起来像训练数据的新数据，但实际上是完全新的数据。
- en: We then expand our network’s usefulness by including convolution layers, which
    enable us to work directly with images and other 2D data. We will train a convolution-based
    autoencoder to *denoise* grainy images, giving us back a clean input. We wrap
    the chapter up with a look at *variational* *autoencoders*, which create a more
    nicely organized representation of the encoded data. This makes it even easier
    to use the second part as a generator, since we will have better control over
    what kind of data it will produce.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过包含卷积层来扩展网络的功能，使我们能够直接处理图像和其他二维数据。我们将训练一个基于卷积的自编码器来*去噪*颗粒状图像，从而恢复干净的输入。我们在本章结束时将讨论*变分*
    *自编码器*，它能够创建一个更为整齐的编码数据表示。这使得将自编码器的第二部分作为生成器使用变得更加容易，因为我们能够更好地控制它将生成何种数据。
- en: Introduction to Encoding
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码简介
- en: Compressing files is useful throughout computing. Many people listen to their
    music saved in the MP3 format, which can reduce an audio file by a huge amount
    while still sounding acceptably close to the original (Wikipedia 2020b). We often
    view images using the JPG format, which can compress image files down by as much
    as a factor of 20 while still looking acceptably close to the original image (Wikipedia
    2020a). In both cases, the compressed file is only an approximation of the original.
    The more we compress the file (that is, the less information we save), the easier
    it is to detect the differences between the original and the compressed version.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩文件在计算中非常有用。许多人听着以MP3格式保存的音乐，这种格式能够大幅压缩音频文件，同时仍然保持与原音接近的音质（维基百科2020b）。我们常常使用JPG格式查看图像，这种格式能够将图像文件压缩到原始大小的20倍，同时仍然看起来与原图接近（维基百科2020a）。在这两种情况下，压缩后的文件只是原始文件的近似值。我们压缩文件越多（即保存的信息越少），就越容易检测到原始文件和压缩版本之间的差异。
- en: We refer to the act of compressing data, or reducing the amount of memory required
    to store it, as *encoding*. Encoders are part of everyday computer use. We say
    that both MP3 and JPG take an *input* and *encode* it; then we *decode* or *decompress*
    that version to *recover* or *reconstruct* some version of the original. Generally
    speaking, the smaller the compressed file, the less well the recovered version
    matches the original
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将压缩数据或减少存储数据所需内存的行为称为*编码*。编码器是日常计算机使用的一部分。我们说MP3和JPG都会接受*输入*并*编码*它；然后我们*解码*或*解压*这个版本，*恢复*或*重建*原始的某个版本。一般来说，压缩文件越小，恢复后的版本与原始版本的匹配度越差。
- en: The MP3 and JPG encoders are entirely different, but they are both examples
    of *lossy encoding*. Let’s see what this means.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: MP3和JPG的编码器完全不同，但它们都是*有损编码*的例子。让我们来看一下这是什么意思。
- en: Lossless and Lossy Encoding
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无损与有损编码
- en: In previous chapters we used the word *loss* as a synonym for error, so our
    network’s error function was also called its loss function. In this section, we
    use the word with a slightly different meaning, referring to the degradation of
    a piece of data that has been compressed and then decompressed. The greater the
    mismatch between the original and the decompressed version, the greater the loss.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们使用*损失*一词作为错误的同义词，因此我们网络的误差函数也被称为其损失函数。在这一节中，我们使用这个词时有稍微不同的含义，指的是压缩后再解压时数据的退化程度。原始数据与解压后数据之间的差距越大，损失就越大。
- en: The idea of loss, or degradation of the input, is distinct from the idea of
    making the input smaller. For example, in Chapter 6, we saw how to use Morse code
    to carry information. The translation of letters to Morse code symbols carries
    no loss, because we can exactly reconstruct the original message from the Morse
    version. We say that converting, or encoding, our message into Morse code is a
    *lossless* transformation. We’re just changing format, like changing a book’s
    typeface or type color.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 损失的概念，或者输入的退化，和将输入变小的概念是不同的。例如，在第六章中，我们看到了如何使用摩尔斯电码传递信息。字母到摩尔斯电码符号的转换没有损失，因为我们可以从摩尔斯版本精确地重建原始信息。我们说将我们的信息转换或编码成摩尔斯电码是*无损*转换。我们只是改变了格式，就像改变一本书的字体或字体颜色一样。
- en: To see where loss can get involved, let’s suppose that we’re camping in the
    mountains. On a nearby mountain, our friend Sara is enjoying her birthday. We
    don’t have radios or phones, but both groups have mirrors, and we’ve found we
    can communicate between the mountains by reflecting sunlight off our mirrors,
    sending Morse code back and forth. Suppose that we want to send the message, “HAPPY
    BIRTHDAY SARA BEST WISHES FROM DIANA” (for simplicity, we leave out punctuation).
    Counting spaces, that’s 42 characters. That’s a lot of mirror-wiggling. We decide
    to leave out the vowels, and send “HPP BRTHD SR BST WSHS FRM DN” instead. That’s
    only 28 letters, so we can send this in about two-thirds the time of the full
    message.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看出损失是如何发生的，假设我们正在山中露营。在附近的山上，我们的朋友Sara正在庆祝她的生日。我们没有收音机或电话，但两组人都有镜子，我们发现可以通过反射阳光来进行摩尔斯电码通信。假设我们想发送消息：“HAPPY
    BIRTHDAY SARA BEST WISHES FROM DIANA”（为简便起见，我们省略了标点符号）。如果算上空格，这有42个字符。那是很多镜子摆动。于是我们决定省略元音，只发送“HPP
    BRTHD SR BST WSHS FRM DN”。这只有28个字母，所以我们可以在完整消息的三分之二的时间内发送它。
- en: Our new message has lost some information (the vowels) by being compressed in
    this way. We say that this is a *lossy* method of compression.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新消息通过这种方式被压缩，丢失了一些信息（元音）。我们说这是一种*有损*的压缩方法。
- en: We can’t make a blanket statement about whether it is or isn’t okay to lose
    some information from any message. If there is loss, then the amount of loss we
    can tolerate depends on the message and all the context around it. For example,
    suppose that our friend Sara is camping with her friend Suri, and it just happens
    that they share a birthday. In this context, “HPP BRTHD SR” is ambiguous, because
    they can’t tell who we’re addressing.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能简单地说丢失一些信息是否可以接受。若发生损失，我们能容忍的损失量取决于消息及其上下文。例如，假设我们的朋友Sara和她的朋友Suri正在露营，而且恰好她们是同一天生日。在这种情况下，“HPP
    BRTHD SR”就有歧义，因为她们无法知道我们在称呼谁。
- en: An easy way to test if a transformation is lossy or lossless is to consider
    if it can be *inverted*, or run backward, to recover the original data. In the
    case of standard Morse code, we can turn our letters into dot-dash patterns and
    then back to letters again with nothing lost in the process. But when we deleted
    the vowels from our message, those letters were lost forever. We can usually guess
    at them, but we’re only guessing and we can get it wrong. Removing the vowels
    creates a compressed version that is not invertible.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 测试一个转换是否有损或无损的简单方法是考虑它是否可以被*反转*，或者倒回去，以恢复原始数据。在标准的摩尔斯电码中，我们可以将字母转化为点划模式，然后再转换回字母，在这个过程中没有丢失任何信息。但是，当我们删除了消息中的元音字母时，这些字母就永远丢失了。我们通常可以猜测它们，但我们只是猜测，可能会猜错。去除元音字母会生成一个不可逆的压缩版本。
- en: Both MP3 and JPG are lossy systems for compressing data. In fact, they’re very
    lossy. But both of these compression standards were carefully designed to throw
    away just the “right” information so that in most everyday cases, we can’t tell
    the compressed version from the original.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MP3和JPG都是有损数据压缩系统。事实上，它们的有损程度非常高。但这两种压缩标准都是经过精心设计的，旨在丢弃“正确”的信息，以便在大多数日常情况下，我们无法分辨压缩版本和原始版本的差异。
- en: This was achieved by carefully studying the properties of each kind of data
    and how it was perceived. For example, the MP3 standard is based not just on the
    properties of sound in general, but on the properties of music and of the human
    auditory system. In the same way, the JPG algorithm is not only specialized toward
    the structure of data within images, but it also builds on science describing
    the human visual system.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过仔细研究每种数据类型的特性以及它们是如何被感知的实现的。例如，MP3标准不仅基于声音的一般特性，还基于音乐和人类听觉系统的特性。同样，JPG算法不仅专注于图像中数据的结构，还依赖于描述人类视觉系统的科学。
- en: 'In a perfect but impossible world, compressed files are tiny, and their decompressed
    versions match their corresponding originals perfectly. In the real world, we
    trade off the *fidelity*, or accuracy, of the decompressed image for the file
    size. Generally speaking, the bigger the file, the better the decompressed file
    matches the original. This makes sense in terms of information theory: a smaller
    file holds less information than a larger one. When the original file has redundancy,
    we can exploit that to make a lossless compression in a smaller file (for example,
    when we compress a text file using the ZIP format). But in general, compression
    usually implies some loss.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个完美但不可能的世界里，压缩文件非常小，而且解压后的版本与其原始文件完全匹配。在现实世界中，我们为了文件大小而牺牲了*保真度*，即解压图像的准确性。一般来说，文件越大，解压后的文件与原始文件越匹配。这在信息理论上是有道理的：较小的文件比较大的文件包含的信息少。当原始文件存在冗余时，我们可以利用这一点，通过无损压缩来生成更小的文件（例如，当我们使用ZIP格式压缩文本文件时）。但一般而言，压缩通常意味着某种丢失。
- en: The designers of lossy compression algorithms work hard to selectively lose
    just the information that matters to us the least for that particular type of
    file. Often this question of “what matters” to a person is an issue of debate,
    leading to a variety of different lossy encoders (such as FLAC and AAC for audio,
    and JPEG and JPEG 2000 for images).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有损压缩算法的设计师们努力工作，选择性地丢弃那些对我们来说在特定文件类型中最不重要的信息。通常，“什么重要”这个问题是一个争议问题，这也导致了多种不同的有损编码器（例如FLAC和AAC用于音频，JPEG和JPEG
    2000用于图像）。
- en: Blending Representations
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示的融合
- en: Later in this chapter, we will find numerical representations of multiple inputs
    and then *blend* those to create new data that has aspects of each input. There
    are two general approaches to blending data. We can describe the first as *content
    blending*. That’s where we blend the content of two pieces of data with each other.
    For example, content blending the images of a cow and zebra gives us something
    like [Figure 18-1](#figure18-1).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面，我们将找到多个输入的数值表示，然后*融合*这些数据，以创建具有每个输入特征的新数据。数据融合有两种常见方法。我们可以将第一种方法描述为*内容融合*。这就是将两组数据的内容相互融合。例如，将牛和斑马的图像进行内容融合，就会得到类似[图18-1](#figure18-1)的效果。
- en: '![F18001](Images/F18001.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![F18001](Images/F18001.png)'
- en: 'Figure 18-1: Content blending images of a cow and zebra. Scaling each by 50
    percent and adding the results together gives us a superposition of the two images,
    rather than a single animal that’s half cow and half zebra.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-1：混合牛和斑马的图像内容。将每张图像缩放50%，然后将结果加在一起，我们得到的是两张图像的叠加，而不是一个半牛半斑马的单一动物。
- en: The result is a combination of the two images, not an in-between animal that
    is half cow and half zebra. To get a hybrid animal, we would use a second approach,
    called *parametric blending*, or *representation blending*. Here we work with
    parameters that describe the thing we’re interested in. By blending two sets of
    parameters, depending on the nature of the parameters and the algorithm we use
    to create the object, we can create results that blend the inherent qualities
    of the things themselves.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是两个图像的组合，而不是一个介于牛和斑马之间的混合动物。要得到一个混合动物，我们将使用第二种方法，称为*参数混合*，或*表示混合*。在这里，我们使用描述我们感兴趣事物的参数。通过混合两组参数，根据参数的性质和我们用来创建对象的算法，我们可以创建出混合了事物固有特征的结果。
- en: For example, suppose we have two circles, each described by a center, radius,
    and color, as in [Figure 18-2](#figure18-2).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有两个圆形，每个圆形由一个中心、半径和颜色来描述，如[图18-2](#figure18-2)所示。
- en: '![F18002](Images/F18002.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![F18002](Images/F18002.png)'
- en: 'Figure 18-2: Two circles we’d like to blend'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-2：我们想要混合的两个圆
- en: If we blend the parameters (that is, we blend the two values representing the
    x component of the circle’s center with each other, and the two values for y,
    and similarly for radius and color) then we get an in-between circle, as in [Figure
    18-3](#figure18-3).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们混合这些参数（即，我们将表示圆形中心的x分量的两个值相互混合，将y的两个值相互混合，同样地，半径和颜色也如此），那么我们得到一个介于两个圆之间的圆形，如[图18-3](#figure18-3)所示。
- en: '![F18003](Images/F18003.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![F18003](Images/F18003.png)'
- en: 'Figure 18-3: Parametric blending of the two circles means blending their parameters
    (center, radius, and color).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-3：对两个圆进行参数混合意味着混合它们的参数（中心、半径和颜色）。
- en: This works well for uncompressed objects. But if we try this with compressedobjects,
    we rarely get reasonable in-between results. The problem is that the compressed
    form may have little in common with the internal structure that we need to meaningfully
    blend the objects. For example, let’s take the sounds of the words *cherry* and
    *orange*. These sounds are our objects. We can blend these sounds together by
    having two people say the words at the same time, creating the audio version of
    our cow and zebra in [Figure 18-1](#figure18-1).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法对于未压缩的对象效果很好。但如果我们尝试对压缩过的对象进行混合，通常无法得到合理的中间结果。问题在于，压缩后的形式可能与我们需要用来有意义地混合对象的内部结构相去甚远。例如，假设我们将“cherry”和“orange”这两个词的声音作为我们的对象进行混合。我们可以通过让两个人同时说出这两个词来混合这些声音，创造出[图18-1](#figure18-1)中牛和斑马的音频版本。
- en: We can think of turning these sounds into written language as a form of compression.
    If it takes a half-second to say the word *cherry*, then if we use MP3 at a popular
    compression setting of 128 Kbps, we need about 8,000 bytes (AudioMountain 2020).
    If we use the Unicode UTF-32 standard (which requires 4 bytes per letter), the
    written form requires only 24 bytes, which is vastly smaller than 8,000\. Since
    the letters are drawn from the alphabet, which has a given order, we can blend
    the representations by blending the letters through the alphabet. This isn’t going
    to work for letters, but let’s follow the process through because a version of
    this will work for us later.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些声音转化为书面语言看作是一种压缩方式。如果说出“cherry”这个词需要半秒钟，那么如果我们使用MP3格式，并选择流行的128 Kbps压缩设置，我们大约需要8,000字节（AudioMountain
    2020）。如果我们使用Unicode UTF-32标准（每个字母需要4个字节），书面形式只需要24个字节，这比8,000小得多。由于字母来自字母表，且字母表有固定的顺序，我们可以通过在字母表中混合字母来混合这些表示形式。这种方法对字母不起作用，但让我们按照这个过程走下去，因为稍后某种形式的这个方法将对我们有用。
- en: The first letters of “cherry” and “orange” are C and O. In the alphabet, the
    region spanned by these letters is CDEFGHIJKLMNO. Right in the middle is the letter
    I, so that’s the first letter of our blend. When the first letter appears later
    in the alphabet than the second, as in E to A, we count backward. When there’s
    an even number of letters in the span, we chosen the earlier one. As shown in
    [Figure 18-4](#figure18-4) this blending gives us the sequence IMCPMO.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: “cherry”和“orange”这两个词的首字母分别是C和O。在字母表中，这两个字母所跨越的区域是CDEFGHIJKLMNO。正中间是字母I，因此I是我们混合的首字母。当第一个字母在字母表中的位置晚于第二个字母时，如E到A，我们会倒数计算。当跨度中的字母数量是偶数时，我们选择较早的字母。如[图18-4](#figure18-4)所示，这种混合方式给我们带来了IMCPMO这个字母序列。
- en: '![F18004](Images/F18004.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![F18004](Images/F18004.png)'
- en: 'Figure 18-4: Blending the written words *cherry* and *orange* by finding the
    midpoint of each letter in the alphabet'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-4：通过找到字母表中每个字母的中点来混合书写的“cherry”和“orange”这两个词
- en: What we wanted was something that, when uncompressed, sounded like a blend between
    the sound of *cherry* and the sound of *orange*. Saying the word *imcpmo* out
    loud definitely does not satisfy that goal. Beyond that, it’s a meaningless string
    of letters that doesn’t correspond to any fruit, or even any word in English.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要的是一种压缩后，当解压时，声音听起来像是*樱桃*和*橙子*的声音混合在一起。大声说出*imcpmo*这个词显然无法达到这个目标。更重要的是，它只是一个没有任何意义的字母串，既不代表任何水果，也不代表英语中的任何单词。
- en: In this case, blending the compressed representations doesn’t give us anything
    like the blended objects. We will see that a remarkable feature of autoencoders,
    including the variational autoencoder we see at the end of the chapter, is that
    they do allow us to blend the compressed versions, and (to a point) recover blended
    versions of the original data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，混合压缩后的表示并不能给我们带来类似混合物体的效果。我们将看到，自编码器，尤其是本章末尾提到的变分自编码器的一个显著特点是，它们确实允许我们混合压缩版本，并且（在一定程度上）恢复出混合后的原始数据版本。
- en: The Simplest Autoencoder
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最简单的自编码器
- en: We can build a deep learning system to figure out a compression scheme for any
    data we want. The key idea is to create a place in the network where the entire
    dataset has to be represented by fewer numbers than there are in the input. That,
    after all, is what compression is all about.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建一个深度学习系统来为我们想要的数据找出压缩方案。关键思想是在网络中创建一个地方，在这个地方，整个数据集必须用比输入数字少的数字来表示。毕竟，这就是压缩的核心。
- en: For instance, let’s suppose that our input consists of grayscale images of animals,
    saved at a resolution of 100 by 100\. Each image has 100 × 100 = 10,000 pixels,
    so our input layer has 10,000 numbers. Let’s arbitrarily say we want to find the
    best way to represent those images using only 20 numbers.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们的输入由动物的灰度图像组成，分辨率为100×100。每张图片有100 × 100 = 10,000个像素，所以我们的输入层有10,000个数字。假设我们随意选择，想要用仅仅20个数字来表示这些图像的最佳方式。
- en: One way to do this is to build a network as in [Figure 18-5](#figure18-5). It’s
    just one layer!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一种实现方式是构建一个如[图18-5](#figure18-5)所示的网络。它只有一层！
- en: '![F18005](Images/F18005.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![F18005](Images/F18005.png)'
- en: 'Figure 18-5: Our first encoder is a single dense, or fully connected, layer
    that turns 10,000 numbers into 20 numbers.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-5：我们的第一个编码器是一个单一的密集层，或者说全连接层，将10,000个数字转换为20个数字。
- en: Our input is 10,000 elements, going into a fully connected layer of only 20
    neurons. The output of those neurons for any given input is our compressed version
    of that image. In other words, with just one layer, we’ve built an encoder.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入是10,000个元素，进入一个只有20个神经元的全连接层。对于任何给定的输入，这些神经元的输出就是我们压缩后的图像版本。换句话说，通过这一层，我们构建了一个编码器。
- en: The real trick now would be to be able to recover the original 10,000 pixel
    values, or even anything close to them, starting from just these 20 numbers. To
    do that, we follow the encoder with a decoder, as in [Figure 18-6](#figure18-6).
    In this case, we just make a fully connected layer with 10,000 neurons, one for
    each output pixel.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的真正难题是，能够从这20个数字中恢复出原始的10,000个像素值，或者至少恢复接近它们的值。为此，我们在编码器后面加上一个解码器，如[图18-6](#figure18-6)所示。在这种情况下，我们只需创建一个包含10,000个神经元的全连接层，每个神经元对应一个输出像素。
- en: '![F18006](Images/F18006.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![F18006](Images/F18006.png)'
- en: 'Figure 18-6: An encoder (in blue) turns our 10,000 inputs into 20 variables,
    then a decoder (in beige) turns those back into 10,000 values.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-6：一个编码器（蓝色）将我们的10,000个输入转换为20个变量，然后一个解码器（米色）将它们转换回10,000个值。
- en: Because the amount of data is 10,000 elements at the start, 20 in the middle,
    and 10,000 again at the end, we say that we’ve created a *bottleneck*. [Figure
    18-7](#figure18-7) shows the idea.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因为数据量最开始是10,000个元素，中间是20个，最后又是10,000个，所以我们说我们已经创建了一个*瓶颈*。[图18-7](#figure18-7)展示了这一理念。
- en: '![F18007](Images/F18007.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![F18007](Images/F18007.png)'
- en: 'Figure 18-7: We say the middle of a network like the one shown in [Figure 18-6](#figure18-6)
    is a bottleneck because it’s shaped like a bottle with a narrow top, or neck.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-7：我们说像[图18-6](#figure18-6)所示的网络中间部分是瓶颈，因为它的形状像一个瓶子，顶部狭窄，像瓶颈。
- en: Now we can train our system. Each input image is also the output target. This
    tiny autoencoder tries to find the best way to crunch the input into just 20 numbers
    that can be uncrunched to match the target, which is the input itself. The compressed
    representation at the bottleneck is called the *code*, or the *latent variables*
    (*latent* suggests that these values are inherent in the input data, just waiting
    for us to discover them). Usually we make the bottleneck using a small layer in
    the middle of a deep network, as in [Figure 18-6](#figure18-6). Naturally enough,
    this layer is often called the *latent layer* or the *bottleneck layer*. The outputs
    of the neurons on this layer are the latent variables. The idea is that these
    values represent the image in some way.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始训练我们的系统。每个输入图像也是输出目标。这个小型自编码器试图找到最佳方法，将输入压缩成仅有20个数字，然后再将其解压缩回原始目标，即输入本身。瓶颈处的压缩表示被称为*编码*，或*潜在变量*（*潜在*意味着这些值本质上存在于输入数据中，只是等待我们去发现它们）。通常我们通过深度网络中间的一个小层来构建瓶颈，正如在[图18-6](#figure18-6)中所示。这个层通常被称为*潜在层*或*瓶颈层*。这个层上神经元的输出就是潜在变量。这个想法是，这些值以某种方式表示了图像。
- en: This network has no category labels (as with a categorizer) or targets (as with
    a regression model). We don’t have any other information for the system other
    than the input we want it to compress and then decompress. We say that an autoencoder
    is an example of *semi-supervised learning*. It sort-of issupervised learning
    because we give the system explicit goal data (the output should be the same as
    the input), and it sort-of isn’tsupervised learning because we don’t have any
    manually determined labels or targets on the inputs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络没有类别标签（像分类器那样）或目标（像回归模型那样）。除了我们希望它进行压缩然后解压缩的输入外，系统没有其他信息。我们说自编码器是*半监督学习*的一个例子。它某种程度上是监督学习，因为我们给系统提供了明确的目标数据（输出应该与输入相同），而它又某种程度上不是监督学习，因为我们没有任何手动确定的标签或目标来标注输入。
- en: Let’s train our tiny autoencoder of [Figure 18-6](#figure18-6) on an image of
    a tiger and see how it does. We’ll feed it the tiger image over and over, encouraging
    the system to output a full-size image of the tiger, despite the compression down
    to just 20 numbers at the bottleneck. The loss function compares the pixels of
    the original tiger with the pixels from the autoencoder’s output and adds up the
    differences, so the more the pixels differ, the larger the loss. We trained it
    until it stopped improving. [Figure 18-8](#figure18-8) shows the result. Each
    error value shown on the far right is the original pixel value minus the corresponding
    output pixel value (the pixels were scaled to the range [0,1]).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练我们的小型自编码器，使用[图18-6](#figure18-6)中的老虎图像，看看它的表现如何。我们会反复输入这张老虎图像，鼓励系统输出一张完整的老虎图像，尽管在瓶颈部分压缩到了仅仅20个数字。损失函数比较原始老虎图像的像素与自编码器输出的像素，并计算差异，因此像素差异越大，损失也越大。我们训练到它不再改进为止。[图18-8](#figure18-8)显示了结果。右侧显示的每个误差值是原始像素值减去对应的输出像素值（像素已缩放到[0,1]的范围内）。
- en: '![F18008](Images/F18008.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![F18008](Images/F18008.png)'
- en: 'Figure 18-8: Training our autoencoder of [Figure 18-6](#figure18-6) on a tiger.
    Left: The original, input tiger. Middle: The output. Right: The pixel-by-pixel
    differences between the original and output tiger (pixels are in the range [0,1]).
    The autoencoder seems to have done an amazing job since the bottleneck had only
    20 numbers.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-8：训练我们的小型自编码器（见[图18-6](#figure18-6)）来处理老虎图像。左侧：原始输入的老虎图像。中间：输出图像。右侧：原始与输出老虎图像之间的逐像素差异（像素值范围为[0,1]）。自编码器似乎做得非常好，尽管瓶颈部分只有20个数字。
- en: This is fantastic! Our system took a picture composed of 10,000 pixel values
    and crunched them down to 20 numbers, and now it appears to have recovered the
    entire picture again, right down to the thin, wispy whiskers. The biggest error
    in any pixel was about 1 part in 100\. It looks like we’ve found a fantastic way
    to do compression!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这太棒了！我们的系统将由10,000个像素值组成的图像压缩成了20个数字，现在它似乎又恢复了整张图像，甚至细致到那些细长、飘逸的胡须。任何像素的最大误差大约是100分之一。看起来我们找到了一个非常棒的压缩方法！
- en: But wait a second. This doesn’t make sense. There’s just no way to rebuild that
    tiger image from 20 numbers without doing something sneaky. In this case, the
    sneaky thing is that the network has utterly overfit and memorized the image.
    It simply set up all 10,000 output neurons to take those 20 input numbers and
    reconstruct the original 10,000 input values. Put more bluntly, the network merely
    memorized the tiger. We didn’t really compress anything at all. Each of the 10,000
    inputs went to each of the 20 neurons in the bottleneck layer, requiring 20 ×
    10,000 = 200,000 weights, and then the 20 bottleneck results all went to each
    of the 10,000 neurons in the output layer, requiring another 200,000 weights,
    which then produced the picture of the tiger. We basically found a way to store
    10,000 numbers using only 400,000 numbers. Hooray?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但是等一下。这不合理。没有办法仅凭 20 个数字重建那张老虎图像，除非做些“偷偷摸摸”的事情。在这种情况下，这种“偷偷摸摸”的事情就是网络完全过拟合并记住了图像。它简单地将所有
    10,000 个输出神经元设置为接收这 20 个输入数字，并重建原始的 10,000 个输入值。更直白地说，网络仅仅记住了老虎。我们实际上并没有压缩任何东西。每个
    10,000 个输入都传递到瓶颈层的 20 个神经元中，需要 20 × 10,000 = 200,000 个权重，然后这 20 个瓶颈结果都传递到输出层的
    10,000 个神经元中，需要另外 200,000 个权重，这样就生成了老虎图像。我们基本上找到了一种方法，只使用 400,000 个数字就能存储 10,000
    个数字。耶？
- en: In fact, most of those numbers are irrelevant. Remember that each neuron has
    a bias that’s added alongside the incoming weighted inputs. The output neurons
    are relying on mostly their bias values and not too much on the inputs. To test
    this, [Figure 18-9](#figure18-9) shows the result of giving the autoencoder a
    picture of a flight of stairs. It doesn’t do a poor job of compressing and decompressing
    the stairs. Instead, it mostly ignores the stairs, and gives us back the memorized
    tiger. The output isn’t exactly the input tiger, as shown by the rightmost image,
    but if we just look at the output, it’s hard to see any hint of the stairs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，很多数字是无关紧要的。记住，每个神经元都有一个偏置，它与输入的加权值一起相加。输出神经元主要依赖它们的偏置值，而不是输入值。为了测试这一点，[图
    18-9](#figure18-9)展示了给自编码器输入一张楼梯的图片的结果。它压缩和解压楼梯的表现并不差。相反，它主要忽略了楼梯，给我们返回了记住的老虎。输出图像并不完全是输入的老虎，如最右边的图像所示，但如果我们只看输出，几乎看不出楼梯的任何痕迹。
- en: '![F18009](Images/F18009.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![F18009](Images/F18009.png)'
- en: 'Figure 18-9: Left: We present our tiny autoencoder trained on just the tiger
    with an image of a stairway. Middle: The output is the tiger! Right: The difference
    between the output image and the original tiger.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-9：左：我们展示了一个仅针对老虎训练的小型自编码器，并输入了一张楼梯图像。中：输出是老虎！右：输出图像与原始老虎之间的差异。
- en: The error bar on the right of [Figure 18-9](#figure18-9) shows that our errors
    are much larger than those of [Figure 18-8](#figure18-8), but the tiger still
    looks a lot like the original.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 18-9](#figure18-9)右侧的误差条显示，我们的误差远大于[图 18-8](#figure18-8)中的误差，但老虎看起来仍然与原始图像相似。'
- en: Let’s make a real stress test of the idea that the network is mostly relying
    on the bias values. We can feed the autoencoder an input image that is zero everywhere.
    Then it has no input values to work with, and only the bias values contribute
    to the output. [Figure 18-10](#figure18-10) shows the result.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对“网络主要依赖偏置值”这一观点进行真正的压力测试。我们可以给自编码器输入一张到处都是零的图像。这样它就没有输入值可供处理，只有偏置值参与输出。[图
    18-10](#figure18-10)展示了结果。
- en: '![F18010](Images/F18010.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![F18010](Images/F18010.png)'
- en: 'Figure 18-10: When we give our tiny autoencoder a field of pure black, it uses
    the bias values to give us back a low quality, but recognizable, version of the
    tiger. Left: The black input. Middle: The output. Right: The difference between
    the output and the original tiger. Note that the range of differences runs from
    0 to almost 1, unlike [Figure 18-9](#figure18-9) where they ran from about −0.4
    to 0\.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-10：当我们给我们的小型自编码器输入一张纯黑的图像时，它使用偏置值给我们返回一张低质量但可识别的老虎图像。左：黑色输入。中：输出。右：输出与原始老虎之间的差异。注意，差异的范围从
    0 到几乎 1，不同于[图 18-9](#figure18-9)，其中差异的范围从大约 −0.4 到 0。
- en: No matter what input we give to this network, we will always get back some version
    of the tiger as output. The autoencoder has trained itself to produce the tiger
    every time.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们给这个网络什么输入，我们总是会得到某种版本的老虎作为输出。自编码器已经训练自己每次都生成老虎。
- en: A real test of this autoencoder would be to teach it a bunchof images and see
    how well it compresses them. Let’s try again with a set of 25 photographs, shown
    in [Figure 18-11](#figure18-11).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个自编码器的真正考验是教它一堆图片，并观察它压缩这些图片的效果。让我们用一组25张照片再试一次，见[图18-11](#figure18-11)。
- en: '![F18011](Images/F18011.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![F18011](Images/F18011.png)'
- en: 'Figure 18-11: The 25 photographs that we used, in addition to the tiger, to
    train our tiny autoencoder. Each image was rotated by 90 degrees, 180 degrees,
    and 270 degrees during training.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-11：除了老虎，我们还使用了这25张照片来训练我们的微型自编码器。在训练过程中，每张图片都旋转了90度、180度和270度。
- en: We made the database larger by training not just on each image, but also on
    each image rotated by 90 degrees, 180 degrees, and 270 degrees. Our training set
    was the tiger (and its three rotations) and the 100 images of [Figure 18-11](#figure18-11)
    with rotations, for a total of 104 images.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过不仅训练每张图像，还训练每张图像分别旋转90度、180度和270度来扩展数据库。我们的训练集包括老虎（及其三种旋转版本）和[图18-11](#figure18-11)中的100张带有旋转的图片，共计104张图像。
- en: Now that the system is trying to remember how to represent all 104 of these
    pictures with just 20 numbers, it should be no surprise that it can’t do a very
    good job. [Figure 18-12](#figure18-12) shows what this autoencoder produces when
    we ask it to compress and decompress the tiger.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，系统必须记住如何用仅20个数字来表示这104张图片，结果它做得并不好也不奇怪。[图18-12](#figure18-12)展示了当我们要求它压缩和解压老虎图像时，这个自编码器的表现。
- en: '![F18012](Images/F18012.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![F18012](Images/F18012.png)'
- en: 'Figure 18-12: We trained our autoencoder of [Figure 18-6](#figure18-6) with
    the 100 images of [Figure 18-11](#figure18-11) (each image plus its rotated versions),
    along with the four rotations of the tiger. Using this training, we gave it the
    tiger on the left, and it produced the output in the middle.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-12：我们用[图18-6](#figure18-6)中的自编码器训练了[图18-11](#figure18-11)中的100张图片（每张图片及其旋转版本），以及老虎的四种旋转。通过这个训练，我们给了它左边的老虎图片，它生成了中间的输出。
- en: Now that the system isn’t allowed to cheat, the result doesn’t look like a tiger
    at all, and everything makes sense again. We can see a little bit of four-way
    rotational symmetry in the result, owing to our training on the rotated versions
    of the input images. We could do better by increasing the number of neurons in
    the bottleneck, or latent, layer. But since we want to compress our inputs as
    much as possible, adding more values to the bottleneck should be a last resort.
    We’d rather do the best possible job we can with as few values as we can get away
    with.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于系统不能作弊，结果完全不像老虎，一切都变得有道理了。我们可以看到，结果中有一些四向旋转对称性，这是由于我们训练了旋转过的输入图像版本。通过增加瓶颈层（或潜在层）中的神经元数量，我们可以做得更好。但由于我们希望尽可能压缩输入，增加瓶颈的值应作为最后的手段。我们更愿意尽可能少地使用值来完成最佳的工作。
- en: Let’s try to improve the performance by considering a more complex architecture
    than just the two dense layers we’ve been using so far.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试通过考虑比目前使用的两个全连接层更复杂的架构来提高性能。
- en: A Better Autoencoder
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更好的自编码器
- en: In this section, we’ll explore a variety of autoencoder architectures. To compare
    them, we’ll use the MNIST database we saw in Chapter 17\. To recap, this is a
    big, free database of hand-drawn, grayscale digits from 0 to 9, saved at a resolution
    of 28 by 28 pixels. [Figure 18-13](#figure18-13) shows some typical digit images
    from the MNIST dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索多种自编码器架构。为了进行比较，我们将使用第17章中看到的MNIST数据库。回顾一下，这是一个大型的免费数据库，包含从0到9的手绘灰度数字，分辨率为28×28像素。[图18-13](#figure18-13)展示了一些来自MNIST数据集的典型数字图像。
- en: '![F18013](Images/F18013.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![F18013](Images/F18013.png)'
- en: 'Figure 18-13: A sampling of the handwritten digits from the MNIST dataset'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-13：MNIST数据集中的手写数字样本
- en: To run our simple autoencoder on this data, we need to change the size of the
    inputs and outputs of [Figure 18-6](#figure18-6) to fit the MNIST data. Each image
    has 28 × 28 = 784 pixels. Thus, our input and output layer now needs 784 elements
    instead of 10,000\. Let’s flatten the 2D image into a single big list before we
    feed it to the network and leave the bottleneck at 20\. [Figure 18-14](#figure18-14)
    shows the new autoencoder. In this diagram, as well as those to come, we won’t
    draw the flatten layer at the start, or the reshaping layer at the end that “undoes”
    the flattening and turns the list of 784 numbers back into a 28 by 28 grid.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要在这些数据上运行我们的简单自编码器，我们需要改变[图18-6](#figure18-6)中输入和输出的大小以适应MNIST数据。每张图像有28 × 28
    = 784个像素。因此，我们的输入和输出层现在需要784个元素，而不是10,000个。让我们在将图像输入网络之前，将二维图像展平为一个大的列表，并将瓶颈保持在20个潜在变量。[图18-14](#figure18-14)显示了新的自编码器。在这个图示以及接下来的图示中，我们不会绘制开始时的展平层，或结束时的重塑层，这些层“还原”了展平操作，将784个数字重新转化为28×28的网格。
- en: '![F18014](Images/F18014.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![F18014](Images/F18014.png)'
- en: 'Figure 18-14: Our two-layer autoencoder for MNIST data'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-14：我们的两层自编码器用于MNIST数据
- en: Let’s train this for 50 epochs (that is, we run through all 60,000 training
    examples 50 times). Some results are shown in [Figure 18-15](#figure18-15).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练50个周期（也就是说，我们将60,000个训练样本跑50遍）。一些结果显示在[图18-15](#figure18-15)中。
- en: '![F18015](Images/F18015.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![F18015](Images/F18015.png)'
- en: 'Figure 18-15: Running five digits from the MNIST dataset through our trained
    autoencoder of [Figure 18-14](#figure18-14), which uses 20 latent variables. Top
    row: Five pieces of input data. Bottom row: The reconstructed images.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-15：通过我们训练过的自编码器（见[图18-14](#figure18-14)）处理MNIST数据集中的五个数字，使用20个潜在变量。上排：五个输入数据。下排：重构后的图像。
- en: '[Figure 18-15](#figure18-15) is pretty amazing. Our two-layer network learned
    how to take each input of 784 pixels, squash it down to just 20 numbers, and then
    blow it back up to 784 pixels. The resulting digits are blurry, but recognizable.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-15](#figure18-15)相当惊人。我们的两层网络学会了如何将每个784像素的输入压缩成仅20个数字，然后再把它扩展回784个像素。生成的数字虽然模糊，但仍然可以识别。'
- en: Let’s try reducing the number of latent variables down to 10\. We expect things
    are going to look a lot worse. [Figure 18-16](#figure18-16) shows that they are
    indeed worse.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试将潜在变量的数量减少到10个。我们预期结果会变得更糟。[图18-16](#figure18-16)显示了情况确实更糟。
- en: '![F18016](Images/F18016.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![F18016](Images/F18016.png)'
- en: 'Figure 18-16: Top row: The original MNIST images. Bottom row: The output of
    our autoencoder using 10 latent variables.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-16：上排：原始的MNIST图像。下排：使用10个潜在变量的自编码器输出。
- en: This is getting pretty bad. The 2 seems to be turning into a 3 with a bite taken
    out of it, and the 4 seems to be turning into a 9\. But that’s what we get for
    crushing these images down to 10 numbers. That’s just not enough to enable the
    system to do a good job of representing the input.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 结果变得相当糟糕。数字2似乎变成了一个咬了一口的3，而数字4似乎变成了9。不过，这就是我们将这些图像压缩到10个数字时的结果。这远不足以让系统有效地表示输入。
- en: The lesson is that our autoencoder needs to have both enough computational power
    (that is, enough neurons and weights) to figure out how to encode the data, and
    enough latent variables to find a useful compressed representation of the input.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教训是，我们的自编码器需要有足够的计算能力（也就是说，足够的神经元和权重）来弄清楚如何编码数据，同时需要足够的潜在变量来找到输入的有用压缩表示。
- en: Let’s see how deeper models perform. We can build the encoder and decoder with
    any types of layers we like. We can make deep autoencoders with lots of layers,
    or shallow ones with only a few, depending on our data. For now, let’s continue
    using fully connected layers, but let’s add some more of them to create a deeper
    autoencoder. We’ll construct the encoder stage from several hidden layers of decreasing
    size until we reach the bottleneck, and then we’ll build a decoder from several
    more hidden layers of increasing size until they reach the same size as the input.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看更深的模型表现如何。我们可以使用任何类型的层来构建编码器和解码器。根据我们的数据，我们可以构建有很多层的深度自编码器，也可以构建只有少数层的浅层自编码器。现在，我们继续使用全连接层，但添加更多层来创建一个更深的自编码器。我们将从几层逐渐变小的隐藏层构建编码器阶段，直到达到瓶颈，然后再从几层逐渐变大的隐藏层构建解码器，直到它们的大小与输入相同。
- en: '[Figure 18-17](#figure18-17) shows this approach where now we have three layers
    of encoding and three of decoding.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-17](#figure18-17)展示了这种方法，现在我们有了三层编码和三层解码。'
- en: '![F18017](Images/F18017.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![F18017](Images/F18017.png)'
- en: 'Figure 18-17: A deep autoencoder built out of fully connected (or dense) layers.
    Blue icons: A three-layer encoder. Beige icons: A three-layer decoder.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-17：一个由全连接（或密集）层构建的深度自动编码器。蓝色图标：一个三层编码器。米色图标：一个三层解码器。
- en: We often build these fully connected layers so that their numbers of neurons
    decrease (and then increase) by a multiple of two, as when we go between 512 and
    256\. That choice often works out well, but there’s no rule enforcing it.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常构建这些全连接层，使它们的神经元数目按2的倍数减少（然后再增加），就像在512和256之间切换一样。这个选择通常效果很好，但没有强制要求。
- en: Let’s train this autoencoder just like the others, for 50 epochs. [Figure 18-18](#figure18-18)
    shows the results.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们像训练其他模型一样，训练这个自动编码器，训练50个epoch。[图 18-18](#figure18-18)展示了结果。
- en: '![F18018](Images/F18018.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![F18018](Images/F18018.png)'
- en: 'Figure 18-18: Predictions from our deep autoencoder of [Figure 18-17](#figure18-17).
    Top row: Images from the MNIST test set. Bottom row: Output from our trained autoencoder
    when presented with the test digits.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-18：我们深度自动编码器对[图 18-17](#figure18-17)的预测。上排：来自MNIST测试集的图像。下排：当我们向训练好的自动编码器提供测试数字时输出的结果。
- en: The results are just a little blurry, but they match the originals unambiguously.
    Compare these results to [Figure 18-15](#figure18-15), which also used 20 latent
    variables. These images are much clearer. By providing additional compute power
    to find those variables (in the encoder), and extra power in turning them back
    into images (in the decoder), we’ve gotten much better results out of our 20 latent
    variables.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 结果稍微有些模糊，但与原图匹配得非常明确。将这些结果与[图 18-15](#figure18-15)进行对比，后者也使用了20个潜在变量，这些图像更清晰。通过提供额外的计算能力来寻找这些变量（在编码器中），以及在将它们转回图像时提供额外的计算能力（在解码器中），我们从20个潜在变量中获得了更好的结果。
- en: Exploring the Autoencoder
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索自动编码器
- en: Let’s look more closely at the results produced by the autoencoder network in
    [Figure 18-17](#figure18-17).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看[图 18-17](#figure18-17)中自动编码器网络生成的结果。
- en: A Closer Look at the Latent Variables
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更深入地观察潜在变量
- en: We’ve seen that the latent variables are a compressed form of the inputs, but
    we haven’t looked at the latent variables themselves. [Figure 18-19](#figure18-19)
    shows graphs of the 20 latent variables produced by the network in [Figure 18-17](#figure18-17)
    in response to our five test images, and the images that the decoder constructs
    from them.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到潜在变量是输入的压缩形式，但我们还没有看过潜在变量本身。[图 18-19](#figure18-19)展示了网络在[图 18-17](#figure18-17)中为我们的五张测试图像生成的20个潜在变量的图表，以及解码器从中构建的图像。
- en: '![F18019](Images/F18019.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![F18019](Images/F18019.png)'
- en: 'Figure 18-19: Top row: The 20 latent variables for each of our five images
    produced by the network in [Figure 18-17](#figure18-17). Bottom row: The images
    decompressed from the latent variables above them.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-19：上排：网络在[图 18-17](#figure18-17)中为我们的五张图像生成的20个潜在变量。下排：从上述潜在变量解压出来的图像。
- en: The latent variables shown in [Figure 18-19](#figure18-19) are typical, in the
    sense that latent variables rarely show any obvious connection to the input data
    from which they were produced. The network has found its own private, highly compressed
    form for representing its inputs, and that form often makes no sense to us. For
    example, we can see a couple of consistent holes in the graphs (in positions 4,
    5, and 14), but there’s no obvious reason from this one set of images why those
    values are 0 (or nearly 0) for these inputs. Looking at more data would surely
    help, but the problem of interpretation remains, in general.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 18-19](#figure18-19)中显示的潜在变量是典型的，因为潜在变量很少与它们所产生的输入数据有明显的关联。网络找到了自己专有的、高度压缩的形式来表示输入，而这种形式通常对我们来说毫无意义。例如，我们可以看到图中有几个一致的空洞（位于第4、5和14个位置），但从这一组图像中看不出为什么这些输入的值为0（或接近0）。查看更多数据肯定会有所帮助，但通常来说，解释的问题仍然存在。
- en: The mysterious nature of the latent variables is fine, because we rarely care
    about directly interpreting these values. Later on, we’ll play with the latent
    values, by blending and averaging them, but we won’t care about what these numbers
    represent. They’re just a private code that the network created during training
    that lets it compress and then decompress each input as well as possible.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在变量的神秘性是可以接受的，因为我们很少关心直接解释这些值。稍后，我们会通过混合和平均潜在值来玩这些变量，但我们并不关心这些数字代表什么。它们只是网络在训练过程中创建的一个私人代码，让它能够尽可能好地压缩和解压每个输入。
- en: The Parameter Space
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数空间
- en: Though we usually don’t care about the numerical values in the latent variables,
    it is still useful to get a feeling for what latent variables are produced by
    similar and different inputs. For example, if we feed the system two images of
    a seven that are almost the same, will the images be assigned almost the same
    latent variables? Or might they be wildly far apart?
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们通常不关心潜在变量中的数值，但了解相似和不同输入所产生的潜在变量仍然是有用的。例如，如果我们输入两张几乎相同的七的图像，图像是否会被分配到几乎相同的潜在变量？或者它们可能会相隔非常远？
- en: To answer these questions, let’s continue with the simple deep autoencoder of
    [Figure 18-17](#figure18-17). But instead of making the last stage of the encoder
    a fully connected layer of 20 neurons, let’s drop that to merely two neurons,
    so we have just two latent variables. The point of this is that we can plot the
    two variables on the page as (x,y) pairs. Of course, if we generate images from
    just two latent variables, those images will come out extremely blurry, but it’s
    worth the exercise so we can see the structure of these simple latent variables.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，让我们继续使用[图18-17](#figure18-17)中的简单深度自编码器。但我们将编码器的最后一层从一个包含20个神经元的全连接层缩减为仅仅两个神经元，这样我们就只有两个潜在变量了。这样做的目的是，我们可以将这两个变量作为（x，y）对绘制在页面上。当然，如果我们仅用两个潜在变量生成图像，这些图像将会非常模糊，但这个练习是值得的，因为它让我们能够看到这些简单潜在变量的结构。
- en: In [Figure 18-20](#figure18-20) we encoded 10,000 MNIST images, found each image’s
    two latent variables, and then plotted them as a point. Each dot is color-coded
    for the label assigned to the image it came from. We say that an image like [Figure
    18-20](#figure18-20) is a visualization of *latent variable space*, or more simply,
    *latent space*.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图18-20](#figure18-20)中，我们对10,000个MNIST图像进行了编码，找出了每个图像的两个潜在变量，然后将它们作为点进行绘制。每个点根据其所属图像的标签进行了颜色编码。我们称像[图18-20](#figure18-20)这样的图像是*潜在变量空间*的可视化，或者更简单地说是*潜在空间*。
- en: '![F18020](Images/F18020.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![F18020](Images/F18020.png)'
- en: 'Figure 18-20: After training a deep autoencoder with only two latent variables,
    we show the latent variables assigned to each of 10,000 MNIST images.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-20：在只使用两个潜在变量训练深度自编码器后，我们展示了每个10,000个MNIST图像分配的潜在变量。
- en: There’s a lot of structure here! The latent variables aren’t being assigned
    numerical values totally at random. Instead, similar images are getting assigned
    similar latent variables. The 1’s, 3s, and 0s seem to fall into their own zones.
    Many of the other digits seem to be scrambled in the lower-left of the plot, getting
    assigned similar values. [Figure 18-21](#figure18-21) shows a close-up view of
    that region.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多结构！潜在变量并不是完全随机地被赋予数值。相反，相似的图像被分配了相似的潜在变量。1、3和0似乎落在它们各自的区域里。许多其他数字似乎在图的左下方被打乱，分配了相似的值。[图18-21](#figure18-21)展示了该区域的特写视图。
- en: '![F18021](Images/F18021.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![F18021](Images/F18021.png)'
- en: 'Figure 18-21: A close-up of the lower-left corner of [Figure 18-20](#figure18-20)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-21：是[图18-20](#figure18-20)左下角的特写图。
- en: It’s not a total jumble. The 0’s have their own band, and while the others are
    a bit mixed up, we can see they all seem to fall into well-defined zones.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 它并不是完全杂乱的。0的区域有自己的带状区，而其他数字虽然有些混杂，但我们可以看到它们似乎都落入了明确的区域。
- en: Though we expect the images to be blurry, let’s make pictures from these 2D
    latent values. We can see from [Figure 18-20](#figure18-20) that the first latent
    variable (which we’re drawing on the X axis) takes on values from 0 to about 40,
    and the second latent variable (which we’re drawing on the Y axis) takes on values
    from 0 to almost 70\.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们预期这些图像会模糊，但我们还是用这些二维潜在值生成图像。从[图18-20](#figure18-20)中我们可以看到，第一个潜在变量（我们在X轴上绘制的）取值范围从0到大约40，第二个潜在变量（我们在Y轴上绘制的）取值范围从0到接近70。
- en: Let’s make a square grid of decoded images, following the recipe in [Figure
    18-22](#figure18-22). We can make a box that runs from 0 to 55 along each axis
    (that’s a little too short in Y, but a little too long in X). We can pick (x,y)
    points inside this grid, and then feed those two numbers to the decoder, producing
    a picture. Then we can draw the picture at that (x,y) position in a corresponding
    grid. We found that 23 steps on each axis produced a nice image that’s dense,
    but not overly so.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照[图18-22](#figure18-22)中的方法，制作一个解码图像的方形网格。我们可以制作一个沿每个轴从0到55的框（Y轴略短，但X轴略长）。然后我们可以选择这个网格内的（x，y）点，将这两个数输入解码器，生成一张图片。然后我们可以将这张图片绘制在对应网格的（x，y）位置。我们发现，在每个轴上取23个步骤生成的图像既密集又不会过于拥挤。
- en: '[Figure 18-23](#figure18-23) shows the result.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-23](#figure18-23)显示了结果。'
- en: '![F18022](Images/F18022.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![F18022](Images/F18022.png)'
- en: 'Figure 18-22: Making a grid of images by decoding (x,y) pairs from [Figure
    18-20](#figure18-20) (and a little beyond). On the left, we select an (x,y) pair
    located at about (22,8). Then we pass these two numbers through the decoder, creating
    the tiny output image on the right.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-22：通过解码来自[图18-20](#figure18-20)（以及稍微超出）中的(x,y)对来生成图像网格。左侧，我们选择一个位于大约(22,8)的(x,y)对。然后，我们将这两个数字传递给解码器，创建右侧的微小输出图像。
- en: '![F18023](Images/F18023.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![F18023](Images/F18023.png)'
- en: 'Figure 18-23: Images generated from latent variables in the range of [Figure
    18-22](#figure18-22)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-23：从[图18-22](#figure18-22)中的潜在变量范围生成的图像
- en: The 1’s spray along the top, as expected. Surprisingly, the 7’s dominate the
    right side. As before, let’s look at the images in a close-up of the lower-left
    corner, shown in [Figure 18-24](#figure18-24).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，1的喷洒位于顶部。令人惊讶的是，7占据了右侧的主导地位。像以前一样，让我们在[图18-24](#figure18-24)中看一下左下角的图像的特写。
- en: '![F18024](Images/F18024.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![F18024](Images/F18024.png)'
- en: 'Figure 18-24: Images from the close-up range of latent variables in [Figure
    18-23](#figure18-23)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-24：从[图18-23](#figure18-23)中的潜在变量特写范围生成的图像
- en: The digits are frequently fuzzy, and they don’t fall into clear zones. This
    is not just because we’re using a very simple encoder, but because we’re encoding
    our inputs into just two latent variables. With more latent variables, things
    become more separated and distinct, but we can’t draw simple pictures of those
    high-dimensional spaces. Nevertheless, this shows us that even with an extreme
    compression down to just two latent variables, the system assigned those values
    in ways that grouped similar digits together.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数字经常模糊，而且它们没有落入清晰的区域。这不仅仅是因为我们使用了一个非常简单的编码器，而是因为我们将输入编码为仅两个潜在变量。使用更多的潜在变量时，事物变得更加分离和独特，但我们无法画出这些高维空间的简单图像。尽管如此，这仍然向我们展示了，即使是将潜在变量压缩到仅两个变量的极限，系统也以某种方式将这些值分配，形成了相似数字的聚类。
- en: Let’s look more closely at this space. [Figure 18-25](#figure18-25) shows the
    images produced by taking (x,y) values along four lines through the plot and feeding
    those to the decoder to produce images.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地观察这个空间。[图18-25](#figure18-25)显示了通过沿着四条线从图中提取(x,y)值并将其输入解码器生成的图像。
- en: This confirms that the encoder assigned similar latent variables to similar
    images and seemed to build clusters of different images, with each variation of
    the image in its own region. That’s a whole lot of structure. As we increase our
    number of latent variables from this ridiculously small value of two, the encoder
    continues to produce clustered regions, but they become more distinct and there
    is less overlapping.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这证实了编码器将相似的潜在变量分配给相似的图像，并似乎构建了不同图像的簇，每种图像的变化都位于自己的区域。这是非常强的结构。随着我们将潜在变量的数量从这个极小的值（仅为两个）增加，编码器继续生成聚类区域，但它们变得更加清晰，重叠较少。
- en: '![F18025](Images/F18025.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![F18025](Images/F18025.png)'
- en: 'Figure 18-25: For each arrow, we took eight equally spaced steps from the start
    to the end, producing eight (x,y) pairs. The decoded images for these pairs are
    shown in the corresponding row.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-25：对于每个箭头，我们从起点到终点进行了八个等间距的步骤，生成了八个(x,y)对。这些对的解码图像显示在相应的行中。
- en: Blending Latent Variables
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合潜在变量
- en: Now that we’ve seen the structure inherent in the latent variables, we can put
    it to use. In particular, let’s blend some pairs of latent variables together
    and see if we get an intermediate image. In other words, let’s do parametric blending
    on the images, as we discussed earlier, where the latent variables are the parameters.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了潜在变量中固有的结构，我们可以加以利用。特别是，让我们将一些潜在变量对混合在一起，看看是否能得到一张中间图像。换句话说，让我们对图像进行参数化混合，正如我们之前讨论的那样，其中潜在变量就是参数。
- en: We actually did this in [Figure 18-25](#figure18-25) as we blended the two latent
    variables from one end of an arrow to the other. But there, we were using an autoencoder
    with just two latent variables, so it wasn’t able to represent the images very
    well. The results were mostly blurry. Let’s use some more latent variables so
    we can get a feeling for what this kind of blending, or *interpolation*, looks
    like in more complex models.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们在[图18-25](#figure18-25)中已经做了这个操作，通过将箭头一端到另一端的两个潜在变量混合。但在这里，我们使用的是只有两个潜在变量的自动编码器，因此它无法很好地表示图像，结果大多模糊。让我们使用更多的潜在变量，这样我们就能更好地理解这种混合或*插值*在更复杂模型中的表现。
- en: Let’s return to the six-layer version of our deep autoencoder of [Figure 18-17](#figure18-17),
    which has 20 latent variables. We can pick out pairs of images, find the latent
    variables for each one, and then simply average each pair of latent variables.
    That is, we have a list of 20 numbers for the first image (its latent variables)
    and a list of 20 numbers for the second image. We blend the first number in each
    list together, then the second number in each list, and so on, until we have a
    new list of 20 numbers. This is the new set of latent variables that we hand to
    the decoder, which produces an image.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们深度自编码器的六层版本，如[图18-17](#figure18-17)所示，它有20个潜在变量。我们可以挑选出一对图像，找出每个图像的潜在变量，然后简单地平均每对潜在变量。也就是说，我们有一组包含20个数字的列表，表示第一张图像的潜在变量，和另一组20个数字的列表，表示第二张图像的潜在变量。我们将每个列表中的第一个数字合成，然后是第二个数字，以此类推，直到我们得到一个新的20个数字的列表。这就是我们传递给解码器的新潜在变量集合，解码器将生成一张图像。
- en: '[Figure 18-26](#figure18-26) shows five pairs of images blended this way.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-26](#figure18-26)展示了通过这种方式合成的五对图像。'
- en: As we expect, the system isn’t simply blending the images with content blending
    (like we did for the cow and zebra in [Figure 18-1](#figure18-1)). Instead, the
    autoencoder is producing intermediate images that have qualities of both inputs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所预期的那样，系统并不仅仅是将图像进行内容混合（就像我们在[图18-1](#figure18-1)中对牛和斑马所做的那样）。相反，自编码器正在生成具有两个输入特征的中间图像。
- en: '![F18026](Images/F18026.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![F18026](Images/F18026.png)'
- en: 'Figure 18-26: Examples of blending latent variables in our deep autoencoder.
    Top row: Five images from the MNIST dataset. Middle row: Five other images. Bottom
    row: The image resulting from averaging the latent variables of the two images
    directly above, and then decoding.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-26：在我们的深度自编码器中混合潜在变量的示例。第一行：来自MNIST数据集的五张图像。第二行：另外五张图像。第三行：通过直接平均上面两张图像的潜在变量，然后解码得到的图像。
- en: These results aren’t absurd. For example, in the second column, the blend between
    a 2 and 4 looks like a partial 8\. That makes sense. [Figure 18-23](#figure18-23)
    shows us that the 2s, 4s, and 8s are close together in the diagram with only 2
    latent variables, so it’s reasonable that they could still be near one another
    in a 20-dimensional diagram with 20 latent variables.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果并不荒谬。例如，在第二列，2和4之间的混合看起来像是一个部分的8。这是有道理的。[图18-23](#figure18-23)告诉我们，数字2、4和8在只有2个潜在变量的图中相距很近，因此它们在具有20个潜在变量的20维图中仍然可能靠得很近。
- en: Let’s look at this kind of blending of latent variables more closely. [Figure
    18-27](#figure18-27) shows three new pairs of digits with six equally spaced steps
    of interpolation.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这种潜在变量的混合。[图18-27](#figure18-27)展示了三个新的数字对，并且通过六个等间距的插值步骤进行混合。
- en: '![F18027](Images/F18027.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![F18027](Images/F18027.png)'
- en: 'Figure 18-27: Blending the latent variables. For each row, we blend between
    the leftmost and rightmost sets of latent variables.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-27：混合潜在变量。对于每一行，我们在最左侧和最右侧的潜在变量集之间进行混合。
- en: The far left and right of each row are images from the MNIST data. We found
    the 20 latent variables for each endpoint, created six equally spaced blends of
    those latent variables, and then ran those blended latents through the decoder.
    The system is trying to move from one image to another, but it’s not producing
    very reasonable intermediate digits. Even when going from a 5 to a 5 in the middle
    row, the intermediate values almost break up into two separate pieces before rejoining.
    Some of the blends near the middle of the top and bottom rows don’t look like
    any digits at all. Although the ends are recognizable, the blends fall apart very
    quickly. Blending latent parameters in this autoencoder smoothly changes the image
    from one digit to another, but the in-betweens are just weird shapes, rather than
    some kind of blended digits. We’ve seen that sometimes this is due to moving through
    dense regions where similar latent variables encode different digits. A bigger
    problem is conceptual. These examples may not even be wrong, since it’s not clear
    what a digit that’s partly 0 and partly 1 *should* look like, were we able to
    make one. Maybe the 0 should get thinner? Maybe the 1 should curl up into a circle?
    So although these blends don’t look like digits, they’re reasonable results.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行的最左边和最右边是来自MNIST数据的图像。我们为每个端点找到了20个潜在变量，创建了六个均匀间隔的潜在变量混合体，然后将这些混合的潜在变量传递给解码器。系统试图从一幅图像转换到另一幅，但它并没有生成非常合理的中间数字。即使是在中间行中从5转换到5，过渡值几乎在两者之间分裂成两个独立的部分，然后才重新合并。顶部和底部行中靠近中间的一些混合图像根本不像任何数字。尽管两端可以识别，但这些混合图像迅速崩溃。自编码器中的潜在参数混合平滑地将图像从一个数字转换为另一个，但过渡图像只是奇怪的形状，而不是某种混合的数字。我们已经看到，这有时是因为经过密集区域，其中相似的潜在变量编码了不同的数字。一个更大的问题是概念上的。这些例子可能甚至不算错，因为如果我们能制造出一个部分为0、部分为1的数字，它应该是什么样子还不清楚。也许0应该变得更瘦？也许1应该卷成一个圆圈？所以尽管这些混合图像不像数字，但它们是合理的结果。
- en: Some of these interpolated latent values can land in regions of latent space
    where there’s no nearby data. In other words, we’re asking the decoder to reconstruct
    an image from values of latent variables that don’t have any nearby neighbors
    in latent space. The decoder is producing *something*, and that output has some
    qualities of the nearby regions, but the decoder is essentially guessing.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这些插值的潜在值可能会落入潜在空间中的区域，这些区域没有附近的数据。换句话说，我们要求解码器从没有任何邻近数据的潜在变量值中重建图像。解码器正在生成*某种东西*，而且这个输出具有附近区域的一些特性，但解码器本质上是在猜测。
- en: Predicting from Novel Input
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从新输入进行预测
- en: Let’s try to use this deep autoencoder trained on MNIST data to compress and
    then decompress our tiger image. We will shrink the tiger to 28 by 28 pixels to
    match the network’s input size, so it’s going to look very blurry.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用在MNIST数据上训练的深度自编码器来压缩然后解压我们的老虎图像。我们将把老虎图像缩小到28×28像素，以匹配网络的输入大小，因此它看起来会非常模糊。
- en: The tiger is like nothing the network has ever seen before, so it’s completely
    ill-equipped to deal with this data. It tries to “see” a digit in the image and
    produces a corresponding output. [Figure 18-28](#figure18-28) shows the results.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这只老虎是网络从未见过的，所以它完全没有能力处理这些数据。它尝试在图像中“看”一个数字并生成相应的输出。[图18-28](#figure18-28)展示了结果。
- en: '![F18028](Images/F18028.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![F18028](Images/F18028.png)'
- en: 'Figure 18-28: Encoding and then decoding a 28 by 28 version of our tiger of
    [Figure 18-8](#figure18-8) with our deep autoencoder of 20 latent variables, trained
    on the MNIST handwritten digit dataset'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-28：使用我们训练在MNIST手写数字数据集上的20个潜在变量的深度自编码器，编码然后解码我们老虎图像的28×28版本，见[图18-8](#figure18-8)
- en: It looks like the algorithm has tried to find a spot that combines several different
    digits. The splotch in the middle isn’t much of a match to the tiger, but there’s
    no reason it should be.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来算法尝试找出一个位置，将几个不同的数字组合起来。中间的污点与老虎并不匹配，但也没有理由它应该匹配。
- en: Using information learned from digits to compress and decompress a tiger is
    like trying to build a guitar using parts taken from pencil sharpeners. Even if
    we do our best, the result isn’t likely to be a good guitar. An autoencoder can
    only meaningfully encode and decode the type of data it’s been trained on because
    it created meaning for the latent variables only to represent that data. When
    we surprise it with something completely different, it does its best, but it’s
    not going to be very good.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用从数字中学到的信息来压缩和解压缩一只老虎，就像试图用削笔器的零件来建造一把吉他一样。即使我们尽最大努力，结果也不太可能是把好吉他。自编码器只能有意义地编码和解码它所训练过的数据类型，因为它只为潜在变量创建了表示该数据的意义。当我们用完全不同的东西来“惊讶”它时，它尽力而为，但效果不会很好。
- en: There are several variations on the basic autoencoder concept. Since we’re working
    with images, and convolution is a natural approach for that kind of data, let’s
    build an autoencoder using convolution layers.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的自编码器概念有几个变种。由于我们处理的是图像数据，而卷积是处理此类数据的自然方法，所以让我们使用卷积层来构建一个自编码器。
- en: Convolutional Autoencoders
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积自编码器
- en: We said earlier that our encoding and decoding stages could contain any kind
    of layers we wanted. Since our running example uses image data, let’s use convolutional
    layers. In other words, let’s build a *convolutional autoencoder*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，编码和解码阶段可以包含我们想要的任何类型的层。由于我们的示例使用的是图像数据，让我们使用卷积层。换句话说，我们来构建一个*卷积自编码器*。
- en: We will design an encoder to use several layers of convolution to scale down
    the original 28 by 28 MNIST image in stages until it’s just 7 by 7\. All of our
    convolutions will use 3 by 3 filters, and zero-padding. As shown in [Figure 18-29](#figure18-29),
    we start with a convolution layer with 16 filters and follow it by a maximum pooling
    layer with a 2 by 2 cell, giving us a tensor that is 14 by 14 by 16 (we could
    have used striding during convolution, but we’ve separated the steps here for
    clarity). Then we apply another convolution, this time with 8 filters, and follow
    that with pooling, producing a tensor that’s 7 by 7 by 8\. The final encoder layer
    uses three filters, producing a tensor that’s 7 by 7 by 3 at the bottleneck. Thus,
    our bottleneck represents the 768 inputs with 7 × 7 × 3 = 147 latent variables.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将设计一个编码器，使用多个卷积层逐步缩小原始 28 × 28 MNIST 图像，直到它变成 7 × 7。我们所有的卷积都会使用 3 × 3 的滤波器，并且会进行零填充。如[图
    18-29](#figure18-29)所示，我们从一个包含 16 个滤波器的卷积层开始，然后跟一个 2 × 2 单元的最大池化层，得到一个形状为 14 ×
    14 × 16 的张量（我们本可以在卷积时使用步幅，但为了清晰起见，这里将步骤分开）。然后我们应用另一个卷积层，这次使用 8 个滤波器，并跟随一个池化层，生成一个形状为
    7 × 7 × 8 的张量。最终的编码器层使用三个滤波器，生成一个形状为 7 × 7 × 3 的张量作为瓶颈。这样，我们的瓶颈用 7 × 7 × 3 = 147
    个潜在变量表示了 768 个输入。
- en: '![F18029](Images/F18029.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![F18029](Images/F18029.png)'
- en: 'Figure 18-29: The architecture of our convolutional autoencoder. In the encoding
    stage (blue), we have three convolutional layers. The first two layers are each
    followed by a pooling layer, so by the end of the third convolutional layer, we
    have an intermediate tensor of shape 7 by 7 by 3\. The decoder (beige) uses convolution
    and upsampling to grow the bottleneck tensor back into a 28 by 28 by 1 output.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-29：我们卷积自编码器的架构。在编码阶段（蓝色），我们有三层卷积层。前两层每层后面跟一个池化层，因此到第三层卷积层结束时，我们得到了一个形状为
    7 × 7 × 3 的中间张量。解码器（米色）使用卷积和上采样将瓶颈张量恢复为 28 × 28 × 1 的输出。
- en: Our decoder runs the process in reverse. The first upsampling layer produces
    a tensor that’s 14 by 14 by 3\. The following convolution and upsampling gives
    us a tensor that’s 28 by 28 by 16, and the final convolution produces a tensor
    of shape 28 by 28 by 1\. As before, we’re leaving out the flattening step at the
    start and the reshaping step at the end.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解码器反向执行这个过程。第一个上采样层生成一个形状为 14 × 14 × 3 的张量。接下来的卷积和上采样给我们一个形状为 28 × 28 × 16
    的张量，最后的卷积产生一个形状为 28 × 28 × 1 的张量。如前所述，我们省略了开始时的扁平化步骤和结束时的重塑步骤。
- en: Since we’ve got 147 latent variables, along with the power of the convolutional
    layers, we should expect better results than with our previous autoencoder of
    just 20 latent variables. We trained this network for 50 epochs, just as before.
    The model was still improving at that point, but we stopped at 50 epochs for the
    sake of comparison with the previous models.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有 147 个潜在变量，加上卷积层的强大功能，我们应该期望比之前仅有 20 个潜在变量的自编码器得到更好的结果。我们像之前一样训练了这个网络 50
    个周期。在那个时候，模型仍在改进，但为了与之前的模型进行比较，我们在 50 个周期时停止了训练。
- en: '[Figure 18-30](#figure18-30) shows five examples from the test set and their
    decompressed versions after running through our convolutional autoencoder.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-30](#figure18-30)展示了来自测试集的五个例子，以及它们经过我们的卷积自编码器处理后解压的版本。'
- en: '![F18030](Images/F18030.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![F18030](Images/F18030.png)'
- en: 'Figure 18-30: Top row: Five elements from the MNIST test set. Bottom row: The
    images produced by our convolutional autoencoder given the image above it as input.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-30：顶部行：来自MNIST测试集的五个元素。底部行：通过我们的卷积自编码器生成的图像，输入为上面一行的图像。
- en: These results are pretty great. The images aren’t identical, but they’re very
    close.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果相当不错。图像并不完全相同，但非常接近。
- en: Just for fun, let’s try giving the decoder step nothing but noise. Since our
    latent variables are a tensor of size 7 by 7 by 3, our noise values need to be
    a 3D volume of the same shape. Rather than try to draw such a block of numbers,
    we will just show the topmost 7 by 7 slice of the block. [Figure 18-31](#figure18-31)
    shows the results.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 只是为了好玩，尝试给解码器输入纯噪声。由于我们的潜在变量是一个7×7×3的张量，因此我们的噪声值需要是一个形状相同的3D体积。我们不打算画出这些数字块，而是只展示这个块的最上面一层7×7切片。[图18-31](#figure18-31)展示了结果。
- en: '![F18031](Images/F18031.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![F18031](Images/F18031.png)'
- en: 'Figure 18-31: Images produced by handing an input tensor of random values to
    the decoder stage of our convolutional neural network'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-31：通过将随机值输入到我们的卷积神经网络解码阶段生成的图像
- en: This just produces randomly splotchy images, which seems a fair output for a
    random input.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这只产生了随机的斑点图像，对于随机输入来说，这是一个合理的输出。
- en: Blending Latent Variables
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合潜在变量
- en: Let’s blend the latent variables in our convolutional autoencoder and see how
    it goes. In [Figure 18-32](#figure18-32) we show our grid using the same images
    as in [Figure 18-26](#figure18-26). We find the latent variables for each image
    in the top two rows, blend them equally, and then decode the interpolated variables
    to create the bottom row.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在卷积自编码器中混合潜在变量，看看效果如何。[图18-32](#figure18-32)展示了我们的网格，使用的图像与[图18-26](#figure18-26)中的相同。我们找到顶部两行每张图像的潜在变量，将它们等量混合，然后解码插值变量，生成底部行。
- en: The results are pretty gloppy, though some have a feeling of being a mix of
    the images from the rows above. Again, we shouldn’t be too surprised, since it’s
    not clear what a digit halfway between, say, 7 and 3 ought to look like.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 结果有些模糊，尽管某些图像看起来像是上面几行图像的混合。然而，我们不应该太惊讶，因为我们并不清楚比如说，7和3之间的数字应该是什么样子。
- en: '![F18032](Images/F18032.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![F18032](Images/F18032.png)'
- en: 'Figure 18-32: Blending latent variables in the convolutional autoencoder. Top
    two rows: Samples from the MNIST dataset. Bottom row: The result of an equal blend
    of the latent variables from each of the above images.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-32：在卷积自编码器中混合潜在变量。顶部两行：来自MNIST数据集的样本。底部行：每张图像潜在变量等量混合后的结果。
- en: Let’s look at multiple steps along the way in the same three blends that we
    used before in [Figure 18-27](#figure18-27). The results are shown in [Figure
    18-33](#figure18-33).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下之前在[图18-27](#figure18-27)中使用的三种混合方式的多个步骤。结果如[图18-33](#figure18-33)所示。
- en: '![F18033](Images/F18033.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![F18033](Images/F18033.png)'
- en: 'Figure 18-33: Blending the latent variables of two MNIST test images and then
    decoding'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-33：将两张MNIST测试图像的潜在变量进行混合，然后解码
- en: The left and right ends of each row are images created by encoding and decoding
    an MNIST image. In between are the results of blending their latent variables
    and then decoding. This isn’t looking a whole lot better than our simpler autoencoder.
    So just because we have more latent variables, we still run into trouble when
    we try to reconstruct using inputs that are too unlike the samples that the system
    was trained on. For example, in the top row we didn’t train on any input images
    that were in some way “between” a 4 and 3, so the system didn’t have any good
    information on how to produce images from latent values representing such a thing.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 每行的左右两端是通过编码和解码MNIST图像生成的图像。中间部分是通过混合它们的潜在变量并解码得到的结果。这看起来比我们的简单自编码器好不了多少。所以，仅仅因为我们有更多的潜在变量，当我们尝试用那些与系统训练样本差异较大的输入进行重建时，依然会遇到问题。例如，在顶部一行中，我们并没有训练任何在某种意义上“介于”4和3之间的输入图像，因此系统没有关于如何从代表这种情况的潜在值生成图像的有用信息。
- en: Predicting from Novel Input
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从新输入中进行预测
- en: Let’s repeat our completely unfair test by giving the low-resolution tiger to
    our convolutional neural net. The results are shown in [Figure 18-34](#figure18-34).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过给卷积神经网络输入低分辨率的老虎图像，重复我们这个完全不公平的测试。结果如[图18-34](#figure18-34)所示。
- en: If we squint, it looks like the major dark regions around the eyes, the sides
    of the mouth, and the nose, have been preserved. Maybe. Or maybe that’s just imagination.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们眯起眼睛看，它看起来像是眼睛周围、嘴巴两侧和鼻子周围的主要暗区被保留下来了。也许吧。或者这也可能只是想象。
- en: As with our earlier autoencoder built from fully connected layers, our convolutional
    autoencoder is trying to find a tiger somewhere in the latent space of digits.
    We shouldn’t expect it to do well.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 和我们之前用全连接层构建的自动编码器一样，我们的卷积自动编码器试图在数字的潜在空间中找到某个地方的老虎。我们不应该期待它做得很好。
- en: '![F18034](Images/F18034.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![F18034](Images/F18034.png)'
- en: 'Figure 18-34: The low-resolution tiger we applied to our convolutional autoencoder,
    and the result. It’s not very tiger-like.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-34：我们应用于卷积自动编码器的低分辨率老虎图像，以及结果。看起来不像老虎。
- en: Denoising
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去噪
- en: A popular use of autoencoders is to remove noise from samples. A particularly
    interesting application is to remove the speckling that sometimes appears in computer-generated
    images (Bako et al. 2017; Chaitanya 2017). These bright and dark points, which
    can look like static, or snow, can be produced when we generate an image quickly,
    without refining all the results.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器的一个流行用途是去除样本中的噪声。一种特别有趣的应用是去除计算机生成图像中有时会出现的斑点（Bako et al. 2017; Chaitanya
    2017）。这些明亮和暗淡的点，看起来像静态噪声或雪花，通常是在我们快速生成图像时出现的，未对所有结果进行细化。
- en: Let’s see how to use an autoencoder to remove bright and dark dots in an image.
    We will use the MNIST dataset again, but this time, we’ll add some random noise
    to our images. At every pixel, we pick a value from a Gaussian distribution with
    a mean of 0, so we get positive and negative values, add them in, and then clip
    the resulting values to the range 0 to 1\. [Figure 18-35](#figure18-35) shows
    some MNIST training images with this random noise applied.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用自动编码器去除图像中的亮点和暗点。我们将再次使用MNIST数据集，但这次，我们会在图像中添加一些随机噪声。在每个像素点，我们从均值为0的高斯分布中选择一个值，因此我们得到正值和负值，将它们加进来，然后将结果值截断到0到1的范围内。[图18-35](#figure18-35)展示了应用了这种随机噪声的一些MNIST训练图像。
- en: '![F18035](Images/F18035.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![F18035](Images/F18035.png)'
- en: 'Figure 18-35: Top: MNIST training digits. Bottom: The same digits but with
    random noise.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-35：上：MNIST训练数字。下：相同的数字，但加了随机噪声。
- en: Our goal is to give our trained autoencoder the noisy versions of the digits
    in the bottom row of [Figure 18-35](#figure18-35) and have it return cleaned-up
    versions like the top row of in [Figure 18-35](#figure18-35). Our hope is that
    the latent variables won’t encode the noise, so we’ll get back just the digits.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将训练过的自动编码器输入[图18-35](#figure18-35)底部行的带噪数字版本，让它返回清理后的版本，就像[图18-35](#figure18-35)顶部行中的数字一样。我们的希望是潜在变量不会编码噪声，所以我们只会得到数字。
- en: We’ll use an autoencoder with the same general structure as [Figure 18-29](#figure18-29),
    though with different numbers of filters (Chollet 2017). [Figure 18-36](#figure18-36)
    shows the architecture.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与[图18-29](#figure18-29)中相同的总体结构的自动编码器，尽管它有不同数量的滤波器（Chollet 2017）。[图18-36](#figure18-36)显示了架构。
- en: '![F18036](Images/F18036.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![F18036](Images/F18036.png)'
- en: 'Figure 18-36: A denoising autoencoder'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-36：去噪自动编码器
- en: To train our autoencoder, we’ll give it noisy image inputs and their corresponding
    clean, noise-free versions as the targets we want it to produce. We’ll train with
    all 60,000 images for 100 epochs.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的自动编码器，我们将给它带噪的图像输入和它们相应的干净、无噪声的版本作为我们希望它生成的目标。我们将使用所有60,000张图像进行100轮训练。
- en: The tensor at the end of the decoding step in [Figure 18-35](#figure18-35) (that
    is, after the third convolution) has size 7 by 7 by 32, for a total of 1,568 numbers.
    So our “bottleneck” in this model is twice the size of the input. That would be
    bad if our goal was compression, but here we’re trying to remove noise, so minimizing
    the number of latent variables isn’t as much of a concern.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图18-35](#figure18-35)解码步骤的最后（也就是第三个卷积之后），张量的大小为7 x 7 x 32，共计1,568个数字。所以这个模型中的“瓶颈”是输入大小的两倍。如果我们的目标是压缩，这将是糟糕的，但在这里我们是要去除噪声，所以最小化潜在变量的数量并不是特别关注的问题。
- en: How well does it perform? [Figure 18-37](#figure18-37) shows some of the noisy
    inputs and the autoencoder’s outputs. It cleaned up the pixels very well, giving
    us great-looking results.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 它表现如何？[图18-37](#figure18-37)展示了一些噪声输入和自动编码器的输出。它很好地清理了像素，给我们带来了很棒的结果。
- en: '![F18037](Images/F18037.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![F18037](Images/F18037.png)'
- en: 'Figure 18-37: Top row: Digits with noise added. Bottom row: The same digits
    denoised by our model of [Figure 18-36](#figure18-36).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-37：上排：添加噪声的数字。下排：通过我们模型（见[图18-36](#figure18-36)）去噪后的相同数字。
- en: In Chapter 16, we discussed that explicit upsampling and downsampling layers
    are falling out of favor, replaced by striding and transposed convolution. Let’s
    follow that trend to simplify our model of [Figure 18-36](#figure18-36) to make
    [Figure 18-38](#figure18-38), which is now made up of nothing but a sequence of
    five convolution layers. The first two convolutions use striding to replace explicit
    downsampling layers, and the last two layers use repetition instead of explicit
    upsampling layers. Recall that we’re assuming zero-padding in each convolution
    layer.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在第16章中，我们讨论了显式的上采样和下采样层正在逐渐被淘汰，取而代之的是步幅和转置卷积。让我们遵循这一趋势，简化[图18-36](#figure18-36)中的模型，得到[图18-38](#figure18-38)，该模型现在仅由五个卷积层组成。前两个卷积层使用步幅代替显式的下采样层，最后两个层使用重复代替显式的上采样层。请记住，我们假设在每个卷积层中都使用了零填充。
- en: '![F18038](Images/F18038.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![F18038](Images/F18038.png)'
- en: 'Figure 18-38: The autoencoder of [Figure 18-36](#figure18-36) but using downsampling
    and upsampling inside the convolution layers, as shown by the wedges attached
    to the convolution icons'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-38：[图18-36](#figure18-36)的自编码器，但在卷积层中使用了下采样和上采样，如卷积图标附带的楔形所示
- en: '[Figure 18-39](#figure18-39) shows the results.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-39](#figure18-39)显示了结果。'
- en: '![F18039](Images/F18039.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![F18039](Images/F18039.png)'
- en: 'Figure 18-39: The results of our denoising model of [Figure 18-38](#figure18-38)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-39：我们去噪模型的结果，[图18-38](#figure18-38)所示
- en: The outputs are quite close, though there are small differences (for example,
    look at the bottom-left of the 0). The first model, [Figure 18-36](#figure18-36),
    with explicit layers for upsampling and downsampling, took roughly 300 seconds
    per epoch on a late 2014 iMac with no GPU support. The simpler model of [Figure
    18-38](#figure18-38) took only about 200 seconds per epoch so it shaved off about
    one-third of the training time.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 输出非常接近，尽管存在一些小的差异（例如，看看0的左下角）。第一个模型，[图18-36](#figure18-36)，具有显式的上采样和下采样层，在2014年底的iMac上每个训练周期大约需要300秒，并且没有GPU支持。较简单的模型，[图18-38](#figure18-38)，每个训练周期只需约200秒，因此节省了大约三分之一的训练时间。
- en: It would require a more careful problem statement, testing, and review of the
    results to decide if either of these models produces better results than the other
    for this task.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 需要更小心的问题陈述、测试和结果审查，以决定这些模型中的任何一个是否比另一个在此任务中产生更好的结果。
- en: Variational Autoencoders
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: The autoencoders we’ve seen so far have tried to find the most efficient way
    to compress an input so that it can later be re-created. A *variational autoencoder
    (VAE)* shares the same general architecture as those networks but does an even
    better job of clumping the latent variables and filling up the latent space.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到的自编码器都试图找到最有效的方式来压缩输入，以便之后能够重新创建它。*变分自编码器（VAE）*与这些网络具有相同的基本架构，但在聚集潜在变量和填充潜在空间方面做得更好。
- en: VAEs also differ from our previous autoencoders because they have some unpredictability.
    Our previous autoencoders were deterministic. That is, given the same input, they
    always produce the same latent variables, and those latent variables always then
    produce the same output. But a VAE uses probabilistic ideas (that is, random numbers)
    in the encoding stage; if we run the same input through the system multiple times,
    we get a slightly different output each time. We say that a VAE is *nondeterministic.*
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE）与我们之前的自编码器有所不同，因为它们具有一些不可预测性。我们之前的自编码器是确定性的。也就是说，给定相同的输入，它们总是产生相同的潜在变量，而这些潜在变量总是会产生相同的输出。但VAE在编码阶段使用了概率思想（即随机数）；如果我们多次将相同的输入通过系统，得到的输出每次都会略有不同。我们称VAE为*非确定性*的。
- en: 'As we look at the VAE, let’s continue to phrase our discussion in terms of
    images (and pixels) for concreteness, but like all of our other machine learning
    algorithms, a VAE can be applied to any kind of data: sound, weather, movie preferences,
    or anything else we can represent numerically.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论VAE时，为了具体化，我们继续以图像（和像素）为例进行讨论，但和我们其他的机器学习算法一样，VAE可以应用于任何类型的数据：声音、天气、电影偏好，或任何我们可以用数字表示的东西。
- en: Distribution of Latent Variables
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 潜在变量的分布
- en: In our previous autoencoders we didn’t impose any conditions on the structure
    of the latent variables. In [Figure 18-20](#figure18-20), we saw that a fully
    connected encoder seemed to naturally group the latent variables into blobs radiating
    to the right and upward from a common starting point at (0,0). That structure
    wasn’t a design goal. It just came out that way as a result of the nature of the
    network we built. The convolutional network in [Figure 18-38](#figure18-38) produces
    similar results when we reduce the bottleneck to two latent variables, shown here
    in [Figure 18-40](#figure18-40).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![F18040](Images/F18040.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18-40: The latent variables produced by the convolutional autoencoder
    of [Figure 18-38](#figure18-38) with a bottleneck of two latent variables. These
    samples are drawn from both densely mixed and sparse regions.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 18-40](#figure18-40) shows decoded images generated from latent variables
    chosen from both dense and sparse regions.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 18-22](#figure18-22) we saw that we can pick any pair of latent variables
    and run those values through a decoder to make an image. [Figure 18-40](#figure18-40)
    shows that if we pick these points in the dense zones, or the unoccupied zones,
    we often get back images that don’t look like digits. It would be great if we
    could find a way to set things up so that any pair of inputs always (or almost
    always) produces a good-looking digit.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, it would be great if each digit had its own zone, the zones didn’t
    overlap, and we didn’t have any big, empty spaces. There’s not much we can do
    about filling in empty zones, since those are places where we just don’t have
    input data. But we can try to break apart the mixed zones so that each digit occupies
    its own region of the latent space.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how a variational autoencoder does a good job of meeting these goals.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Variational Autoencoder Structure
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As so often happens with good ideas, the VAEwas invented simultaneously but
    independently by at least two different groups (Kingma and Welling 2014; Rezende,
    Mohamed, and Wierstra 2014). Understanding the technique in detail requires working
    through some math (Dürr 2016), so instead, let’s take an approximate and conceptual
    approach. Because our intent is to capture the gist of the method rather than
    its precise mechanics, we will skip some details and gloss over others.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to create a generator that can take in random latent variables and
    produce new outputs that are reasonably like inputs that had similar latent values.
    Recall that the distribution of the latent variables is created together by the
    encoder and decoder during training. During that process, in addition to making
    sure the latent variables let us reconstruct the inputs, we also desire that the
    latent variables obey three properties.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: First, all of the latent variables should be gathered into one region of latent
    space so we know what the ranges should be for our random values. Second, latent
    variables produced by similar inputs (that is, images that show the same digit)
    should be clumped together. Third, we want to minimize empty regions in the latent
    space.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，所有潜在变量应该集中在潜在空间的一个区域，以便我们知道随机值的范围应该是多少。其次，由相似输入（即显示相同数字的图像）生成的潜在变量应该聚集在一起。第三，我们希望最小化潜在空间中的空白区域。
- en: To satisfy these criteria, we can use a more complicated error term that punishes
    the system when it makes latent samples that don’t follow the rules. Since the
    whole point of learning is to minimize the error, the system will learn how to
    create latent values that are structured the way we want. The architecture and
    the error term are designed to work together. Let’s see what that error term looks
    like.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些标准，我们可以使用一个更复杂的误差项，当系统生成不符合规则的潜在样本时，给予惩罚。由于学习的整个目的是最小化误差，系统将学会如何创建结构符合我们要求的潜在值。架构和误差项是协同设计的。让我们看看这个误差项是什么样的。
- en: Clustering the Latent Variables
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 聚类潜在变量
- en: Let’s first tackle the idea of keeping all latent variables together in one
    place. We can do that by imposing a rule, or constraint, which we build into the
    error term.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来处理将所有潜在变量集中在一个地方的想法。我们可以通过强加一个规则或约束来实现这一点，并将其构建到误差项中。
- en: Our constraint is that the values for each latent variable, when plotted, come
    close to forming a unit Gaussian distribution. Recall from Chapter 2 that a Gaussian
    is the famous bell curve, illustrated in [Figure 18-41](#figure18-41).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的约束是，每个潜在变量的值在绘制时，接近形成单位高斯分布。回想一下第二章，高斯分布是著名的钟形曲线，如[图18-41](#figure18-41)所示。
- en: '![F18041](Images/F18041.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![F18041](Images/F18041.png)'
- en: 'Figure 18-41: A Gaussian curve'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-41：高斯曲线
- en: When we place two Gaussians at right angles to one another, we get a bump above
    the plane, as in [Figure 18-42](#figure18-42).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将两个高斯分布彼此垂直放置时，我们得到一个位于平面上的凸起，如[图18-42](#figure18-42)所示。
- en: '![F18042](Images/F18042.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![F18042](Images/F18042.png)'
- en: 'Figure 18-42: In 3D, we can place two Gaussians at right angles. Together,
    they form a bump over the plane.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-42：在3D空间中，我们可以将两个高斯分布放置在垂直方向上。它们一起形成了一个位于平面上的凸起。
- en: '[Figure 18-42](#figure18-42) shows a 3D visualization of a 2D distribution.
    We can create an actual 3D distribution by including another Gaussian on the Z
    axis. If we think of the resulting bump as a density, then this 3D Gaussian is
    like a dandelion puff, which is dense in the center but becomes sparser as we
    move outward.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-42](#figure18-42)展示了二维分布的三维可视化。我们可以通过在Z轴上加入另一个高斯分布来创建一个实际的三维分布。如果我们把结果的凸起看作密度，那么这个三维高斯分布就像一朵蒲公英的花絮，中心密集，但向外扩展时变得更加稀疏。'
- en: '![F18043](Images/F18043.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![F18043](Images/F18043.png)'
- en: 'Figure 18-43: A Gaussian is described by its mean (the location of its center),
    and its standard deviation (the symmetrical distance that contains about 68 percent
    of its area). Here we have a center of 0, and a standard deviation of 1.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-43：一个高斯分布由其均值（即其中心位置）和标准差（即对称的距离，约68%的面积包含在此范围内）来描述。这里的均值为0，标准差为1。
- en: By analogy, we can imagine a Gaussian of any number of dimensions, just by saying
    that each dimension’s density follows a Gaussian curve on its axis. And that’s
    what we do here. We tell the VAE to learn values for the latent variables so that,
    when we look at the latent variables for lots of training samples and we count
    up how many times each value occurs, every variable’s counts form a distribution
    like a Gaussian that has its mean (or center) at 0, and a standard deviation (that
    is, its spread) of 1, as in [Figure 18-43](#figure18-43). Recall from Chapter
    2 that this means that about 68 percent of the values we produce for this latent
    variable fall between −1 and 1.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 类比地，我们可以想象一个任意维度的高斯分布，只需要假设每个维度的密度沿其轴线遵循高斯曲线。这就是我们在这里做的。我们告诉VAE学习潜在变量的值，使得当我们查看大量训练样本的潜在变量并统计每个值出现的次数时，每个变量的计数形成类似高斯分布的分布，其均值（或中心）为0，标准差（即其扩展范围）为1，如[图18-43](#figure18-43)所示。回想一下第二章，这意味着大约68%的潜在变量值位于−1到1之间。
- en: When we’re done training, we know that our latent variables will be distributed
    according to this pattern. If we pick new values to feed to the decoder, and we
    select them from this distribution (where we’re more likely to pick each value
    near its center and within the bulk of its bump rather than off to the edges),
    we are likely to generate sets of latent values that are near values we learned
    from the training set, and thus we can create an output that is also like the
    training set. This naturally also keeps the samples together in the same area,
    since they’re all trying to match a Gaussian distribution with a center of 0.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们完成训练后，我们知道我们的潜在变量将按照这种模式分布。如果我们选择新的值并将其输入解码器，并且从这个分布中选择这些值（我们更有可能选择靠近中心和分布主要部分的值，而不是选择边缘的值），那么我们很可能生成与训练集中学到的值相近的潜在变量集，从而生成与训练集相似的输出。这也自然地将样本保持在同一区域，因为它们都试图匹配以0为中心的高斯分布。
- en: Getting the latent variables to fall within unit Gaussians, as shown in [Figure
    18-43](#figure18-43), is an ideal we rarely achieve. There’s a tradeoff between
    how well the variables match Gaussians and how accurate the system can be in re-creating
    inputs (Frans 2016). The system automatically learns that tradeoff during the
    training session, striving to keep the latents Gaussian-like while also reconstructing
    the inputs well.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图18-43](#figure18-43)所示，将潜在变量落在单位高斯分布内是一个理想状态，但我们很少能做到这一点。变量与高斯分布的匹配度和系统在重建输入时的准确度之间存在权衡（Frans
    2016）。系统在训练过程中自动学习这种权衡，努力保持潜在变量的高斯形态，同时也能很好地重建输入。
- en: Clumping Digits Together
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将数字聚集在一起
- en: Our next goal is getting the latent values of all images with the same digits
    to clump together. To do this, let’s use a clever trick that involves some randomness.
    It’s a bit subtle.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个目标是让所有相同数字的图像的潜在值聚集在一起。为此，让我们使用一个巧妙的技巧，涉及一些随机性。这有点微妙。
- en: Let’s start by assuming that we’ve *already achieved this goal*. We will see
    what this implies from a particular point of view, and that will tell us how to
    actually bring it about. For example, we’re assuming that every set of latent
    variables for an image of, say, the digit 2 is near every other set of latent
    variables for images of the digit 2\. We can do even better, though. Some 2s have
    a loop in the lower-left corner. So, in addition to having all the 2s clumped
    together, we can keep all the 2s with loops together and all the 2s without loops
    together, and the region between those clumps is filled with the latent variables
    of 2s that sort-of have a loop, as in [Figure 18-44](#figure18-44).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们*已经实现了这个目标*，让我们从一个特定的角度来看这个结果，这将告诉我们如何真正实现它。例如，我们假设每个图像的潜在变量集（例如数字2的图像）与所有其他数字2图像的潜在变量集相近。不过，我们可以做得更好。一些2在左下角有一个环。因此，除了将所有2聚集在一起外，我们还可以将所有有环的2聚在一起，将所有没有环的2聚在一起，而这两个聚集区域之间的区域则由那些大致有环的2的潜在变量填充，正如在[图18-44](#figure18-44)所示。
- en: 'Now let’s carry this idea to its limit. Whatever the shape and style and line
    thickness and tilt and so on of every image that’s labeled a 2, we’ll assign that
    image latent variables that are near other images labeled 2 that show about the
    same shape and style. We can gather together all the 2s with a loop and all those
    without, all those drawn with straight lines and all those drawn with curves,
    all those with a thick stroke and all those with a thin one, all the 2s that are
    tall, and so on. That’s the major value of using lots of latent variables: they
    let us clump together all of the different combinations of these features, which
    wouldn’t be possible in just two dimensions. In one place we have thin straight
    no-loop 2s, another region has thick curved no-loop 2s, and so on, for every combination.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们把这个想法推向极限。无论每个标记为2的图像的形状、风格、线条粗细、倾斜度等如何，我们都会为这些图像分配潜在变量，这些变量与其他标记为2的图像相近，而这些图像呈现相似的形状和风格。我们可以用一个循环将所有标记为2的图像聚集在一起，再将那些没有标记为2的图像分开，将所有用直线绘制的图像和所有用曲线绘制的图像、所有线条粗的图像和所有线条细的图像、所有高的2等聚在一起。这就是使用大量潜在变量的主要价值：它们让我们能够将这些特征的所有不同组合聚集在一起，而这在二维空间中是无法做到的。在一个区域，我们有细直线的无环2，另一个区域则有粗曲线的无环2，以此类推，涵盖了每一种组合。
- en: '![F18044](Images/F18044.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![F18044](Images/F18044.png)'
- en: 'Figure 18-44: A grid of 2s organized so that neighbors are all like one another.
    We want the latent variables for these 2s to follow roughly this kind of structure.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-44：一个2的网格被组织成使得相邻的2看起来相似。我们希望这些2的潜在变量大致遵循这样的结构。
- en: If we had to identify all of these features ourselves, this scheme wouldn’t
    be very practical. But a VAE not only learns the different features, it automatically
    creates all the different groupings for us as it learns. As usual, we just feed
    in images and the system does all the rest of the work.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们必须自己识别所有这些特征，这种方案就不太实用了。但 VAE 不仅学习不同的特征，还会在学习过程中自动为我们创建所有不同的分组。像往常一样，我们只需输入图像，系统就会完成其余的工作。
- en: 'This “nearness” criterion is measured in latent space, where there’s one dimension
    for each latent variable. In two dimensions, each set of latent variables creates
    a point on the plane, and their distance (or “nearness”) is the length of the
    line between them. We can generalize this idea to any number of dimensions, so
    we can always find the distance between two sets of latent variables, even if
    each one has 30 or even 300 values: we just measure the length of the line that
    joins them.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这个“接近度”标准是在潜在空间中衡量的，其中每个潜在变量都有一个维度。在二维空间中，每组潜在变量在平面上形成一个点，它们之间的距离（或“接近度”）是连接它们的直线的长度。我们可以将这个概念推广到任意维度，因此即使每组潜在变量有
    30 或甚至 300 个值，我们也可以始终找到它们之间的距离：我们只需测量连接它们的直线长度。
- en: We want the system to clump together similar-looking inputs. But recall that
    we also want each latent variable to form a Gaussian distribution. These two criteria
    can come into conflict. By introducing some randomness, we can tell the system
    to “usually” clump the latent variables for similar input, and “usually” also
    distribute those variables along a Gaussian curve. Let’s see how randomness lets
    us make that happen.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望系统将相似的输入聚集在一起。但请记住，我们也希望每个潜在变量形成高斯分布。这两个标准可能会发生冲突。通过引入一些随机性，我们可以告诉系统“通常”将相似的输入潜在变量聚集在一起，并“通常”沿高斯曲线分布这些变量。让我们看看随机性如何让这一目标得以实现。
- en: Introducing Randomness
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 引入随机性
- en: Suppose that our system is given an input of an image of the digit 2, and as
    usual, the encoder finds the latent variables for it. Before we hand these to
    the decoder to produce an output image, let’s add a little randomness to each
    of the latent variables and pass those modified values to the decoder, as in [Figure
    18-45](#figure18-45).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的系统输入的是数字 2 的图像，并且像往常一样，编码器为其找到潜在变量。在将这些潜在变量传递给解码器生成输出图像之前，让我们在每个潜在变量上添加一点随机性，并将这些修改后的值传递给解码器，如[图
    18-45](#figure18-45)所示。
- en: '![F18045](Images/F18045.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![F18045](Images/F18045.png)'
- en: 'Figure 18-45: One way to add randomness to the output of the encoder is to
    add a random value to each latent variable before passing them to the decoder.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-45：向编码器输出添加随机性的一个方法是，在将每个潜在变量传递给解码器之前，向其添加一个随机值。
- en: Because we’re assuming that all of the examples of the same style are clumped
    together, the output image we generate from the perturbed latent variables will
    be similar to (but different from) our input, and thus the error that measures
    the difference between the images will also be low. Then we can make lots of new
    2’s that are like the input 2, just by adding different small random numbers to
    the same set of latent values.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们假设相同风格的所有示例都被聚集在一起，所以我们从扰动后的潜在变量生成的输出图像将与输入图像相似（但有所不同），因此，衡量图像之间差异的误差也会较低。然后，我们只需通过向同一组潜在值添加不同的小随机数，就可以生成许多类似于输入
    2 的新 2。
- en: That’s how it works once the clumping has already been done. To get the clumping
    done in the first place, all we have to do is give the network a big error score
    during training when this perturbed output doesn’t come very close to matching
    the input. Because the system wants to minimize the error, it learns that latent
    values that are close to the input’s original latent values should produce images
    that are close to the input image. As a result, the latent values for similar
    inputs get clumped together, just as we desired.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦聚类完成，这就是它的工作原理。为了首先完成聚类，我们只需在训练过程中，当这个扰动后的输出与输入差距较大时，给予网络一个大的误差分数。因为系统希望最小化误差，它会学习到与输入原始潜在值接近的潜在值应该生成接近输入图像的图像。结果，相似输入的潜在值会聚集在一起，正如我们所期望的那样。
- en: But we took a shortcut just now that we can’t follow in practice. If we just
    add random numbers as in [Figure 18-45](#figure18-45), we won’t be able to use
    the backpropagation algorithm we saw in Chapter 14 to train the model. The problem
    comes about because backpropagation needs to compute the gradients flowing through
    the network, but the mathematics of an operation like [Figure 18-45](#figure18-45)
    don’t let us calculate the gradients the way we need to. And without backpropagation,
    our whole learning process disappears in a puff of smoke.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们刚才走了一条在实践中不能继续的捷径。如果我们像在[图 18-45](#figure18-45)中那样直接加上随机数，我们就无法使用第14章中看到的反向传播算法来训练模型。问题出在反向传播需要计算网络中流动的梯度，但像[图
    18-45](#figure18-45)这样的操作的数学方法无法让我们以需要的方式计算梯度。如果没有反向传播，我们整个学习过程就会像消失在一阵烟雾中一样。
- en: VAEs use a clever idea to get around this problem, replacing the process of
    adding random values with a similar idea that does about the same job, but which
    lets us compute the gradient. It’s a little bit of mathematical substitution that
    lets backpropagation work again. This is called the *reparameterization trick*.
    (As we’ve seen a few times, mathematicians sometimes use the word *trick* as a
    compliment when referring to a clever idea.)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE）采用了一种巧妙的想法来解决这个问题，它将添加随机值的过程替换为一个相似的概念，完成相似的工作，但让我们可以计算梯度。这是一种小小的数学替代方法，使得反向传播算法可以再次起作用。这被称为*重参数化技巧*。（正如我们之前看到的，数学家们有时会用*技巧*这个词来赞扬一个聪明的想法。）
- en: 'It’s worth knowing about this trick because it often comes up when we’re reading
    about VAEs (there are other mathematical tricks involved, but we won’t go into
    them). The trick is this: instead of just picking a random number from thin air
    for each latent variable and adding it in, as in [Figure 18-45](#figure18-45),
    we draw a random variable from a probability distribution. That value now becomes
    our latent variable (Doersch 2016). In other words, rather than start with a latent
    value and then add a random offset to it to create a new latent value, we use
    the latent value to control a random number generation process, and the result
    of that process becomes the new latent value.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这个技巧很有价值，因为在阅读变分自编码器（VAE）相关内容时，通常会遇到这个技巧（还有其他数学技巧涉及其中，但我们不会深入探讨）。这个技巧是这样的：我们不再像在[图
    18-45](#figure18-45)中那样直接为每个潜在变量从空中选取一个随机数并加进去，而是从概率分布中抽取一个随机变量。这个值现在成为我们的潜在变量（Doersch
    2016）。换句话说，我们不是从一个潜在值开始，再加上一个随机偏移来创建一个新的潜在值，而是利用潜在值来控制一个随机数生成过程，那个过程的结果就是新的潜在值。
- en: Recall from Chapter 2 that a probability distribution can give us random numbers,
    where some are more likely than others. In this case, we use a Gaussian distribution.
    This means that when we ask for a random value, we’re most likely to get a number
    near where the bump is high, and we’re less and less likely to get numbers that
    are farther away from the center of the bump.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾第二章，概率分布可以为我们提供随机数，其中一些数值比其他数值更可能出现。在这种情况下，我们使用高斯分布。这意味着，当我们请求一个随机值时，我们最有可能得到一个靠近“峰值”高的位置的数值，而我们越远离“峰值”的中心，得到这些数值的可能性就越小。
- en: 'Since each Gaussian requires a center (the mean) and a spread (the standard
    deviation), the encoder produces this pair of numbers for each latent variable.
    If our system has eight latent variables, then the encoder produces eight pairs
    of numbers: the center and spread for a Gaussian distribution for each one. Once
    we have them, then for each pair of values, we pick a random number from the distribution
    they define, and that’s the value of the latent variable that we then give to
    the decoder. In other words, we create a new value for each latent variable that’s
    pretty close to where it was, but has some randomness built in. The restructuring
    of the perturbation process lets us apply backpropagation to the network.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个高斯分布需要一个中心（均值）和一个扩展（标准差），编码器为每个潜在变量生成这对数值。如果我们的系统有八个潜在变量，那么编码器将生成八对数值：每个潜在变量对应一个高斯分布的中心和扩展。一旦得到了这些数值，对于每一对值，我们从它们定义的分布中选择一个随机数，这个值就是我们随后提供给解码器的潜在变量值。换句话说，我们为每个潜在变量创建一个新的值，这个值与原来的非常接近，但内含一定的随机性。扰动过程的重构使我们能够将反向传播应用于网络。
- en: '[Figure 18-46](#figure18-46) shows the idea.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 18-46](#figure18-46)展示了这个想法。'
- en: 'The structure of our autoencoder, as shown in [Figure 18-46](#figure18-46),
    requires the network to *split* after the computation of the latent value. Splitting
    is a new technique for our repertoire of deep learning architectures: it just
    takes a tensor and duplicates it, sending the two copies to two different subsequent
    layers. After the split, we use one layer to compute the center of the Gaussian
    and one to compute the spread. We sample this Gaussian and that gives us our new
    latent value.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的自动编码器结构，如[图 18-46](#figure18-46)所示，要求网络在计算潜在值后进行*分裂*。分裂是我们深度学习架构中新采用的技术：它仅仅是对一个张量进行复制，并将两个副本发送到两个不同的后续层。在分裂之后，我们使用一层来计算高斯分布的中心，另一层计算其扩展。我们从这个高斯分布中进行采样，得到我们的新潜在值。
- en: '![f18046](Images/f18046.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![f18046](Images/f18046.png)'
- en: 'Figure 18-46: We use the computed latent value to get the center and spread
    of a Gaussian bump. We pick a number from that bump, and that becomes our new
    latent value.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-46：我们使用计算得到的潜在值来获得高斯波峰的中心和扩展。我们从这个波峰中选择一个数字，这个数字成为我们的新潜在值。
- en: To apply our sampling idea of [Figure 18-46](#figure18-46), we create a Gaussian
    for each latent variable and sample it. Then we feed all of the new latent values
    into a *merge* or *combination* layer, which simply places all of its inputs one
    after the other to form a single list (in practice, we often combine the sampling
    and merging steps together into one layer). [Figure 18-47](#figure18-47) shows
    how we’d process a latent vector with three values.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用我们在[图 18-46](#figure18-46)中的采样思想，我们为每个潜在变量创建一个高斯分布并进行采样。然后，我们将所有的新潜在值输入到一个*合并*或*组合*层中，该层只是简单地将所有输入依次放在一起形成一个单一的列表（实际上，我们通常将采样和合并步骤合并成一个层）。[图
    18-47](#figure18-47)展示了如何处理一个具有三个值的潜在向量。
- en: '![f18047](Images/f18047.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![f18047](Images/f18047.png)'
- en: 'Figure 18-47: Picturing the split-and-combine sampling step of a VAE for three
    latent variables'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-47：展示了具有三个潜在变量的 VAE 的分裂与合并采样步骤
- en: In [Figure 18-47](#figure18-47), the encoder ends with three latent variables,
    and for each one, we compute a center and spread. Those three different Gaussian
    bumps are then randomly sampled, and those selected values are merged, or combined,
    to form the final latent variables computed for that input. These variables are
    the output of the encoder section.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 18-47](#figure18-47)中，编码器以三个潜在变量结束，对于每一个，我们都会计算一个中心和扩展。这三个不同的高斯波峰随后会被随机采样，所选的值会被合并或组合，形成为该输入计算出的最终潜在变量。这些变量是编码器部分的输出。
- en: During the learning process, the network learns what the centers and spreads
    should be for each Gaussian.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程中，网络会学习每个高斯分布的中心和扩展应该是什么。
- en: This operation is why we said earlier that each time we send a sample into a
    trained VAE (that is, after learning is done), we get back a slightly different
    result. The encoder is deterministic up to and including the split. But then the
    system picks a random value for each latent variable from its Gaussian, and those
    are different each time.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作就是我们之前所说的，每次将一个样本输入经过训练的 VAE（也就是说，学习完成后），我们得到的结果都会略有不同。编码器在分裂之前是确定性的。但之后，系统从每个潜在变量的高斯分布中随机选择一个值，而这些值每次都会不同。
- en: Exploring the VAE
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 VAE
- en: '[Figure 18-48](#figure18-48) shows the architecture of a fully connected VAE.
    It’s just like our deep autoencoder built from fully connected layers of [Figure
    18-17](#figure18-17), but with two changes (we chose fully connected layers rather
    than convolution layers for simplicity).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 18-48](#figure18-48)展示了一个完全连接的 VAE 的架构。它就像我们从完全连接的层构建的深度自动编码器（参见[图 18-17](#figure18-17)），但有两个变化（我们选择完全连接的层而不是卷积层，以简化模型）。'
- en: '![f18048](Images/f18048.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![f18048](Images/f18048.png)'
- en: 'Figure 18-48: The architecture of our VAE for MNIST data. There are 20 latent
    values.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-48：我们用于 MNIST 数据的 VAE 架构。共有 20 个潜在值。
- en: The first change is that we now we have the split-select-merge process at the
    end of the encoder. The second change is that we use our new loss, or error, function.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个变化是现在我们在编码器的末端采用了分裂-选择-合并过程。第二个变化是我们使用了新的损失函数，或误差函数。
- en: Another job we’ll assign to our new loss function is to measure the similarity
    between the fully connected layers of the encoding and decoding stages. After
    all, whatever the encoding stage is doing, we want the decoding stage to undo
    it.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会给新的损失函数分配的另一个任务是测量编码和解码阶段完全连接层之间的相似性。毕竟，无论编码阶段做了什么，我们希望解码阶段能将其还原。
- en: The perfect way to measure this is with the Kullback–Leibler (or KL) divergence
    that we saw in Chapter 6\. Recall that this measures the error we get from compressing
    information using an encoding that is different from the optimal one. In this
    case, we’re asserting that the optimal encoder is the opposite of the decoder,
    and vice versa. The big picture is that as the network tries to decrease the error,
    it is therefore decreasing the differences between the encoder and decoder, bringing
    them closer to mirroring each other (Altosaar 2020).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 测量这个的最佳方式是使用我们在第六章看到的 Kullback-Leibler（或 KL）散度。回想一下，这度量了使用不同于最优编码的编码方式压缩信息时的误差。在这种情况下，我们认为最优编码器是解码器的反向操作，反之亦然。总的来说，随着网络试图减小误差，它也在减少编码器和解码器之间的差异，使它们越来越接近彼此镜像（Altosaar
    2020）。
- en: Working with the MNIST Samples
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理 MNIST 样本
- en: Let’s see what comes out of this VAE for some of our MNIST samples. [Figure
    18-49](#figure18-49) shows the result.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个 VAE 对我们的一些 MNIST 样本会输出什么。[图 18-49](#figure18-49)展示了结果。
- en: '![F18049](Images/F18049.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![F18049](Images/F18049.png)'
- en: 'Figure 18-49: Predictions from our VAE of [Figure 18-48](#figure18-48). Top
    row: Input MNIST data. Bottom row: Output of the variational autoencoder.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18-49: 我们的 VAE 对 [图 18-48](#figure18-48) 的预测。顶行：输入的 MNIST 数据。底行：变分自编码器的输出。'
- en: It’s no surprise that these are pretty good matches. Our network is using a
    lot of compute power to make these images! But as we have seen from its architecture,
    the VAE produces different outputs each time it sees the same image. Let’s take
    the image of the two from this test set and run it through the VAE eight times.
    The results are in [Figure 18-50](#figure18-50).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这些匹配得相当不错并不奇怪。我们的网络使用了大量的计算资源来生成这些图像！但是正如我们从其架构中看到的，VAE 每次看到相同图像时都会产生不同的输出。让我们拿出这组测试集中的两个图像，并将其通过
    VAE 运行八次。结果见[图 18-50](#figure18-50)。
- en: '![F18050](Images/F18050.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![F18050](Images/F18050.png)'
- en: 'Figure 18-50: The VAE produces a different result each time it sees the same
    input. Top row: The input image. Middle row: The output from the VAE after processing
    the input eight times. Bottom row: The pixel by pixel differences between the
    input and each output. Increasing red means larger positive differences, increasing
    blue means larger negative differences.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18-50: VAE 每次看到相同输入时都会产生不同的结果。顶行：输入图像。中行：VAE 处理输入八次后的输出。底行：输入和每个输出之间的逐像素差异。红色越深表示正差异越大，蓝色越深表示负差异越大。'
- en: These eight results from the VAE are similar to each other, but we can see obvious
    differences.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这八个 VAE 结果彼此相似，但我们可以看到明显的差异。
- en: Let’s go back to our eight images from [Figure 18-49](#figure18-49) but add
    additional noise to the latent variables that come out of the encoder. That is,
    just before the decoder stage, we add some noise to the latent variables. This
    gives us a good test of how clumped-together the training images are in latent
    space.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到[图 18-49](#figure18-49)中的八个图像，但向从编码器输出的潜在变量中添加额外的噪声。也就是说，在解码器阶段之前，我们向潜在变量中添加了一些噪声。这为我们提供了一个很好的测试，来查看训练图像在潜在空间中是如何紧密聚集的。
- en: Let’s try adding a random value that’s up to 10 percent of each latent variable’s
    amount. [Figure 18-51](#figure18-51) shows the result of adding this moderate
    amount of noise to the latent variables.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试向每个潜在变量的值中添加最多10%的随机值。[图 18-51](#figure18-51)展示了在潜在变量中加入这种适量噪声后的结果。
- en: '![F18051](Images/F18051.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![F18051](Images/F18051.png)'
- en: 'Figure 18-51: Adding 10 percent noise to the latent variables coming out of
    the VAE encoder. Top row: Input images from MNIST. Bottom row: The decoder output
    after adding noise to the latent variables produced by the encoder.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18-51: 向从 VAE 编码器输出的潜在变量中添加 10% 的噪声。顶行：来自 MNIST 的输入图像。底行：向编码器输出的潜在变量添加噪声后的解码器输出。'
- en: Adding noise doesn’t seem to change the images much at all. That’s great, because
    it’s telling us that these noisy values are still “near” the original inputs.
    Let’s crank up the noise, adding in a random number as much as 30 percent of the
    latent variable’s value. [Figure 18-52](#figure18-52) shows the result.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 添加噪声似乎并不会显著改变图像。这很好，因为这告诉我们这些噪声值仍然“接近”原始输入。让我们增加噪声，加入一个随机值，最大为潜在变量值的30%。[图 18-52](#figure18-52)展示了结果。
- en: '![F18052](Images/F18052.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![F18052](Images/F18052.png)'
- en: 'Figure 18-52: Perturbing the latent variables by up to 30 percent. Top row:
    The MNIST input images. Bottom row: The results from the VAE decoder.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18-52: 对潜在变量进行最多 30% 的扰动。顶行：MNIST 输入图像。底行：来自 VAE 解码器的结果。'
- en: Even with a lot of noise, the images still look like digits. For example, the
    7 changes significantly, but it changes into a bent 7, not a random splotch.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有很多噪声，这些图像仍然看起来像数字。例如，7的变化很大，但它变成了一个弯曲的7，而不是一个随机的斑点。
- en: Let’s try blending the parameters for our digits and see how that looks. [Figure
    18-53](#figure18-53) shows the equal blends for the five pairs of digits we’ve
    seen before.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试混合我们数字的参数，看看效果如何。[图18-53](#figure18-53)显示了我们之前看到的五对数字的等比例混合结果。
- en: '![F18053](Images/F18053.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![F18053](Images/F18053.png)'
- en: 'Figure 18-53: Blending latent variables in the VAE. Top and middle row: MNIST
    input images. Bottom row: An equal blend of the latent variables for each image,
    decoded.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-53：VAE中的潜在变量混合。顶部和中间的行：MNIST输入图像。底部行：每个图像的潜在变量的等比例混合，解码后的结果。
- en: The interesting thing here is that these are all looking roughly like digits
    (the leftmost image is the worst in terms of being a digit, but it’s still a coherent
    shape). That’s because there’s less unoccupied territory in latent space, so the
    intermediate values are less likely to land in a zone far from other data (and
    thus produce a strange, nondigit image).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有趣的地方在于，这些图像大致看起来像数字（最左边的图像在作为数字的表现上最差，但它仍然是一个一致的形状）。这是因为潜在空间中没有那么多空白区域，所以中间值不太可能落入远离其他数据的区域（从而产生一个奇怪的、非数字的图像）。
- en: Let’s look at some linear blends. [Figure 18-54](#figure18-54) shows the intermediate
    steps for the three pairs of digits we’ve seen before.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些线性混合。[图18-54](#figure18-54)显示了我们之前见过的三对数字的中间步骤。
- en: '![F18054](Images/F18054.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![F18054](Images/F18054.png)'
- en: 'Figure 18-54: Linear interpolation of the latent variables in a VAE. The leftmost
    and rightmost image in each row are the output of the VAE for an MNIST sample.
    The images in between are decoded versions of the blended latent variables.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-54：VAE中潜在变量的线性插值。每行的最左边和最右边的图像是VAE对一个MNIST样本的输出。中间的图像是混合潜在变量的解码版本。
- en: The 5 is looking great, moving through a space of 5s from one version to another.
    The top and bottom rows have plenty of images that aren’t digits. They might be
    passing through empty zones in latent space, but as we mentioned before, it’s
    not clear that these are wrong in any sense. After all, what should an image partly
    between a four and a three look like?
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这个5看起来很不错，正从一个版本移动到另一个版本的5空间中。顶部和底部的行有很多不是数字的图像。它们可能正在通过潜在空间中的空白区域，但正如我们之前提到的，并不清楚这些在任何意义上是错误的。毕竟，一个介于四和三之间的图像应该是什么样子呢？
- en: Let’s run our tiger through the system just for fun. Remember, this is a completely
    unfair thing to do, and we shouldn’t expect anything meaningful to come out. [Figure
    18-55](#figure18-55) shows the result.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为了好玩将老虎图像输入系统。记住，这是一个完全不公平的操作，我们不应该期望得到任何有意义的结果。[图18-55](#figure18-55)显示了结果。
- en: '![F18055](Images/F18055.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![F18055](Images/F18055.png)'
- en: 'Figure 18-55: Running our low-resolution tiger through the VAE'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-55：将我们低分辨率的老虎图像输入VAE模型
- en: The VAE created something with a coherent structure, but it’s not much like
    a digit.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: VAE创建了一些具有一致结构的东西，但它与数字不像。
- en: Working with Two Latent Variables
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用两个潜在变量
- en: For comparison to our other autoencoders, we trained our VAE with just 2 latent
    variables (rather than the 20 we’ve been using), and plotted them in [Figure 18-56](#figure18-56).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与其他自编码器进行对比，我们使用了仅有两个潜在变量的VAE进行训练（而不是我们一直在使用的20个），并在[图18-56](#figure18-56)中绘制了它们。
- en: '![F18056](Images/F18056.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![F18056](Images/F18056.png)'
- en: 'Figure 18-56: The placement of latent variables for 10,000 MNIST images from
    our VAE trained with two latent variables'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图18-56：我们训练的VAE模型中，使用两个潜在变量对10,000个MNIST图像的潜在变量位置进行展示。
- en: This is a great result. The standard deviation of the Gaussian bump the latent
    variables are trying to stay within is represented here by a black circle, and
    it seems pretty well populated. The various digits are generally well clumped.
    There’s some confusion in the middle, but remember that this image uses just two
    latent variables. Curiously, the 2s seem to form two clusters. To see what’s going
    on, let’s make a grid of images that correspond to our two latent variables using
    the recipe shown in [Figure 18-22](#figure18-22), but using the latent variables
    of [Figure 18-56](#figure18-56). Let’s take the x and y values of each point on
    the grid and feed them to the decoder as though they were latent variables. Our
    range is −3 to 3 on each axis, like the circle in [Figure 18-56](#figure18-56).
    The result is [Figure 18-57](#figure18-57).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的结果。潜在变量试图保持的高斯峰值的标准差在这里由一个黑色圆圈表示，并且似乎分布得相当好。各种数字通常被很好地聚集在一起。中间有些混乱，但请记住，这张图像仅使用了两个潜在变量。有趣的是，2
    似乎形成了两个簇。为了了解发生了什么，让我们创建一个对应于我们两个潜在变量的图像网格，使用 [图 18-22](#figure18-22) 中显示的公式，但使用
    [图 18-56](#figure18-56) 中的潜在变量。让我们将网格上每个点的 x 和 y 值输入到解码器中，假设它们是潜在变量。我们的范围是每个轴从
    −3 到 3，就像 [图 18-56](#figure18-56) 中的圆圈一样。结果是 [图 18-57](#figure18-57)。
- en: '![F18057](Images/F18057.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![F18057](Images/F18057.png)'
- en: 'Figure 18-57: The output of the VAE treating the x and y coordinates as the
    two latent variables. Each axis runs from −3 to 3.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-57：VAE 输出，将 x 和 y 坐标作为两个潜在变量处理。每个轴的范围从 −3 到 3。
- en: The 2s without a loop are grouped together near the lower middle, and the 2s
    with a loop are grouped in the middle left. The system decided that these were
    so different that they didn’t need to be near each other.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 没有环的 2 紧密地聚集在中下部，而带环的 2 则聚集在中左部。系统认为这些数字差异如此之大，以至于它们不需要靠近彼此。
- en: Looking over this figure, we can see how nicely the digits have been clumped
    together. This is a far more organized and uniform structure than we saw in [Figure
    18-23](#figure18-23). In a few places the images get fuzzy, but even with just
    two latent variables, most of the images are digit-like.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览这张图像，我们可以看到数字是如何紧密聚集在一起的。这比我们在 [图 18-23](#figure18-23) 中看到的结构更有组织，也更加均匀。在一些地方，图像有些模糊，但即使只有两个潜在变量，大部分图像看起来仍像数字。
- en: Producing New Input
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成新的输入
- en: In [Figure 18-58](#figure18-58) we’ve isolated the decoder part of the VAE to
    use as a generator. For the moment, we’ll continue to use a version where we’ve
    reduced the 20 latent values to just 2\.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 18-58](#figure18-58) 中，我们已经将 VAE 的解码器部分隔离出来，作为生成器使用。暂时，我们将继续使用一个版本，其中将
    20 个潜在值减少到仅 2 个。
- en: '![F18058](Images/F18058.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![F18058](Images/F18058.png)'
- en: 'Figure 18-58: The decoder stage of our VAE'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-58：我们的 VAE 解码器阶段
- en: Since we have only two latent variables, we randomly picked 80 random (x,y)
    pairs from a circle centered at (0,0) with radius 3, fed them into the decoder,
    and gathered the resulting images together into [Figure 18-59](#figure18-59).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只有两个潜在变量，我们从以 (0,0) 为中心，半径为 3 的圆内随机选择了 80 对随机 (x, y) 值，输入到解码器中，并将生成的图像汇集在一起，形成
    [图 18-59](#figure18-59)。
- en: '![F18059](Images/F18059.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![F18059](Images/F18059.png)'
- en: 'Figure 18-59: Images produced by the VAE decoder when presented with two random
    latent variables'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-59：VAE 解码器在输入两个随机潜在变量时生成的图像
- en: These are looking pretty great for the most part. Some aren’t quite legible,
    but overall, most of the images are recognizable digits. Many of the mushiest
    shapes seem to have come from the boundary between the 8s and the 1s, leading
    to narrow and thin 8s.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分图像看起来相当不错。有些图像不太清晰，但总体上，大多数图像都是可辨识的数字。许多模糊的形状似乎来自于 8 和 1 之间的边界，导致了狭窄且纤细的
    8。
- en: Most of these digits are fuzzy, because we’re using only two latent variables.
    Let’s sharpen things up by training and then using a deeper VAE with more latent
    variables. [Figure 18-60](#figure18-60) shows our new VAE. This architecture is
    based on the MLP autoencoder that’s part of the *Caffe* machine-learning library
    (Jia and Shelhamer 2020; Donahue 2015). (Recall that MLP stands for multilayer
    perceptron, or a network built only out of fully connected layers.)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字大多是模糊的，因为我们仅使用了两个潜在变量。让我们通过训练并使用一个具有更多潜在变量的更深层 VAE 来使结果更加清晰。[图 18-60](#figure18-60)
    展示了我们的新 VAE。这个架构基于 *Caffe* 机器学习库中的 MLP 自编码器（Jia 和 Shelhamer 2020；Donahue 2015）。(回忆一下，MLP
    代表多层感知器，即一个完全由全连接层构成的网络。)
- en: '![f18060](Images/f18060.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![f18060](Images/f18060.png)'
- en: 'Figure 18-60: The architecture of a deeper VAE'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18-60：更深层 VAE 的架构
- en: We trained this system with 50 latent variables for 25 epochs and then generated
    another grid of random images. As before, we used just the decoder stage, shown
    in [Figure 18-61](#figure18-61).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '![F18061](Images/F18061.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18-61: We generate new output using just the decoder stage of our deeper
    VAE, feeding in 50 random numbers to produce images.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: The results are in [Figure 18-62](#figure18-62).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '![F18062](Images/F18062.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18-62: Images produced by our bigger VAE when provided with random latent
    variables'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: These images have significantly crisper edges than the images in [Figure 18-59](#figure18-59).
    For the most part, we’ve generated entirely recognizable and plausible digits
    from purely random latent variables, though, as usual, some weird images that
    aren’t much like digits show up. These are probably coming from the empty zones
    between digits, or zones where different digits are near one another, causing
    oddball blends of the shapes.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Once our VAE has been trained, if we want to make more digit-like data, we can
    ignore the encoder and save the decoder. This is now a generator that we can use
    to create as many new digit-like images as we like. If we were to train the VAE
    on images of tractors, songbirds, or rivers, we could generate more of those types
    of images, too.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter we saw how autoencoders learn to represent a set of inputs with
    latent variables. Usually there are fewer of these latent variables than there
    are values in the input, so we say that the autoencoder compresses the input by
    forcing it through a bottleneck. Because some information is lost along the way,
    it’s a form of lossy compression.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: By feeding latent values of our own choice directly into the second half of
    a trained autoencoder, we can view that set of layers as a generator, capable
    of producing new output data that is like the input, but wholly novel.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders may be built using many kinds of layers. In this chapter, we saw
    examples of networks built from fully connected layers and convolution layers.
    We looked at the structure of 2D latent variables generated by a trained autoencoder
    built of fully connected layers, and found that it had a surprising amount of
    organization and structure. Picking new pairs of latents from a populated region
    of these latents and handing them to a generator usually produced an output that
    was blurry (because we had only two latent values), but plausibly like the input.
    We then looked at convolutional autoencoders, built primarily (or exclusively)
    with convolution layers.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: We saw that we could blend latent variables, in essence creating a series of
    in-between latent variables between the endpoints. The more latent variables we
    used in our bottleneck, the better these interpolated outputs appeared. We then
    saw that an autoencoder can be trained to denoise the input, simply by telling
    it to generate the clean value of a noisy input. Finally, we looked at variational
    autoencoders, which do a better job of clumping similar inputs and filling up
    a region of the latent space, at the cost of introducing some randomization into
    the process.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现可以混合潜在变量，本质上是在端点之间创建一系列中间潜在变量。我们在瓶颈中使用的潜在变量越多，这些插值输出看起来就越好。接着，我们发现可以训练自编码器去噪输入，只需告诉它生成噪声输入的干净值。最后，我们研究了变分自编码器，它能够更好地聚集相似的输入，并填充潜在空间的区域，但代价是引入了某种随机化过程。
- en: Autoencoders are often used for denoising and simplifying datasets, but people
    have found creative ways to use them for many kinds of tasks, such as creating
    music and modifying input data to help networks learn better and faster (Raffel
    2019).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器常用于去噪和简化数据集，但人们已经找到了创造性的方法，将其用于许多不同的任务，例如创作音乐和修改输入数据，以帮助网络更好、更快地学习（Raffel
    2019）。
