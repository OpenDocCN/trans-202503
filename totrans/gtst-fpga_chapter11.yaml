- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">11</samp> <samp class="SANS_Dogma_OT_Bold_B_11">GETTING
    DATA IN AND OUT WITH I/O AND SERDES</samp>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../images/opener-img.png)'
  prefs: []
  type: TYPE_IMG
- en: Throughout this book, we’ve focused on the internals of FPGAs, and that’s typical
    of the FPGA design process. FPGA design largely centers around writing Verilog
    or VHDL code targeting internal components like flip-flops, LUTs, block RAMs,
    and DSP blocks. But what’s going on at the edge of the device, where data enters
    and exits the FPGA?
  prefs: []
  type: TYPE_NORMAL
- en: There’s a surprising amount of complexity involved in getting data into and
    out of an FPGA. In my experience, this is where most of the trickier FPGA design
    problems occur. Understanding how the *input/output (I/O)* works will help you
    tackle those problems. You’ll be able to spend less time worrying about external
    interfaces, and more time solving the internal task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Working with I/O is where the boundary between being a “software person” and
    a “hardware person” lies. You need to understand the details of the electrical
    signals that you’re interfacing to in order to configure the FPGA pins correctly.
    What voltage do they operate at? Are the signals single-ended or differential?
    (And what does that even mean?) How can you use double data rate or a serializer/deserializer
    to send data at very high speeds? This chapter answers these questions and more.
    Even if you don’t have an electrical engineering background, you’ll learn the
    fundamentals of interfacing FPGAs to the outside world.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">Working with GPIO Pins</samp>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most pins on the FPGA are *general purpose input/output (GPIO)* pins, meaning
    they can function as a digital input or output. We’ve used these pins in the book’s
    projects to take in signals from push buttons and output signals to light up LEDs,
    but we haven’t worried about the details of how this actually works. In this section,
    we’ll look at how GPIO pins interface with an FPGA and how they can be made to
    input data, output data, or both.
  prefs: []
  type: TYPE_NORMAL
- en: When I was first getting into FPGA design, I had no idea of the nuances involved
    in pin configuration. There are many knobs to turn and settings to play with.
    Having a thorough understanding of your GPIO pins is important, especially for
    high-speed designs, because maintaining signal integrity and performance throughout
    your design starts at the pins.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">I/O Buffers</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPIO pins interface with an FPGA through *buffers*, electronic circuit elements
    that isolate their input from their output. These buffers are what allow you to
    configure some pins as inputs and others as outputs. As you’ll see soon, they
    even allow you to toggle a pin between input and output while the FPGA is running.
    [Figure 11-1](#fig11-1) shows a simplified block diagram of a GPIO pin interface
    on an Intel FPGA, to illustrate how a buffer serves as an intermediary between
    the pin and the internal FPGA logic.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/Figure11-1.png)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 11-1: A simplified GPIO
    block diagram</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The box on the right-hand side of the image (with the X inside it) represents
    the physical pin. Immediately to the left of the pin is a block labeled Buffer,
    which represents the I/O buffer. It contains two main components, represented
    by triangles. The triangle pointing to the right is the *output* *buffer*; it
    pushes data out to the pin. The triangle pointing to the left is the *input buffer*;
    it sends data from the pin into the FPGA.
  prefs: []
  type: TYPE_NORMAL
- en: On the far left of the diagram is a block labeled GPIO, representing the internal
    FPGA logic that interacts directly with the pin via the buffers. The main path
    to notice here is OE, which stands for *output enable*. This turns the output
    buffer on or off to control whether the pin will function as an output or an input.
    When OE is high, the output buffer will drive the pin with whatever data is present
    on the output path. If data on the output path is low, the pin will be low, and
    if data on the output path is high, the pin will be high. When OE is low, the
    pin is configured as an input, so the output buffer stops passing its input to
    its output. At this point the buffer’s output becomes *high impedance* (also called
    *hi-Z* or *tri-state*), meaning it will accept very little current. A high-impedance
    output buffer no longer affects anything happening on the pin. Instead, the pin’s
    state is governed by whatever external signal is coming in. The input buffer is
    free to read that signal and pass it along to the input path for use inside the
    FPGA.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11-1](#tab11-1) shows a truth table for an output buffer, summarizing
    this behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Heavy_B_11">Table 11-1:</samp> <samp class="SANS_Futura_Std_Book_11">Truth
    Table for an Output Buffer</samp>
  prefs: []
  type: TYPE_NORMAL
- en: '| <samp class="SANS_Futura_Std_Heavy_B_11">Input</samp> | <samp class="SANS_Futura_Std_Heavy_B_11">OE</samp>
    | <samp class="SANS_Futura_Std_Heavy_B_11">Output</samp> |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| <samp class="SANS_Futura_Std_Book_11">0</samp> | <samp class="SANS_Futura_Std_Book_11">0</samp>
    | <samp class="SANS_Futura_Std_Book_11">Z</samp> |'
  prefs: []
  type: TYPE_TB
- en: '| <samp class="SANS_Futura_Std_Book_11">1</samp> | <samp class="SANS_Futura_Std_Book_11">0</samp>
    | <samp class="SANS_Futura_Std_Book_11">Z</samp> |'
  prefs: []
  type: TYPE_TB
- en: '| <samp class="SANS_Futura_Std_Book_11">0</samp> | <samp class="SANS_Futura_Std_Book_11">1</samp>
    | <samp class="SANS_Futura_Std_Book_11">0</samp> |'
  prefs: []
  type: TYPE_TB
- en: '| <samp class="SANS_Futura_Std_Book_11">1</samp> | <samp class="SANS_Futura_Std_Book_11">1</samp>
    | <samp class="SANS_Futura_Std_Book_11">1</samp> |'
  prefs: []
  type: TYPE_TB
- en: Looking at this table, we can see that when OE is high, the value on the buffer’s
    input is simply passed to its output. However, when OE is low, the buffer’s output
    is high impedance (conventionally represented by a *Z*), regardless of the value
    on the input.
  prefs: []
  type: TYPE_NORMAL
- en: In the projects in this book, we’ve defined the input and output signals at
    the top level of the design code. Inputs are represented with the keyword <samp
    class="SANS_TheSansMonoCd_W5Regular_11">input</samp> (Verilog) or <samp class="SANS_TheSansMonoCd_W5Regular_11">in</samp>
    (VHDL), while outputs are indicated by the keyword <samp class="SANS_TheSansMonoCd_W5Regular_11">output</samp>
    (Verilog) or <samp class="SANS_TheSansMonoCd_W5Regular_11">out</samp> (VHDL).
    When building the FPGA, the synthesis tools see which signals are defined for
    each direction and set up the buffers accordingly. If the signal is an input,
    OE will be set low. If the signal in an output, OE will be set high. Then, during
    the place and route process, the physical constraints file maps the signals to
    the specific pins on the FPGA. This is how GPIO pins get configured as dedicated
    input or output pins for your design.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Bidirectional Data for Half-Duplex
    Communication</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While most pins in a design are typically fixed as either input or output, a
    GPIO pin can be configured to be *bidirectional*, meaning it can switch between
    functioning as input and output within the same design. When the FPGA needs to
    output data through the bidirectional pin, it drives the OE signal high, then
    puts the data to transmit onto the output path. When the FPGA needs to receive
    data as input through the bidirectional pin, it drives OE low. This puts the output
    buffer into tri-state (high impedance), enabling the FPGA to listen to the data
    on the pin and pass it to the input path. When a pin is configured to be bidirectional
    like this, it’s acting as a *transceiver*, as opposed to just a transmitter or
    just a receiver.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional pins are useful for *half-duplex* communication, where two devices
    exchange data using a single, shared transmission line (one pin). Either device
    can serve as a transmitter, but only one device can transmit at a time, while
    the other receives. This is in contrast to *full-duplex* communication, where
    both devices can transmit and receive data at the same time. Full-duplex communication
    requires two transmission lines (two pins), one for sending data from device 1
    to device 2 and the other for sending data from device 2 to device 1, as opposed
    to the single transmission line of half-duplex communication.
  prefs: []
  type: TYPE_NORMAL
- en: A common example of half-duplex communication is a two-way radio. The speaker
    is only able to transmit when they hold down the button on the radio. When the
    speaker is transmitting, the listener is unable to transmit, so the speaker and
    listener must agree whose turn it is to talk. This is why people always say “Over”
    in the movies when they’re talking on walkie-talkies; it’s a signal that the speaker
    is done talking and the listener is now free to respond.
  prefs: []
  type: TYPE_NORMAL
- en: With a physical wire, if the two sides don’t take turns sharing the communication
    channel, then there can be a data collision. This collision can corrupt the data,
    so nobody receives anything. To avoid this the devices must agree on a *protocol*,
    a set of rules governing communication. The protocol determines how a device can
    initiate a transaction, establishes well-defined locations in time for other devices
    on the line to talk back (the equivalent of saying “Over”), and so on. Some protocols
    are even able to handle data collisions by detecting when data is corrupted and
    resending the corrupted data, though this requires additional complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Half-duplex communication is usually more complicated than using dedicated transmit
    and receive channels, but it’s still quite common. *I2C* (or I²C, pronounced “eye-squared-see”
    or “eye-two-see” and short for *inter- integrated circuit*), for example, is a
    widely used half-duplex protocol. Countless unique integrated circuits—including
    ADCs, DACs, accelerometers, gyroscopes, temperature sensors, microcontrollers,
    and many others—use I2C to communicate, since it’s relatively simple to implement
    and, thanks to its half-duplex nature, requires a very low pin count. Only two
    pins, clock and data, are used in I2C, which is why you may also see it referred
    to as *TWI (two-wire interface)*.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">A Bidirectional Pin Implementation</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s look at how to code a bidirectional pin using Verilog or VHDL. As you
    examine this code, refer to [Figure 11-2](#fig11-2) to see how the signals in
    the code match the block diagram from [Figure 11-1](#fig11-1):'
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Oblique_BI_11">Verilog</samp>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <samp class="SANS_Futura_Std_Bold_Oblique_BI_11">VHDL</samp>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We declare the bidirectional pin (<samp class="SANS_TheSansMonoCd_W5Regular_11">io_Data</samp>)
    with the keyword <samp class="SANS_TheSansMonoCd_W5Regular_11">inout</samp> in
    both Verilog and VHDL ❶. At this point we can imagine that we’re at the pin, as
    indicated by the label <samp class="SANS_TheSansMonoCd_W5Regular_11">io_Data</samp>
    in [Figure 11-2](#fig11-2). We’ll need to map this signal to one of the FPGA’s
    pins in our physical constraints file. For the input functionality, we simply
    use an assignment to drive <samp class="SANS_TheSansMonoCd_W5Regular_11">w_RX_Data</samp>
    with the data from the pin ❷. On the output side, we selectively enable the output
    buffer using the signal <samp class="SANS_TheSansMonoCd_W5Regular_11">w_TX_En</samp>
    ❸. We use the ternary operator in Verilog or a conditional assignment in VHDL.
    The data driven onto <samp class="SANS_TheSansMonoCd_W5Regular_11">io_Data</samp>
    will either be <samp class="SANS_TheSansMonoCd_W5Regular_11">w_TX_Data</samp>
    or high impedance (indicated by <samp class="SANS_TheSansMonoCd_W5Regular_11">1'bZ</samp>
    in Verilog or <samp class="SANS_TheSansMonoCd_W5Regular_11">'Z'</samp> in VHDL),
    depending on the state of the output enable signal (<samp class="SANS_TheSansMonoCd_W5Regular_11">w_TX_En</samp>).
    This code pattern is very common for bidirectional data. Synthesis tools are smart
    enough to recognize it and infer an I/O buffer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/Figure11-2.png)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 11-2: A labeled bidirectional
    interface</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: One thing you might notice is that any data driven out on <samp class="SANS_TheSansMonoCd_W5Regular_11">w_TX_Data</samp>
    will be received on <samp class="SANS_TheSansMonoCd_W5Regular_11">w_RX_Data</samp>,
    since they’re connected together through <samp class="SANS_TheSansMonoCd_W5Regular_11">io_Data</samp>.
    You’ll need to address this elsewhere in the code by telling your receiver to
    ignore any data on <samp class="SANS_TheSansMonoCd_W5Regular_11">io_Data</samp>
    when <samp class="SANS_TheSansMonoCd_W5Regular_11">w_TX_En</samp> is high. Otherwise,
    your FPGA will be hearing itself talk.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Electrical Characteristics</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are many different electrical characteristics that you can specify for
    each individual GPIO pin. We’re going to talk about three: operating voltage,
    drive strength, and slew rate. We’ll also look at the electrical differences between
    single-ended and differential data transmission.'
  prefs: []
  type: TYPE_NORMAL
- en: As you read, keep in mind that these aren’t the only pin settings you can control.
    For example, you also may be able to set pins to be open drain, include a pull-up
    or pull-down resistor or a termination resistor, and much more. The I/O of your
    FPGA can be configured in many, many ways, depending on which GPIO properties
    are built into the device itself. If you need to implement anything other than
    simple signal interfaces, it’s worth exploring the relevant datasheets to ensure
    you’re working correctly with your I/O buffers. All of the specific information
    about your FPGA’s I/O can usually be found in the I/O user guide, which is a great
    reference for details on what types of electronics your FPGA is capable of interfacing
    to.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Operating Voltage</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *operating voltage* specifies what voltage the pin will be driven to for
    a logic 1 output and sets the expected voltage for a logic 1 input. Most commonly,
    FPGA pins use 0 V to represent a 0 and 3.3 V to represent a 1\. This standard
    is called *LVCMOS33* (LVCMOS is short for *low-voltage complementary metal oxide
    semiconductor*). Another standard you might come across is 0 V to represent a
    0 and 5 V to represent a 1\. This is called *TTL*, short for *transistor–transistor
    logic*. TTL is less common in FPGAs these days, since many don’t allow voltages
    as high as 5 V internally. There’s also the LVCMOS25 standard, which uses 0 V
    to represent a 0 and 2.5 V to represent a 1.
  prefs: []
  type: TYPE_NORMAL
- en: LVCMOS33, LVCMOS25, and TTL are all examples of *single-ended* I/O standards,
    meaning the signals involved are referenced to ground. As you’ll see soon, there
    are also *differential* standards, where the signals aren’t referenced to ground.
    There are many more single-ended standards than the three I’ve mentioned. A typical
    FPGA supports about a dozen single-ended I/O standards.
  prefs: []
  type: TYPE_NORMAL
- en: One important note about setting your operating voltage is that all signals
    on a single bank need to be at the same operating voltage. A *bank* is a group
    of pins that all operate with a common reference voltage, usually called *VCCIO*.
    You might have eight banks on your FPGA, and each bank can use a unique operating
    voltage. For example, you might configure bank 1 to use 1.8 V, bank 2 to use 3.3
    V, and bank 3 to use 2.5 V. What’s critical is that all the pins within a single
    bank are operating at the same voltage. You can’t put an LVCMOS33 pin on the same
    bank as an LVCMOS25 pin, because the former requires a VCCIO of 3.3 V while the
    latter requires a VCCIO of 2.5 V. When doing your schematic review, always check
    to make sure that the signals on each bank share the same reference voltage. If
    you try to mix voltages in the same bank, the place and route tool will likely
    generate an error, or at least a very strong warning.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Drive Strength</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *drive strength* of a pin determines how much current (in milliamps, or
    mA) can be driven into or out of the pin. For example, a pin set to an 8 mA drive
    strength will be capable of sinking or sourcing up to 8 mA of current. The drive
    strength can be changed on an individual pin basis and should be high enough to
    match the needs of the circuit you’re interfacing to. Most often, the drive strength
    settings can be left at the default for all of the pins on your FPGA. Unless you
    have some high current needs, it’s unlikely you’ll need to modify the default
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Slew Rate</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *slew rate* sets the rate of change allowed for an output signal. It’s usually
    specified in qualitative terms, such as *fast*, *medium*, or *slow*. The slew
    rate setting affects how quickly a pin can change from a 0 to a 1 or from a 1
    to a 0\. Like drive strength, the slew rate can often be left at the default setting
    for each of your pins. The exception is if you’re interfacing to some component
    that requires very fast data rates, in which case you might want to select the
    fastest option. However, selecting a faster slew rate can increase system noise,
    so it’s not recommended to slew faster unless you really need it.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Differential Signaling</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Differential signaling* is a method of sending data where you have two signals
    that aren’t referenced to ground, but rather to each other. As I hinted earlier,
    this is in contrast to *single-ended signaling*, where you have one data signal
    referenced to ground. [Figure 11-3](#fig11-3) illustrates the difference.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/Figure11-3.png)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 11-3: Single-ended vs.
    differential interfaces</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: 'The top half of the figure shows a single-ended configuration: we have device
    1 transmitting data to device 2 on a single wire, and another wire for the ground
    path. There’s no data on this ground wire, but it’s needed to maintain a consistent
    ground reference between the devices. Data is sent as a voltage on the data wire:
    0 V for a 0 or some other value (such as 3.3 V) for a 1, depending on the operating
    voltage. If we wanted to add another data path, we could just add another single
    wire between the two devices; the ground reference can work for multiple data
    paths.'
  prefs: []
  type: TYPE_NORMAL
- en: The bottom half of the image shows a differential configuration. Here, we don’t
    have a ground reference passed between the devices. Instead, we have a pair of
    data lines. Notice the bubble at the start of the upper line, on the output of
    device 1’s transmit buffer. This looks like the bubble we saw when looking at
    NOT gates and NAND gates back in [Chapter 3](chapter3.xhtml), and it’s a common
    indication that we have a differential pair. If the difference between the + and
    – terminals on the receiver is a positive voltage above some threshold, then the
    signal is decoded as a 1; if the difference is a negative voltage below some threshold,
    then the signal is decoded as a 0\. The details depend on the differential standard.
    For example, TIA/EIA-644, more commonly called *LVDS (low-voltage differential
    signaling)*, specifies that there should be a difference of about +/– 350 millivolts
    (mV) between the two wires. This voltage is quite a bit lower than most single-ended
    signals use, meaning the system can operate at less power, which is one advantage
    of differential communication over single-ended communication. A typical FPGA
    supports about the same number of differential standards as single-ended standards
    (a dozen or so).
  prefs: []
  type: TYPE_NORMAL
- en: One disadvantage you might have picked up on is that differential communication
    requires twice as many wires for every data path. In the case of single-ended
    data transfer, there’s just one wire for each data path we want to create. If
    we want 10 data paths, we need 10 wires (and usually at least 1 ground wire).
    To create the same 10 data paths with differential signaling, we’d need 20 wires
    (but no ground wires). This extra wiring costs money and will require a larger
    connector. Still, differential signals have some unique properties that may make
    this trade-off worthwhile in certain applications.
  prefs: []
  type: TYPE_NORMAL
- en: One important advantage is that differential signals are much more immune to
    *noise*, or *electromagnetic interference (EMI)*, than single-ended signals. EMI
    is a phenomenon caused by changing electrical and magnetic fields—for example,
    from a nearby microwave oven, cell phone, or power line—that can cause disturbances
    in other systems. You can think of a wire that carries data as a small antenna
    that receives all sorts of unwanted electrical signals, creating noise that shows
    up as a voltage blip on the wire. A large enough voltage blip on a single-ended
    signal could corrupt the data, causing a 0 to turn into a 1, or a 1 into a 0\.
    With a differential signal, however, the voltage blip will affect both wires equally,
    meaning the voltage difference between the wires will remain constant. Since it’s
    the voltage difference, and not the exact value of the voltage itself, that matters,
    the noise is effectively canceled out.
  prefs: []
  type: TYPE_NORMAL
- en: An additional benefit of differential communication is that the transmitter
    and the receiver can be referenced to different ground voltages and still send
    and receive data reliably. It might seem strange, but ground isn’t always exactly
    0 V. The ground in a system can be affected by noise, just as data lines can be,
    so problems can arise when you rely on ground as a source of truth throughout
    your system. In particular, it’s difficult to maintain a common ground reference
    for two devices that are far apart, which is why differential signals are often
    used to send data over long distances. For example, RS-485, a differential electrical
    standard, can send data at 10 megabits per second (Mb/s) over a distance of nearly
    1 mile, which would be impossible with a single-ended signal. Even at closer distances,
    there are situations where one system might not be referenced to ground at all.
    Instead, it might be *floating* or *isolated* from a ground reference. To communicate
    with an isolated system, you need a method of communication that doesn’t rely
    on a shared ground reference; differential communication is one such method.
  prefs: []
  type: TYPE_NORMAL
- en: Differential signals are also able to send data at faster rates than single-ended
    signals. When a transmitter needs to change from a 0 to a 1, it must drive the
    line all the way from the voltage corresponding to a 0 to the voltage corresponding
    to a 1, and that process takes some amount of time (the slew rate). The bigger
    the difference between the voltages, the more current must be driven onto the
    line, and the longer the process will take. Since single-ended protocols typically
    require wider voltage swings between a 0 and a 1, they’re inherently slower than
    differential protocols. For example, the LVCMOS33 voltage swing of 3.3 V is much
    greater than the LVDS voltage swing of +/– 350 mV. For this reason, almost all
    high-speed applications use differential signals. We’ll get into more detail about
    this later in the chapter when we discuss SerDes, but interfaces like USB, SATA,
    and Ethernet all use differential signals for the highest possible data rates.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">How to Modify Pin Settings</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you want to specify the operating voltage, drive strength, or slew rate
    values for your pins, or control which pins are for single-ended signals and which
    are for differential signals, the place to do it is your physical constraints
    file. Recall that this file lists how the pins on your FPGA connect to the signals
    in your design. In addition to specifying the pin mapping, you can also add these
    other parameters to further define the I/O behavior. Here’s an excerpt from a
    Lattice constraint file that includes some additional parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first line maps the signal <samp class="SANS_TheSansMonoCd_W5Regular_11">o_Data</samp>
    to the pin A13\. The second line sets the operating voltage to <samp class="SANS_TheSansMonoCd_W5Regular_11">LVCMOS33</samp>,
    the drive strength to <samp class="SANS_TheSansMonoCd_W5Regular_11">8</samp>,
    and the slew rate to <samp class="SANS_TheSansMonoCd_W5Regular_11">FAST</samp>.
    You should refer to the constraints user guide for your particular FPGA to see
    how to set these parameters; the syntax isn’t universal across devices. You can
    also use the GUI in your IDE to set these parameters without having to learn the
    exact syntax required.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Faster Data Transmission
    with Double Data Rate</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sending data quickly is where FPGAs can really shine, and one way to speed up
    transmission is to use *double data rate (DDR)*. Up until this point, I’ve stated
    that the signals in your FPGA should be synchronized to the rising edges of the
    clock. With double data rate, however, signals change on the rising *and* falling
    edges of the clock. This enables you to send twice the amount of data with the
    same clock frequency, as shown in [Figure 11-4](#fig11-4).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/Figure11-4.png)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 11-4: Single vs. double
    data rate</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, with single data rate, where data is sent on each rising clock
    edge, you’re able to move three bits of data (D0 through D2) during three clock
    cycles. In comparison, with double data rate, where data is sent on both rising
    and falling edges, you can send six bits of data (D0 through D5) during the same
    three clock cycles. This technique is known for its use in LPDDR memory, short
    for *low-power double data rate*, a type of RAM commonly found in computers, smartphones,
    and other electronics. Changing the data on both edges of the clock increases
    the bandwidth of the memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to create double data rate output (ODDR) buffers anywhere you want
    to use DDR for data transfer. The details vary between FPGA manufacturers, but
    I generally recommend creating these ODDR buffers directly within your Verilog
    or VHDL, using instantiation, since they aren’t terribly complicated to configure.
    As an example, let’s take a look at an instantiation template for an ODDR buffer
    from AMD’s Virtex-7 Library User Guide:'
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Oblique_BI_11">Verilog</samp>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: <samp class="SANS_Futura_Std_Bold_Oblique_BI_11">VHDL</samp>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: It’s not critical to understand every line here. The most important connection
    is the output pin itself ❶ ; this is where the <samp class="SANS_TheSansMonoCd_W5Regular_11">ODDR</samp>
    block is connected to the pin. The two data inputs, <samp class="SANS_TheSansMonoCd_W5Regular_11">D1</samp>
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">D2</samp>, will be used in an
    alternating pattern to drive the data to the output pin. <samp class="SANS_TheSansMonoCd_W5Regular_11">D1</samp>
    is driven on rising (or positive) edges and <samp class="SANS_TheSansMonoCd_W5Regular_11">D2</samp>
    on falling (or negative) edges.
  prefs: []
  type: TYPE_NORMAL
- en: Double data rate allows you to speed up data transmission, but if you really
    want to get your data flying, some FPGAs have a specialized type of interface
    called SerDes that allows for even speedier input and output. We’ll examine this
    exciting FPGA feature next.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">SerDes</samp>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *SerDes*, short for *serializer/deserializer*, is a primitive of some (but
    not all) FPGAs responsible for inputting or outputting data at very high speeds,
    into the gigabits per second (Gb/s). At a high level, it works by taking a parallel
    data stream and converting it into a serial data stream for high-speed transmission.
    On the receiving end, the serial data is converted back to parallel. This is how
    the FPGA can exchange data with other devices at very fast data rates. It may
    sound counterintuitive that sending data serially, one bit at a time, is faster
    than sending data in parallel, several bits at a time, but that’s the magic of
    SerDes. We’ll discuss why this works soon.
  prefs: []
  type: TYPE_NORMAL
- en: SerDes primitives are sometimes called *SerDes transceivers*, which reflects
    that they can send and receive data. That said, SerDes transceivers are almost
    always full-duplex, meaning they don’t have to switch back and forth between being
    a transmitter and a receiver like we saw previously with bidirectional communication.
    You usually set up one data path as a transmitter out of your FPGA, and another
    as a receiver into your FPGA.
  prefs: []
  type: TYPE_NORMAL
- en: SerDes transceivers help FPGAs excel at sending or receiving very large amounts
    of data at a rate that wouldn’t be possible with other devices. This is a killer
    feature that makes FPGAs attractive for use cases such as receiving data from
    a video camera. A high-resolution camera might have a pixel space of 1,920×1,080,
    with 32 bits of data per pixel, and new images captured at a rate of 60 Hz. If
    we multiply those numbers, that translates to 3.9Gb of uncompressed raw data per
    second—quite a lot!—and some cameras can go even higher, up to 4K and 120 Hz.
    SerDes transceivers allow an FPGA to receive an absolute firehose of data and
    unpack it in such a way that the FPGA can process it correctly. Another common
    use case is networking, where you have Ethernet packets flying around at hundreds
    of gigabits per second. You might have multiple SerDes transceivers working together
    on a single device to route packets correctly, again at very fast data rates that
    wouldn’t be possible to achieve on a standard I/O pin.
  prefs: []
  type: TYPE_NORMAL
- en: At its heart, SerDes revolves around converting between parallel and serial
    data. To understand why this conversion is necessary, we need to take a closer
    look at the differences between serial and parallel data transfer.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Parallel vs. Serial
    Communication</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Parallel communication* means we’re using multiple communication channels
    (usually wires) to send data, with the data split up across the different channels.
    *Serial communication* means we’re sending the data on a single channel, one bit
    at a time. [Figure 11-5](#fig11-5) illustrates the difference.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/Figure11-5.png)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 11-5: Parallel vs. serial
    interfaces</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: The top half of the figure shows an 8-bit-wide synchronous parallel interface.
    *Synchronous* means that we have a clock signal sent between the devices, and
    the data is aligned with the clock. With this interface, we can send a whole byte
    of data on a single clock cycle, with one bit on each of the eight wires. In this
    case, we’re sending the value <samp class="SANS_TheSansMonoCd_W5Regular_11">01100011</samp>,
    or <samp class="SANS_TheSansMonoCd_W5Regular_11">0x63</samp>. While we have eight
    parallel data paths in this example, you could theoretically create a parallel
    interface with any width—you could have a 2-bit-wide interface, a 32-bit-wide
    interface, a 64-bit-wide interface, or any other arbitrary size.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom half of the figure shows the same data transfer of the value <samp
    class="SANS_TheSansMonoCd_W5Regular_11">01100011</samp>, but it’s sent in a synchronous
    serial stream. Once again, there’s a clock signal shared between the devices,
    but now the data is sent across a single wire, one bit per clock cycle. This way,
    it takes eight clock cycles to send <samp class="SANS_TheSansMonoCd_W5Regular_11">0x63</samp>,
    rather than just one clock cycle in the parallel case.
  prefs: []
  type: TYPE_NORMAL
- en: Since parallel communication allows you to send multiple bits in a single clock
    cycle, it might seem logical that transmitting data in parallel will always allow
    you to send more data per unit time than sending the data serially. In fact, parallel
    data transfer runs up against some serious limitations as the bandwidth increases.
    The physics can’t easily scale to support today’s high-speed data needs, which
    is why parallel interfaces are far less common today than they used to be.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re old enough to remember the days of ribbon printers, those would connect
    to your computer using an LPT port, which is a type of parallel interface. Another
    example was the old PCI bus that was a common way to plug devices like modems
    and sound cards into your desktop motherboard. Neither of these interfaces is
    used very much anymore; they couldn’t keep up with our need for faster data.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate why, let’s consider how your data transfer speed (or bandwidth)
    is calculated on a parallel interface like PCI. The first version of PCI operated
    at a clock rate of 33 MHz and was 32 bits wide, meaning there were 32 individual
    data wires that needed to be connected between two devices. Multiplying the numbers
    out, we get 1,056Mb/s, or 132 megabytes per second (MB/s), of bandwidth. This
    was sufficient for the computing needs of the 1990s, but the demand for more data
    soon began to increase. We wanted better graphics cards, for example, and the
    bus to support that data transfer needed to grow accordingly. PCI designers answered
    the demand by doubling the clock rate to 66 MHz, which doubled the total bandwidth
    from 132MB/s to 264MB/s. That bought a few years, but it wasn’t enough, so PCI
    next doubled the width of the connector from 32 bits to 64 bits, meaning now we
    have 64 data wires. This provided around 528MB/s of bandwidth, which again bought
    a few years, but still it wasn’t enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'By this time, PCI was reaching a point of diminishing returns. There are only
    two ways to increase data throughput with a parallel interface like PCI: make
    the bus wider or increase the clock speed. At 64 bits wide, PCI connectors were
    already a few inches long. To go any wider—say, to 128 bits—the connectors would
    need to be enormous. Routing all those connections in the circuit board gets very
    challenging, too. It simply doesn’t make sense to continue to widen the bus.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also big challenges with increasing the clock speed. When you have
    a wide data bus and you’re sending a clock signal alongside the data, as in the
    synchronous parallel interface in [Figure 11-5](#fig11-5), you need to maintain
    a tight relationship between the clock and the data. The clock will be fed into
    the clock input of each flip-flop on the receiving side: for a 128-bit-wide bus,
    for example, there are 128 individual flip-flops that all need that same clock.
    With so many parallel flip-flops, however, you start to run into problems with
    *clock skew*, a phenomenon where the same clock arrives at different times to
    each flip-flop due to propagation delay.'
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in [Chapter 7](chapter7.xhtml), signals don’t travel instantly;
    rather, there’s some delay, and the longer the signals have to travel (that is,
    the longer the wire) the longer the propagation delay becomes. With a 128-bit-wide
    bus, the distance the clock signal travels to get to the bit-0 flip-flop can be
    quite different from the distance the clock signal travels to get to the bit-127
    flip-flop. As the clock frequency increases, this difference can create enough
    clock skew to trigger metastable conditions and corrupt the data.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, parallel communication has issues. There are fundamental limits to
    how fast you can go and how many bits you can send at once. Ultimately, the problem
    comes down to the need to send a separate clock signal alongside the data. The
    solution is to send the clock and the data together, serially, as part of a single
    combined signal.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Self-Clocking Signals</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Combining the clock and the data into one signal gives you something called
    a *self-clocking signal*. The process of creating this signal is sometimes referred
    to as *embedding the clock in the data*, and it’s the key technique that makes
    high-speed serial data transfer via SerDes possible. If you send the clock and
    the data together as one signal, then the issue of clock skew is no longer a problem,
    since your clock is received exactly when your data is received; they’re the same
    signal! With clock skew out of the picture, you’re able to increase the clock
    frequency (and therefore the data frequency) tremendously.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different systems, called *encoding schemes*, for embedding the
    clock in the data. Common ones include Manchester code, High-Level Data Link Control
    (HDLC), and 8B/10B. We’ll focus on Manchester code, since it’s a relatively simple
    encoding scheme, to illustrate one way to create a self-clocking signal. [Figure
    11-6](#fig11-6) shows how Manchester code works.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/Figure11-6.png)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 11-6: Embedding the
    clock in the data with Manchester code</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: To implement Manchester code, you take the XOR (exclusive OR) of the clock and
    data signals, producing a new signal that combines the two. In any given clock
    period, the data signal will be a 1 or a 0, while the clock signal is a 1 for
    the first half of the period and a 0 for the second half of the period. The resulting
    Manchester code signal thus changes halfway through the clock period as well,
    in response to the transition in the clock signal. Depending on the data value
    in that period, the Manchester signal will either be low then high (when the data
    is a 1) or high then low (when the data is a 0). [Table 11-2](#tab11-2) is a truth
    table for the Manchester signal based on the different data/clock combinations.
    You can use this table to understand the Manchester signal pattern in [Figure
    11-6](#fig11-6).
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Heavy_B_11">Table 11-2:</samp> <samp class="SANS_Futura_Std_Book_11">Truth
    Table for Manchester Code</samp>
  prefs: []
  type: TYPE_NORMAL
- en: '| <samp class="SANS_Futura_Std_Heavy_B_11">Data</samp> | <samp class="SANS_Futura_Std_Heavy_B_11">Clock</samp>
    | <samp class="SANS_Futura_Std_Heavy_B_11">Manchester encoding</samp> |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| <samp class="SANS_Futura_Std_Book_11">0</samp> | <samp class="SANS_Futura_Std_Book_11">0</samp>
    | <samp class="SANS_Futura_Std_Book_11">0</samp> |'
  prefs: []
  type: TYPE_TB
- en: '| <samp class="SANS_Futura_Std_Book_11">0</samp> | <samp class="SANS_Futura_Std_Book_11">1</samp>
    | <samp class="SANS_Futura_Std_Book_11">1</samp> |'
  prefs: []
  type: TYPE_TB
- en: '| <samp class="SANS_Futura_Std_Book_11">1</samp> | <samp class="SANS_Futura_Std_Book_11">0</samp>
    | <samp class="SANS_Futura_Std_Book_11">1</samp> |'
  prefs: []
  type: TYPE_TB
- en: '| <samp class="SANS_Futura_Std_Book_11">1</samp> | <samp class="SANS_Futura_Std_Book_11">1</samp>
    | <samp class="SANS_Futura_Std_Book_11">0</samp> |'
  prefs: []
  type: TYPE_TB
- en: This is simply a truth table for a two-input XOR logic gate, where the inputs
    are the data and clock signals. As we discussed in [Chapter 3](chapter3.xhtml),
    XOR performs the operation either/or, but not both, so the output is high when
    exactly one input is high, but not when both or neither are high. Looking at the
    waveforms in [Figure 11-6](#fig11-6), notice that whenever the clock and data
    are both high, the encoded value is low. The encoded value is also low when the
    clock and data are both low, and it’s high when only one of the two is high.
  prefs: []
  type: TYPE_NORMAL
- en: The Manchester encoded signal allows you to send the clock and data signals
    together on a single wire. As mentioned previously, this is the key enabler of
    high-speed serial data transfer. You no longer need to worry about the alignment
    of the clock to the data, because *the clock is the data* and *the data is the
    clock*.
  prefs: []
  type: TYPE_NORMAL
- en: On the receiving side, you need to separate the clock back out from the data,
    a process called *clock data recovery (CDR)*. This is achieved using an XOR gate
    and some small, additional logic at the receiver. Then you can use the recovered
    clock as your clock input to a flip-flop, and feed the recovered data into the
    data input of that same flip-flop. This way you have perfect synchronization between
    the clock and the data. The issue of clock skew that we saw with parallel data
    goes away, enabling you to crank up the data rates far beyond what parallel data
    transfer could ever achieve.
  prefs: []
  type: TYPE_NORMAL
- en: Manchester code is just one way to generate a self-clocking signal, and it’s
    a simple encoding scheme. It isn’t used for modern, more complicated SerDes applications,
    but it does have some features that are critical. For one, the Manchester encoded
    signal is guaranteed to transition on each clock cycle. If you’re sending a continuous
    stream of 0s in the data, for example, there will still be transitions in the
    encoded signal. These transitions are essential for performing CDR on the receiving
    side. Without guaranteed transitions, the receiver wouldn’t be able to lock onto
    the input stream. It wouldn’t know if the data was being sent at 3 gigabits per
    second (Gb/s), or 1.5Gb/s, or if it was running at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important feature of Manchester code is that it’s *DC balanced*, meaning
    there are an equal number of highs and lows in the resulting data stream. This
    helps maintain signal integrity and overcome non-ideal conditions on the wire
    during high-speed data transfer. We normally consider wires to be perfect conductors,
    but in reality they aren’t; every wire has some capacitance, inductance, and resistance.
    At slow data rates these don’t matter much, but when we get into the Gb/s range,
    we need to consider these effects. For example, since there’s some capacitance
    in the wire, it makes sense that the wire can be charged up like a capacitor.
    When a wire becomes slightly charged, for example to a high state, then it requires
    more energy to discharge it to a low state. Ideally, you don’t want to charge
    up your wires at all: in other words, you want to maintain a DC balance. Sending
    an equal number of high and low transitions in SerDes is critical to maintaining
    good signal integrity, and all clock and data encoding schemes have this feature.'
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">How SerDes Works</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we’ve covered the speed advantages of serial communication over parallel
    communication and examined how to combine the clock and data into one signal,
    we’re ready to look at how SerDes actually works. [Figure 11-7](#fig11-7) shows
    a simplified block diagram of a SerDes interface.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/Figure11-7.png)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 11-7: A simplified SerDes
    block diagram</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the image as a whole, we have a serializer on the left that acts
    as a transmitter, and a deserializer on the right that acts as a receiver. We
    have clock and data signals going into the serializer on the left, and clock and
    data signals coming out of the deserializer on the right. That’s really all we’re
    trying to do with SerDes: send some data and a clock signal from a transmitter
    to a receiver. However, doing this at fast data rates requires a lot more functionality
    than we’ve seen in simpler input and output buffers.'
  prefs: []
  type: TYPE_NORMAL
- en: First, notice that there’s a phase-locked loop on the transmit side, in the
    lower-left corner of [Figure 11-7](#fig11-7). Usually this is a dedicated PLL
    specific to the SerDes transceiver that uses a reference clock (Clk In) to generate
    the clock that will run the serializer. This clock dictates the overall rate at
    which your SerDes will run. The data that you actually want to send (Data In)
    comes into the SerDes block in parallel. I’ve drawn four lines here, but there
    could be any number of wires. The serializer takes the output of the PLL and the
    parallel data, encodes it using an encoding protocol, and generates a serial data
    stream at the desired SerDes rate. For example, if you have a parallel interface
    that takes 4 bits at a time at a 250 MHz clock rate, then the serializer will
    generate a serialized version of this data that can be transferred at 1Gb/s, four
    times that speed.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>
  prefs: []
  type: TYPE_NORMAL
- en: '*Depending on the encoding scheme, the actual serial stream will likely be
    running at above 1Gb/s. For example, the 8B/10B scheme takes 8-bit data and encodes
    it into 10-bit data. It does this for two purposes: to ensure transitions in the
    clock so we can do clock data recovery at the receiver, and to maintain a DC balance.
    Going from 8-bit data to 10-bit data adds a 20 percent overhead, however, so to
    send data at a rate of 1Gb/s we need to send the actual serial stream at 1.2Gb/s.*'
  prefs: []
  type: TYPE_NORMAL
- en: The output stage is next. Notice that the output stage contains a differential
    output buffer. For the reasons discussed previously, such as the ability to send
    data at high rates, at lower power, and with noise immunity, SerDes transceivers
    use differential data. The output stage also performs some extra signal conditioning
    to improve the signal integrity. Once the signal is as conditioned as it can be,
    it passes through the data channel.
  prefs: []
  type: TYPE_NORMAL
- en: Sending data at high speeds requires optimizations in all parts of the data
    path, including the data channel itself. For copper wires, the impedance of the
    material must be controlled to ensure good signal integrity. The channel can also
    be made from a different material than copper. For example, fiber optics can be
    used, where light rather than electricity is sent down thin glass or plastic wires.
    Fiber optics provides excellent signal integrity and is immune to EMI, but it’s
    a more expensive solution than traditional copper wires.
  prefs: []
  type: TYPE_NORMAL
- en: On the receive side, the input stage performs its own signal conditioning to
    extract the best-quality signal possible. The data is then sent to both a CDR
    block and the serial-to-parallel conversion and decoder block. The CDR recovers
    the clock signal from the data stream, and then the deserializer uses that extracted
    clock to sample the data. It might seem a bit odd that you can recover the clock
    and then use that clock to sample data from the same signal, but that’s the magic
    of how SerDes works! Finally, at the output side, the data is again converted
    to parallel. Continuing with the previous example, you would recover your 250
    MHz data stream across the four parallel output wires.
  prefs: []
  type: TYPE_NORMAL
- en: This example referred to a 1Gb/s data rate, but that isn’t really that fast
    anymore. As data rates keep increasing, we need to keep optimizing each part of
    this whole process. Maintaining high signal integrity is critical for SerDes applications.
    At fast data rates, small resistances, capacitances, and inductances can affect
    the signal integrity of the line. Passing data through a connector (for example,
    a USB plug) causes small imperfections in the data path that affect the signal
    integrity too, so optimizing every aspect of the process becomes critical.
  prefs: []
  type: TYPE_NORMAL
- en: SerDes is one of the killer features of modern FPGAs, but there’s a lot of complexity
    involved. As you’ve just seen, something as simple-sounding as high-speed data
    transfer involves a number of steps, including serializing data, transmitting
    it, receiving it, and then deserializing it again. Even with that process, we’re
    fighting physics to get data to travel at faster and faster rates.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">Summary</samp>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To be a successful FPGA designer, it helps to have a strong understanding of
    I/O. This is where the FPGA engineer works at the intersection of electrical engineering
    and software engineering. This chapter explained how FPGAs use buffers to bring
    data in and send data out and explored some of the more common settings that FPGA
    designers need to be aware of when configuring I/O, including the operating voltage,
    drive strength, and slew rate. You learned about the difference between single-ended
    and differential communication, and you saw how DDR uses both rising and falling
    clock edges to send data more quickly. We also explored SerDes, a powerful input/output
    feature that allows FPGAs to excel at high-speed data applications.
  prefs: []
  type: TYPE_NORMAL
