- en: '**9'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AUDIO SIGNALS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we continue exploring the relationship between continuous and
    discrete signals. The *Nyquist–Shannon sampling theorem* relates continuous signals
    and discrete signals. To properly discretize a continuous signal, we must sample
    it at a rate at least twice that of the highest frequency present in the signal.
    The theorem is why compact discs sample audio signals at 44.1 kHz, or 44,100 times
    per second. At that rate, any frequency up to 22,050 Hz will be captured. Note
    that 22 kHz is the theoretical upper limit on the highest frequency a human can
    hear, though most adults have a much lower upper limit; mine is about 13.5 kHz.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explores *compressed sensing* (or compressive sensing), a technique
    for beating Nyquist and Shannon at their own game. With compressed sensing, it
    becomes possible to acquire less data when digitizing a signal than the Nyquist-Shannon
    theorem says is required. This is an exciting real-world inverse problem involving
    randomness.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: We’ll begin by walking through the main points of compressed sensing; we’ll
    cover some of the math, but I encourage you to explore the rest on your own. Then
    we’ll explore compressed sensing in one dimension, audio, to see how it lets us
    break the Nyquist limit. Finally, as unraveled images are, to compressed sensing,
    no different from signals in time, we’ll apply compressed sensing to reconstruct
    images from what seems like too little data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: There’s some matrix-vector math in the first section, but it doesn’t go much
    beyond what we encountered in [Chapter 7](ch07.xhtml) with iterated function systems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '**Compressed Sensing**'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Digitizing a signal usually means reading the output of an analog-to-digital
    converter at a specified but constant time interval. The number of readings per
    second is the sampling rate, which the Nyquist-Shannon theorem is concerned with.
    If we acquire the samples according to the Nyquist-Shannon theorem, we can accurately
    reconstruct the signal from the samples.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: When we sample at a fixed time interval, we are *uniform sampling*. However,
    there are times when uniform sampling isn’t desired, possibly because it’s too
    expensive or there’s too much risk associated with it (for example, in X-ray tomography).
    In such situations, it would be nice to acquire less data but still reconstruct
    the entire signal. For example, if the signal we want is denoted as ***x***, we’ll
    measure some subset of the signal, ***y***, and from ***y*** reconstruct ***x***.
    Mathematically, we can cast this process as a matrix equation
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0256-01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: where we know vector ***y*** because we measured it and matrix ***C*** because
    it dictates the parts of ***x*** we sampled. We want ***x***, the vector we would’ve
    measured following standard sampling theory. Keep [Equation 9.1](ch09.xhtml#ch09equ1)
    in the back of your mind for the time being.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s return to algebra class, which asks us to solve systems of equations,
    usually two equations and two unknowns:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0256-02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: 'Here, *a* through *f* are constants. Because there are two equations and two
    unknowns, we can find *x* and *y* values that satisfy both, assuming one equation
    isn’t a multiple of the other. In matrix form, we write the system of equations
    as:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0256-03.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: The rules of matrix-vector multiplication tell us to multiply each row of the
    matrix, ***A***, by the corresponding elements of the vector, ***x***, then sum.
    This transforms the matrix equation into the system. Regardless of the number
    of elements in the vectors, this rule applies.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The system of equations works because there are as many unknowns as there are
    equations, meaning the number of elements in the vectors, here ***b*** and ***x***,
    matches the number of rows in the matrix, ***A***. For such an equation, the solution
    (if there is one), or the ***x*** vector that makes the equation true, is ***A***^(–1)***b***
    = ***x*** for ***A***^(–1), the inverse matrix of ***A***. For example, this system
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0257-01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: becomes
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0257-02.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: with solution
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0257-03.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: where the inverse of a matrix ***A*** is ***A***^(–1) such that ***AA***^(–1)
    = ***A***^(–1)***A*** = ***I***.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Here, ***I*** is the *identity matrix*—the matrix of all zeros with ones along
    the diagonal. In the world of matrices, ***I*** is akin to the number 1\. Use
    NumPy’s `linalg.inv` function to find ***A***^(–1).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '***A*** is a *square matrix*, meaning it has as many rows as columns. If ***A***
    is square, and the number of rows matches the number of elements in ***b*** and
    ***x***, then we can use ***A***^(–1) to find ***x***.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Now comes the fun part. Return to [Equation 9.1](ch09.xhtml#ch09equ1). By design,
    there are *fewer* elements in ***y***, the values we measure, than in ***x***,
    the full signal. If there are *N* elements in ***y*** and *M* elements in ***x***,
    then ***C*** is an *N*×*M* matrix with *N* rows and *M* columns. There are more
    unknowns in [Equation 9.1](ch09.xhtml#ch09equ1) than there are equations. Such
    a system is called *underdetermined*. Underdetermined systems have an infinite
    number of solutions; there are an infinite number of vectors, ***x***, that, when
    multiplied by ***C***, give ***y***.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: We want to get ***x*** by measuring ***y***, but ***y*** alone doesn’t have
    enough information to tell us *which* of the infinite set of ***x*** vectors we
    want. Compressed sensing comes to the rescue—at least in some cases.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: According to compressed sensing theory, if ***x*** is *sparse*, meaning most
    of its elements are essentially zero, then we can recover ***x*** from ***y***
    by solving the *inverse problem*, which searches for the ***x*** that minimizes
    some measure of the difference ***Ax*** – ***b*** while strongly encouraging ***x***
    to be sparse. As we’ll see, algorithms capable of this kind of optimization exist.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Great! We’re in business. We measure ***y*** containing some subset of the elements
    that would be in ***x***, and we get ***x*** from ***y*** by solving a minimization
    problem. But it isn’t that simple; the optimization trick only works if ***x***
    is sparse. Most signals are not sparse; an audio signal isn’t likely to be. Recall
    working with the waveforms in [Chapter 8](ch08.xhtml). Are we doomed? Not necessarily.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: While an audio signal isn’t sparse, there are Fourier-like transformations that
    map from a signal changing in time to one changing in frequency, and it is often
    the case that the frequency domain signal *is* sparse. Therefore, if we can write
    ***x*** = **Ψ*****s*** for some transformation matrix **Ψ** (psi) and sparse vector
    ***s***, [Equation 9.1](ch09.xhtml#ch09equ1) becomes
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0258-03.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: where **Θ** = ***C*****Ψ**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: While ***x*** isn’t sparse and is therefore unrecoverable, ***s*** is, meaning
    the optimization trick might have a chance of working. The measurements in ***y***
    combined with the external knowledge that ***s*** is sparse will let us find ***s***.
    Once we have ***s***, we get ***x***.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: But, what are all these matrices floating around? The *measurement matrix*,
    ***C***, can mathematically be any matrix of values such that the values in ***C***
    satisfy some notion of *incoherence* in relation to the elements of **Ψ**. For
    us, the elements of ***C*** are binary, zero or one, and serve to select specific
    elements of ***x*** that are actually measured. This requirement supplies the
    necessary mathematical incoherence between ***C*** and **Ψ**. The most important
    point is that ***C*** is somehow *random*. In practice, we don’t explicitly define
    ***C***, but our measurement process uses it implicitly. The random bit is essential
    to the entire operation, however.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: The **Ψ** matrix is a transformation matrix that transforms the sparse vector,
    ***s***, into a new representation, ***x***, which we ultimately want to measure.
    For us, **Ψ** is a Fourier-like *discrete cosine transformation (DCT)*. Signals
    are often sparse in this domain, thereby making it very useful for compressed
    sensing.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Finally, **Θ** represents the combination of the measurement process working
    on **Ψ**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now in a position to try solving ***y*** = **Θ*****s*** for some ***s***
    that both is sparse and leads to ***y***, the set of measurements we have. There
    are multiple algorithms available, but we’ll use the Lasso algorithm, courtesy
    of scikit-learn. Likewise, we need the DCT and its inverse, which SciPy dutifully
    supplies:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Lasso minimizes the following
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0258-01.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: where *n* is the number of samples, or the number of elements in ***y***.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'The double vertical bar notation refers to a *norm*, which is a metric measuring
    distance of some kind. The first term uses the square of the *ℓ*² norm, while
    the second term multiplies the *ℓ*¹ norm by *α*. The *ℓ^p* norm of a vector, ***x***,
    is defined as:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0258-02.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: The *ℓ*² norm is the Euclidean distance. The *ℓ*¹ norm, sometimes called the
    Manhattan or taxicab distance, is the sum of the absolute values of the elements
    of ***x***. Lasso uses this term, scaled by *α*, to find an ***s*** vector that
    minimizes the Euclidean distance between the measurements, ***y***, and **Θ*****s***
    while simultaneously minimizing the sum of the absolute values of the elements
    of ***s***. This latter constraint forces many elements of ***s*** toward zero,
    thereby ensuring sparsity.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: To understand why the *ℓ*¹ norm term is present in the Lasso objective function,
    consider a simple case where we have a two-element vector and a single-element
    output. This is akin to finding a solution to *x* + *y* = *c* that is as sparse
    as possible, where *x* = 0 or *y* = 0\. Geometrically, minimizing the *ℓ*¹ norm
    leads to a situation as on the left of [Figure 9-1](ch09.xhtml#ch09fig01), while
    minimizing the *ℓ*² norm is shown on the right. Minimizing the *ℓ*² norm is standard
    least-squares regression.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/09fig01.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-1: Minimizing the ℓ¹ norm (left) and the ℓ² norm (right)*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The line in [Figure 9-1](ch09.xhtml#ch09fig01) represents the infinite set of
    solutions to *ax* + *by* = *c* for some *c*. The diamond on the left and the circle
    on the right correspond to constant *ℓ*¹ and *ℓ*² norms, respectively. Minimizing
    the *ℓ*¹ norm intersects the line at a point where *y* is zero, while minimizing
    *ℓ*² intersects the line at a point where neither *x* nor *y* is zero. This trend
    continues as the dimensionality increases. In each case, minimizing the *ℓ*¹ norm
    implies sparsity in the solution while minimizing the *ℓ*² norm distributes the
    “energy” throughout each dimension, the opposite of enforcing sparsity.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Let’s sum up. We want to acquire ***x*** by measuring a subset of it, ***y***.
    To solve for random-ish measurement matrix ***A***, we want ***y*** = ***Ax***.
    This expression is underdetermined, meaning there are an infinite number of ***x***
    that work as solutions, so we need extra information to find the one we (likely)
    want. We get this information from expressing ***x*** in some other form (basis)
    where it becomes sparse. If sparse, the probability of finding a meaningful and
    parsimonious solution presents itself. A commonly used basis for this comes from
    the Fourier family of transformations, like the DCT, ***x*** = **Ψ*****s***, where
    **Ψ** encapsulates the DCT and ***s*** is a sparse vector we want to find. If
    we find ***s***, we find ***x***.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Combining the measurement matrix and the DCT gives us a new equation, ***y***
    = **Θ*****s***, where we know ***y*** and **Θ**. It’s still underdetermined, but
    we know ***s*** is sparse. To find ***s***, we use an optimization algorithm that
    knows how to minimize the *ℓ*¹ norm of ***s*** in the process. This enforces sparsity
    and gives us some confidence that we might find a suitable ***s***.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Let’s give the recipe a go and see what happens.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '**Signal Generation**'
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll walk through *cs_signal.py*, which illustrates the compressed sensing
    process and why we need to use random measurements. First, let’s run it, then
    I’ll explain the various plots it generates. Here’s the command line:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Several plots should appear in succession; close each to move to the next. Output
    files are created as well.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: The code first generates a one-second signal, the sum of three sine waves forming
    a C major chord. Standard Nyquist sampling gives this signal for a sample rate
    of 4,096 Hz, that is, ***x***. This is a demonstration, so we start with ***x***
    and then throw much of it away to create a ***y*** that we might have plausibly
    measured in the first place. The command line includes an argument of `0.2`, the
    fraction of ***x*** to retain, meaning that ***y*** has 20 percent of the samples;
    we throw the remaining 80 percent away. The remainder of the command line specifies
    the randomness source (`minstd`) and a seed value (`65536`).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'The signal comes from:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We used similar code in [Chapter 8](ch08.xhtml). The three frequencies (`f0`,
    `f1`, `f2`) are the C major chord. The `samples` vector is the final signal, ***x***.
    It’s a vector of 4,096 elements because the sampling rate is 4,096 Hz and the
    duration is one second.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build ***y*** from ***x***. This process makes implicit use of the measurement
    matrix. We’ll keep 20 percent of the samples in ***x***, first by selecting samples
    at a uniform interval, and then randomly. The uniform samples correspond to measuring
    the signal at some rate below the Nyquist limit:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There are `nsamp` samples in ***y***. The first ***y*** vector is `bu`, uniformly
    sampled, and the second is `br`, randomly sampled. [Figure 9-2](ch09.xhtml#ch09fig02)
    shows the original signal with the uniform and random samples marked (*cs_signal_samples.png*).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/09fig02.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-2: Random (top) and uniform samples (bottom)*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the measurements. Now we need **Θ**, the combination of **Ψ** and the
    measurement matrix. Once we have that, we’re ready to use Lasso. We have two ***y***
    vectors, so we need two **Θ** matrices:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first, `U`, keeps only the uniformly selected measurements. The second,
    `R`, uses the randomly selected measurements. Here, `D` is the discrete Fourier
    transform matrix, **Ψ**, and `U` and `R` are **Θ***[u]* and **Θ***[r]*, respectively.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'We optimize twice, first ***y**[u]* = **Θ***[u]**s**[u]* and then ***y**[r]*
    = **Θ***[r]**s**[r]*, where the subscripts refer now to the uniformly and randomly
    sampled measurements:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`Lasso` follows the scikit-learn convention of creating an instance of a class
    and then calling `fit` to do the optimization. For `Lasso`, the solution vector
    is buried in the `coef_` member variable, which we extract to get `su` and `sr`,
    the uniform and random ***s*** vectors, respectively. [Figure 9-3](ch09.xhtml#ch09fig03)
    shows the two ***s*** vectors (*cs_signal_sparse.png*).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/09fig03.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-3: Random (top) and uniform (bottom) solution vectors,* s'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: The top plot shows ***s**[u]* and the bottom shows ***s**[r]*. The spikes correspond
    to DCT components. Both ***s*** vectors are sparse, with most of the 4,096 elements
    near zero, but the bottom vector has more than 10 nonzero elements while the top
    has only 3 (the shape is sometimes both positive and negative). Recall, ***x***
    is the sum of three sine waves, so the three sine waves and three spikes in the
    ***s**[r]* vector seems promising.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Lasso has solved ***y*** = **Θ*****s*** for us. Now we need ***x*** = **Ψ*****s***,
    which we find by calling the inverse DCT:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Figure 9-4](ch09.xhtml#ch09fig04) shows us ***x**[u]* (`ru`) and ***x**[r]*
    (`rr`); see *cs_signal_recon.png*.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/09fig04.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-4: From top: the original, reconstructed randomly sampled, and reconstructed
    uniformly sampled signals*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: The topmost plot shows the original signal. The middle plot shows the signal
    reconstructed from 20 percent of the original signal using randomly selected measurements.
    Finally, the bottom plot shows the signal reconstructed from the uniformly selected
    measurements, likewise 20 percent of the original number. Which do you think more
    faithfully captured the original signal?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step is to output the signal as a WAV file:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: See [Listing 8-1](ch08.xhtml#ch08list01) to review how `WriteOutputWav` works.
    Play the output files. I think you’ll agree that random sampling produced the
    better result.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Uniform sampling failed for deep mathematical reasons related to coherence between
    the measurement matrix and the DCT transform basis. However, we can intuitively
    understand the failure due to uniform sampling at less than the Nyquist rate,
    which means *aliasing* where higher frequency signals look like lower frequency
    signals, and there’s no way to disentangle the two. On the other hand, with random
    sampling, the likelihood of aliasing decreases, making Lasso more likely to find
    a suitable ***s*** vector.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Rerun *cs_signal.py*, but alter the fraction from 20 percent to smaller and
    higher values. Is there a place where everything falls apart? See if you can re-create
    the signal from only 10 percent, 5 percent, or even 1 percent of the original,
    and then try the opposite direction. Sampling even slightly above 50 percent appears
    to have a dramatic effect on the quality of the uniform sample. Why might that
    be? Consider the Nyquist-Shannon sampling theorem requirements.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '**Unraveled Images**'
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The file *cs_image.py* applies compressed sensing to images. It’s similar to
    *cs_signal.py*, but it unravels the image before selecting the measured components
    (pixels). The image is ***x***, with the selected mask pixels forming ***y***.
    The code expects these command line arguments:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The input image may be grayscale or RGB. If RGB, each channel is processed individually
    using the same random mask. The output directory contains the original image,
    the reconstructed image, and a parameter file.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'The code tries to import from scikit-image. It will run if scikit-image isn’t
    installed, but you can install it with:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If scikit-image is present, the code imports `structural_similarity`, which
    measures the mean structural similarity between two images—here the original image
    and the reconstructed image. Higher similarity is better, with 1.0 indicating
    an exact match.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'The code loads the input image, converts it to RGB, and tests to see if it’s
    really grayscale:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: A grayscale image converted to RGB ends up with every channel the same, hence
    the call to `array_equal`. The test is not entirely foolproof, but it’s good enough
    for us.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step generates the random mask, the subset of actual image pixels
    that construct ***y***:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `mask` vector is 1 for selected pixels.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'The remainder of the code calls `CS` for each image channel, if RGB, or the
    first channel, if grayscale, before dumping the original image, reconstructed
    image, and parameters to the output directory. All the action is in `CS`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `CS` function is a compact version of the essential code in *cs_signal.py*.
    It forms the unraveled image (`f`) and then selects the masked regions to form
    `y`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: To make the code reproducible from a given seed value, we define the local variable,
    `seed`, and pass it to the `Lasso` constructor before calling `fit`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: When `fit` exits, the inverse DCT uses the sparse vector (***s***) to recover
    the image. The image isn’t scaled to [0, 255], so we first scale it to [0, 1]
    and then multiply by 255 and reshape (`oimg`).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Let’s find out whether *cs_image.py* works. This command line
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: attempts to reconstruct the peppers image. It will take several minutes to run
    before producing [Figure 9-5](ch09.xhtml#ch09fig05).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/09fig05.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-5: The original image (left), mask (middle), and reconstructed image
    (right)*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The original image is on the left, the 10 percent mask in the middle, and the
    reconstructed image on the right. This is best viewed in color; look at the files
    in the *peppers* directory. I inverted the mask image to show the selected pixels
    in black.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: The reconstructed image isn’t particularly impressive until you remember that
    90 percent of the original image information was discarded or, in practice, never
    measured in the first place.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: I claimed that Lasso finds sparse ***s*** vectors. The signal example was sparse,
    but what about images? The test images are 128×128 = 16, 384 pixels, meaning ***s***
    has that many elements. A quick test with the *barbara.png* image, keeping 20
    percent of the pixels, returned an ***s*** that’s 70 percent zeros. Dropping down
    to 10 percent jumps to 81 percent zeros, while moving up to 80 percent drops to
    only 15 percent zeros. Fewer measurements imply a sparser ***s***, which seems
    reasonable. Recall that ***s*** is the representation of the image in the discrete
    cosine transform space. If we can find only a few presumably low frequency components
    when attempting to best fit the few measurements in ***y***, we might expect most
    of ***s*** to be zero after imposing *ℓ*¹ regularization.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾声称 Lasso 找到稀疏的 ***s*** 向量。信号示例是稀疏的，但图像呢？测试图像为 128×128 = 16,384 像素，这意味着 ***s***
    含有这么多元素。使用 *barbara.png* 图像进行快速测试，保留 20% 的像素，返回的 ***s*** 中有 70% 是零。降到 10% 时，零的比例上升到
    81%，而升到 80% 时，零的比例降至仅 15%。测量减少意味着 ***s*** 更稀疏，这似乎是合理的。回想一下，***s*** 是图像在离散余弦变换空间中的表示。如果我们在尝试最优拟合
    ***y*** 的少量测量时，能找到少数低频成分，我们可能会期待大部分 ***s*** 在施加 *ℓ*¹ 正则化后变为零。
- en: The *cs_image_test* script runs *cs_image.py* repeatedly on the same test image
    while varying the measured fraction of pixels from 1 percent up to 80 percent.
    [Figure 9-6](ch09.xhtml#ch09fig06) shows the resulting reconstructed images.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*cs_image_test* 脚本反复运行 *cs_image.py*，在同一测试图像上，测量像素的比例从 1% 到 80% 不等。[图 9-6](ch09.xhtml#ch09fig06)
    显示了重建图像的结果。'
- en: '![Image](../images/09fig06.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/09fig06.jpg)'
- en: '*Figure 9-6: Reconstructions of the* zelda.png *image by varying the fraction
    of the original pixels*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-6：通过改变原始像素的比例重建 *zelda.png* 图像*'
- en: At 10 percent, we can start to recognize the image, but it isn’t clear that
    it’s a person’s face until 20 percent. Note that I altered the intensity of the
    original *zelda.png* image to use the entire range [0, 255]; this makes it as
    bright as the reconstructions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在 10% 时，我们已经能够开始辨认图像，但直到 20% 时，才清楚地知道这是一张人的面孔。注意，我调整了原始 *zelda.png* 图像的亮度，使其使用整个
    [0, 255] 范围；这使其亮度与重建图像一致。
- en: '[Figure 9-7](ch09.xhtml#ch09fig07) shows a plot of the mean structural similarity
    index (SSIM) between the reconstructions and the original image.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-7](ch09.xhtml#ch09fig07) 显示了重建图像与原始图像之间的平均结构相似性指数 (SSIM) 的变化曲线。'
- en: '![Image](../images/09fig07.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/09fig07.jpg)'
- en: '*Figure 9-7: The mean structural similarity index as a function of measurements*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-7：平均结构相似性指数随测量数量的变化*'
- en: As we might anticipate, the index increases rapidly as the number of pixels
    measured increases. The results are encouraging, because there’s little perceptual
    difference between the original image and the one made from 20 percent fewer measurements.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，随着测量像素数量的增加，指数迅速增加。结果令人鼓舞，因为原始图像和从减少 20% 测量中重建的图像之间几乎没有感知上的差异。
- en: '**Compressed Sensing Applications**'
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**压缩感知应用**'
- en: Compressed sensing is used in many places, including medical imaging, where
    its use has improved acquisition times in magnetic resonance imaging and various
    forms of tomography. Applying compressed sensing to tomography implies collecting
    fewer projections, leading to a substantial reduction in the amount of X-ray energy
    used (ionizing radiation).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩感知被广泛应用于许多领域，包括医学影像学，在磁共振成像和各种断层扫描形式中，压缩感知的应用显著提高了获取时间。将压缩感知应用于断层扫描意味着采集更少的投影，从而大幅减少使用的
    X 光能量（电离辐射）。
- en: Magnetic resonance imaging is a natural target for compressed sensing. The image
    acquisition process literally measures in *k-space*, or Fourier space, equivalent
    to measuring ***s*** directly. The desired image is recovered by a two-dimensional
    inverse Fourier transform, just as we recovered ***x*** from ***s*** via the inverse
    discrete cosine transform. Many k-space sampling strategies have been developed
    to speed image acquisition while still producing clinically valuable images. How
    magnetic resonance image acquisition works makes the simple random sampling in
    this chapter impractical, but alternative approaches for sampling k-space in a
    mathematically incoherent manner exist and lead to reduced acquisition times.
    For example, GE’s *HyperSense*, an advanced compressed sensing method, can reduce
    scan times by up to 50 percent. Faster scan times mean less scanner time for the
    patient.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: The future of compressed sensing is, however, a bit unclear. Deep neural networks
    are also quite good at solving inverse linear problems—in fact, likely better
    than traditional compressed sensing. Using deep neural networks in place of traditional
    CS, or in combination with it, is an active research area.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This brief chapter included two experiments, first with one-dimensional signals,
    then with images expressed as one-dimensional vectors. Here are some possible
    avenues for further exploration:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in *cs_signal.py* worked with the entire one-second sound sample.
    How might you modify this basic approach to compress an arbitrary WAV file? Hint:
    try keeping only a random subset of each few hundred milliseconds of sound and
    reconstructing each.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assuming you build an arbitrary WAV filesystem, can you get away with using
    the same measurement matrix (the same random sampling) for each subset, or is
    it better to alter that in some way—maybe by using a fixed pseudorandom seed and
    selecting measurements in blocks as needed?
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of our image experiments used *α* = 0.001\. Try varying *α* from near 0
    up to, or even beyond, 1\. If *α* = 0, the *ℓ*¹ regularization term in Lasso vanishes,
    and the optimization becomes standard least-squares using only the *ℓ*² norm.
    Does compressed sensing work well when *α* is very small? Note that the scikit-learn
    documentation for Lasso warns not to use *α* = 0, so, for that case, replace `Lasso`
    with `LinearRegression`.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *cs_image.py* file includes checks to see if the supplied randomness source
    is `quasi` and, if so, to interpret the seed value as the quasi-random generator
    base. What happens if you use `quasi` for different prime bases like 2, 3, or
    13? Can you explain the results you see?
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We process RGB images color channel by color channel. As an alternative, we
    can unravel the full RGB image into a vector three times larger and then perform
    the optimization (remember to re-form the RGB image on output). Alter *cs_image.py*
    to do this. Does it matter? Does it help or hurt?
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are all random measurement matrices created equal?
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Summary**'
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compressed sensing breaks the Nyquist-Shannon sampling theorem limit and allows
    signals to be reconstructed from fewer samples than initially thought possible.
    In this chapter, we experimented with a basic form of compressed sensing and applied
    it to audio signals and images.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: First, we discussed the core concepts in compressed sensing, including sparsity
    and *ℓ*¹ regularization. We then expressed the compressed sensing problem as an
    inverse linear problem of the form ***y*** = ***Cx*** for measured vector ***y***
    and desired output vector ***x***. In practice, sparsity constraints means using
    an alternate form of ***x*** = **Ψ*****s*** for sparse vector ***s*** and basis
    **Ψ**. For us, **Ψ** came from the discrete cosine transform in which signals
    are known to be sparse.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: The compressed sensing problem then became one of finding solution vector ***s***
    such that the *ℓ*² distance between ***y*** and ***C*****Ψ*****s*** = **Θ*****s***
    was as small as possible, subject to the constraint that ∥***s***∥[1] was also
    as small as possible. We discovered that Lasso regression accomplishes this goal
    quite nicely.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Theory in hand, we performed two sets of experiments. The first sought to reconstruct
    a one-second audio signal, a C major chord, using uniform sampling below the Nyquist
    limit and random sampling. Uniform sampling couldn’t recover the signal until
    the sampling rate exceeded half the playback rate (in which more than 50 percent
    of samples were kept). On the other hand, random sampling with compressed sensing
    returned good results even after discarding up to 90 percent of the original data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: In the second experiment, we worked with both grayscale and RGB images. As with
    signals, we successfully used compressed sensing and the discrete cosine transform
    to recover images from as little as 10 percent of the original pixels, often with
    considerable noise. The DCT isn’t necessarily the best basis for images, but better
    ones, like wavelets, are beyond the scope of this book.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: We closed the chapter by pointing out that compressed sensing has been a boon
    to medical imaging to improve patient comfort and reduce exposure to ionizing
    radiation. Finally, we noted that recent advances in deep neural networks will
    likely substantially impact the future of compressed sensing.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: We’ll take a break from experimenting in the next chapter to explore how we
    use randomness in experiments themselves. Modern science critically depends on
    properly designed experiments, and randomness is a powerful player in that process.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
