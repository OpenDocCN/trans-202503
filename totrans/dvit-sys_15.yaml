- en: '15'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**LOOKING AHEAD: OTHER PARALLEL SYSTEMS AND PARALLEL PROGRAMMING MODELS**'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous chapter, we discussed shared memory parallelism and multithreaded
    programming. In this chapter, we introduce other parallel programming models and
    languages for different classes of architecture. Namely, we introduce parallelism
    for hardware accelerators focusing on graphics processing units (GPUs) and general-purpose
    computing on GPUs (GPGPU computing), using CUDA as an example; distributed memory
    systems and message passing, using MPI as an example; and cloud computing, using
    MapReduce and Apache Spark as examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Whole New World: Flynn’s Taxonomy of Architecture'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Flynn’s taxonomy* is commonly used to describe the ecosystem of modern computing
    architecture ([Figure 15-1](ch15.xhtml#ch15fig1)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/15fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-1: Flynn’s taxonomy classifies the ways in which a processor applies
    instructions.*'
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal axis refers to the data stream, whereas the vertical axis refers
    to the instruction stream. A *stream* in this context is a flow of data or instructions.
    A *single stream* issues one element per time unit, similar to a queue. In contrast,
    *multiple streams* typically issue many elements per time unit (think of multiple
    queues). Thus, a single instruction stream (SI) issues a single instruction per
    time unit, whereas a multiple instruction stream (MI) issues many instructions
    per time unit. Likewise, a single data stream (SD) issues one data element per
    time unit, whereas a multiple data stream (MD) issues many data elements per time
    unit.
  prefs: []
  type: TYPE_NORMAL
- en: A processor can be classified into one of four categories based on the types
    of streams it employs.
  prefs: []
  type: TYPE_NORMAL
- en: '**SISD**   Single instruction/single data systems have a single control unit
    processing a single stream of instructions, allowing it to execute only one instruction
    at a time. Likewise, the processor can process only a single stream of data or
    process one data unit at a time. Most commercially available processors prior
    to the mid-2000s were SISD machines.'
  prefs: []
  type: TYPE_NORMAL
- en: '**MISD**   Multiple instruction/single data systems have multiple instruction
    units performing on a single data stream. MISD systems were typically designed
    for incorporating fault tolerance in mission-critical systems, such as the flight
    control programs for NASA shuttles. That said, MISD machines are rarely used in
    practice anymore.'
  prefs: []
  type: TYPE_NORMAL
- en: '**SIMD**   Single instruction/multiple data systems execute the *same* instruction
    on multiple data simultaneously and in lockstep fashion. During “lockstep” execution,
    all instructions are placed into a queue, while data is distributed among different
    compute units. During execution, each compute unit executes the first instruction
    in the queue simultaneously, before simultaneously executing the next instruction
    in the queue, and then the next, and so forth. The most well-known example of
    the SIMD architecture is the graphics processing unit. Early supercomputers also
    followed the SIMD architecture. We discuss GPUs more in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: '**MIMD**   Multiple instruction/multiple data systems represent the most widely
    used architecture class. They are extremely flexible and have the ability to work
    on multiple instructions or multiple data streams. Since nearly all modern computers
    use multicore CPUs, most are classified as MIMD machines. We discuss another class
    of MIMD systems, distributed memory systems, in “Distributed Memory Systems, Message
    Passing, and MPI” on [page 746](ch15.xhtml#lev1_115).'
  prefs: []
  type: TYPE_NORMAL
- en: '15.1 Heterogeneous Computing: Hardware Accelerators, GPGPU Computing, and CUDA'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Heterogeneous computing* is computing using multiple, different processing
    units found in a computer. These processing units often have different ISAs, some
    managed by the OS, and others not. Typically, heterogeneous computing means support
    for parallel computing using the computer’s CPU cores and one or more of its accelerator
    units such as *graphics processing units* (GPUs) or *field programmable gate arrays*
    (FPGAs).^([1](ch15.xhtml#fn15_1))'
  prefs: []
  type: TYPE_NORMAL
- en: It is increasingly common for developers to implement heterogeneous computing
    solutions to large, data-intensive and computation-intensive problems. These types
    of problems are pervasive in scientific computing as well as in a more diverse
    range of applications to Big Data processing, analysis, and information extraction.
    By making use of the processing capabilities of both the CPU and the accelerator
    units that are available on a computer, a programmer can increase the degree of
    parallel execution in their application, resulting in improved performance and
    scalability.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we introduce heterogeneous computing using hardware accelerators
    to support general-purpose parallel computing. We focus on GPUs and the CUDA programming
    language.
  prefs: []
  type: TYPE_NORMAL
- en: 15.1.1 Hardware Accelerators
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to the CPU, computers have other processing units that are designed
    to perform specific tasks. These units are not general-purpose processing units
    like the CPU, but are special-purpose hardware that is optimized to implement
    functionality that is specific to certain devices or that is used to perform specialized
    types of processing in the system. FPGAs, Cell processors, and GPUs are three
    examples of these types of processing units.
  prefs: []
  type: TYPE_NORMAL
- en: FPGAs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An FPGA is an integrated circuit that consists of gates, memory, and interconnection
    components. They are reprogrammable, meaning that they can be reconfigured to
    implement specific functionality in hardware, and they are often used to prototype
    application-specific integrated circuits (ASICs). FPGAs typically require less
    power to run than a full CPU, resulting in energy-efficient operation. Some example
    ways in which FPGAs are integrated into a computer system include as device controllers,
    for sensor data processing, for cryptography, and for testing new hardware designs
    (because they are reprogrammable, designs can be implemented, debugged, and tested
    on an FPGA). FPGAs can be designed as a circuit with a high number of simple processing
    units. FPGAs are also low-latency devices that can be directly connected to system
    buses. As a result, they have been used to implement very fast parallel computation
    that consists of regular patterns of independent parallel processing on several
    data input channels. However, reprogramming FPGAs takes a long time, and their
    use is limited to supporting fast execution of specific parts of parallel workloads
    or for running a fixed program workload.^([2](ch15.xhtml#fn15_2))
  prefs: []
  type: TYPE_NORMAL
- en: GPUs and Cell Processors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A Cell processor is a multicore processor that consists of one general-purpose
    processor and multiple coprocessors that are specialized to accelerate a specific
    type of computation, such as multimedia processing. The Sony PlayStation 3 gaming
    system was the first Cell architecture, using the Cell coprocessors for fast graphics.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs perform computer graphics computations—they operate on image data to enable
    high-speed graphics rendering and image processing. A GPU writes its results to
    a frame buffer, which delivers the data to the computer’s display. Driven by computer
    gaming applications, today sophisticated GPUs come standard in desktop and laptop
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the mid 2000s, parallel computing researchers recognized the potential of
    using accelerators in combination with a computer’s CPU cores to support general-purpose
    parallel computing.
  prefs: []
  type: TYPE_NORMAL
- en: 15.1.2 GPU Architecture Overview
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GPU hardware is designed for computer graphics and image processing. Historically,
    GPU development has been driven by the video game industry. To support more detailed
    graphics and faster frame rendering, a GPU device consists of thousands of special-purpose
    processors, specifically designed to efficiently manipulate image data, such as
    the individual pixel values of a two-dimensional image, in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: The hardware execution model implemented by GPUs is *single instruction*/*multiple
    thread* (SIMT), a variation of SIMD. SIMT is like multithreaded SIMD, where a
    single instruction is executed in lockstep by multiple threads running on the
    processing units. In SIMT, the total number of threads can be larger than the
    total number of processing units, requiring the scheduling of multiple groups
    of threads on the processors to execute the same sequence of instructions.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, NVIDIA GPUs consist of several streaming multiprocessors (SMs),
    each of which has its own execution control units and memory space (registers,
    L1 cache, and shared memory). Each SM consists of several scalar processor (SP)
    cores. The SM includes a warp scheduler that schedules *warps*, or sets of application
    threads, to execute in lockstep on its SP cores. In lockstep execution, each thread
    in a warp executes the same instruction each cycle but on different data. For
    example, if an application is changing a color image to grayscale, then each thread
    in a warp executes the same sequence of instructions at the same time to set a
    pixel’s RGB value to its grayscale equivalent. Each thread in the warp executes
    these instructions on a different pixel data value, resulting in multiple pixels
    of the image being updated in parallel. Because the threads are executed in lockstep,
    the processor design can be simplified so that multiple cores share the same instruction
    control units. Each unit contains cache memory and multiple registers that it
    uses to hold data as it’s manipulated in lockstep by the parallel processing cores.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-2](ch15.xhtml#ch15fig2) shows a simplified GPU architecture that
    includes a detailed view of one of its SM units. Each SM consists of multiple
    SP cores, a warp scheduler, an execution control unit, an L1 cache, and shared
    memory space.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/15fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-2: An example of a simplified GPU architecture with 2,048 cores.
    This shows the GPU divided into 64 SM units, and the details of one SM consisting
    of 32 SP cores. The SM’s warp scheduler schedules thread warps on its SPs. A warp
    of threads executes in lockstep on the SP cores.*'
  prefs: []
  type: TYPE_NORMAL
- en: 15.1.3 GPGPU Computing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*General Purpose GPU* (GPGPU) computing applies special-purpose GPU processors
    to general-purpose parallel computing tasks. GPGPU computing combines computation
    on the host CPU cores with SIMT computation on the GPU processors. GPGPU computing
    performs best on parallel applications (or parts of applications) that can be
    constructed as a stream processing computation on a grid of multidimensional data.'
  prefs: []
  type: TYPE_NORMAL
- en: The host operating system does not manage the GPU’s processors or memory. As
    a result, space for program data needs to be allocated on the GPU and the data
    copied between the host memory and the GPU memory by the programmer. GPGPU programming
    languages and libraries typically provide programming interfaces to GPU memory
    that hide some or all of the difficulty of explicitly managing GPU memory from
    the programmer. For example, in CUDA a programmer can include calls to CUDA library
    functions to explicitly allocate CUDA memory on the GPU and to copy data between
    CUDA memory on the GPU and host memory. A CUDA programmer can also use CUDA unified
    memory, which is CUDA’s abstraction of a single memory space on top of host and
    GPU memory. CUDA unified memory hides the separate GPU and host memory, and the
    memory copies between the two, from the CUDA programmer.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs also provide limited support for thread synchronization, which means that
    GPGPU parallel computing performs particularly well for parallel applications
    that are either embarrassingly parallel or have large extents of independent parallel
    stream-based computation with very few synchronization points. GPUs are massively
    parallel processors, and any program that performs long sequences of independent
    identical (or mostly identical) computation steps on data may perform well as
    a GPGPU parallel application. GPGPU computing also performs well when there are
    few memory copies between host and device memory. If GPU–CPU data transfer dominates
    execution time, or if an application requires fine-grained synchronization, GPGPU
    computing may not perform well or provide much, if any, gain over a multithreaded
    CPU version of the program.
  prefs: []
  type: TYPE_NORMAL
- en: 15.1.4 CUDA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA (Compute Unified Device Architecture)^([3](ch15.xhtml#fn15_3)) is NVIDIA’s
    programming interface for GPGPU computing on its graphics devices. CUDA is designed
    for heterogeneous computing in which some program functions run on the host CPU,
    and others run on the GPU device. Programmers typically write CUDA programs in
    C or C++ with annotations that specify CUDA kernel functions, and they make calls
    to CUDA library functions to manage GPU device memory. A CUDA *kernel function*
    is a function that is executed on the GPU, and a CUDA *thread* is the basic unit
    of execution in a CUDA program. CUDA threads are scheduled in warps that execute
    in lockstep on the GPU’s SMs, executing CUDA kernel code on their part of data
    stored in GPU memory. Kernel functions are annotated with `__global__` to distinguish
    them from host functions. CUDA `__device__` functions are helper functions that
    can be called from a CUDA kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The memory space of a CUDA program is separated into host and GPU memory. The
    program must explicitly allocate and free GPU memory space to store program data
    manipulated by CUDA kernels. The CUDA programmer must either explicitly copy data
    to and from the host and GPU memory, or use CUDA unified memory that presents
    a view of memory space that is directly shared by the GPU and host. Here is an
    example of CUDA’s basic memory allocation, memory deallocation, and explicit memory
    copy functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'CUDA threads are organized into *blocks*, and the blocks are organized into
    a *grid*. Grids can be organized into one-, two-, or three-dimensional groupings
    of blocks. Blocks, likewise, can be organized into one-, two-, or three-dimensional
    groupings of threads. Each thread is uniquely identified by its thread (*x*,*y*,*z*)
    position in its containing block’s (*x*,*y*,*z*) position in the grid. For example,
    a programmer could define two-dimensional block and grid dimensions as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When a kernel is invoked, its blocks/grid and thread/block layout is specified
    in the call. For example, here is a call to a kernel function named `do_something`
    specifying the grid and block layout using `gridDim` and `blockDim` defined above
    (and passing parameters `dev_array` and 100):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 15-3](ch15.xhtml#ch15fig3) shows an example of a two-dimensional arrangement
    of thread blocks. In this example, the grid is a 3 × 2 array of blocks, and each
    block is a 4× 3 array of threads.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/15fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-3: The CUDA thread model. A grid of blocks of threads. Blocks and
    threads can be organized into one-, two-, or three-dimensional layouts. This example
    shows a grid of two-dimensional blocks, 3 × 2 blocks per grid, and each block
    has a two-dimensional set of threads, 4 × 3 threads per block).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'A thread’s position in this layout is given by the (*x*,*y*) coordinate in
    its containing block (`threadId.x`, `threadId.y`) and by the (*x*,*y*) coordinate
    of its block in the grid (`blockIdx.x`, `blockIdx.y`). Note that block and thread
    coordinates are (*x*,*y*) based, with the *x*-axis being horizontal, and the *y*-axis
    vertical. The (0,0) element is in the upper left. The CUDA kernel also has variables
    that are defined to the block dimensions (`blockDim.x` and `blockDim.y`). Thus,
    for any thread executing the kernel, its (row, col) position in the two-dimensional
    array of threads in the two-dimensional array of blocks can be logically identified
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Although not strictly necessary, CUDA programmers often organize blocks and
    threads to match the logical organization of program data. For example, if a program
    is manipulating a two-dimensional matrix, it often makes sense to organize threads
    and blocks into a two-dimensional arrangement. This way, a thread’s block (*x*,*y*)
    and its thread (*x*,*y*) within a block can be used to associate a thread’s position
    in the two-dimensional blocks of threads with one or more data values in the two-dimensional
    array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example CUDA Program: Scalar Multiply'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As an example, consider a CUDA program that performs scalar multiplication
    of a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Because the program data comprises one-dimensional arrays, using a one-dimensional
    layout of blocks/grid and threads/block works well. This is not necessary, but
    it makes the mapping of threads to data easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'When run, the main function of this program will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Allocate host-side memory for the vector `x` and initialize it.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Allocate device-side memory for the vector `x` and copy it from host memory
    to GPU memory.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Invoke a CUDA kernel function to perform vector scalar multiply in parallel,
    passing as arguments the device address of the vector `x` and the scalar value
    `a`.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Copy the result from GPU memory to host memory vector `x`.
  prefs: []
  type: TYPE_NORMAL
- en: In the example that follows, we show a CUDA program that performs these steps
    to implement scalar vector multiplication. We have removed some error handling
    and details from the code listing, but the full solution is available online.^([4](ch15.xhtml#fn15_4))
  prefs: []
  type: TYPE_NORMAL
- en: 'The main function of the CUDA program performs the aforementioned steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Each CUDA thread executes the CUDA kernel function `scalar_multiply`. A CUDA
    kernel function is written from an individual thread’s point of view. It typically
    consists of two main steps: (1) the calling thread determines which portion of
    the data it is responsible for based on its thread’s position in its enclosing
    block and its block’s position in the grid; (2) the calling thread performs application-specific
    computation on its portion of the data. In this example, each thread is responsible
    for computing scalar multiplication on exactly one element in the array. The kernel
    function code first calculates a unique index value based on the calling thread’s
    block and thread identifier. It then uses this value as an index into the array
    of data to perform scalar multiplication on its array element (`array[index] =`
    `array[index] * scalar`). CUDA threads running on the GPU’s SM units each compute
    a different index value to update array elements in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: CUDA Thread Scheduling and Synchronization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Each CUDA thread block is run by a GPU SM unit. An SM schedules a warp of threads
    from the same thread block to run its processor cores. All threads in a warp execute
    the same set of instructions in lockstep, typically on different data. Threads
    share the instruction pipeline but get their own registers and stack space for
    local variables and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Because blocks of threads are scheduled on individual SMs, increasing the threads
    per block increases the degree of parallel execution. Because the SM schedules
    thread warps to run on its processing units, if the number of threads per block
    is a multiple of the warp size, then no SM processor cores are wasted in the computation.
    In practice, using a number of threads per block that is a small multiple of the
    number of processing cores of an SM works well.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA guarantees that all threads from a single kernel call complete before any
    threads from a subsequent kernel call are scheduled. Thus, there is an implicit
    synchronization point between separate kernel calls. Within a single kernel call,
    however, thread blocks are scheduled to run the kernel code in any order on the
    GPU SMs. As a result, a programmer should not assume any ordering of execution
    between threads in different thread blocks. CUDA provides some support for synchronizing
    threads, but only for threads that are in the same thread block.
  prefs: []
  type: TYPE_NORMAL
- en: 15.1.5 Other Languages for GPGPU Programming
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are other programming languages for GPGPU computing. OpenCL, OpenACC,
    and OpenHMPP are three examples of languages that can be used to program any graphics
    device (they are not specific to NVIDIA devices). OpenCL (Open Computing Language)
    has a similar programming model to CUDA’s; both implement a lower-level programming
    model (or implement a thinner programming abstraction) on top of the target architectures.
    OpenCL targets a wide range of heterogeneous computing platforms that include
    a host CPU combined with other compute units, which could include CPUs or accelerators
    such as GPUs and FPGAs. OpenACC (Open Accelerator) is a higher-level abstraction
    programming model than CUDA or OpenCL. It is designed for portability and programmer
    ease. A programmer annotates portions of their code for parallel execution, and
    the compiler generates parallel code that can run on GPUs. OpenHMPP (Open Hybrid
    Multicore Programming) is another language that provides a higher-level programming
    abstraction for heterogeneous programming.
  prefs: []
  type: TYPE_NORMAL
- en: 15.2 Distributed Memory Systems, Message Passing, and MPI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Chapter 14](ch14.xhtml#ch14) describes mechanisms like Pthreads (see “Hello
    Threading! Writing Your First Multithreaded Program” on [page 677](ch14.xhtml#lev1_106))
    and OpenMP (see “Implicit Threading with OpenMP” on [page 729](ch14.xhtml#lev1_111))
    that programs use to take advantage of multiple CPU cores on a *shared memory
    system*. In such systems, each core shares the same physical memory hardware,
    allowing them to communicate data and synchronize their behavior by reading from
    and writing to shared memory addresses. Although shared memory systems make communication
    relatively easy, their scalability is limited by the number of CPU cores in the
    system.'
  prefs: []
  type: TYPE_NORMAL
- en: As of 2019, high-end commercial server CPUs generally provide a maximum of 64
    cores. For some tasks, though, even a few hundred CPU cores isn’t close enough.
    For example, imagine trying to simulate the fluid dynamics of the Earth’s oceans
    or index the entire contents of the World Wide Web to build a search application.
    Such massive tasks require more physical memory and processors than any single
    computer can provide. Thus, applications that require a large number of CPU cores
    run on systems that forego shared memory. Instead, they execute on systems built
    from multiple computers, each with their own CPU(s) and memory, that communicate
    over a network to coordinate their behavior.
  prefs: []
  type: TYPE_NORMAL
- en: A collection of computers working together is known as a *distributed memory
    system* (or often just *distributed system*).
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning A NOTE ON CHRONOLOGY**'
  prefs: []
  type: TYPE_NORMAL
- en: Despite the order in which they’re presented in this book, systems designers
    built distributed systems long before mechanisms like threads or OpenMP existed.
  prefs: []
  type: TYPE_NORMAL
- en: Some distributed memory systems integrate hardware more closely than others.
    For example, a *supercomputer* is a high-performance system in which many *compute
    nodes* are tightly coupled (closely integrated) to a fast interconnection network.
    Each compute node contains its own CPU(s), GPU(s), and memory, but multiple nodes
    might share auxiliary resources like secondary storage and power supplies. The
    exact level of hardware sharing varies from one supercomputer to another.
  prefs: []
  type: TYPE_NORMAL
- en: On the other end of the spectrum, a distributed application might run on a loosely
    coupled (less integrated) collection of fully autonomous computers (*nodes*) connected
    by a traditional local area network (LAN) technology like Ethernet. Such a collection
    of nodes is known as a *commodity off-the-shelf* (COTS) cluster. COTS clusters
    typically employ a *shared-nothing architecture* in which each node contains its
    own set of computation hardware (i.e., CPU(s), GPU(s), memory, and storage). [Figure
    15-4](ch15.xhtml#ch15fig4) illustrates a shared-nothing distributed system consisting
    of two shared-memory computers.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/15fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-4: The major components of a shared-nothing distributed memory architecture
    built from two compute nodes*'
  prefs: []
  type: TYPE_NORMAL
- en: 15.2.1 Parallel and Distributed Processing Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Application designers often organize distributed applications using tried-and-true
    designs. Adopting application models like these helps developers reason about
    an application because its behavior will conform to well-understood norms. Each
    model has its unique benefits and drawbacks—there’s no one-size-fits-all solution.
    We briefly characterize a few of the more common models in the subsections that
    follow, but note that we’re not presenting an exhaustive list.
  prefs: []
  type: TYPE_NORMAL
- en: Client/Server
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The *client/server model* is an extremely common application model that divides
    an application’s responsibilities among two actors: client processes and server
    processes. A server process provides a service to clients that ask for something
    to be done. Server processes typically wait at well-known addresses to receive
    incoming connections from clients. Upon making a connection, a client sends requests
    to the server process, which either satisfies those requests (e.g., by fetching
    a requested file) or reports an error (e.g., the file doesn’t exist or the client
    can’t be properly authenticated).'
  prefs: []
  type: TYPE_NORMAL
- en: Although you may not have considered it, you access web pages via the client/server
    model! Your web browser (client) connects to a website (server) at a public address
    (e.g., `[diveintosystems.org](http://diveintosystems.org)`) to retrieve the page’s
    contents.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The *pipeline model* divides an application into a distinct sequence of steps,
    each of which can process data independently. This model works well for applications
    whose workflow involves linear, repetitive tasks over large data inputs. For example,
    consider the production of computer-animated films. Each frame of the film must
    be processed through a sequence of steps that transform the frame (e.g., adding
    textures or applying lighting). Because each step happens independently in a sequence,
    animators can speed up rendering by processing frames in parallel across a large
    cluster of computers.
  prefs: []
  type: TYPE_NORMAL
- en: Boss/Worker
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the *boss/worker model*, one process acts as a central coordinator and distributes
    work among the processes at other nodes. This model works well for problems that
    require processing a large, divisible input. The boss divides the input into smaller
    pieces and assigns one or more pieces to each worker. In some applications, the
    boss might statically assign each worker exactly one piece of the input. In other
    cases, the workers might repeatedly finish a piece of the input and then return
    to the boss to dynamically retrieve the next input chunk. Later in this section,
    we’ll present an example program in which a boss divides an array among many workers
    to perform scalar multiplication on an array.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this model is sometimes called other names, like “master/ worker”
    or other variants, but the main idea is the same.
  prefs: []
  type: TYPE_NORMAL
- en: Peer-to-Peer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Unlike the boss/worker model, a *peer-to-peer* application avoids relying on
    a centralized control process. Instead, peer processes self-organize the application
    into a structure in which they each take on roughly the same responsibilities.
    For example, in the BitTorrent file sharing protocol, each peer repeatedly exchanges
    parts of a file with others until they’ve all received the entire file.
  prefs: []
  type: TYPE_NORMAL
- en: Lacking a centralized component, peer-to-peer applications are generally robust
    to node failures. On the other hand, peer-to-peer applications typically require
    complex coordination algorithms, making them difficult to build and rigorously
    test.
  prefs: []
  type: TYPE_NORMAL
- en: 15.2.2 Communication Protocols
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Whether they are part of a supercomputer or a COTS cluster, processes in a distributed
    memory system communicate via *message passing*, whereby one process explicitly
    sends a message to processes on one or more other nodes, which receive it. It’s
    up to the applications running on the system to determine how to utilize the network—some
    applications require frequent communication to tightly coordinate the behavior
    of processes across many nodes, whereas other applications communicate to divide
    up a large input among processes and then mostly work independently.
  prefs: []
  type: TYPE_NORMAL
- en: 'A distributed application formalizes its communication expectations by defining
    a communication *protocol*, which describes a set of rules that govern its use
    of the network, including:'
  prefs: []
  type: TYPE_NORMAL
- en: When a process should send a message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To which process(es) it should send the message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to format the message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without a protocol, an application might fail to interpret messages properly
    or even deadlock (see “Deadlock” on [page 700](ch14.xhtml#lev3_120)). For example,
    if an application consists of two processes, and each process waits for the other
    to send it a message, neither process will ever make progress. Protocols add structure
    to communication to reduce the likelihood of such failures.
  prefs: []
  type: TYPE_NORMAL
- en: To implement a communication protocol, applications require basic functionality
    for tasks like sending and receiving messages, naming processes (addressing),
    and synchronizing process execution. Many applications look to the Message Passing
    Interface for such functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 15.2.3 Message Passing Interface
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *Message Passing Interface* (MPI) defines (but does not itself implement)
    a standardized interface that applications can use to communicate in a distributed
    memory system. By adopting the MPI communication standard, applications become
    *portable*, meaning that they can be compiled and executed on many different systems.
    In other words, as long as an MPI implementation is installed, a portable application
    can move from one system to another and expect to execute properly, even if the
    systems have different underlying characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: MPI allows a programmer to divide an application into multiple processes. It
    assigns each of an application’s processes a unique identifier, known as a *rank*,
    which ranges from 0 to *N –* 1 for an application with *N* processes. A process
    can learn its rank by calling the `MPI_Comm_rank` function, and it can learn how
    many processes are executing in the application by calling `MPI_Comm` `_size`.
    To send a message, a process calls `MPI_Send` and specifies the rank of the intended
    recipient. Similarly, a process calls `MPI_Recv` to receive a message, and it
    specifies whether to wait for a message from a specific node or to receive a message
    from any sender (using the constant `MPI_ANY_SOURCE` as the rank).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the basic send and receive functions, MPI also defines a variety
    of functions that make it easier for one process to communicate data to multiple
    recipients. For example, `MPI_Bcast` allows one process to send a message to every
    other process in the application with just one function call. It also defines
    a pair of functions, `MPI_Scatter` and `MPI_Gather`, that allow one process to
    divide up an array and distribute the pieces among processes (scatter), operate
    on the data, and then later retrieve all the data to coalesce the results (gather).
  prefs: []
  type: TYPE_NORMAL
- en: Because MPI *specifies* only a set of functions and how they should behave,
    each system designer can implement MPI’s functionality in a way that matches the
    capabilities of their particular system. For example, a system with an interconnect
    network that supports broadcasting (sending one copy of a message to multiple
    recipients at the same time) might be able to implement MPI’s `MPI_Bcast` function
    more efficiently than a system without such support.
  prefs: []
  type: TYPE_NORMAL
- en: 15.2.4 MPI Hello World
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As an introduction to MPI programming, consider the “Hello World” program^([5](ch15.xhtml#fn15_5))
    presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: When starting this program, MPI simultaneously executes multiple copies of it
    as independent processes across one or more computers. Each process makes calls
    to MPI to determine how many total processes are executing (with `MPI_Comm_size`)
    and which process it is among those processes (the process’s rank, with `MPI_Comm_rank`).
    After looking up this information, each process prints a short message containing
    the rank and name of the computer (`hostname`) it’s running on before terminating.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note RUNNING MPI CODE**'
  prefs: []
  type: TYPE_NORMAL
- en: To run these MPI examples, you’ll need an MPI implementation like OpenMPI^([6](ch15.xhtml#fn15_6))
    or MPICH^([7](ch15.xhtml#fn15_7)) installed on your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compile this example, invoke the `mpicc` compiler program, which executes
    an MPI-aware version of GCC to build the program and link it against MPI libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To execute the program, use the `mpirun` utility to start up several parallel
    processes with MPI. The `mpirun` command needs to be told which computers to run
    processes on (`--hostfile`) and how many processes to run at each machine (`-np`).
    Here, we provide it with a file named `hosts.txt` that tells `mpirun` to create
    four processes across two computers, one named `lemon`, and another named `orange`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Warning MPI EXECUTION ORDER**'
  prefs: []
  type: TYPE_NORMAL
- en: You should *never* make any assumptions about the order in which MPI pro- cesses
    will execute. The processes start up on multiple machines, each of which has its
    own OS and process scheduler. If the correctness of your program requires that
    processes run in a particular order, you must ensure that the proper order occurs—for
    example, by forcing certain processes to pause until they receive a message.
  prefs: []
  type: TYPE_NORMAL
- en: 15.2.5 MPI Scalar Multiplication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For a more substantive MPI example, consider performing scalar multiplication
    on an array. This example adopts the boss/worker model—one process divides the
    array into smaller pieces and distributes them among worker processes. Note that
    in this implementation of scalar multiplication, the boss process also behaves
    as a worker and multiplies part of the array after distributing sections to the
    other workers.
  prefs: []
  type: TYPE_NORMAL
- en: To benefit from working in parallel, each process multiplies just its local
    piece of the array by the scalar value, and then the workers all send the results
    back to the boss process to form the final result. At several points in the program,
    the code checks to see whether the rank of the process is zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This check ensures that only one process (the one with rank 0) plays the role
    of the boss. By convention, MPI applications often choose rank 0 to perform one-time
    tasks because no matter how many processes there are, one will always be given
    rank 0 (even if just a single process is executing).
  prefs: []
  type: TYPE_NORMAL
- en: MPI Communication
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The boss process begins by determining the scalar value and initial input array.
    In a real scientific computing application, the boss would likely read such values
    from an input file. To simplify this example, the boss uses a constant scalar
    value (10) and generates a simple 40-element array (containing the sequence 0
    to 39) for illustrative purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This program requires communication between MPI processes for three important
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The boss sends the scalar value and the size of the array to *all* of the
    workers.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The boss divides the initial array into pieces and sends a piece to each
    worker.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Each worker multiplies the values in its piece of the array by the scalar
    and then sends the updated values back to the boss.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting Important Values
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To send the scalar value to the workers, the example program uses the `MPI_Bcast`
    function, which allows one MPI process to send the same value to all the other
    MPI processes with one function call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This call sends one integer (`MPI_INT`) starting from the address of the `scalar`
    variable from the process with rank 0 to every other process (`MPI_COMM` `_WORLD`).
    All the worker processes (those with nonzero rank) receive the broadcast into
    their local copy of the `scalar` variable, so when this call completes, every
    process knows the scalar value to use.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note MPI_BCAST BEHAVIOR**'
  prefs: []
  type: TYPE_NORMAL
- en: Every process executes `MPI_Bcast`, but it behaves differently depending on
    the rank of the calling process. If the rank matches that of the fourth argument,
    then the caller assumes the role of the sender. All other processes that call
    `MPI_Bcast` act as receivers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the boss broadcasts the total size of the array to every other process.
    After learning the total array size, each process sets a `local_size` variable
    by dividing the total array size by the number of MPI processes. The `local_size`
    variable represents how many elements each worker’s piece of the array will contain.
    For example, if the input array contains 40 elements and the application consists
    of eight processes, each process is responsible for a five-element piece of the
    array (40 / 8 = 5). To keep the example simple, it assumes that the number of
    processes evenly divides the size of the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Distributing the Array
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now that each process knows the scalar value and how many values it’s responsible
    for multiplying, the boss must divide the array into pieces and distribute them
    among the workers. Note that in this implementation, the boss (rank 0) also participates
    as a worker. For example, with a 40-element array and eight processes (ranks 0–7),
    the boss should keep array elements 0–4 for itself (rank 0), send elements 5–9
    to rank 1, elements 10–14 to rank 2, and so on. [Figure 15-5](ch15.xhtml#ch15fig5)
    shows how the boss assigns pieces of the array to each MPI process.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/15fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-5: The distribution of a 40-element array among eight MPI processes
    (ranks 0–7)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'One option for distributing pieces of the array to each worker combines `{MPI_Send}`
    calls at the boss with an `{MPI_Recv}` call at each worker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this code, the boss executes a loop that executes once for each worker process,
    in which it sends the worker a piece of the array. It starts sending data from
    the address of `array` at an offset of `(i * local_size)` to ensure that each
    worker gets a unique piece of the array. That is, the worker with rank 1 gets
    a piece of the array starting at index 5, rank 2 gets a piece of the array starting
    at index 10, etc., as shown in [Figure 15-5](ch15.xhtml#ch15fig5).
  prefs: []
  type: TYPE_NORMAL
- en: Each call to `MPI_Send` sends `local_size` (5) integers worth of data (20 bytes)
    to the process with rank i. The `0` argument toward the end represents a message
    tag, which is an advanced feature that this program doesn’t need—setting it to
    `0` treats all messages equally.
  prefs: []
  type: TYPE_NORMAL
- en: The workers all call `MPI_Recv` to retrieve their piece of the array, which
    they store in memory at the address to which `local_array` refers. They receive
    `local_size` (5) integers worth of data (20 bytes) from the node with rank 0\.
    Note that `MPI_Recv` is a *blocking* call, which means that a process that calls
    it will pause until it receives data. Because the `MPI_Recv` call blocks, no worker
    will proceed until the boss sends its piece of the array.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Execution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: After a worker has received its piece of the array, it can begin multiplying
    each array value by the scalar. Because each worker gets a unique subset of the
    array, they can execute independently, in parallel, without the need to communicate.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating Results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Finally, after workers complete their multiplication, they send the updated
    array values back to the boss, which aggregates the results. Using `MPI_Send`
    and `MPI_Recv`, this process looks similar to the array distribution code we looked
    at earlier, except the roles of sender and receiver are reversed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Recall that `MPI_Recv` *blocks* or pauses execution, so each call in the `for`
    loop causes the boss to wait until it receives a piece of the array from worker
    *i*.
  prefs: []
  type: TYPE_NORMAL
- en: Scatter/Gather
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Although the `for` loops in the previous example correctly distribute data
    with `MPI_Send` and `MPI_Recv`, they don’t succinctly capture the *intent* behind
    them. That is, they appear to MPI as a series of send and receive calls without
    the obvious goal of distributing an array across MPI processes. Because parallel
    applications frequently need to distribute and collect data like this example
    array, MPI provides functions for exactly this purpose: `MPI_Scatter` and `MPI_Gather`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These functions provide two major benefits: they allow the entire code blocks
    in the previous example to each be expressed as a single MPI function call, which
    simplifies the code, and they express the *intent* of the operation to the underlying
    MPI implementation, which may be able to better optimize their performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To replace the first loop in the previous example, each process could call
    `MPI_Scatter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This function automatically distributes the contents of memory starting at `array`
    in pieces containing `local_size` integers to the `local_array` destination variable.
    The `0` argument specifies that the process with rank 0 (the boss) is the sender,
    so it reads and distributes the `array` source to other processes (including sending
    one piece to itself). Every other process acts as a receiver and receives data
    into its `local_array` destination.
  prefs: []
  type: TYPE_NORMAL
- en: 'After this single call, the workers can each multiply the array in parallel.
    When they finish, each process calls `MPI_Gather` to aggregate the results back
    in the boss’s `array` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This call behaves like the opposite of `MPI_Scatter`: this time, the `0` argument
    specifies that the process with rank 0 (the boss) is the receiver, so it updates
    the `array` variable, and workers each send `local_size` integers from their `local_array`
    variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Full Code for MPI Scalar Multiply
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Here’s a full MPI scalar multiply code listing that uses `MPI_Scatter` and `MPI_Gather`:^([8](ch15.xhtml#fn15_8))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the `main` function, the boss sets up the problem and creates an array. If
    this were solving a real problem (e.g., a scientific computing application), the
    boss would likely read its initial data from an input file. After initializing
    the array, the boss needs to send information about the size of the array and
    the scalar to use for multiplication to all the other worker processes, so it
    broadcasts those variables to every process.
  prefs: []
  type: TYPE_NORMAL
- en: Now that each process knows the size of the array and how many processes there
    are, they can each divide to determine how many elements of the array they’re
    responsible for multiplying. For simplicity, this code assumes that the array
    is evenly divisible by the number of processes.
  prefs: []
  type: TYPE_NORMAL
- en: The boss then uses the `MPI_Scatter` function to send an equal portion of the
    array to each worker process (including itself). Now the workers have all the
    information they need, so they each perform multiplication over their portion
    of the array in parallel. Finally, as the workers complete their multiplication,
    the boss collects each worker’s piece of the array using `MPI_Gather` to report
    the final results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compiling and executing this program looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 15.2.6 Distributed Systems Challenges
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In general, coordinating the behavior of multiple processes in distributed systems
    is notoriously difficult. If a hardware component (e.g., CPU or power supply)
    fails in a shared memory system, the entire system becomes inoperable. In a distributed
    system though, autonomous nodes can fail independently. For example, an application
    must decide how to proceed if one node disappears and the others are still running.
    Similarly, the interconnection network could fail, making it appear to each process
    as if all the others failed.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed systems also face challenges due to a lack of shared hardware, namely
    clocks. Due to unpredictable delays in network transmission, autonomous nodes
    cannot easily determine the order in which messages are sent. Solving these challenges
    (and many others) is beyond the scope of this book. Fortunately, distributed software
    designers have constructed several frameworks that ease the development of distributed
    applications. We characterize some of these frameworks in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: MPI Resources
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'MPI is large and complex, and this section hardly scratches the surface. For
    more information about MPI, we suggest:'
  prefs: []
  type: TYPE_NORMAL
- en: The Lawrence Livermore National Lab’s MPI tutorial, by Blaise Barney.^([9](ch15.xhtml#fn15_9))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CSinParallel’s MPI Patterns.^([10](ch15.xhtml#fn15_10))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '15.3 To Exascale and Beyond: Cloud Computing, Big Data, and the Future of Computing'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Advances in technology have made it possible for humanity to produce data at
    a rate never seen before. Scientific instruments such as telescopes, biological
    sequencers, and sensors produce high-fidelity scientific data at low cost. As
    scientists struggle to analyze this “data deluge,” they increasingly rely on sophisticated
    multinode supercomputers, which form the foundation of *high-performance computing*
    (HPC).
  prefs: []
  type: TYPE_NORMAL
- en: HPC applications are typically written in languages like C, C++, or Fortran,
    with multithreading and message passing enabled with libraries such as POSIX threads,
    OpenMP, and MPI. Thus far, the vast majority of this book has described architectural
    features, languages, and libraries commonly leveraged on HPC systems. Companies,
    national laboratories, and other organizations interested in advancing science
    typically use HPC systems and form the core of the computational science ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the proliferation of internet-enabled devices and the ubiquity of
    social media have caused humanity to effortlessly produce large volumes of online
    multimedia, in the form of web pages, pictures, videos, tweets, and social media
    posts. It is estimated that 90% of all online data was produced in the past two
    years, and that society produces 30 terabytes of user data per second (or 2.5
    exabytes per day). The deluge of *user data* offers companies and organizations
    a wealth of information about the habits, interests, and behavior of its users,
    and it facilitates the construction of data-rich customer profiles to better tailor
    commercial products and services. To analyze user data, companies typically rely
    on multinode data centers that share many of the hardware architecture components
    of typical supercomputers. However, these data centers rely on a different software
    stack designed specifically for internet-based data. The computer systems used
    for the storage and analysis of large-scale internet-based data are sometimes
    referred to as *high-end data analysis* (HDA) systems. Companies like Amazon,
    Google, Microsoft, and Facebook have a vested interest in the analysis of internet
    data, and form the core of the data analytics ecosystem. The HDA and data analytics
    revolution started around 2010, and now is a dominant area of cloud computing
    research.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-6](ch15.xhtml#ch15fig6) highlights the key differences in software
    utilized by the HDA and HPC communities. Note that both communities use similar
    cluster hardware that follows a distributed memory model, where each compute node
    typically has one or more multicore processors and frequently a GPU. The cluster
    hardware typically includes a *distributed filesystem* that allows users and applications
    common access to files that reside locally on multiple nodes in the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/15fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-6: Comparison of HDA vs. HPC frameworks. Based on a figure by Jack
    Dongarra and Daniel Reed.^([11](ch15.xhtml#fn15_11))*'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike supercomputers, which are typically built and optimized for HPC use,
    the HDA community relies on *data centers*, which consist of a large collection
    of general-purpose compute nodes typically networked together via Ethernet. At
    a software level, data centers typically employ virtual machines, large distributed
    databases, and frameworks that enable high-throughput analysis of internet data.
    The term *cloud* refers to the data storage and computing power components of
    HDA data centers.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we take a brief look at cloud computing, some of the software
    commonly used to enable cloud computing (specifically MapReduce), and some challenges
    for the future. Please note that this section is not meant to be an in-depth look
    at these concepts; we encourage interested readers to explore the referenced sources
    for greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: 15.3.1 Cloud Computing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Cloud computing* is the use or lease of the cloud for a variety of services.
    Cloud computing enables computing infrastructure to act as a “utility”: a few
    central providers give users and organizations access to (a seemingly infinite
    amount of) compute power through the internet, with users and organizations choosing
    to use as much as they want and paying according to their level of use. Cloud
    computing has three main pillars: software as a service (SaaS), infrastructure
    as a service (IaaS), and platform as a service (PaaS).^([12](ch15.xhtml#fn15_12))'
  prefs: []
  type: TYPE_NORMAL
- en: Software as a Service
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Software as a service* (SaaS) refers to software provided directly to users
    through the cloud. Most people utilize this pillar of cloud computing without
    even realizing it. Applications that many people use daily (e.g., web mail, social
    media, and video streaming) depend upon cloud infrastructure. Consider the classic
    application of web mail. Users are able to log on and access their web mail from
    any device, send and receive mail, and seemingly never run out of storage space.
    Interested organizations can in turn “rent” cloud email services to provide email
    to their own clients and employees, without incurring the hardware and maintenance
    cost of running the service themselves. Services in the SaaS pillar are managed
    completely by cloud providers; organizations and users do not (beyond configuring
    a few settings, perhaps) manage any part of the application, data, software, or
    hardware infrastructure, all which would be necessary if they were trying to set
    up the service on their own hardware. Prior to the advent of cloud computing,
    organizations interested in providing web mail for their users would need their
    own infrastructure and dedicated IT support staff to maintain it. Popular examples
    of SaaS providers include Google’s G Suite and Microsoft’s Office 365.'
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as a Service
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Infrastructure as a service* (IaaS) allows people and organizations to “rent
    out” computational resources to meet their needs, usually in the form of accessing
    virtual machines that are either general purpose or preconfigured for a particular
    application. One classic example is Amazon’s Elastic Compute Cloud (EC2) service
    from Amazon Web Services (AWS). EC2 enables users to create fully customizable
    virtual machines. The term *elastic* in EC2 refers to a user’s ability to grow
    or shrink their compute resource requests as needed, paying as they go. For example,
    an organization may use an IaaS provider to host its website or deploy its own
    series of custom-built applications to users. Some research labs and classrooms
    use IaaS services in lieu of lab machines, running experiments in the cloud or
    offering a virtual platform for their students to learn. In all cases, the goal
    is to eliminate the maintenance and capital needed to maintain a personal cluster
    or server for similar purposes. Unlike use cases in the SaaS pillar, use cases
    in the IaaS pillar require clients to configure applications, data, and in some
    cases the virtual machine’s OS itself. However, the host OS and hardware infrastructure
    is set up and managed by the cloud provider. Popular IaaS providers include Amazon
    AWS, Google Cloud Services, and Microsoft Azure.'
  prefs: []
  type: TYPE_NORMAL
- en: Platform as a Service
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Platform as a service* (PaaS) allows individuals and organizations to develop
    and deploy their own web applications for the cloud, eliminating the need for
    local configuration or maintenance. Most PaaS providers enable developers to write
    their applications in a variety of languages and offer a choice of APIs to use.
    For example, Microsoft Azure’s service allows users to code web applications in
    the Visual Studio IDE and deploy their applications to Azure for testing. Google
    App Engine enables developers to build and test custom mobile applications in
    the cloud in a variety of languages. Heroku and CloudBees are other prominent
    examples. Note that developers have control over their applications and data only;
    the cloud provider controls the rest of the software infrastructure and all of
    the underlying hardware infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: 15.3.2 MapReduce
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Perhaps the most famous programming paradigm used on cloud systems is MapReduce.^([13](ch15.xhtml#fn15_13))
    Although MapReduce’s origins lay in functional programming’s Map and Reduce operations,
    Google was the first to apply the concept to analyzing large quantities of web
    data. MapReduce enabled Google to perform web queries faster than its competitors,
    and enabled Google’s meteoric rise as the go-to web service provider and internet
    giant it is today.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Map and Reduce Operations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `map` and `reduce` functions in the MapReduce paradigm are based on the
    mathematical operations of Map and Reduce from functional programming. In this
    section, we briefly discuss how these mathematical operations work by revisiting
    some examples presented earlier in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Map operation typically applies the same function to all the elements in
    a collection. Readers familiar with Python may recognize this functionality most
    readily in the list comprehension feature in Python. For example, the following
    two code snippets perform scalar multiplication in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: Regular scalar multiply
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Scalar multiply with list comprehension
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The list comprehension applies the same function (in this case, multiplying
    an array element with scalar value `s`) to every element `x` in `array`.
  prefs: []
  type: TYPE_NORMAL
- en: A single Reduce operation takes a collection of elements and combines them together
    into a single value using some common function. For example, the Python function
    `sum` acts similarly to a Reduce operation, as it takes a collection (typically
    a Python list) and combines all the elements together using addition. So, for
    example, applying addition to all the elements in the `result` array returned
    from the `scalarMultiply` function yields a combined sum of 50.
  prefs: []
  type: TYPE_NORMAL
- en: The MapReduce Programming Model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A key feature of MapReduce is its simplified programming model. Developers need
    to implement only two types of functions, `map` and `reduce`; the underlying MapReduce
    framework automates the rest of the work.
  prefs: []
  type: TYPE_NORMAL
- en: The programmer-written `map` function takes an input (*key*,*value*) pair and
    outputs a series of intermediate (*key*,*value*) pairs that are written to a distributed
    filesystem shared among all the nodes. A combiner that is typically defined by
    the MapReduce framework then aggregates (*key*,*value*) pairs by key, to produce
    (*key*,list(*value*)) pairs that are passed to the programmer-defined `reduce`
    function. The `reduce` function then takes as input a (*key*,list(*value*)) pair
    and combines all the values together through some programmer-defined operation
    to form a final (*key*,*value*), where the *value* in this output corresponds
    to the result of the reduction operation. The output from the `reduce` function
    is written to the distributed filesystem and usually output to the user.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how to use the MapReduce model to parallelize a program, we discuss
    the Word Frequency program. The goal of Word Frequency is to determine the frequency
    of each word in a large text corpus.
  prefs: []
  type: TYPE_NORMAL
- en: A C programmer may implement the following `map` function for the Word Frequency
    program:^(13)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This `map` function receives as input a string (`key`) that corresponds to the
    name of the file, and a separate string (`value`) that contains a component of
    file data. The function then parses words from the input `value` and emits each
    word (`words[i]`) separately with the string value `"1"`. The `emit` function
    is provided by the MapReduce framework and writes the intermediate (*key*,*value*)
    pairs to the distributed filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete the Word Frequency program, a programmer may implement the following
    `reduce` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This `reduce` function receives as input a string (`key`) that corresponds to
    a particular word, and an `Iterator` struct (again, provided by the MapReduce
    framework) that consists of an aggregated array of items associated with the key
    (`items`), and the length of that array (`length`). In the Word Frequency application,
    `items` corresponds to a list of counts. The function then extracts the number
    of words from the `length` field of the `Iterator` struct, and the array of counts
    from the `items` field. It then loops over all the counts, aggregating the values
    into the variable `total`. Since the `emit` function requires `char *` parameters,
    the function converts `total` to a string prior to calling `emit`.
  prefs: []
  type: TYPE_NORMAL
- en: After implementing `map` and `reduce`, the programmer’s responsibility ends.
    The MapReduce framework automates the rest of the work, including partitioning
    the input, generating and managing the processes that run the `map` function (map
    tasks), aggregating and sorting intermediate (*key*,*value*) pairs, generating
    and managing the separate processes that run the `reduce` function (reduce tasks),
    and generating a final output file.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, in [Figure 15-7](ch15.xhtml#ch15fig7) we illustrate how MapReduce
    parallelizes the opening lines of the popular Jonathan Coulton song “Code Monkey”:
    *code monkey get up get coffee, code monkey go to job*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/15fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-7: Parallelization of the opening lines of the song “Code Monkey”
    using the MapReduce framework*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-7](ch15.xhtml#ch15fig7) gives an overview of this process. Prior
    to execution, the boss node first partitions the input into *M* parts, where *M*
    corresponds to the number of map tasks. In [Figure 15-7](ch15.xhtml#ch15fig7),
    *M* = 3, and the input file (`coulton.txt`) is split into three parts. During
    the map phase, the boss node distributes the map tasks among one or more worker
    nodes, with each map task executing independently and in parallel. For example,
    the first map task parses the snippet *code monkey get up* into separate words
    and emits the following four (*key*,*value*) pairs: (`code`,`1`), (`monkey`,`1`),
    (`get`,`1`), (`up`,`1`). Each map task then emits its intermediate values to a
    distributed filesystem that takes up a certain amount of storage on each node.'
  prefs: []
  type: TYPE_NORMAL
- en: Prior to the start of the reduce phase, the framework aggregates and combines
    the intermediate (*key*,*value*) pairs into (*key*,list(*value*)) pairs. In [Figure
    15-7](ch15.xhtml#ch15fig7), for example, the (*key*,*value*) pair (`get`,`1`)
    is emitted by two separate map tasks. The MapReduce framework aggregates these
    separate (*key*,*value*) pairs into the single (*key*,list(*value*)) pair (`get`,`[1,1]`).
    The aggregated intermediate pairs are written to the distributed filesystem on
    disk.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the MapReduce framework directs the boss node to generate *R* reduce tasks.
    In [Figure 15-7](ch15.xhtml#ch15fig7), *R* = 8\. The framework then distributes
    the tasks among its worker nodes. Once again, each reduce task executes independently
    and in parallel. In the reduce phase of this example, the (*key*,list(*value*))
    pair (`get`,`[1,1]`) is reduced to the (*key*,*value*) pair (`get`,`2`). Each
    worker node appends the output of its set of reduce tasks to a final file, which
    is available to the user upon completion.
  prefs: []
  type: TYPE_NORMAL
- en: Fault Tolerance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Data centers typically contain thousands of nodes. Consequently, the rate of
    failure is high; consider that if an individual node in a data center has a 2%
    chance of hardware failure, there is a greater than 99.99% chance that some node
    in a 1,000-node data center will fail. Software written for data centers must
    therefore be *fault tolerant*, meaning that it must be able to continue operation
    in the face of hardware failures (or else fail gracefully).
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce was designed with fault tolerance in mind. For any MapReduce run,
    there is one boss node and potentially thousands of worker nodes. The chance that
    a worker node will fail is therefore high. To remedy this, the boss node pings
    individual worker nodes periodically. If the boss node does not receive a response
    from a worker node, the boss redistributes the worker’s assigned workload to a
    different node and re-executes the task.^(13) If the boss node fails (a low probability
    given that it is only one node), the MapReduce job aborts and must be rerun on
    a separate node. Note that sometimes a worker node may fail to respond to the
    boss node’s pings because the worker is bogged down by tasks. MapReduce therefore
    uses the same pinging and work redistribution strategy to limit the effect of
    slow (or straggler) worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop and Apache Spark
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The development of MapReduce took the computing world by storm. However, Google’s
    implementation of MapReduce is closed source. As a result, engineers at Yahoo!
    developed Hadoop,^([14](ch15.xhtml#fn15_14)) an open source implementation of
    MapReduce, which was later adopted by the Apache Foundation. The Hadoop project
    consists of an ecosystem of tools for Apache Hadoop, including the Hadoop Distributed
    File System or HDFS (an open source alternative to Google File System), and HBase
    (modeled after Google’s BigTable).
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop has a few key limitations. First, it is difficult to chain multiple MapReduce
    jobs together into a larger workflow. Second, the writing of intermediates to
    the HDFS proves to be a bottleneck, especially for small jobs (smaller than one
    gigabyte). Apache Spark^([15](ch15.xhtml#fn15_15)) was designed to address these
    issues, among others. Due to its optimizations and ability to largely process
    intermediate data in memory, Apache Spark is up to 100 times faster than Hadoop
    on some applications.^([16](ch15.xhtml#fn15_16))
  prefs: []
  type: TYPE_NORMAL
- en: '15.3.3 Looking Toward the Future: Opportunities and Challenges'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Despite the innovations in the internet data analytics community, the amount
    of data produced by humanity continues to grow. Most new data is produced in so-called
    *edge environments*, or near sensors and other data-generating instruments that
    are by definition on the other end of the network from commercial cloud providers
    and HPC systems. Traditionally, scientists and practitioners gather data and analyze
    it using a local cluster, or they move it to a supercomputer or data center for
    analysis. This “centralized” view of computing is no longer a viable strategy
    as improvements in sensor technology have exacerbated the data deluge.
  prefs: []
  type: TYPE_NORMAL
- en: One reason for this explosive growth is the proliferation of small internet-enabled
    devices that contain a variety of sensors. These *Internet of Things* (IoT) devices
    have led to the generation of large and diverse datasets in edge environments.
    Transferring large datasets from the edge to the cloud is difficult, as larger
    datasets take more time and energy to move. To mitigate the logistic issues of
    so-called “Big Data,” the research community has begun to create techniques that
    aggressively summarize data at each transfer point between the edge and the cloud.^([17](ch15.xhtml#fn15_17))
    There is intense interest in the computing research community in creating infrastructure
    that is capable of processing, storing, and summarizing data in edge environments
    in a unified platform; this area is known as *edge* (or *fog*) computing. Edge
    computing flips the traditional analysis model of Big Data; instead of analysis
    occurring at the supercomputer or data center (“last mile”), analysis instead
    occurs at the source of data production (“first mile”).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to data movement logistics, the other cross-cutting concern for
    the analysis of Big Data is power management. Large, centralized resources such
    as supercomputers and data centers require a lot of energy; modern supercomputers
    require several megawatts (million watts) to power and cool. An old adage in the
    supercomputing community is that “a megawatt costs a megabuck”; in other words,
    it costs roughly $1 million annually to maintain the power requirement of one
    megawatt.^([18](ch15.xhtml#fn15_18)) Local data processing in edge environments
    helps mitigate the logistical issue of moving large datasets, but the computing
    infrastructure in such environments must likewise use the minimal energy possible.
    At the same time, increasing the energy efficiency of large supercomputers and
    data centers is paramount.
  prefs: []
  type: TYPE_NORMAL
- en: There is also interest in figuring out ways to converge the HPC and cloud computing
    ecosystems to create a common set of frameworks, infrastructure and tools for
    large-scale data analysis. In recent years, many scientists have used techniques
    and tools developed by researchers in the cloud computing community to analyze
    traditional HPC datasets, and vice versa. Converging these two software ecosystems
    will allow for the cross-pollination of research and lead to the development of
    a unified system that allows both communities to tackle the coming onslaught of
    data and potentially share resources. The Big Data Exascale Computing (BDEC) working
    group^([19](ch15.xhtml#fn15_19)) argues that instead of seeing HPC and cloud computing
    as two fundamentally different paradigms, it is perhaps more useful to view cloud
    computing as a “digitally empowered” phase of scientific computing, in which data
    sources are increasingly generated over the internet.^(17) In addition, a convergence
    of culture, training, and tools is necessary to fully integrate the HPC and cloud
    computing software and research communities. BDEC also suggests a model in which
    supercomputers and data centers are “nodes” in a very large network of computing
    resources, all working in concert to deal with data flooding from multiple sources.
    Each node aggressively summarizes the data flowing to it, releasing it to a larger
    computational resource node only when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: As the cloud computing and HPC ecosystems look for unification and gird themselves
    against an increasing onslaught of data, the future of computer systems brims
    with exciting possibilities. New fields like artificial intelligence and quantum
    computing are leading to the creation of new *domain-specific architectures* (DSAs)
    and *application-specific integrated circuits* (ASICS) that will be able to handle
    custom workflows more energy efficiently than before (see the TPU^([20](ch15.xhtml#fn15_20))
    for one example). In addition, the security of such architectures, long overlooked
    by the community, will become critical as the data they analyze increases in importance.
    New architectures will also lead to new languages needed to program them, and
    perhaps even new operating systems to manage their various interfaces. To learn
    more about what the future of computer architecture may look like, we encourage
    readers to peruse an article by the 2017 ACM Turing Award winners and computer
    architecture giants, John Hennessy and David Patterson.^([21](ch15.xhtml#fn15_21))
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[1.](ch15.xhtml#rfn15_1) Sparsh Mittal, “A Survey Of Techniques for Architecting
    and Managing Asymmetric Multicore Processors,” *ACM Computing Surveys* 48(3),
    February 2016.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2.](ch15.xhtml#rfn15_2) “FPGAs and the Road to Reprogrammable HPC,” inside
    HPC, July 2019, *[https://insidehpc.com/2019/07/fpgas-and-the-road-to-reprogrammable-hpc/](https://insidehpc.com/2019/07/fpgas-and-the-road-to-reprogrammable-hpc/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[3.](ch15.xhtml#rfn15_3) “GPU Programming,” from CSinParallel: *[https://csinparallel.org/csinparallel/modules/gpu_programming.html](https://csinparallel.org/csinparallel/modules/gpu_programming.html)*;
    CSinParallel has other GPU programming modules: *[https://csinparallel.org](https://csinparallel.org)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[4.](ch15.xhtml#rfn15_4) *[https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_cuda.cu](https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_cuda.cu)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[5.](ch15.xhtml#rfn15_5) *[https://diveintosystems.org/book/C15-Parallel/_attachments/hello_world_mpi.c](https://diveintosystems.org/book/C15-Parallel/_attachments/hello_world_mpi.c)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[6.](ch15.xhtml#rfn15_6) *[https://www.open-mpi.org/](https://www.open-mpi.org/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[7.](ch15.xhtml#rfn15_7) *[https://www.mpich.org/](https://www.mpich.org/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[8.](ch15.xhtml#rfn15_8) Available at *[https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_mpi.c](https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_mpi.c)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[9.](ch15.xhtml#rfn15_9) *[https://hpc-tutorials.llnl.gov/mpi/](https://hpc-tutorials.llnl.gov/mpi/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[10.](ch15.xhtml#rfn15_10) *[http://selkie.macalester.edu/csinparallel/modules/Patternlets/build/html/MessagePassing/MPI_Patternlets.html](http://selkie.macalester.edu/csinparallel/modules/Patternlets/build/html/MessagePassing/MPI_Patternlets.html)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[11.](ch15.xhtml#rfn15_11) D. A. Reed and J. Dongarra, “Exascale Computing
    and Big Data,” *Communications of the ACM* 58(7), 56–68, 2015.'
  prefs: []
  type: TYPE_NORMAL
- en: '[12.](ch15.xhtml#rfn15_12) M. Armbrust et al., “A View of Cloud Computing,”
    *Communications of the ACM* 53(4), 50–58, 2010.'
  prefs: []
  type: TYPE_NORMAL
- en: '[13.](ch15.xhtml#rfn15_13) Jeffrey Dean and Sanjay Ghemawat, “MapReduce: Simplified
    Data Processing on Large Clusters,” *Proceedings of the Sixth Conference on Operating
    Systems Design and Implementation*, Vol. 6, USENIX, 2004.'
  prefs: []
  type: TYPE_NORMAL
- en: '[14.](ch15.xhtml#rfn15_14) *[https://hadoop.apache.org/](https://hadoop.apache.org/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[15.](ch15.xhtml#rfn15_15) *[https://spark.apache.org/](https://spark.apache.org/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[16.](ch15.xhtml#rfn15_16) DataBricks, “Apache Spark,” *[https://databricks.com/spark/about](https://databricks.com/spark/about)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[17.](ch15.xhtml#rfn15_17) M. Asch et al., “Big Data and Extreme-Scale Computing:
    Pathways to Convergence – Toward a shaping strategy for a future software and
    data ecosystem for scientific inquiry,” *The International Journal of High Performance
    Computing Applications* 32(4), 435–479, 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[18.](ch15.xhtml#rfn15_18) M. Halper, “Supercomputing’s Super Energy Needs,
    and What to Do About Them,” CACM News, *[https://cacm.acm.org/news/192296-supercomputings-super-energy-needs-and-what-to-do-about-them/fulltext](https://cacm.acm.org/news/192296-supercomputings-super-energy-needs-and-what-to-do-about-them/fulltext)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[19.](ch15.xhtml#rfn15_19) *[https://www.exascale.org/bdec/](https://www.exascale.org/bdec/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[20.](ch15.xhtml#rfn15_20) N. P. Jouppi et al., “In-Datacenter Performance
    Analysis of a Tensor Processing Unit,” *Proceedings of the 44th Annual International
    Symposium on Computer Architecture*, ACM, 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[21.](ch15.xhtml#rfn15_21) J. Hennessy and D. Patterson, “A New Golden Age
    for Computer Architecture,” *Communications of the ACM* 62(2), 48–60, 2019.'
  prefs: []
  type: TYPE_NORMAL
