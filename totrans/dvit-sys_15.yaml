- en: '15'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '15'
- en: '**LOOKING AHEAD: OTHER PARALLEL SYSTEMS AND PARALLEL PROGRAMMING MODELS**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**展望未来：其他并行系统和并行编程模型**'
- en: '![image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common.jpg)'
- en: In the previous chapter, we discussed shared memory parallelism and multithreaded
    programming. In this chapter, we introduce other parallel programming models and
    languages for different classes of architecture. Namely, we introduce parallelism
    for hardware accelerators focusing on graphics processing units (GPUs) and general-purpose
    computing on GPUs (GPGPU computing), using CUDA as an example; distributed memory
    systems and message passing, using MPI as an example; and cloud computing, using
    MapReduce and Apache Spark as examples.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了共享内存并行性和多线程编程。在本章中，我们介绍了适用于不同架构类别的其他并行编程模型和语言。具体而言，我们介绍了面向硬件加速器的并行性，重点讨论图形处理单元（GPU）和基于GPU的通用计算（GPGPU计算），以CUDA为例；分布式内存系统和消息传递，以MPI为例；以及云计算，举例说明MapReduce和Apache
    Spark。
- en: 'A Whole New World: Flynn’s Taxonomy of Architecture'
  id: totrans-4
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 全新世界：弗林架构分类法
- en: '*Flynn’s taxonomy* is commonly used to describe the ecosystem of modern computing
    architecture ([Figure 15-1](ch15.xhtml#ch15fig1)).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*弗林分类法*通常用于描述现代计算架构的生态系统（[图15-1](ch15.xhtml#ch15fig1)）。'
- en: '![image](../images/15fig01.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/15fig01.jpg)'
- en: '*Figure 15-1: Flynn’s taxonomy classifies the ways in which a processor applies
    instructions.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-1：弗林分类法对处理器如何应用指令进行了分类。*'
- en: The horizontal axis refers to the data stream, whereas the vertical axis refers
    to the instruction stream. A *stream* in this context is a flow of data or instructions.
    A *single stream* issues one element per time unit, similar to a queue. In contrast,
    *multiple streams* typically issue many elements per time unit (think of multiple
    queues). Thus, a single instruction stream (SI) issues a single instruction per
    time unit, whereas a multiple instruction stream (MI) issues many instructions
    per time unit. Likewise, a single data stream (SD) issues one data element per
    time unit, whereas a multiple data stream (MD) issues many data elements per time
    unit.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 水平轴表示数据流，而垂直轴表示指令流。在这个上下文中，*流*指的是数据或指令的流动。*单流*每个时间单位发出一个元素，类似于队列。相反，*多流*通常每个时间单位发出多个元素（可以想象成多个队列）。因此，单一指令流（SI）每个时间单位发出一个指令，而多重指令流（MI）每个时间单位发出多个指令。同样，单一数据流（SD）每个时间单位发出一个数据元素，而多重数据流（MD）每个时间单位发出多个数据元素。
- en: A processor can be classified into one of four categories based on the types
    of streams it employs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器可以根据它使用的流的类型被分类为四种类型之一。
- en: '**SISD**   Single instruction/single data systems have a single control unit
    processing a single stream of instructions, allowing it to execute only one instruction
    at a time. Likewise, the processor can process only a single stream of data or
    process one data unit at a time. Most commercially available processors prior
    to the mid-2000s were SISD machines.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**SISD** 单指令/单数据系统具有一个控制单元，处理一个指令流，只能一次执行一个指令。同样，处理器只能处理一个数据流或每次处理一个数据单元。2000年代中期之前，大多数商业化的处理器都是SISD机器。'
- en: '**MISD**   Multiple instruction/single data systems have multiple instruction
    units performing on a single data stream. MISD systems were typically designed
    for incorporating fault tolerance in mission-critical systems, such as the flight
    control programs for NASA shuttles. That said, MISD machines are rarely used in
    practice anymore.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**MISD** 多指令/单数据系统具有多个指令单元，在一个数据流上执行。MISD系统通常是为在关键任务系统中集成容错功能而设计的，例如NASA航天飞机的飞行控制程序。尽管如此，MISD机器如今在实践中已经很少使用。'
- en: '**SIMD**   Single instruction/multiple data systems execute the *same* instruction
    on multiple data simultaneously and in lockstep fashion. During “lockstep” execution,
    all instructions are placed into a queue, while data is distributed among different
    compute units. During execution, each compute unit executes the first instruction
    in the queue simultaneously, before simultaneously executing the next instruction
    in the queue, and then the next, and so forth. The most well-known example of
    the SIMD architecture is the graphics processing unit. Early supercomputers also
    followed the SIMD architecture. We discuss GPUs more in the next section.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '**MIMD**   Multiple instruction/multiple data systems represent the most widely
    used architecture class. They are extremely flexible and have the ability to work
    on multiple instructions or multiple data streams. Since nearly all modern computers
    use multicore CPUs, most are classified as MIMD machines. We discuss another class
    of MIMD systems, distributed memory systems, in “Distributed Memory Systems, Message
    Passing, and MPI” on [page 746](ch15.xhtml#lev1_115).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '15.1 Heterogeneous Computing: Hardware Accelerators, GPGPU Computing, and CUDA'
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Heterogeneous computing* is computing using multiple, different processing
    units found in a computer. These processing units often have different ISAs, some
    managed by the OS, and others not. Typically, heterogeneous computing means support
    for parallel computing using the computer’s CPU cores and one or more of its accelerator
    units such as *graphics processing units* (GPUs) or *field programmable gate arrays*
    (FPGAs).^([1](ch15.xhtml#fn15_1))'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: It is increasingly common for developers to implement heterogeneous computing
    solutions to large, data-intensive and computation-intensive problems. These types
    of problems are pervasive in scientific computing as well as in a more diverse
    range of applications to Big Data processing, analysis, and information extraction.
    By making use of the processing capabilities of both the CPU and the accelerator
    units that are available on a computer, a programmer can increase the degree of
    parallel execution in their application, resulting in improved performance and
    scalability.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we introduce heterogeneous computing using hardware accelerators
    to support general-purpose parallel computing. We focus on GPUs and the CUDA programming
    language.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 15.1.1 Hardware Accelerators
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to the CPU, computers have other processing units that are designed
    to perform specific tasks. These units are not general-purpose processing units
    like the CPU, but are special-purpose hardware that is optimized to implement
    functionality that is specific to certain devices or that is used to perform specialized
    types of processing in the system. FPGAs, Cell processors, and GPUs are three
    examples of these types of processing units.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: FPGAs
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An FPGA is an integrated circuit that consists of gates, memory, and interconnection
    components. They are reprogrammable, meaning that they can be reconfigured to
    implement specific functionality in hardware, and they are often used to prototype
    application-specific integrated circuits (ASICs). FPGAs typically require less
    power to run than a full CPU, resulting in energy-efficient operation. Some example
    ways in which FPGAs are integrated into a computer system include as device controllers,
    for sensor data processing, for cryptography, and for testing new hardware designs
    (because they are reprogrammable, designs can be implemented, debugged, and tested
    on an FPGA). FPGAs can be designed as a circuit with a high number of simple processing
    units. FPGAs are also low-latency devices that can be directly connected to system
    buses. As a result, they have been used to implement very fast parallel computation
    that consists of regular patterns of independent parallel processing on several
    data input channels. However, reprogramming FPGAs takes a long time, and their
    use is limited to supporting fast execution of specific parts of parallel workloads
    or for running a fixed program workload.^([2](ch15.xhtml#fn15_2))
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: FPGA 是一种集成电路，由门、电池和互联组件组成。它们是可重新编程的，意味着可以重新配置以实现硬件中的特定功能，通常用于原型设计应用特定集成电路（ASIC）。与完整的
    CPU 相比，FPGA 通常消耗更少的功耗，从而实现更高效的能源使用。FPGA 集成到计算机系统中的一些常见方式包括作为设备控制器、传感器数据处理、加密和测试新硬件设计（由于其可重新编程的特性，可以在
    FPGA 上实现、调试和测试设计）。FPGA 可以设计为包含大量简单处理单元的电路。FPGA 也是低延迟设备，可以直接连接到系统总线。因此，它们被用于实现非常快速的并行计算，执行由若干数据输入通道上的独立并行处理所构成的规则模式。然而，重新编程
    FPGA 需要较长时间，它们的使用被局限于支持并行工作负载的特定部分的快速执行或运行固定程序工作负载。^([2](ch15.xhtml#fn15_2))
- en: GPUs and Cell Processors
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPU 与 Cell 处理器
- en: A Cell processor is a multicore processor that consists of one general-purpose
    processor and multiple coprocessors that are specialized to accelerate a specific
    type of computation, such as multimedia processing. The Sony PlayStation 3 gaming
    system was the first Cell architecture, using the Cell coprocessors for fast graphics.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Cell 处理器是一种多核处理器，包含一个通用处理器和多个专门加速特定类型计算（如多媒体处理）的协处理器。索尼 PlayStation 3 游戏系统是第一个采用
    Cell 架构的设备，使用 Cell 协处理器来加速图形处理。
- en: GPUs perform computer graphics computations—they operate on image data to enable
    high-speed graphics rendering and image processing. A GPU writes its results to
    a frame buffer, which delivers the data to the computer’s display. Driven by computer
    gaming applications, today sophisticated GPUs come standard in desktop and laptop
    systems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 执行计算机图形计算——它们操作图像数据以实现高速图形渲染和图像处理。GPU 将结果写入帧缓冲区，将数据传输到计算机显示器。受计算机游戏应用的推动，如今先进的
    GPU 已成为台式机和笔记本系统的标准配置。
- en: In the mid 2000s, parallel computing researchers recognized the potential of
    using accelerators in combination with a computer’s CPU cores to support general-purpose
    parallel computing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2000 年代中期，平行计算研究人员认识到，将加速器与计算机的 CPU 核心结合使用以支持通用并行计算的潜力。
- en: 15.1.2 GPU Architecture Overview
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.1.2 GPU 架构概述
- en: GPU hardware is designed for computer graphics and image processing. Historically,
    GPU development has been driven by the video game industry. To support more detailed
    graphics and faster frame rendering, a GPU device consists of thousands of special-purpose
    processors, specifically designed to efficiently manipulate image data, such as
    the individual pixel values of a two-dimensional image, in parallel.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 硬件是为计算机图形和图像处理设计的。GPU 的发展历程主要受到视频游戏行业的推动。为了支持更细致的图形和更快的帧渲染，GPU 设备由成千上万的专用处理器组成，专门设计用来高效地并行处理图像数据，比如二维图像中的单个像素值。
- en: The hardware execution model implemented by GPUs is *single instruction*/*multiple
    thread* (SIMT), a variation of SIMD. SIMT is like multithreaded SIMD, where a
    single instruction is executed in lockstep by multiple threads running on the
    processing units. In SIMT, the total number of threads can be larger than the
    total number of processing units, requiring the scheduling of multiple groups
    of threads on the processors to execute the same sequence of instructions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GPU实现的硬件执行模型是*单指令*/*多线程*（SIMT），是SIMD的变种。SIMT类似于多线程的SIMD，其中单个指令由在处理单元上运行的多个线程同步执行。在SIMT中，线程的总数可以大于处理单元的总数，因此需要在处理器上调度多个线程组，以执行相同的指令序列。
- en: As an example, NVIDIA GPUs consist of several streaming multiprocessors (SMs),
    each of which has its own execution control units and memory space (registers,
    L1 cache, and shared memory). Each SM consists of several scalar processor (SP)
    cores. The SM includes a warp scheduler that schedules *warps*, or sets of application
    threads, to execute in lockstep on its SP cores. In lockstep execution, each thread
    in a warp executes the same instruction each cycle but on different data. For
    example, if an application is changing a color image to grayscale, then each thread
    in a warp executes the same sequence of instructions at the same time to set a
    pixel’s RGB value to its grayscale equivalent. Each thread in the warp executes
    these instructions on a different pixel data value, resulting in multiple pixels
    of the image being updated in parallel. Because the threads are executed in lockstep,
    the processor design can be simplified so that multiple cores share the same instruction
    control units. Each unit contains cache memory and multiple registers that it
    uses to hold data as it’s manipulated in lockstep by the parallel processing cores.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以NVIDIA的GPU为例，它们由多个流式多处理器（SM）组成，每个SM都有自己的执行控制单元和内存空间（寄存器、L1缓存和共享内存）。每个SM由多个标量处理器（SP）核心组成。SM包括一个warp调度器，调度*warp*，即一组应用程序线程，在其SP核心上同步执行。在同步执行中，warp中的每个线程每个周期执行相同的指令，但操作不同的数据。例如，如果一个应用程序正在将彩色图像转换为灰度图像，那么warp中的每个线程将同时执行相同的指令序列，将像素的RGB值设置为其灰度值。warp中的每个线程在不同的像素数据值上执行这些指令，从而使图像的多个像素并行更新。由于线程是同步执行的，处理器设计可以简化，使得多个核心共享相同的指令控制单元。每个单元包含缓存内存和多个寄存器，用于在并行处理核心的同步操作中存储数据。
- en: '[Figure 15-2](ch15.xhtml#ch15fig2) shows a simplified GPU architecture that
    includes a detailed view of one of its SM units. Each SM consists of multiple
    SP cores, a warp scheduler, an execution control unit, an L1 cache, and shared
    memory space.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15-2](ch15.xhtml#ch15fig2)展示了一个简化的GPU架构，包含其某个SM单元的详细视图。每个SM由多个SP核心、一个warp调度器、一个执行控制单元、一个L1缓存和共享内存空间组成。'
- en: '![image](../images/15fig02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/15fig02.jpg)'
- en: '*Figure 15-2: An example of a simplified GPU architecture with 2,048 cores.
    This shows the GPU divided into 64 SM units, and the details of one SM consisting
    of 32 SP cores. The SM’s warp scheduler schedules thread warps on its SPs. A warp
    of threads executes in lockstep on the SP cores.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-2：一个简化的GPU架构示例，包含2,048个核心。该图展示了GPU被划分为64个SM单元，以及其中一个SM的详细信息，包含32个SP核心。SM的warp调度器在其SP核心上调度线程warp。线程warp在SP核心上同步执行。*'
- en: 15.1.3 GPGPU Computing
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.1.3 GPGPU计算
- en: '*General Purpose GPU* (GPGPU) computing applies special-purpose GPU processors
    to general-purpose parallel computing tasks. GPGPU computing combines computation
    on the host CPU cores with SIMT computation on the GPU processors. GPGPU computing
    performs best on parallel applications (or parts of applications) that can be
    constructed as a stream processing computation on a grid of multidimensional data.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*通用GPU*（GPGPU）计算将专用GPU处理器应用于通用并行计算任务。GPGPU计算结合了主机CPU核心上的计算和GPU处理器上的SIMT计算。GPGPU计算在能够作为网格多维数据流处理计算构建的并行应用程序（或应用程序的一部分）上表现最佳。'
- en: The host operating system does not manage the GPU’s processors or memory. As
    a result, space for program data needs to be allocated on the GPU and the data
    copied between the host memory and the GPU memory by the programmer. GPGPU programming
    languages and libraries typically provide programming interfaces to GPU memory
    that hide some or all of the difficulty of explicitly managing GPU memory from
    the programmer. For example, in CUDA a programmer can include calls to CUDA library
    functions to explicitly allocate CUDA memory on the GPU and to copy data between
    CUDA memory on the GPU and host memory. A CUDA programmer can also use CUDA unified
    memory, which is CUDA’s abstraction of a single memory space on top of host and
    GPU memory. CUDA unified memory hides the separate GPU and host memory, and the
    memory copies between the two, from the CUDA programmer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 主机操作系统不管理GPU的处理器或内存。因此，程序数据需要在GPU上分配空间，并由程序员在主机内存和GPU内存之间复制数据。GPGPU编程语言和库通常提供GPU内存的编程接口，隐藏了程序员在显式管理GPU内存时的一些或全部困难。例如，在CUDA中，程序员可以调用CUDA库函数，显式地在GPU上分配CUDA内存，并在GPU上的CUDA内存和主机内存之间复制数据。CUDA程序员还可以使用CUDA统一内存，这是CUDA对主机和GPU内存之上的单一内存空间的抽象。CUDA统一内存隐藏了独立的GPU和主机内存，以及它们之间的内存复制，免去了CUDA程序员的处理。
- en: GPUs also provide limited support for thread synchronization, which means that
    GPGPU parallel computing performs particularly well for parallel applications
    that are either embarrassingly parallel or have large extents of independent parallel
    stream-based computation with very few synchronization points. GPUs are massively
    parallel processors, and any program that performs long sequences of independent
    identical (or mostly identical) computation steps on data may perform well as
    a GPGPU parallel application. GPGPU computing also performs well when there are
    few memory copies between host and device memory. If GPU–CPU data transfer dominates
    execution time, or if an application requires fine-grained synchronization, GPGPU
    computing may not perform well or provide much, if any, gain over a multithreaded
    CPU version of the program.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: GPU还提供了有限的线程同步支持，这意味着GPGPU并行计算对于那些典型的并行应用表现尤为出色，尤其是那些轻松并行或具有大范围独立并行流计算且几乎没有同步点的应用。GPU是大规模并行处理器，任何在数据上执行长时间序列独立相同（或大部分相同）计算步骤的程序，都可能作为GPGPU并行应用表现良好。GPGPU计算在主机与设备内存之间进行较少内存复制时也表现良好。如果GPU-CPU数据传输占据了执行时间，或应用程序需要精细的同步，GPGPU计算可能无法提供良好的性能，甚至无法超越多线程的CPU版本。
- en: 15.1.4 CUDA
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.1.4 CUDA
- en: CUDA (Compute Unified Device Architecture)^([3](ch15.xhtml#fn15_3)) is NVIDIA’s
    programming interface for GPGPU computing on its graphics devices. CUDA is designed
    for heterogeneous computing in which some program functions run on the host CPU,
    and others run on the GPU device. Programmers typically write CUDA programs in
    C or C++ with annotations that specify CUDA kernel functions, and they make calls
    to CUDA library functions to manage GPU device memory. A CUDA *kernel function*
    is a function that is executed on the GPU, and a CUDA *thread* is the basic unit
    of execution in a CUDA program. CUDA threads are scheduled in warps that execute
    in lockstep on the GPU’s SMs, executing CUDA kernel code on their part of data
    stored in GPU memory. Kernel functions are annotated with `__global__` to distinguish
    them from host functions. CUDA `__device__` functions are helper functions that
    can be called from a CUDA kernel function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA（计算统一设备架构）^([3](ch15.xhtml#fn15_3))是NVIDIA为其图形设备上的GPGPU计算提供的编程接口。CUDA旨在支持异构计算，其中一些程序功能在主机CPU上运行，其他则在GPU设备上运行。程序员通常用C或C++编写CUDA程序，并添加注释来指定CUDA内核函数，然后调用CUDA库函数来管理GPU设备内存。CUDA
    *内核函数*是执行在GPU上的函数，CUDA *线程*是CUDA程序中基本的执行单元。CUDA线程在GPU的SM单元中以warp的形式调度执行，并在其数据部分（存储在GPU内存中）上执行CUDA内核代码。内核函数使用`__global__`进行注释，以将其与主机函数区分开。CUDA的`__device__`函数是可以从CUDA内核函数中调用的辅助函数。
- en: 'The memory space of a CUDA program is separated into host and GPU memory. The
    program must explicitly allocate and free GPU memory space to store program data
    manipulated by CUDA kernels. The CUDA programmer must either explicitly copy data
    to and from the host and GPU memory, or use CUDA unified memory that presents
    a view of memory space that is directly shared by the GPU and host. Here is an
    example of CUDA’s basic memory allocation, memory deallocation, and explicit memory
    copy functions:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA程序的内存空间分为主机内存和GPU内存。程序必须显式地分配和释放GPU内存空间，以存储CUDA内核操作的数据。CUDA程序员必须显式地将数据从主机和GPU内存之间复制，或者使用CUDA统一内存，该内存呈现一个直接由GPU和主机共享的内存空间视图。以下是CUDA基本内存分配、内存释放和显式内存复制函数的示例：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'CUDA threads are organized into *blocks*, and the blocks are organized into
    a *grid*. Grids can be organized into one-, two-, or three-dimensional groupings
    of blocks. Blocks, likewise, can be organized into one-, two-, or three-dimensional
    groupings of threads. Each thread is uniquely identified by its thread (*x*,*y*,*z*)
    position in its containing block’s (*x*,*y*,*z*) position in the grid. For example,
    a programmer could define two-dimensional block and grid dimensions as the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA线程被组织成*块*，而块又被组织成*网格*。网格可以组织成一维、二维或三维的块分组。同样，块可以组织成一维、二维或三维的线程分组。每个线程通过其在包含块中的(*x*,*y*,*z*)位置以及其在网格中的(*x*,*y*,*z*)位置被唯一标识。例如，程序员可以将二维块和网格维度定义如下：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When a kernel is invoked, its blocks/grid and thread/block layout is specified
    in the call. For example, here is a call to a kernel function named `do_something`
    specifying the grid and block layout using `gridDim` and `blockDim` defined above
    (and passing parameters `dev_array` and 100):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用一个内核时，它的块/网格和线程/块布局在调用中被指定。例如，这里是调用一个名为`do_something`的内核函数，使用上述定义的`gridDim`和`blockDim`来指定网格和块的布局（并传递参数`dev_array`和100）：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Figure 15-3](ch15.xhtml#ch15fig3) shows an example of a two-dimensional arrangement
    of thread blocks. In this example, the grid is a 3 × 2 array of blocks, and each
    block is a 4× 3 array of threads.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-3](ch15.xhtml#ch15fig3)展示了一个线程块二维排列的示例。在此示例中，网格是一个3 × 2的块数组，每个块是一个4 ×
    3的线程数组。'
- en: '![image](../images/15fig03.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/15fig03.jpg)'
- en: '*Figure 15-3: The CUDA thread model. A grid of blocks of threads. Blocks and
    threads can be organized into one-, two-, or three-dimensional layouts. This example
    shows a grid of two-dimensional blocks, 3 × 2 blocks per grid, and each block
    has a two-dimensional set of threads, 4 × 3 threads per block).*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-3：CUDA线程模型。一个由线程块组成的网格。块和线程可以组织成一维、二维或三维布局。此示例显示了一个二维块的网格，每个网格有3 × 2个块，每个块有4
    × 3个线程。*'
- en: 'A thread’s position in this layout is given by the (*x*,*y*) coordinate in
    its containing block (`threadId.x`, `threadId.y`) and by the (*x*,*y*) coordinate
    of its block in the grid (`blockIdx.x`, `blockIdx.y`). Note that block and thread
    coordinates are (*x*,*y*) based, with the *x*-axis being horizontal, and the *y*-axis
    vertical. The (0,0) element is in the upper left. The CUDA kernel also has variables
    that are defined to the block dimensions (`blockDim.x` and `blockDim.y`). Thus,
    for any thread executing the kernel, its (row, col) position in the two-dimensional
    array of threads in the two-dimensional array of blocks can be logically identified
    as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 线程在此布局中的位置由其所在块的(*x*,*y*)坐标（`threadId.x`，`threadId.y`）和其在网格中的块的(*x*,*y*)坐标（`blockIdx.x`，`blockIdx.y`）给出。请注意，块和线程的坐标是基于(*x*,*y*)的，其中*x*轴是水平的，*y*轴是垂直的。元素(0,0)位于左上角。CUDA内核还有一些变量，用于定义块的维度（`blockDim.x`和`blockDim.y`）。因此，对于执行内核的任何线程，其在二维线程数组中的（行，列）位置可以逻辑上表示为如下：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Although not strictly necessary, CUDA programmers often organize blocks and
    threads to match the logical organization of program data. For example, if a program
    is manipulating a two-dimensional matrix, it often makes sense to organize threads
    and blocks into a two-dimensional arrangement. This way, a thread’s block (*x*,*y*)
    and its thread (*x*,*y*) within a block can be used to associate a thread’s position
    in the two-dimensional blocks of threads with one or more data values in the two-dimensional
    array.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管并非严格必要，CUDA程序员通常会将块和线程组织成与程序数据的逻辑结构相匹配。例如，如果程序正在处理二维矩阵，通常会将线程和块组织成二维的排列方式。这样，线程的块(*x*,*y*)及其块内的线程(*x*,*y*)可以用来将线程在二维线程块中的位置与二维数组中的一个或多个数据值关联起来。
- en: 'Example CUDA Program: Scalar Multiply'
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例CUDA程序：标量乘法
- en: 'As an example, consider a CUDA program that performs scalar multiplication
    of a vector:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，考虑一个执行向量标量乘法的CUDA程序：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Because the program data comprises one-dimensional arrays, using a one-dimensional
    layout of blocks/grid and threads/block works well. This is not necessary, but
    it makes the mapping of threads to data easier.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于程序数据包含一维数组，使用一维的块/网格布局和线程/块的方式效果良好。这不是必须的，但它使线程与数据的映射更加容易。
- en: 'When run, the main function of this program will do the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 程序运行时，主函数将执行以下操作：
- en: 1\. Allocate host-side memory for the vector `x` and initialize it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 为向量`x`分配主机端内存并初始化。
- en: 2\. Allocate device-side memory for the vector `x` and copy it from host memory
    to GPU memory.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 为向量`x`分配设备端内存，并将其从主机内存复制到GPU内存。
- en: 3\. Invoke a CUDA kernel function to perform vector scalar multiply in parallel,
    passing as arguments the device address of the vector `x` and the scalar value
    `a`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 调用一个CUDA内核函数，执行向量标量乘法并行计算，传入向量`x`的设备地址和标量值`a`作为参数。
- en: 4\. Copy the result from GPU memory to host memory vector `x`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 将结果从GPU内存复制到主机内存中的向量`x`。
- en: In the example that follows, we show a CUDA program that performs these steps
    to implement scalar vector multiplication. We have removed some error handling
    and details from the code listing, but the full solution is available online.^([4](ch15.xhtml#fn15_4))
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们展示了一个执行这些步骤以实现标量向量乘法的CUDA程序。我们已从代码清单中删除了一些错误处理和细节，但完整的解决方案可以在网上找到。^([4](ch15.xhtml#fn15_4))
- en: 'The main function of the CUDA program performs the aforementioned steps:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA程序的主函数执行上述步骤：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Each CUDA thread executes the CUDA kernel function `scalar_multiply`. A CUDA
    kernel function is written from an individual thread’s point of view. It typically
    consists of two main steps: (1) the calling thread determines which portion of
    the data it is responsible for based on its thread’s position in its enclosing
    block and its block’s position in the grid; (2) the calling thread performs application-specific
    computation on its portion of the data. In this example, each thread is responsible
    for computing scalar multiplication on exactly one element in the array. The kernel
    function code first calculates a unique index value based on the calling thread’s
    block and thread identifier. It then uses this value as an index into the array
    of data to perform scalar multiplication on its array element (`array[index] =`
    `array[index] * scalar`). CUDA threads running on the GPU’s SM units each compute
    a different index value to update array elements in parallel.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每个CUDA线程执行CUDA内核函数`scalar_multiply`。CUDA内核函数是从单个线程的角度编写的。它通常包括两个主要步骤：（1）调用线程根据其在线程块中的位置和块在网格中的位置，确定它负责处理的数据部分；（2）调用线程对其负责的数据部分执行应用程序特定的计算。在这个示例中，每个线程负责计算数组中的一个元素的标量乘法。内核函数的代码首先根据调用线程的块和线程标识符计算出一个唯一的索引值。然后，它使用该值作为数组数据的索引，执行标量乘法操作（`array[index]
    =` `array[index] * scalar`）。运行在GPU的SM单元上的CUDA线程每个计算一个不同的索引值，以并行更新数组元素。
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: CUDA Thread Scheduling and Synchronization
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CUDA线程调度与同步
- en: Each CUDA thread block is run by a GPU SM unit. An SM schedules a warp of threads
    from the same thread block to run its processor cores. All threads in a warp execute
    the same set of instructions in lockstep, typically on different data. Threads
    share the instruction pipeline but get their own registers and stack space for
    local variables and parameters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Because blocks of threads are scheduled on individual SMs, increasing the threads
    per block increases the degree of parallel execution. Because the SM schedules
    thread warps to run on its processing units, if the number of threads per block
    is a multiple of the warp size, then no SM processor cores are wasted in the computation.
    In practice, using a number of threads per block that is a small multiple of the
    number of processing cores of an SM works well.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: CUDA guarantees that all threads from a single kernel call complete before any
    threads from a subsequent kernel call are scheduled. Thus, there is an implicit
    synchronization point between separate kernel calls. Within a single kernel call,
    however, thread blocks are scheduled to run the kernel code in any order on the
    GPU SMs. As a result, a programmer should not assume any ordering of execution
    between threads in different thread blocks. CUDA provides some support for synchronizing
    threads, but only for threads that are in the same thread block.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 15.1.5 Other Languages for GPGPU Programming
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are other programming languages for GPGPU computing. OpenCL, OpenACC,
    and OpenHMPP are three examples of languages that can be used to program any graphics
    device (they are not specific to NVIDIA devices). OpenCL (Open Computing Language)
    has a similar programming model to CUDA’s; both implement a lower-level programming
    model (or implement a thinner programming abstraction) on top of the target architectures.
    OpenCL targets a wide range of heterogeneous computing platforms that include
    a host CPU combined with other compute units, which could include CPUs or accelerators
    such as GPUs and FPGAs. OpenACC (Open Accelerator) is a higher-level abstraction
    programming model than CUDA or OpenCL. It is designed for portability and programmer
    ease. A programmer annotates portions of their code for parallel execution, and
    the compiler generates parallel code that can run on GPUs. OpenHMPP (Open Hybrid
    Multicore Programming) is another language that provides a higher-level programming
    abstraction for heterogeneous programming.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 15.2 Distributed Memory Systems, Message Passing, and MPI
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Chapter 14](ch14.xhtml#ch14) describes mechanisms like Pthreads (see “Hello
    Threading! Writing Your First Multithreaded Program” on [page 677](ch14.xhtml#lev1_106))
    and OpenMP (see “Implicit Threading with OpenMP” on [page 729](ch14.xhtml#lev1_111))
    that programs use to take advantage of multiple CPU cores on a *shared memory
    system*. In such systems, each core shares the same physical memory hardware,
    allowing them to communicate data and synchronize their behavior by reading from
    and writing to shared memory addresses. Although shared memory systems make communication
    relatively easy, their scalability is limited by the number of CPU cores in the
    system.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[第14章](ch14.xhtml#ch14)描述了像Pthreads（见“你好线程！编写你的第一个多线程程序”[第677页](ch14.xhtml#lev1_106)）和OpenMP（见“使用OpenMP的隐式线程”[第729页](ch14.xhtml#lev1_111)）等机制，程序使用这些机制来利用*共享内存系统*中的多个CPU核心。在这种系统中，每个核心共享相同的物理内存硬件，允许它们通过读取和写入共享内存地址来交换数据和同步行为。尽管共享内存系统使得通信相对容易，但其可扩展性受到系统中CPU核心数量的限制。'
- en: As of 2019, high-end commercial server CPUs generally provide a maximum of 64
    cores. For some tasks, though, even a few hundred CPU cores isn’t close enough.
    For example, imagine trying to simulate the fluid dynamics of the Earth’s oceans
    or index the entire contents of the World Wide Web to build a search application.
    Such massive tasks require more physical memory and processors than any single
    computer can provide. Thus, applications that require a large number of CPU cores
    run on systems that forego shared memory. Instead, they execute on systems built
    from multiple computers, each with their own CPU(s) and memory, that communicate
    over a network to coordinate their behavior.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2019年，高端商业服务器CPU通常提供最多64个核心。然而，对于某些任务，即使有几百个CPU核心也远远不够。例如，想象一下尝试模拟地球海洋的流体动力学，或索引整个万维网的内容以构建搜索应用程序。这些庞大的任务需要比任何单一计算机能提供的更多的物理内存和处理器。因此，要求大量CPU核心的应用程序运行在摒弃共享内存的系统上。相反，它们在由多台计算机构成的系统上运行，每台计算机都有自己的CPU和内存，通过网络通信来协调它们的行为。
- en: A collection of computers working together is known as a *distributed memory
    system* (or often just *distributed system*).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一组计算机共同工作被称为*分布式内存系统*（或通常称为*分布式系统*）。
- en: '**Warning A NOTE ON CHRONOLOGY**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告 关于时间顺序的说明**'
- en: Despite the order in which they’re presented in this book, systems designers
    built distributed systems long before mechanisms like threads or OpenMP existed.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书中按顺序介绍了它们，但系统设计师在线程或OpenMP等机制出现之前就已经构建了分布式系统。
- en: Some distributed memory systems integrate hardware more closely than others.
    For example, a *supercomputer* is a high-performance system in which many *compute
    nodes* are tightly coupled (closely integrated) to a fast interconnection network.
    Each compute node contains its own CPU(s), GPU(s), and memory, but multiple nodes
    might share auxiliary resources like secondary storage and power supplies. The
    exact level of hardware sharing varies from one supercomputer to another.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一些分布式内存系统比其他系统更紧密地集成硬件。例如，*超级计算机*是一种高性能系统，其中许多*计算节点*被紧密耦合（紧密集成）到一个快速互连网络中。每个计算节点包含自己的CPU、GPU和内存，但多个节点可能共享辅助资源，如二级存储和电源供应。硬件共享的具体程度在不同的超级计算机之间有所不同。
- en: On the other end of the spectrum, a distributed application might run on a loosely
    coupled (less integrated) collection of fully autonomous computers (*nodes*) connected
    by a traditional local area network (LAN) technology like Ethernet. Such a collection
    of nodes is known as a *commodity off-the-shelf* (COTS) cluster. COTS clusters
    typically employ a *shared-nothing architecture* in which each node contains its
    own set of computation hardware (i.e., CPU(s), GPU(s), memory, and storage). [Figure
    15-4](ch15.xhtml#ch15fig4) illustrates a shared-nothing distributed system consisting
    of two shared-memory computers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，分布式应用程序可能运行在一组松散耦合（集成度较低）的完全自主计算机（*节点*）上，这些计算机通过像以太网这样的传统局域网（LAN）技术连接。这样的节点集合被称为*商用现成*（COTS）集群。COTS集群通常采用*无共享架构*，其中每个节点包含自己的计算硬件（即CPU、GPU、内存和存储）。[图15-4](ch15.xhtml#ch15fig4)展示了一个由两个共享内存计算机组成的无共享分布式系统。
- en: '![image](../images/15fig04.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/15fig04.jpg)'
- en: '*Figure 15-4: The major components of a shared-nothing distributed memory architecture
    built from two compute nodes*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-4：由两个计算节点构建的无共享分布式内存架构的主要组件*'
- en: 15.2.1 Parallel and Distributed Processing Models
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.2.1 并行与分布式处理模型
- en: Application designers often organize distributed applications using tried-and-true
    designs. Adopting application models like these helps developers reason about
    an application because its behavior will conform to well-understood norms. Each
    model has its unique benefits and drawbacks—there’s no one-size-fits-all solution.
    We briefly characterize a few of the more common models in the subsections that
    follow, but note that we’re not presenting an exhaustive list.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Client/Server
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The *client/server model* is an extremely common application model that divides
    an application’s responsibilities among two actors: client processes and server
    processes. A server process provides a service to clients that ask for something
    to be done. Server processes typically wait at well-known addresses to receive
    incoming connections from clients. Upon making a connection, a client sends requests
    to the server process, which either satisfies those requests (e.g., by fetching
    a requested file) or reports an error (e.g., the file doesn’t exist or the client
    can’t be properly authenticated).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Although you may not have considered it, you access web pages via the client/server
    model! Your web browser (client) connects to a website (server) at a public address
    (e.g., `[diveintosystems.org](http://diveintosystems.org)`) to retrieve the page’s
    contents.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The *pipeline model* divides an application into a distinct sequence of steps,
    each of which can process data independently. This model works well for applications
    whose workflow involves linear, repetitive tasks over large data inputs. For example,
    consider the production of computer-animated films. Each frame of the film must
    be processed through a sequence of steps that transform the frame (e.g., adding
    textures or applying lighting). Because each step happens independently in a sequence,
    animators can speed up rendering by processing frames in parallel across a large
    cluster of computers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Boss/Worker
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the *boss/worker model*, one process acts as a central coordinator and distributes
    work among the processes at other nodes. This model works well for problems that
    require processing a large, divisible input. The boss divides the input into smaller
    pieces and assigns one or more pieces to each worker. In some applications, the
    boss might statically assign each worker exactly one piece of the input. In other
    cases, the workers might repeatedly finish a piece of the input and then return
    to the boss to dynamically retrieve the next input chunk. Later in this section,
    we’ll present an example program in which a boss divides an array among many workers
    to perform scalar multiplication on an array.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Note that this model is sometimes called other names, like “master/ worker”
    or other variants, but the main idea is the same.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Peer-to-Peer
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Unlike the boss/worker model, a *peer-to-peer* application avoids relying on
    a centralized control process. Instead, peer processes self-organize the application
    into a structure in which they each take on roughly the same responsibilities.
    For example, in the BitTorrent file sharing protocol, each peer repeatedly exchanges
    parts of a file with others until they’ve all received the entire file.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Lacking a centralized component, peer-to-peer applications are generally robust
    to node failures. On the other hand, peer-to-peer applications typically require
    complex coordination algorithms, making them difficult to build and rigorously
    test.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 15.2.2 Communication Protocols
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Whether they are part of a supercomputer or a COTS cluster, processes in a distributed
    memory system communicate via *message passing*, whereby one process explicitly
    sends a message to processes on one or more other nodes, which receive it. It’s
    up to the applications running on the system to determine how to utilize the network—some
    applications require frequent communication to tightly coordinate the behavior
    of processes across many nodes, whereas other applications communicate to divide
    up a large input among processes and then mostly work independently.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'A distributed application formalizes its communication expectations by defining
    a communication *protocol*, which describes a set of rules that govern its use
    of the network, including:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: When a process should send a message
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To which process(es) it should send the message
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to format the message
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without a protocol, an application might fail to interpret messages properly
    or even deadlock (see “Deadlock” on [page 700](ch14.xhtml#lev3_120)). For example,
    if an application consists of two processes, and each process waits for the other
    to send it a message, neither process will ever make progress. Protocols add structure
    to communication to reduce the likelihood of such failures.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: To implement a communication protocol, applications require basic functionality
    for tasks like sending and receiving messages, naming processes (addressing),
    and synchronizing process execution. Many applications look to the Message Passing
    Interface for such functionality.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 15.2.3 Message Passing Interface
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *Message Passing Interface* (MPI) defines (but does not itself implement)
    a standardized interface that applications can use to communicate in a distributed
    memory system. By adopting the MPI communication standard, applications become
    *portable*, meaning that they can be compiled and executed on many different systems.
    In other words, as long as an MPI implementation is installed, a portable application
    can move from one system to another and expect to execute properly, even if the
    systems have different underlying characteristics.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: MPI allows a programmer to divide an application into multiple processes. It
    assigns each of an application’s processes a unique identifier, known as a *rank*,
    which ranges from 0 to *N –* 1 for an application with *N* processes. A process
    can learn its rank by calling the `MPI_Comm_rank` function, and it can learn how
    many processes are executing in the application by calling `MPI_Comm` `_size`.
    To send a message, a process calls `MPI_Send` and specifies the rank of the intended
    recipient. Similarly, a process calls `MPI_Recv` to receive a message, and it
    specifies whether to wait for a message from a specific node or to receive a message
    from any sender (using the constant `MPI_ANY_SOURCE` as the rank).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the basic send and receive functions, MPI also defines a variety
    of functions that make it easier for one process to communicate data to multiple
    recipients. For example, `MPI_Bcast` allows one process to send a message to every
    other process in the application with just one function call. It also defines
    a pair of functions, `MPI_Scatter` and `MPI_Gather`, that allow one process to
    divide up an array and distribute the pieces among processes (scatter), operate
    on the data, and then later retrieve all the data to coalesce the results (gather).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Because MPI *specifies* only a set of functions and how they should behave,
    each system designer can implement MPI’s functionality in a way that matches the
    capabilities of their particular system. For example, a system with an interconnect
    network that supports broadcasting (sending one copy of a message to multiple
    recipients at the same time) might be able to implement MPI’s `MPI_Bcast` function
    more efficiently than a system without such support.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 15.2.4 MPI Hello World
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As an introduction to MPI programming, consider the “Hello World” program^([5](ch15.xhtml#fn15_5))
    presented here:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When starting this program, MPI simultaneously executes multiple copies of it
    as independent processes across one or more computers. Each process makes calls
    to MPI to determine how many total processes are executing (with `MPI_Comm_size`)
    and which process it is among those processes (the process’s rank, with `MPI_Comm_rank`).
    After looking up this information, each process prints a short message containing
    the rank and name of the computer (`hostname`) it’s running on before terminating.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '**Note RUNNING MPI CODE**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: To run these MPI examples, you’ll need an MPI implementation like OpenMPI^([6](ch15.xhtml#fn15_6))
    or MPICH^([7](ch15.xhtml#fn15_7)) installed on your system.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'To compile this example, invoke the `mpicc` compiler program, which executes
    an MPI-aware version of GCC to build the program and link it against MPI libraries:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To execute the program, use the `mpirun` utility to start up several parallel
    processes with MPI. The `mpirun` command needs to be told which computers to run
    processes on (`--hostfile`) and how many processes to run at each machine (`-np`).
    Here, we provide it with a file named `hosts.txt` that tells `mpirun` to create
    four processes across two computers, one named `lemon`, and another named `orange`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Warning MPI EXECUTION ORDER**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: You should *never* make any assumptions about the order in which MPI pro- cesses
    will execute. The processes start up on multiple machines, each of which has its
    own OS and process scheduler. If the correctness of your program requires that
    processes run in a particular order, you must ensure that the proper order occurs—for
    example, by forcing certain processes to pause until they receive a message.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 15.2.5 MPI Scalar Multiplication
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For a more substantive MPI example, consider performing scalar multiplication
    on an array. This example adopts the boss/worker model—one process divides the
    array into smaller pieces and distributes them among worker processes. Note that
    in this implementation of scalar multiplication, the boss process also behaves
    as a worker and multiplies part of the array after distributing sections to the
    other workers.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: To benefit from working in parallel, each process multiplies just its local
    piece of the array by the scalar value, and then the workers all send the results
    back to the boss process to form the final result. At several points in the program,
    the code checks to see whether the rank of the process is zero.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This check ensures that only one process (the one with rank 0) plays the role
    of the boss. By convention, MPI applications often choose rank 0 to perform one-time
    tasks because no matter how many processes there are, one will always be given
    rank 0 (even if just a single process is executing).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: MPI Communication
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The boss process begins by determining the scalar value and initial input array.
    In a real scientific computing application, the boss would likely read such values
    from an input file. To simplify this example, the boss uses a constant scalar
    value (10) and generates a simple 40-element array (containing the sequence 0
    to 39) for illustrative purposes.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'This program requires communication between MPI processes for three important
    tasks:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The boss sends the scalar value and the size of the array to *all* of the
    workers.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The boss divides the initial array into pieces and sends a piece to each
    worker.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Each worker multiplies the values in its piece of the array by the scalar
    and then sends the updated values back to the boss.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting Important Values
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To send the scalar value to the workers, the example program uses the `MPI_Bcast`
    function, which allows one MPI process to send the same value to all the other
    MPI processes with one function call:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This call sends one integer (`MPI_INT`) starting from the address of the `scalar`
    variable from the process with rank 0 to every other process (`MPI_COMM` `_WORLD`).
    All the worker processes (those with nonzero rank) receive the broadcast into
    their local copy of the `scalar` variable, so when this call completes, every
    process knows the scalar value to use.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '**Note MPI_BCAST BEHAVIOR**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Every process executes `MPI_Bcast`, but it behaves differently depending on
    the rank of the calling process. If the rank matches that of the fourth argument,
    then the caller assumes the role of the sender. All other processes that call
    `MPI_Bcast` act as receivers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the boss broadcasts the total size of the array to every other process.
    After learning the total array size, each process sets a `local_size` variable
    by dividing the total array size by the number of MPI processes. The `local_size`
    variable represents how many elements each worker’s piece of the array will contain.
    For example, if the input array contains 40 elements and the application consists
    of eight processes, each process is responsible for a five-element piece of the
    array (40 / 8 = 5). To keep the example simple, it assumes that the number of
    processes evenly divides the size of the array:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Distributing the Array
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now that each process knows the scalar value and how many values it’s responsible
    for multiplying, the boss must divide the array into pieces and distribute them
    among the workers. Note that in this implementation, the boss (rank 0) also participates
    as a worker. For example, with a 40-element array and eight processes (ranks 0–7),
    the boss should keep array elements 0–4 for itself (rank 0), send elements 5–9
    to rank 1, elements 10–14 to rank 2, and so on. [Figure 15-5](ch15.xhtml#ch15fig5)
    shows how the boss assigns pieces of the array to each MPI process.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/15fig05.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-5: The distribution of a 40-element array among eight MPI processes
    (ranks 0–7)*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'One option for distributing pieces of the array to each worker combines `{MPI_Send}`
    calls at the boss with an `{MPI_Recv}` call at each worker:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this code, the boss executes a loop that executes once for each worker process,
    in which it sends the worker a piece of the array. It starts sending data from
    the address of `array` at an offset of `(i * local_size)` to ensure that each
    worker gets a unique piece of the array. That is, the worker with rank 1 gets
    a piece of the array starting at index 5, rank 2 gets a piece of the array starting
    at index 10, etc., as shown in [Figure 15-5](ch15.xhtml#ch15fig5).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Each call to `MPI_Send` sends `local_size` (5) integers worth of data (20 bytes)
    to the process with rank i. The `0` argument toward the end represents a message
    tag, which is an advanced feature that this program doesn’t need—setting it to
    `0` treats all messages equally.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The workers all call `MPI_Recv` to retrieve their piece of the array, which
    they store in memory at the address to which `local_array` refers. They receive
    `local_size` (5) integers worth of data (20 bytes) from the node with rank 0\.
    Note that `MPI_Recv` is a *blocking* call, which means that a process that calls
    it will pause until it receives data. Because the `MPI_Recv` call blocks, no worker
    will proceed until the boss sends its piece of the array.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Execution
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: After a worker has received its piece of the array, it can begin multiplying
    each array value by the scalar. Because each worker gets a unique subset of the
    array, they can execute independently, in parallel, without the need to communicate.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating Results
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Finally, after workers complete their multiplication, they send the updated
    array values back to the boss, which aggregates the results. Using `MPI_Send`
    and `MPI_Recv`, this process looks similar to the array distribution code we looked
    at earlier, except the roles of sender and receiver are reversed:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Recall that `MPI_Recv` *blocks* or pauses execution, so each call in the `for`
    loop causes the boss to wait until it receives a piece of the array from worker
    *i*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Scatter/Gather
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Although the `for` loops in the previous example correctly distribute data
    with `MPI_Send` and `MPI_Recv`, they don’t succinctly capture the *intent* behind
    them. That is, they appear to MPI as a series of send and receive calls without
    the obvious goal of distributing an array across MPI processes. Because parallel
    applications frequently need to distribute and collect data like this example
    array, MPI provides functions for exactly this purpose: `MPI_Scatter` and `MPI_Gather`.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'These functions provide two major benefits: they allow the entire code blocks
    in the previous example to each be expressed as a single MPI function call, which
    simplifies the code, and they express the *intent* of the operation to the underlying
    MPI implementation, which may be able to better optimize their performance.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'To replace the first loop in the previous example, each process could call
    `MPI_Scatter`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This function automatically distributes the contents of memory starting at `array`
    in pieces containing `local_size` integers to the `local_array` destination variable.
    The `0` argument specifies that the process with rank 0 (the boss) is the sender,
    so it reads and distributes the `array` source to other processes (including sending
    one piece to itself). Every other process acts as a receiver and receives data
    into its `local_array` destination.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'After this single call, the workers can each multiply the array in parallel.
    When they finish, each process calls `MPI_Gather` to aggregate the results back
    in the boss’s `array` variable:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This call behaves like the opposite of `MPI_Scatter`: this time, the `0` argument
    specifies that the process with rank 0 (the boss) is the receiver, so it updates
    the `array` variable, and workers each send `local_size` integers from their `local_array`
    variables.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Full Code for MPI Scalar Multiply
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Here’s a full MPI scalar multiply code listing that uses `MPI_Scatter` and `MPI_Gather`:^([8](ch15.xhtml#fn15_8))
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the `main` function, the boss sets up the problem and creates an array. If
    this were solving a real problem (e.g., a scientific computing application), the
    boss would likely read its initial data from an input file. After initializing
    the array, the boss needs to send information about the size of the array and
    the scalar to use for multiplication to all the other worker processes, so it
    broadcasts those variables to every process.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Now that each process knows the size of the array and how many processes there
    are, they can each divide to determine how many elements of the array they’re
    responsible for multiplying. For simplicity, this code assumes that the array
    is evenly divisible by the number of processes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: The boss then uses the `MPI_Scatter` function to send an equal portion of the
    array to each worker process (including itself). Now the workers have all the
    information they need, so they each perform multiplication over their portion
    of the array in parallel. Finally, as the workers complete their multiplication,
    the boss collects each worker’s piece of the array using `MPI_Gather` to report
    the final results.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Compiling and executing this program looks like this:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 15.2.6 Distributed Systems Challenges
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In general, coordinating the behavior of multiple processes in distributed systems
    is notoriously difficult. If a hardware component (e.g., CPU or power supply)
    fails in a shared memory system, the entire system becomes inoperable. In a distributed
    system though, autonomous nodes can fail independently. For example, an application
    must decide how to proceed if one node disappears and the others are still running.
    Similarly, the interconnection network could fail, making it appear to each process
    as if all the others failed.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Distributed systems also face challenges due to a lack of shared hardware, namely
    clocks. Due to unpredictable delays in network transmission, autonomous nodes
    cannot easily determine the order in which messages are sent. Solving these challenges
    (and many others) is beyond the scope of this book. Fortunately, distributed software
    designers have constructed several frameworks that ease the development of distributed
    applications. We characterize some of these frameworks in the next section.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: MPI Resources
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'MPI is large and complex, and this section hardly scratches the surface. For
    more information about MPI, we suggest:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The Lawrence Livermore National Lab’s MPI tutorial, by Blaise Barney.^([9](ch15.xhtml#fn15_9))
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CSinParallel’s MPI Patterns.^([10](ch15.xhtml#fn15_10))
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '15.3 To Exascale and Beyond: Cloud Computing, Big Data, and the Future of Computing'
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Advances in technology have made it possible for humanity to produce data at
    a rate never seen before. Scientific instruments such as telescopes, biological
    sequencers, and sensors produce high-fidelity scientific data at low cost. As
    scientists struggle to analyze this “data deluge,” they increasingly rely on sophisticated
    multinode supercomputers, which form the foundation of *high-performance computing*
    (HPC).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: HPC applications are typically written in languages like C, C++, or Fortran,
    with multithreading and message passing enabled with libraries such as POSIX threads,
    OpenMP, and MPI. Thus far, the vast majority of this book has described architectural
    features, languages, and libraries commonly leveraged on HPC systems. Companies,
    national laboratories, and other organizations interested in advancing science
    typically use HPC systems and form the core of the computational science ecosystem.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the proliferation of internet-enabled devices and the ubiquity of
    social media have caused humanity to effortlessly produce large volumes of online
    multimedia, in the form of web pages, pictures, videos, tweets, and social media
    posts. It is estimated that 90% of all online data was produced in the past two
    years, and that society produces 30 terabytes of user data per second (or 2.5
    exabytes per day). The deluge of *user data* offers companies and organizations
    a wealth of information about the habits, interests, and behavior of its users,
    and it facilitates the construction of data-rich customer profiles to better tailor
    commercial products and services. To analyze user data, companies typically rely
    on multinode data centers that share many of the hardware architecture components
    of typical supercomputers. However, these data centers rely on a different software
    stack designed specifically for internet-based data. The computer systems used
    for the storage and analysis of large-scale internet-based data are sometimes
    referred to as *high-end data analysis* (HDA) systems. Companies like Amazon,
    Google, Microsoft, and Facebook have a vested interest in the analysis of internet
    data, and form the core of the data analytics ecosystem. The HDA and data analytics
    revolution started around 2010, and now is a dominant area of cloud computing
    research.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-6](ch15.xhtml#ch15fig6) highlights the key differences in software
    utilized by the HDA and HPC communities. Note that both communities use similar
    cluster hardware that follows a distributed memory model, where each compute node
    typically has one or more multicore processors and frequently a GPU. The cluster
    hardware typically includes a *distributed filesystem* that allows users and applications
    common access to files that reside locally on multiple nodes in the cluster.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/15fig06.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-6: Comparison of HDA vs. HPC frameworks. Based on a figure by Jack
    Dongarra and Daniel Reed.^([11](ch15.xhtml#fn15_11))*'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Unlike supercomputers, which are typically built and optimized for HPC use,
    the HDA community relies on *data centers*, which consist of a large collection
    of general-purpose compute nodes typically networked together via Ethernet. At
    a software level, data centers typically employ virtual machines, large distributed
    databases, and frameworks that enable high-throughput analysis of internet data.
    The term *cloud* refers to the data storage and computing power components of
    HDA data centers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we take a brief look at cloud computing, some of the software
    commonly used to enable cloud computing (specifically MapReduce), and some challenges
    for the future. Please note that this section is not meant to be an in-depth look
    at these concepts; we encourage interested readers to explore the referenced sources
    for greater detail.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 15.3.1 Cloud Computing
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Cloud computing* is the use or lease of the cloud for a variety of services.
    Cloud computing enables computing infrastructure to act as a “utility”: a few
    central providers give users and organizations access to (a seemingly infinite
    amount of) compute power through the internet, with users and organizations choosing
    to use as much as they want and paying according to their level of use. Cloud
    computing has three main pillars: software as a service (SaaS), infrastructure
    as a service (IaaS), and platform as a service (PaaS).^([12](ch15.xhtml#fn15_12))'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Software as a Service
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Software as a service* (SaaS) refers to software provided directly to users
    through the cloud. Most people utilize this pillar of cloud computing without
    even realizing it. Applications that many people use daily (e.g., web mail, social
    media, and video streaming) depend upon cloud infrastructure. Consider the classic
    application of web mail. Users are able to log on and access their web mail from
    any device, send and receive mail, and seemingly never run out of storage space.
    Interested organizations can in turn “rent” cloud email services to provide email
    to their own clients and employees, without incurring the hardware and maintenance
    cost of running the service themselves. Services in the SaaS pillar are managed
    completely by cloud providers; organizations and users do not (beyond configuring
    a few settings, perhaps) manage any part of the application, data, software, or
    hardware infrastructure, all which would be necessary if they were trying to set
    up the service on their own hardware. Prior to the advent of cloud computing,
    organizations interested in providing web mail for their users would need their
    own infrastructure and dedicated IT support staff to maintain it. Popular examples
    of SaaS providers include Google’s G Suite and Microsoft’s Office 365.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as a Service
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Infrastructure as a service* (IaaS) allows people and organizations to “rent
    out” computational resources to meet their needs, usually in the form of accessing
    virtual machines that are either general purpose or preconfigured for a particular
    application. One classic example is Amazon’s Elastic Compute Cloud (EC2) service
    from Amazon Web Services (AWS). EC2 enables users to create fully customizable
    virtual machines. The term *elastic* in EC2 refers to a user’s ability to grow
    or shrink their compute resource requests as needed, paying as they go. For example,
    an organization may use an IaaS provider to host its website or deploy its own
    series of custom-built applications to users. Some research labs and classrooms
    use IaaS services in lieu of lab machines, running experiments in the cloud or
    offering a virtual platform for their students to learn. In all cases, the goal
    is to eliminate the maintenance and capital needed to maintain a personal cluster
    or server for similar purposes. Unlike use cases in the SaaS pillar, use cases
    in the IaaS pillar require clients to configure applications, data, and in some
    cases the virtual machine’s OS itself. However, the host OS and hardware infrastructure
    is set up and managed by the cloud provider. Popular IaaS providers include Amazon
    AWS, Google Cloud Services, and Microsoft Azure.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Platform as a Service
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Platform as a service* (PaaS) allows individuals and organizations to develop
    and deploy their own web applications for the cloud, eliminating the need for
    local configuration or maintenance. Most PaaS providers enable developers to write
    their applications in a variety of languages and offer a choice of APIs to use.
    For example, Microsoft Azure’s service allows users to code web applications in
    the Visual Studio IDE and deploy their applications to Azure for testing. Google
    App Engine enables developers to build and test custom mobile applications in
    the cloud in a variety of languages. Heroku and CloudBees are other prominent
    examples. Note that developers have control over their applications and data only;
    the cloud provider controls the rest of the software infrastructure and all of
    the underlying hardware infrastructure.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 15.3.2 MapReduce
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Perhaps the most famous programming paradigm used on cloud systems is MapReduce.^([13](ch15.xhtml#fn15_13))
    Although MapReduce’s origins lay in functional programming’s Map and Reduce operations,
    Google was the first to apply the concept to analyzing large quantities of web
    data. MapReduce enabled Google to perform web queries faster than its competitors,
    and enabled Google’s meteoric rise as the go-to web service provider and internet
    giant it is today.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Map and Reduce Operations
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `map` and `reduce` functions in the MapReduce paradigm are based on the
    mathematical operations of Map and Reduce from functional programming. In this
    section, we briefly discuss how these mathematical operations work by revisiting
    some examples presented earlier in the book.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'The Map operation typically applies the same function to all the elements in
    a collection. Readers familiar with Python may recognize this functionality most
    readily in the list comprehension feature in Python. For example, the following
    two code snippets perform scalar multiplication in Python:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Regular scalar multiply
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Scalar multiply with list comprehension
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The list comprehension applies the same function (in this case, multiplying
    an array element with scalar value `s`) to every element `x` in `array`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: A single Reduce operation takes a collection of elements and combines them together
    into a single value using some common function. For example, the Python function
    `sum` acts similarly to a Reduce operation, as it takes a collection (typically
    a Python list) and combines all the elements together using addition. So, for
    example, applying addition to all the elements in the `result` array returned
    from the `scalarMultiply` function yields a combined sum of 50.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The MapReduce Programming Model
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A key feature of MapReduce is its simplified programming model. Developers need
    to implement only two types of functions, `map` and `reduce`; the underlying MapReduce
    framework automates the rest of the work.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: The programmer-written `map` function takes an input (*key*,*value*) pair and
    outputs a series of intermediate (*key*,*value*) pairs that are written to a distributed
    filesystem shared among all the nodes. A combiner that is typically defined by
    the MapReduce framework then aggregates (*key*,*value*) pairs by key, to produce
    (*key*,list(*value*)) pairs that are passed to the programmer-defined `reduce`
    function. The `reduce` function then takes as input a (*key*,list(*value*)) pair
    and combines all the values together through some programmer-defined operation
    to form a final (*key*,*value*), where the *value* in this output corresponds
    to the result of the reduction operation. The output from the `reduce` function
    is written to the distributed filesystem and usually output to the user.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how to use the MapReduce model to parallelize a program, we discuss
    the Word Frequency program. The goal of Word Frequency is to determine the frequency
    of each word in a large text corpus.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: A C programmer may implement the following `map` function for the Word Frequency
    program:^(13)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This `map` function receives as input a string (`key`) that corresponds to the
    name of the file, and a separate string (`value`) that contains a component of
    file data. The function then parses words from the input `value` and emits each
    word (`words[i]`) separately with the string value `"1"`. The `emit` function
    is provided by the MapReduce framework and writes the intermediate (*key*,*value*)
    pairs to the distributed filesystem.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete the Word Frequency program, a programmer may implement the following
    `reduce` function:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This `reduce` function receives as input a string (`key`) that corresponds to
    a particular word, and an `Iterator` struct (again, provided by the MapReduce
    framework) that consists of an aggregated array of items associated with the key
    (`items`), and the length of that array (`length`). In the Word Frequency application,
    `items` corresponds to a list of counts. The function then extracts the number
    of words from the `length` field of the `Iterator` struct, and the array of counts
    from the `items` field. It then loops over all the counts, aggregating the values
    into the variable `total`. Since the `emit` function requires `char *` parameters,
    the function converts `total` to a string prior to calling `emit`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: After implementing `map` and `reduce`, the programmer’s responsibility ends.
    The MapReduce framework automates the rest of the work, including partitioning
    the input, generating and managing the processes that run the `map` function (map
    tasks), aggregating and sorting intermediate (*key*,*value*) pairs, generating
    and managing the separate processes that run the `reduce` function (reduce tasks),
    and generating a final output file.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, in [Figure 15-7](ch15.xhtml#ch15fig7) we illustrate how MapReduce
    parallelizes the opening lines of the popular Jonathan Coulton song “Code Monkey”:
    *code monkey get up get coffee, code monkey go to job*.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/15fig07.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-7: Parallelization of the opening lines of the song “Code Monkey”
    using the MapReduce framework*'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-7](ch15.xhtml#ch15fig7) gives an overview of this process. Prior
    to execution, the boss node first partitions the input into *M* parts, where *M*
    corresponds to the number of map tasks. In [Figure 15-7](ch15.xhtml#ch15fig7),
    *M* = 3, and the input file (`coulton.txt`) is split into three parts. During
    the map phase, the boss node distributes the map tasks among one or more worker
    nodes, with each map task executing independently and in parallel. For example,
    the first map task parses the snippet *code monkey get up* into separate words
    and emits the following four (*key*,*value*) pairs: (`code`,`1`), (`monkey`,`1`),
    (`get`,`1`), (`up`,`1`). Each map task then emits its intermediate values to a
    distributed filesystem that takes up a certain amount of storage on each node.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Prior to the start of the reduce phase, the framework aggregates and combines
    the intermediate (*key*,*value*) pairs into (*key*,list(*value*)) pairs. In [Figure
    15-7](ch15.xhtml#ch15fig7), for example, the (*key*,*value*) pair (`get`,`1`)
    is emitted by two separate map tasks. The MapReduce framework aggregates these
    separate (*key*,*value*) pairs into the single (*key*,list(*value*)) pair (`get`,`[1,1]`).
    The aggregated intermediate pairs are written to the distributed filesystem on
    disk.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Next, the MapReduce framework directs the boss node to generate *R* reduce tasks.
    In [Figure 15-7](ch15.xhtml#ch15fig7), *R* = 8\. The framework then distributes
    the tasks among its worker nodes. Once again, each reduce task executes independently
    and in parallel. In the reduce phase of this example, the (*key*,list(*value*))
    pair (`get`,`[1,1]`) is reduced to the (*key*,*value*) pair (`get`,`2`). Each
    worker node appends the output of its set of reduce tasks to a final file, which
    is available to the user upon completion.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Fault Tolerance
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Data centers typically contain thousands of nodes. Consequently, the rate of
    failure is high; consider that if an individual node in a data center has a 2%
    chance of hardware failure, there is a greater than 99.99% chance that some node
    in a 1,000-node data center will fail. Software written for data centers must
    therefore be *fault tolerant*, meaning that it must be able to continue operation
    in the face of hardware failures (or else fail gracefully).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce was designed with fault tolerance in mind. For any MapReduce run,
    there is one boss node and potentially thousands of worker nodes. The chance that
    a worker node will fail is therefore high. To remedy this, the boss node pings
    individual worker nodes periodically. If the boss node does not receive a response
    from a worker node, the boss redistributes the worker’s assigned workload to a
    different node and re-executes the task.^(13) If the boss node fails (a low probability
    given that it is only one node), the MapReduce job aborts and must be rerun on
    a separate node. Note that sometimes a worker node may fail to respond to the
    boss node’s pings because the worker is bogged down by tasks. MapReduce therefore
    uses the same pinging and work redistribution strategy to limit the effect of
    slow (or straggler) worker nodes.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop and Apache Spark
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The development of MapReduce took the computing world by storm. However, Google’s
    implementation of MapReduce is closed source. As a result, engineers at Yahoo!
    developed Hadoop,^([14](ch15.xhtml#fn15_14)) an open source implementation of
    MapReduce, which was later adopted by the Apache Foundation. The Hadoop project
    consists of an ecosystem of tools for Apache Hadoop, including the Hadoop Distributed
    File System or HDFS (an open source alternative to Google File System), and HBase
    (modeled after Google’s BigTable).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop has a few key limitations. First, it is difficult to chain multiple MapReduce
    jobs together into a larger workflow. Second, the writing of intermediates to
    the HDFS proves to be a bottleneck, especially for small jobs (smaller than one
    gigabyte). Apache Spark^([15](ch15.xhtml#fn15_15)) was designed to address these
    issues, among others. Due to its optimizations and ability to largely process
    intermediate data in memory, Apache Spark is up to 100 times faster than Hadoop
    on some applications.^([16](ch15.xhtml#fn15_16))
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '15.3.3 Looking Toward the Future: Opportunities and Challenges'
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Despite the innovations in the internet data analytics community, the amount
    of data produced by humanity continues to grow. Most new data is produced in so-called
    *edge environments*, or near sensors and other data-generating instruments that
    are by definition on the other end of the network from commercial cloud providers
    and HPC systems. Traditionally, scientists and practitioners gather data and analyze
    it using a local cluster, or they move it to a supercomputer or data center for
    analysis. This “centralized” view of computing is no longer a viable strategy
    as improvements in sensor technology have exacerbated the data deluge.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: One reason for this explosive growth is the proliferation of small internet-enabled
    devices that contain a variety of sensors. These *Internet of Things* (IoT) devices
    have led to the generation of large and diverse datasets in edge environments.
    Transferring large datasets from the edge to the cloud is difficult, as larger
    datasets take more time and energy to move. To mitigate the logistic issues of
    so-called “Big Data,” the research community has begun to create techniques that
    aggressively summarize data at each transfer point between the edge and the cloud.^([17](ch15.xhtml#fn15_17))
    There is intense interest in the computing research community in creating infrastructure
    that is capable of processing, storing, and summarizing data in edge environments
    in a unified platform; this area is known as *edge* (or *fog*) computing. Edge
    computing flips the traditional analysis model of Big Data; instead of analysis
    occurring at the supercomputer or data center (“last mile”), analysis instead
    occurs at the source of data production (“first mile”).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: In addition to data movement logistics, the other cross-cutting concern for
    the analysis of Big Data is power management. Large, centralized resources such
    as supercomputers and data centers require a lot of energy; modern supercomputers
    require several megawatts (million watts) to power and cool. An old adage in the
    supercomputing community is that “a megawatt costs a megabuck”; in other words,
    it costs roughly $1 million annually to maintain the power requirement of one
    megawatt.^([18](ch15.xhtml#fn15_18)) Local data processing in edge environments
    helps mitigate the logistical issue of moving large datasets, but the computing
    infrastructure in such environments must likewise use the minimal energy possible.
    At the same time, increasing the energy efficiency of large supercomputers and
    data centers is paramount.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: There is also interest in figuring out ways to converge the HPC and cloud computing
    ecosystems to create a common set of frameworks, infrastructure and tools for
    large-scale data analysis. In recent years, many scientists have used techniques
    and tools developed by researchers in the cloud computing community to analyze
    traditional HPC datasets, and vice versa. Converging these two software ecosystems
    will allow for the cross-pollination of research and lead to the development of
    a unified system that allows both communities to tackle the coming onslaught of
    data and potentially share resources. The Big Data Exascale Computing (BDEC) working
    group^([19](ch15.xhtml#fn15_19)) argues that instead of seeing HPC and cloud computing
    as two fundamentally different paradigms, it is perhaps more useful to view cloud
    computing as a “digitally empowered” phase of scientific computing, in which data
    sources are increasingly generated over the internet.^(17) In addition, a convergence
    of culture, training, and tools is necessary to fully integrate the HPC and cloud
    computing software and research communities. BDEC also suggests a model in which
    supercomputers and data centers are “nodes” in a very large network of computing
    resources, all working in concert to deal with data flooding from multiple sources.
    Each node aggressively summarizes the data flowing to it, releasing it to a larger
    computational resource node only when necessary.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: As the cloud computing and HPC ecosystems look for unification and gird themselves
    against an increasing onslaught of data, the future of computer systems brims
    with exciting possibilities. New fields like artificial intelligence and quantum
    computing are leading to the creation of new *domain-specific architectures* (DSAs)
    and *application-specific integrated circuits* (ASICS) that will be able to handle
    custom workflows more energy efficiently than before (see the TPU^([20](ch15.xhtml#fn15_20))
    for one example). In addition, the security of such architectures, long overlooked
    by the community, will become critical as the data they analyze increases in importance.
    New architectures will also lead to new languages needed to program them, and
    perhaps even new operating systems to manage their various interfaces. To learn
    more about what the future of computer architecture may look like, we encourage
    readers to peruse an article by the 2017 ACM Turing Award winners and computer
    architecture giants, John Hennessy and David Patterson.^([21](ch15.xhtml#fn15_21))
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[1.](ch15.xhtml#rfn15_1) Sparsh Mittal, “A Survey Of Techniques for Architecting
    and Managing Asymmetric Multicore Processors,” *ACM Computing Surveys* 48(3),
    February 2016.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[2.](ch15.xhtml#rfn15_2) “FPGAs and the Road to Reprogrammable HPC,” inside
    HPC, July 2019, *[https://insidehpc.com/2019/07/fpgas-and-the-road-to-reprogrammable-hpc/](https://insidehpc.com/2019/07/fpgas-and-the-road-to-reprogrammable-hpc/)*'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[3.](ch15.xhtml#rfn15_3) “GPU Programming,” from CSinParallel: *[https://csinparallel.org/csinparallel/modules/gpu_programming.html](https://csinparallel.org/csinparallel/modules/gpu_programming.html)*;
    CSinParallel has other GPU programming modules: *[https://csinparallel.org](https://csinparallel.org)*'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[4.](ch15.xhtml#rfn15_4) *[https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_cuda.cu](https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_cuda.cu)*'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[5.](ch15.xhtml#rfn15_5) *[https://diveintosystems.org/book/C15-Parallel/_attachments/hello_world_mpi.c](https://diveintosystems.org/book/C15-Parallel/_attachments/hello_world_mpi.c)*'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[6.](ch15.xhtml#rfn15_6) *[https://www.open-mpi.org/](https://www.open-mpi.org/)*'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[7.](ch15.xhtml#rfn15_7) *[https://www.mpich.org/](https://www.mpich.org/)*'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[8.](ch15.xhtml#rfn15_8) Available at *[https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_mpi.c](https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_mpi.c)*'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[9.](ch15.xhtml#rfn15_9) *[https://hpc-tutorials.llnl.gov/mpi/](https://hpc-tutorials.llnl.gov/mpi/)*'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[10.](ch15.xhtml#rfn15_10) *[http://selkie.macalester.edu/csinparallel/modules/Patternlets/build/html/MessagePassing/MPI_Patternlets.html](http://selkie.macalester.edu/csinparallel/modules/Patternlets/build/html/MessagePassing/MPI_Patternlets.html)*'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[11.](ch15.xhtml#rfn15_11) D. A. Reed and J. Dongarra, “Exascale Computing
    and Big Data,” *Communications of the ACM* 58(7), 56–68, 2015.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[12.](ch15.xhtml#rfn15_12) M. Armbrust et al., “A View of Cloud Computing,”
    *Communications of the ACM* 53(4), 50–58, 2010.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[13.](ch15.xhtml#rfn15_13) Jeffrey Dean and Sanjay Ghemawat, “MapReduce: Simplified
    Data Processing on Large Clusters,” *Proceedings of the Sixth Conference on Operating
    Systems Design and Implementation*, Vol. 6, USENIX, 2004.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[14.](ch15.xhtml#rfn15_14) *[https://hadoop.apache.org/](https://hadoop.apache.org/)*'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[15.](ch15.xhtml#rfn15_15) *[https://spark.apache.org/](https://spark.apache.org/)*'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[16.](ch15.xhtml#rfn15_16) DataBricks, “Apache Spark,” *[https://databricks.com/spark/about](https://databricks.com/spark/about)*'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[17.](ch15.xhtml#rfn15_17) M. Asch et al., “Big Data and Extreme-Scale Computing:
    Pathways to Convergence – Toward a shaping strategy for a future software and
    data ecosystem for scientific inquiry,” *The International Journal of High Performance
    Computing Applications* 32(4), 435–479, 2018.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[18.](ch15.xhtml#rfn15_18) M. Halper, “Supercomputing’s Super Energy Needs,
    and What to Do About Them,” CACM News, *[https://cacm.acm.org/news/192296-supercomputings-super-energy-needs-and-what-to-do-about-them/fulltext](https://cacm.acm.org/news/192296-supercomputings-super-energy-needs-and-what-to-do-about-them/fulltext)*'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[19.](ch15.xhtml#rfn15_19) *[https://www.exascale.org/bdec/](https://www.exascale.org/bdec/)*'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[20.](ch15.xhtml#rfn15_20) N. P. Jouppi et al., “In-Datacenter Performance
    Analysis of a Tensor Processing Unit,” *Proceedings of the 44th Annual International
    Symposium on Computer Architecture*, ACM, 2017.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[21.](ch15.xhtml#rfn15_21) J. Hennessy and D. Patterson, “A New Golden Age
    for Computer Architecture,” *Communications of the ACM* 62(2), 48–60, 2019.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
