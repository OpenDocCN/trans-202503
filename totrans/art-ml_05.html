<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h1" id="ch04"><span epub:type="pagebreak" id="page_61" class="calibre2"/><strong class="calibre3"><span class="big">4</span><br class="calibre18"/>DEALING WITH LARGE NUMBERS OF FEATURES</strong></h2>
<div class="imagec"><img alt="Image" src="../images/common.jpg" class="calibre14"/></div>
<p class="noindent">In the previous chapter, we talked about <em class="calibre13">overfitting</em>—that is, using too many features in a given setting. Having a large number of features may also cause issues with long computation times. This chapter is all about reducing the size of our feature set—in other words, <em class="calibre13">dimension reduction</em>.</p>
<p class="indent">Note that it’s not just a need to use <em class="calibre13">fewer</em> features; we also need to decide <em class="calibre13">which</em> features, or even which <em class="calibre13">combinations</em> of features, to use. We’ll cover principal component analysis (PCA), one of the best-known techniques for dealing with large values of <em class="calibre13">p</em>, which is based on forming new features by combining old ones and then using just a few of the new ones as our feature set.</p>
<h3 class="h2" id="ch04lev1">4.1 Pitfall: Computational Issues in Large Datasets</h3>
<p class="noindent">Again, overfitting is a major issue in ML. As noted in the last chapter, plugging the term into Google gave me 6,560,000 results! And, in addition to predictive-accuracy problems of overfitting, we also need to worry about computation. The larger the number of features, the longer our computation will take.</p>
<p class="indent"><span epub:type="pagebreak" id="page_62"/>In some cases, computation times can be extremely challenging. For example, one write-up of AlexNet, a neural network for image classification, reported that the network takes <em class="calibre13">five or six days</em> to train on two extremely powerful computers.<sup class="calibre11"><a id="ch4fn1b" class="calibre12"/><a href="footnote.xhtml#ch4fn1" class="calibre12">1</a></sup></p>
<p class="indent">Moreover, the energy usage in the computation can be staggering.<sup class="calibre11"><a id="ch4fn2b" class="calibre12"/><a href="footnote.xhtml#ch4fn2" class="calibre12">2</a></sup> Training a large natural language processing model can use energy whose production results in emission of over 78,000 pounds of CO<sub class="calibre27">2</sub> into the atmosphere. By comparison, the average automobile will emit about 126,000 pounds of CO<sub class="calibre27">2</sub> in its lifetime.</p>
<p class="indent">Most readers of this book will not encounter exceptionally large applications like the above. But computation can be a significant problem even on merely “large-ish” datasets, such as some considered in this chapter. Your code’s run time—for a single function call—may not be measured in days, but it certainly could run into minutes or, in some cases, even hours.</p>
<p class="indent">In addition, a large dataset may use too much memory. A dataset with a million rows and 1,000 columns has a billion elements. At 8 bytes each, that’s 8GB of RAM. The amount of memory the algorithm uses may be several multiples of that.</p>
<p class="indent">Discussions of dimension reduction seldom mention excessive data loss due to NA values, but it can be a major factor. If there is even one NA value in a given row of our data, most ML software libraries will discard the entire row. The more columns we have, the more likely it is that any given row will have at least one NA value, and thus the more likely the row will be discarded. In other words:</p>
<p class="block">If our data is prone to NA values, the larger <em class="calibre13">p</em> is, the smaller our effective value of <em class="calibre13">n</em>.</p>
<p class="noindent">Thus we have yet another incentive to drop some features. This will result in an increase in the number of complete cases included in our analysis, resulting in better prediction ability.</p>
<p class="indent">As usual, it’s a trade-off: if we remove too many features, some may have substantial predictive ability. So, we hope to discard a few less-important ones.</p>
<h3 class="h2" id="ch04lev2">4.2 Introduction to Dimension Reduction</h3>
<p class="noindent">In this chapter, and in this book generally, our primary tool for attacking the problems posed by big data will be to reduce <em class="calibre13">p</em>, the number of features in our dataset. This is known as <em class="calibre13">dimension reduction</em>. While we could take the approach of simply removing features we believe won’t be very helpful, there are other, more systematic methods that can be applied.<span epub:type="pagebreak" id="page_63"/></p>
<p class="indent">Dimension reduction has two goals, which are both equally important:</p>
<ol class="calibre17">
<li class="noindent3">Avoid overfitting. If we don’t have <img alt="Image" class="middle3" src="../images/prootn.jpg"/> (see <a href="ch03.xhtml#ch03lev1sec3" class="calibre12">Section 3.1.3</a>), we should consider possibly reducing <em class="calibre13">p</em>.</li>
<li class="noindent3">Reduce computation. With larger datasets, k-NN and most of the methods in this book will have challenging computational requirements, and one obvious solution is to reduce <em class="calibre13">p</em>.</li>
</ol>
<h4 class="h3" id="ch04lev2sec1"><em class="calibre22"><strong class="calibre3">4.2.1 Example: The Million Song Dataset</strong></em></h4>
<p class="noindent">Say you’ve run into a recording of an old song without a label specifying its name and you wish to identify it. The Million Song Dataset allows us to predict a song’s release year from 90 audio characteristics, so let’s try it out, since knowing the year might help you find the song’s title. You can download the data from the UC Irvine Machine Learning Repository.<sup class="calibre11"><a id="ch4fn3b" class="calibre12"/><a href="footnote.xhtml#ch4fn3" class="calibre12">3</a></sup> (Actually there are only about 0.5 million songs in this version.)</p>
<p class="indent">A dataset of this size may involve a substantial computational burden. Let’s investigate that, finding the time to do just a single prediction.</p>
<p class="indent">Read in the data from the downloaded file and assign the result to <code>yr</code>.</p>
<pre class="calibre16">&gt; <span class="codestrong">yr &lt;- read.csv('YearPredictionMSD.txt',header=FALSE)</span></pre>
<p class="noindent">Note that in spite of the <em class="calibre13">.txt</em> suffix in the filename, it is actually a CSV file, so we used <code>read.csv()</code>.</p>
<p class="indent">Also, the above file read was slow. The reader may consider using <code>fread()</code> from the <code>data.table</code> package on large files like this. Here the call would be:</p>
<pre class="calibre16">&gt; <span class="codestrong">library(data.table)</span>
&gt; <span class="codestrong">yr &lt;- fread('YearPredictionMSD.txt')</span>
&gt; <span class="codestrong">yr &lt;- as.data.frame(yr)</span></pre>
<p class="noindent">Since <code>kNN()</code> requires data frame or R matrix input (<a href="app03.xhtml#appc" class="calibre12">Appendix C</a>), we needed that last line to convert from a <code>data.table</code> to a data frame.</p>
<p class="indent">Let’s take a look:</p>
<pre class="calibre16">&gt; <span class="codestrong">yr &lt;- read.csv('YearPredictionMSD.txt',header=FALSE)</span>
&gt; <span class="codestrong">yr[1,]</span>
    V1       V2       V3      V4      V5        V6        V7        V8
1 2001 49.94357 21.47114 73.0775 8.74861 -17.40628 -13.09905 -25.01202
         V9     V10      V11     V12      V13      V14      V15      V16
1 -12.23257 7.83089 -2.46783 3.32136 -2.31521 10.20556 611.1091 951.0896
       V17      V18      V19      V20      V21      V22      V23      V24
1 698.1143 408.9848 383.7091 326.5151 238.1133 251.4241 187.1735 100.4265
      V25      V26       V27      V28      V29       V30       V31     V32
1 179.195 -8.41558 -317.8704 95.86266 48.10259 -95.66303 -18.06215 1.96984
       V33     V34    V35     V36      V37       V38       V39       V40
1 34.42438 11.7267 1.3679 7.79444 -0.36994 -133.6785 -83.26165 -37.29765
       V41       V42      V43       V44       V45      V46       V47      V48
1 73.04667 -37.36684 -3.13853 -24.21531 -13.23066 15.93809 -18.60478 82.15479
       V49       V50      V51       V52      V53      V54     V55      V56
1 240.5798 -10.29407 31.58431 -25.38187 -3.90772 13.29258 41.5506 -7.26272
        V57      V58      V59      V60      V61      V62     V63      V64
1 -21.00863 105.5085 64.29856 26.08481 -44.5911 -8.30657 7.93706 -10.7366
        V65       V66       V67     V68      V69      V70     V71       V72
1 -95.44766 -82.03307 -35.59194 4.69525 70.95626 28.09139 6.02015 -37.13767
       V73      V74     V75      V76      V77       V78      V79       V80
1 -41.1245 -8.40816 7.19877 -8.60176 -5.90857 -12.32437 14.68734 -54.32125
       V81     V82       V83      V84      V85     V86       V87      V88
1 40.14786 13.0162 -54.40548 58.99367 15.37344 1.11144 -23.08793 68.40795
       V89       V90     V91
1 -1.82223 -27.46348 2.26327</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_64"/>We see there are over 515,000 rows (that is, 515,000 songs) and 91 columns. That first column is the year of release, followed by 90 columns of arcane audio measurements. That means column 1 is our outcome variable and the remaining 90 columns are (potential) features.</p>
<h4 class="h3" id="ch04lev2sec2"><em class="calibre22"><strong class="calibre3">4.2.2 The Need for Dimension Reduction</strong></em></h4>
<p class="noindent">We have 90 features here. With over 500,000 data points, the rough rule of thumb <img alt="Image" class="middle3" src="../images/prootn.jpg"/> from <a href="ch03.xhtml#ch03lev1sec3" class="calibre12">Section 3.1.3</a> says we probably could use all 90 features without overfitting. But k-NN requires a lot of computation, so dimension is still an important issue.</p>
<p class="indent">As an example of the computational burden, let’s see how long it takes to predict one data point, say, the first row of our data. As explained in <a href="ch01.xhtml#ch01lev17" class="calibre12">Section 1.17</a>, to predict just one data point, it is faster to call <code>kNN()</code> directly rather than calling its wrapper, <code>qeKNN()</code>. The latter operates on a large scale, which we don’t need in this instance, and which will compute many quantities not needed here. (It computes the estimated regression function at every point of the training set. This is effective only if we plan to do a lot of predictions in the future.)</p>
<p class="indent">Recall that the arguments of <code>kNN()</code> are the <em class="calibre13">X</em> data, the <em class="calibre13">Y</em> data, the data point to be predicted, and <em class="calibre13">k</em>. Here, then, is the code:</p>
<pre class="calibre16">&gt; <span class="codestrong">system.time(kNN(yr[,-1],yr[,1],yr[1,-1],25))</span>
  user  system elapsed
30.866   8.330  39.201</pre>
<p class="noindent">That’s over 39 seconds just to predict one data point. If we predict a substantial portion of the entire original dataset, say, with <code>holdout</code> = 100000, that means we incur that 39-second wait 100,000 times, which would be prohibitive.</p>
<p class="indent">Thus we may want to cut down the size of our feature set, whether out of computational concerns, as was the case here, or in other cases because of a need to avoid overfitting.</p>
<p class="indent"><span epub:type="pagebreak" id="page_65"/>Feature selection is yet another aspect of ML that has no perfect solution but for which some pretty good approaches have been developed. Before we get into the main approach this chapter deals with, PCA, let’s look at a few other techniques to get a better sense of the challenges we face when selecting features.</p>
<h3 class="h2" id="ch04lev3">4.3 Methods for Dimension Reduction</h3>
<p class="noindent">Now that we see the need for dimension reduction, how can we achieve it? We’ll cover a few approaches here: consolidation and embedding, <em class="calibre13">all-possible subsets</em>, and PCA. These are generally applicable, and we will present some techniques for some specific ML methods later in the book.</p>
<h4 class="h3" id="ch04lev3sec1"><em class="calibre22"><strong class="calibre3">4.3.1 Consolidation and Embedding</strong></em></h4>
<p class="noindent">Economists talk of <em class="calibre13">proxy</em> variables. We may wish to have data on some variable <em class="calibre13">U</em> but, lacking it, use instead a variable <em class="calibre13">V</em> for which we do have data and that captures much of the essence of <em class="calibre13">U</em>. <em class="calibre13">V</em> is said to be a “proxy” for <em class="calibre13">U</em>. Related techniques in ML are <em class="calibre13">consolidation</em> and <em class="calibre13">embedding</em>.</p>
<p class="indent">In the context of dimension reduction, proxies can have a different use. Say we do have data on <em class="calibre13">U</em>, but this variable is categorical with a huge number of categories (that is, a huge number of levels in the R factor implementation of the variable). That would mean having a huge number of dummy variables, or high dimension. One way to reduce dimension in such settings would be to combine levels, yielding a new categorical variable <em class="calibre13">V</em> with fewer dummies. Or even better, we may be able to use a numerical <em class="calibre13">V</em>, thus no dummies, just that one variable.</p>
<p class="indent">Again consider the ZIP code example of <a href="ch03.xhtml#ch03lev1sec1" class="calibre12">Section 3.1.1</a>, where the hypothetical goal was to estimate parka sales. If there were 42,000 ZIP codes, then in terms of dummy variables, we would have 42,000 dummies. We might cut that down by, say, choosing to use only the first digit of the ZIP code. That would mean, for instance, combining levels 90000, 90001, . . . , 99999 in a single level, 9. (Not all of those ZIP codes actually exist, but the principle is the same.) The ZIP codes for UC Davis and UCLA, 95616 and 90024, respectively, would now both reduce to simply 9.</p>
<p class="indent">Obviously, this would result in loss of information; it would induce some bias into the Bias-Variance Trade-off. But we would still get fairly good geographic detail—for instance, the 9s are all on the West Coast—for the purpose of, say, predicting parka purchases. That way, we’d have a lot of data points for each of the 10 (abbreviated) ZIP codes 0 through 9, thus reducing variance.</p>
<p class="indent">This would reduce 42,000 dummies to 9. Better yet, we might choose to use an <em class="calibre13">embedding</em>. We could fetch the average daily winter temperature per ZIP code from government data sites (remember, we’re selling parkas), and use that temperature instead of ZIP code as our feature. Now we would have only 1 feature, not 42,000, and not even 9.<span epub:type="pagebreak" id="page_66"/></p>
<h4 class="h3" id="ch04lev3sec2"><em class="calibre22"><strong class="calibre3">4.3.2 The All Possible Subsets Method</strong></em></h4>
<p class="noindent">One might think, “To choose a good feature set, why not look at all possible subsets of features? We would find MAPE or OME for each one and then use the subset that minimizes that quantity.” This is called the <em class="calibre13">All Possible Subsets Method</em>.</p>
<p class="indent">In the song dataset, for instance, in predicting holdout data, we would proceed as follows: for each subset of our 90 columns, we could predict our holdout set, then use the set of columns with the best prediction record. There are two big problems here:</p>
<ol class="calibre17">
<li class="noindent3">We’d need tons of computation time. The 2-feature sets alone would number over 4,000. The number of feature sets of all sizes is 2<sup class="calibre11">90</sup>, one of those absurd figures like “number of atoms in the universe” that one often sees in the press.</li>
<li class="noindent3">We’d risk serious p-hacking issues. The chance of some pair of columns accidentally looking very accurate in predicting release year is probably very high.</li>
</ol>
<h4 class="h3" id="ch04lev3sec3"><em class="calibre22"><strong class="calibre3">4.3.3 Principal Components Analysis</strong></em></h4>
<p class="noindent">But this method isn’t infeasible after all if we change our features by using one of the most popular approaches to dimension reduction: PCA. In the song data, for instance, instead of having to investigate an impossible 2<sup class="calibre11">90</sup> number of subsets, we need look only at 90. Here is how it works.</p>
<p class="indent">To apply PCA to the Million Song Dataset, we will replace our 90 features with 90 new ones, to be constructed as 90 combinations of the originals. In our data frame, we’d replace the 90 original feature columns with these 90 new features, known as <em class="calibre13">principal components (PCs)</em>.</p>
<p class="indent">Sounds like no gain, right? We are hoping to <em class="calibre13">reduce</em> the number of features, whereas in the above scenario we simply swapped one set of 90 features for another set of that size. But as will be seen below, we will accomplish that goal by using the All Possible Subsets Method on these new features, though in a certain ordering.</p>
<p class="indent">Our first subset to check will be PC1; then the pair PC1 and PC2; then the triple PC1, PC2, PC3; and so on. That means only 90 subsets in all, and typically we stop well short of 90 anyway.</p>
<p class="indent">You’ll see that it will be a lot easier to choose subsets among these new features than among the original 90.</p>
<h5 class="h4" id="ch04lev3sec3sec1">4.3.3.1 PC Properties</h5>
<p class="noindent">The PCs have two special properties. First, they are uncorrelated; roughly speaking, the value of one has no effect on the others. In that sense, we might think of them as not duplicating each other. Why is that important? If, after reducing our number of predictors, some of them were to partially duplicate each other, it would seem that we should reduce the number even further. Thus having uncorrelated features means unduplicated features, and we feel like we’ve achieved a minimal, nonduplicative set.</p>
<p class="indent"><span epub:type="pagebreak" id="page_67"/>Second, the PCs are arranged in order of decreasing variance. Since a feature with small variance is essentially constant, it probably won’t be a useful feature. Accordingly, we might retain just the PCs that are of substantial variance. Since the variances are ordered, that means retaining, say, the <em class="calibre13">m</em> PCs having the largest variances. Note that <em class="calibre13">m</em> then becomes another hyperparameter.</p>
<p class="indent">Each PC is a <em class="calibre13">linear combination</em> of our original features—that is, a PC is a sum of constants times the features. If, for instance, the latter were Height, Weight, and Age, a PC might be 0.12 Height + 0.59 Weight − 0.02 Age. Recall from algebra that these numbers, 0.12, 0.59, and −0.02, are called <em class="calibre13">coefficients</em>.</p>
<p class="indent">In ML, one’s focus is prediction rather than description. For instance, in the bike rental data, someone may be interested in investigating <em class="calibre13">how</em> the features affect ridership, say, how many extra riders we might have on holidays. That would be a description application, which is not usually a concern in ML. So, those coefficients, such as 0.12, 0.59, and −0.02, are not very relevant for us. Instead, the point is that we are creating new features as combinations of the original ones, and we may not examine the coefficients themselves.</p>
<p class="indent">With these new features, we have many fewer candidate sets to choose from: the first PC, the first two PCs, the first three PCs, and so on. We are choosing from just 90 candidate feature sets rather than the unimaginable 2<sup class="calibre11">90</sup> we would have if we looked at all possible subsets. We can choose among those subsets using cross-validation or another method introduced below. Remember, smaller sets save computation time (for k-NN now and other methods later) and help avoid overfitting and p-hacking.</p>
<h5 class="h4" id="ch04lev3sec3sec2">4.3.3.2 PCA in the Million Song Dataset</h5>
<p class="noindent">Base-R includes several functions to do PCA, including the one we’ll use here, <code>prcomp()</code>. CRAN also has packages that implement especially fast PCA algorithms, such as <code>RSpectra</code>, which are very useful for large datasets.</p>
<p class="indent">We’ll mainly use <code>qePCA()</code>, a <code>regtools</code> function that wraps <code>prcomp()</code>, but you should get at least some exposure to the latter function itself, as follows. Here is the call:</p>
<pre class="calibre16">&gt; <span class="codestrong">pcout &lt;- prcomp(yr[,-1])</span></pre>
<p class="noindent">Remember, PCA is for the <em class="calibre13">X</em> data, not <em class="calibre13">Y</em>, so we skipped the year field here, which is in column 1.</p>
<p class="indent">As an example, let’s say we’ve decided to use the first 20 PCs. We do this by retaining the first 20 columns of the <code>rotation</code> component of the output, which contains the coefficients of the PCs (such as the 0.59 and so on above). Let’s first understand this component:</p>
<pre class="calibre16">&gt; <span class="codestrong">rt &lt;- pcout$rotation</span>
&gt; <span class="codestrong">dim(rt)</span>
[1] 90 90
&gt; <span class="codestrong">rt[1:10,1:10]</span>
              PC1           PC2           PC3           PC4           PC5
V2  -1.492918e-03 -0.0001759914  0.0006706243 -0.0005624011 -0.0027970362
V3  -6.293541e-03  0.0064782264  0.0027845347 -0.0051094877 -0.0239474483
V4  -4.794322e-03 -0.0034153809 -0.0066157025 -0.0044176142  0.0071294386
V5   2.143497e-03  0.0041913051  0.0022151336 -0.0016482743 -0.0087460406
V6   3.013464e-03  0.0013071748 -0.0013400543  0.0032640501  0.0028682836
V7   1.892908e-03  0.0052421454  0.0031921120 -0.0003470488  0.0028807861
V8  -1.518245e-04 -0.0001992038 -0.0004968169  0.0010151582 -0.0061954012
V9   3.160014e-04  0.0009142221 -0.0001189277 -0.0008729237  0.0014144888
V10 -7.203671e-04 -0.0001721222  0.0003838908 -0.0012362764 -0.0009607571
V11  3.348651e-05  0.0011663312  0.0003964205 -0.0008766769 -0.0002092245
              PC6           PC7           PC8           PC9          PC10
V2   3.227906e-04 -0.0012293820  0.0009259120 -0.0009496448  0.0007397941
V3  -1.247137e-02 -0.0185123792  0.0122138115 -0.0186672476  0.0099506738
V4  -9.063568e-03  0.0160957668  0.0123209033 -0.0018904928 -0.0039866354
V5   2.708827e-03 -0.0085000743 -0.0070522158  0.0008992783  0.0003743787
V6  -7.412826e-03 -0.0003856357  0.0003811818 -0.0002785585  0.0022809304
V7   9.840927e-05 -0.0009025075 -0.0006593167  0.0032070603  0.0024775608
V8   3.425829e-03 -0.0038602270 -0.0040122340  0.0005974108  0.0009542331
V9  -1.280174e-03  0.0032705198  0.0059051252 -0.0025076692 -0.0049897536
V10 -4.725944e-05 -0.0035063179 -0.0002710257 -0.0016148579  0.0007367726
V11 -1.940717e-03 -0.0015004583  0.0036280402 -0.0025576335 -0.0025080611</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_68"/>So, <code>rt</code> has 90 rows and 90 columns, with numerical entries. We see that the names of the rows and columns are <code>V2</code>, <code>V3</code>, . . . , and <code>PC1</code>, <code>PC2</code>, . . . , respectively. The row names come from our feature names (the first is <em class="calibre13">Y</em>, not a feature):</p>
<pre class="calibre16">&gt; <span class="codestrong">names(yr)</span>
 [1] "V1"  "V2"  "V3"  "V4"  "V5"  "V6"  "V7"  "V8"  "V9"  "V10" "V11" "V12"
[13] "V13" "V14" "V15" "V16" "V17" "V18" "V19" "V20" "V21" "V22" "V23" "V24"
...
[85] "V85" "V86" "V87" "V88" "V89" "V90" "V91"</pre>
<p class="noindent">The column names are for the PCs. We have 90 features, thus 90 PCs, named <code>PC1</code>, <code>PC2</code>, and so on.</p>
<p class="indent">In this example, we have said that we want to use only the first 20 PCs for dimension reduction. Thus we will discard columns PC21 through PC90 in <code>rt</code>:</p>
<pre class="calibre16">&gt; <span class="codestrong">pcout$rotation &lt;- rt[,1:20]</span></pre>
<p class="noindent">So, we’re now using only those first 20 PCs. Now we convert our original data accordingly:</p>
<pre class="calibre16">&gt; <span class="codestrong">pcX &lt;- predict(pcout,yr[,-1])</span></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_69"/>This is our new <em class="calibre13">X</em> data. We’ll look at how to use it shortly, but first, what happened in that <code>predict()</code> call? The function is not actually predicting anything here. It simply converts the original features to new ones. Let’s take a closer look at that.</p>
<p class="indent">First, recall that, in R, <code>predict()</code> is a <em class="calibre13">generic</em> function (see <a href="ch01.xhtml#ch01lev5sec1" class="calibre12">Section 1.5.1</a>). The <code>prcomp()</code> function returns an object of class <code>'prcomp'</code> (check this by typing <code>class(pcout)</code> for the data above). So, the call to <code>predict()</code> here gets relayed to <code>predict.prcomp()</code>, which the R people wrote for the purpose here—to do conversion from old features to new ones.</p>
<p class="indent">So, our call <code>predict(pcout,yr[,-1])</code> tells R, “Please convert the features in <code>yr[,-1]</code> to PC features, based on what’s in <code>pcout</code>.” Since we previously had altered <code>pcout$rotation</code> to use just 20 PCs, now calling <code>predict()</code> on <code>pcout</code> will generate a new 20-column data frame to use instead of our original 90-column one:</p>
<pre class="calibre16">&gt; <span class="codestrong">dim(pcX)</span>
[1] 515345     20</pre>
<p class="noindent">We have the same number of rows as in the old <em class="calibre13">X</em>—as we should since it’s still the song data (albeit transformed), with 515,345 songs. But now we have 20 columns instead of 90, representing the fact that we now have only 20 features. Note, by the way, that none of these new columns was a column in <code>yr</code>; each new column is a certain mixture of the original ones.</p>
<p class="indent">Now, how would we do k-NN prediction using PCA? Imagine that we have a new case whose 90 audio measures are the same as the eighth song in our dataset, except that there is a value 32.6 in the first column. Let’s store that in <code>w</code>.</p>
<pre class="calibre16">&gt; <span class="codestrong">w &lt;- yr[8,-1]</span>
&gt; <span class="codestrong">w[1] &lt;- 32.6</span></pre>
<p class="noindent">Since we changed our <em class="calibre13">X</em> data in our training set to PCA form, we’ll do the same for <code>w</code>:</p>
<pre class="calibre16">&gt; <span class="codestrong">newW &lt;- predict(pcout,w)</span>
&gt; <span class="codestrong">newW</span>
        PC1       PC2       PC3      PC4      PC5       PC6       PC7      PC8
8 -1129.298 -168.8051 -368.4595 567.6397 -424.265 -291.5593 -168.7774 17.12907
       PC9      PC10      PC11     PC12     PC13     PC14     PC15      PC16
8 243.8396 -134.2792 -35.67449 191.6426 37.70002 50.17716 -66.3604 -72.97002
       PC17      PC18     PC19     PC20
8 -131.9982 -223.3206 -50.0114 52.04066</pre>
<p class="noindent">We would then call <code>qeKNN()</code> on our new data.</p>
<p class="indent">To do so directly, though, would be tedious, and we’ll usually use the labor-saving <code>qePCA()</code> function instead. But let’s put prediction aside for the moment while we discuss the issue of choosing the number of PCs to use.<span epub:type="pagebreak" id="page_70"/></p>
<h4 class="h3" id="ch04lev3sec4"><em class="calibre22"><strong class="calibre3">4.3.4 But Now We Have Two Hyperparameters</strong></em></h4>
<p class="noindent">Before now, to attain the most accurate predictions, we only needed to try a range of values for the number of nearest neighbors <em class="calibre13">k</em>, but in this case, we’ll have to test a range of values for both <em class="calibre13">k</em> and the number of PCs <em class="calibre13">m</em>. It’s nothing new, of course, since we always needed to decide which features to use; overfitting can arise from having both too small a value of <em class="calibre13">k</em> and too many features. Now, by converting to PCA, we at least have formalized the latter in a hyperparameter <em class="calibre13">m</em>. So, we’ve actually made life a little easier.</p>
<p class="indent">Say we try each of 10 <em class="calibre13">k</em> values paired with each of 10 <em class="calibre13">m</em> values and find the holdout MAPE for each pair. We would then choose to use the pair with the smallest MAPE. But that’s 10 × 10 = 100 pairs to try, say, with K-fold cross-validation applied to each pair (that is, running many holdout sets for each pair).</p>
<p class="indent">Recall there are two major problems with this. First, each pair will take substantial computation time, making for a very long total run time, and second, we should be concerned about potential p-hacking (see <a href="ch01.xhtml#ch01lev13" class="calibre12">Section 1.13</a>): one of those 100 pairs may seem to predict song release year especially accurately, simply by coincidence.</p>
<p class="indent">As an alternative, we might try to choose <em class="calibre13">m</em> directly. A common intuitive approach is to look at the variances of the PCs (squares of standard deviations). A variable with small variance is essentially constant and thus presumably of little or no use in predicting <em class="calibre13">Y</em>. So, the idea is to discard PCs having small variance. Let’s take a look:</p>
<pre class="calibre16">&gt; <span class="codestrong">pcout$sdev^2</span>
 [1] 4.471212e+06 1.376757e+06 8.730969e+05 4.709903e+05
 [5] 2.951400e+05 2.163123e+05 1.701326e+05 1.553153e+05
 [9] 1.477271e+05 1.206304e+05 1.099533e+05 9.319449e+04
...</pre>
<p class="noindent">That <code>e</code> notation means powers of 10. The first one, for instance, means 4.471212 × 10<sup class="calibre11">6</sup>.</p>
<p class="indent">We see the variances rapidly decrease. The twelfth one is about 90,000, which is tiny relative to the first, at over 4 million. We might therefore decide to take <em class="calibre13">m</em> = 12. Once we’ve done that, we are back to the case of choosing just one hyperparameter, <em class="calibre13">k</em>.</p>
<p class="indent">There is still no magic formula for choosing the number of PCs <em class="calibre13">m</em>, but the point is that once we decide on a value of <em class="calibre13">m</em>, we can now choose <em class="calibre13">k</em> separately. We may, for instance, choose <em class="calibre13">m</em> in this intuitive manner and then use cross-validation to get <em class="calibre13">k</em>. This may be easier than finding the best (<em class="calibre13">k</em>, <em class="calibre13">m</em>) pair simultaneously.</p>
<p class="indent">It is common for analysts to look at those variances as a cumulative proportion of the total. Here R’s <code>cumsum()</code> (cumulative sums) function will come in handy. This is how that function works:</p>
<pre class="calibre16">&gt; <span class="codestrong">u &lt;- c(12,5,13)</span>
&gt; <span class="codestrong">cumsum(u)</span>  # 12+5 = 17, 12+5+13 = 30
[1] 12 17 30
&gt; <span class="codestrong">cumsum(u) / length(u)</span>  # convert to proportions
[1]  4.000000  5.666667 10.000000</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_71"/>Let’s apply that to the PC variances:</p>
<pre class="calibre16">&gt; <span class="codestrong">pcsds &lt;- pcout$sdev^2</span>
&gt; <span class="codestrong">cumsum(pcsds) / sum(pcsds)</span>
 [1] 0.4691291 0.6135814 0.7051886 0.7546059 0.7855726 0.8082685 0.8261192
 [8] 0.8424152 0.8579151 0.8705719 0.8821084 0.8918866 0.9003893 0.9075907
[15] 0.9146712 0.9212546 0.9269892 0.9324882 0.9376652 0.9416823 0.9455962
[22] 0.9493949 0.9530321 0.9564536 0.9597000 0.9627476 0.9653836 0.9678344
[29] 0.9700491 0.9720936 0.9739898 0.9756223 0.9771978 0.9785780 0.9799187
[36] 0.9811843 0.9824226 0.9836061 0.9847728 0.9858420 0.9868994 0.9879009
[43] 0.9888200 0.9896491 0.9904612 0.9912403 0.9919651 0.9926362 0.9932901
[50] 0.9938841 0.9944387 0.9949139 0.9953413 0.9957644 0.9961658 0.9965495
[57] 0.9968947 0.9971970 0.9974792 0.9977482 0.9979897 0.9982232 0.9984284
[64] 0.9985960 0.9987592 0.9989146 0.9990633 0.9992035 0.9993261 0.9994199
[71] 0.9995115 0.9995965 0.9996681 0.9997329 0.9997858 0.9998289 0.9998700
[78] 0.9999061 0.9999277 0.9999483 0.9999608 0.9999708 0.9999792 0.9999856
[85] 0.9999907 0.9999948 0.9999974 0.9999987 0.9999996 1.0000000</pre>
<p class="indent">So, if, for example, we were to take <em class="calibre13">m</em> = 12, our chosen PCs would make up about 89 percent of total variance in this data. Or we might think that’s too much dimension reduction and opt for, say, a 95 percent cutoff, thus using <em class="calibre13">m</em> = 23 PCs.</p>
<div class="note">
<p class="notet"><strong class="calibre3"><span class="notes">NOTE</span></strong></p>
<p class="notep"><em class="calibre13">Note the phrasing “might,” “if, for example,” and “say” here. As we’ve seen before, for many things in ML, there are no mechanical, formulaic “recipes” for one’s course of action. Once again, keep in mind that ML is an art, not a science. And one becomes skilled in that art with experience, not by recipes.</em></p>
</div>
<h4 class="h3" id="ch04lev3sec5"><em class="calibre22"><strong class="calibre3">4.3.5 Using the qePCA() Wrapper</strong></em></h4>
<p class="noindent">Our goal, then, is to first use PCA for dimension reduction and then do our k-NN prediction with the first <em class="calibre13">m</em> PCs as features. To do this, the following rather elaborate set of actions would need to be performed. Fortunately, there is a function to automate these steps, but as usual, we need to understand those steps first before turning to the convenience function.</p>
<p class="indent">Say the <em class="calibre13">X</em> and <em class="calibre13">Y</em> portions of our training data are stored in <code>trnX</code> and <code>trnY</code>, respectively.</p>
<ol class="calibre17">
<li class="noindent3">We call <code>prcomp(trnX)</code> to calculate the PC coefficients. Let’s name this result <code>pcout</code>, as above.</li>
<li class="noindent3">We limit the number of columns in <code>pcout$rotation</code> to reflect the desired number of PCs.</li>
<li class="noindent3">We call <code>predict(pcout,trnX)</code> to convert our <em class="calibre13">X</em> data to PC form, say, <code>pcX</code>. Now our new training set consists of <code>pcX</code> and <code>trnY</code>.</li>
<li class="noindent3">We now apply <code>qeKNN()</code> on <code>pcX</code> and <code>trnY</code>. Say we name the result <code>knnout</code>.</li>
<li class="noindent3"><span epub:type="pagebreak" id="page_72"/>Subsequently, when new <em class="calibre13">X</em> data comes in, say, <code>newX</code>, we would first call <code>predict(pcout,newX)</code> to convert to PC form. Say we name the results <code>pcNewx</code>.</li>
<li class="noindent3">For our <em class="calibre13">Y</em> prediction, we then call <code>predict(knnout,pcNewX)</code> to obtain our predicted <em class="calibre13">Y</em>.</li>
</ol>
<p class="noindent">By the way, did you notice that we are using <code>predict()</code> in two different ways here, one to convert to PC form and another to do k-NN prediction? Here we see the R concept of generic functions in action. In that first context, R relays the <code>predict()</code> call to <code>predict.prcomp()</code>, while in the second, the relay is to <code>predict.qeKNN()</code>.</p>
<p class="indent">There are a lot of steps in the procedure above, but it does have a pattern, which implies that we should automate it with code. That is the purpose of the wrapper function <code>qePCA()</code>.</p>
<p class="indent">With <code>qePCA()</code>, one specifies the data and <em class="calibre13">Y</em> column name, as in other <code>qe*</code>-series functions, and also specifies the following: the desired ML method (say, k-NN), the usual hyperparameters for that method (<em class="calibre13">k</em>), and the desired proportion of total variance for the PCs.</p>
<p class="indent">For instance, the call</p>
<pre class="calibre16"><span class="codestrong">z &lt;- qePCA(yr,'V1','qeKNN',opts=list(k=25),0.85)</span></pre>
<p class="noindent">states that we want to predict the year (<code>V1</code>) in the song data using k-NN and <em class="calibre13">k</em> = 25. It also says we want as many PCs as will give us 85 percent of the total variance of the features.</p>
<p class="indent">So, how well do we predict using <em class="calibre13">k</em> = 25 and 85 percent total variance?</p>
<pre class="calibre16">&gt; <span class="codestrong">z$testAcc</span>
[1] 7.373</pre>
<p class="noindent">In guessing which year a song was released, we will, on average, be off by about 7 years.</p>
<p class="indent">To later predict a new case—say, <code>w</code>, the example song we used in <a href="ch04.xhtml#ch04lev3sec3sec2" class="calibre12">Section 4.3.3.2</a> —we would make the call:</p>
<pre class="calibre16">&gt; <span class="codestrong">w &lt;- yr[8,-1]</span>
&gt; <span class="codestrong">w[1] &lt;- 32.6</span>
&gt; <span class="codestrong">predict(z,w)</span>
        [,1]
[1,] 1994.96</pre>
<p class="noindent">So, we’d guess that such a song was released around the year 1995.</p>
<p class="indent">All of this is far simpler than the above multistep process; <code>qePCA()</code> saves us a lot of work!<span epub:type="pagebreak" id="page_73"/></p>
<h4 class="h3" id="ch04lev3sec6"><em class="calibre22"><strong class="calibre3">4.3.6 PCs and the Bias-Variance Trade-off</strong></em></h4>
<p class="noindent">Now that we know how to use PCA for dimension reduction and subsequent prediction, let’s return to the issue of how to choose the number of nearest neighbors <em class="calibre13">k</em> and number of PCs <em class="calibre13">m</em>. Note that choosing <em class="calibre13">m</em> is equivalent to choosing the variance proportion in the last argument in <code>qePCA()</code>.</p>
<p class="indent">As usual, we have a bias-variance issue, for both <em class="calibre13">k</em> and <em class="calibre13">m</em>. We’ve discussed the trade-off in terms of <em class="calibre13">k</em> before; what about <em class="calibre13">m</em>?</p>
<p class="indent">Actually, the situation for <em class="calibre13">m</em> is not new either. Recall what we said in <a href="ch03.xhtml#ch03lev1sec1" class="calibre12">Section 3.1.1</a> regarding the <code>mlb</code> dataset:</p>
<p class="blockquote">We might predict weight from height and age. But what if we were to omit height from our feature set? That would induce a bias. Roughly speaking, we’d be tacitly assuming everyone is of middling height, which would result in our tending to overpredict the weight of shorter players while underpredicting that of the taller ones.</p>
<p class="noindent">In other words, omitting a feature induces a bias. Or, equivalently, adding features reduces bias. Since the PCs <em class="calibre13">are</em> features, we see that the more PCs we use, the smaller the bias. But that same section goes on to point out that the more features we have, the higher the variance in our predictions, which is not good. Note, too, that using more PCs results in longer computation time.</p>
<p class="indent">To illustrate this, let’s devise a little experiment to investigate the effect of varying <em class="calibre13">m</em>, the number of PCs.</p>
<p class="indent">Since our song dataset is rather large, let’s consider a random subset and see how well we do with proportions of total variance at various levels. We’ll use a subset of, say, 25,000, and check computation time and prediction accuracy:</p>
<pre class="calibre16">set.seed &lt;- 9999
yr1 &lt;- yr[sample(1:nrow(yr),25000),]  # extract 25,000 random rows
res &lt;- matrix(nrow=9,ncol=4)
for (i in 1:9) {  # loop to do proportions 0.05, 0.15,..., 0.95
   pcaProp &lt;- 0.05 + i*0.10
   st &lt;- system.time(z &lt;- qePCA(yr1,'V1','qeKNN',opts=list(k=25),pcaProp))
   res[i,1] &lt;- pcaProp
   res[i,2] &lt;- st[3]  # the actual elapsed run time
   res[i,3] &lt;- z$qeOut$testAcc
   res[i,4] &lt;- z$numPCs  # m
}</pre>
<p class="indent">The results are shown in <a href="ch04.xhtml#ch4tab1" class="calibre12">Table 4-1</a>. The first column shows the proportion of total variance, leading to a number of PCs <em class="calibre13">m</em> shown in the fourth column. In viewing the latter, remember that the full number of possible PCs <span epub:type="pagebreak" id="page_74"/>is 90. The second column shows run time. It increases with <em class="calibre13">m</em>, of course; a larger <em class="calibre13">m</em> means k-NN must do more computation.</p>
<p class="indent">The most important column in this investigation is the third, the MAPE values. We see that for a large increase in computation time, we attain only a moderate reduction in MAPE. Moreover, the best MAPE uses only 11 of the 90 PCs.</p>
<p class="indent">However, always keep in mind that these MAPE values are subject to sampling variation. They come from holdout sets, and as you know, holdout sets are randomly chosen. For each variance proportion level, we should look at several holdout sets, not just one, using cross-validation. We could do this by applying <code>replicMeans()</code> to the <code>qePCA()</code> call, though it would be time-consuming.</p>
<p class="indent">That means we cannot be sure that <em class="calibre13">m</em> = 11 is best. At the least, though, we see that the data indicates that we should probably use a lot fewer than 90 PCs. Also, in addition to the famous Bias-Variance Trade-off, there is the trade-off involving the analyst’s time. We may feel that setting <em class="calibre13">m</em> = 11 is good enough.</p>
<p class="tabcap" id="ch4tab1"><strong class="calibre5">Table 4-1:</strong> Behavior for Different Values of <em class="calibre13">m</em></p>
<div class="bqparan">
<table class="all">
<colgroup class="calibre28">
<col class="calibre29"/>
<col class="calibre29"/>
<col class="calibre29"/>
<col class="calibre29"/>
</colgroup>
<thead class="calibre30">
<tr class="calibre31">
<th class="table-h"><p class="noindent"><strong class="calibre5">pcaProp</strong></p></th>
<th class="table-h"><p class="noindent"><strong class="calibre5">Time (s.)</strong></p></th>
<th class="table-h"><p class="noindent"><strong class="calibre5">MAPE</strong></p></th>
<th class="table-h"><p class="noindent"><strong class="calibre5"># of PCs</strong></p></th>
</tr>
</thead>
<tbody class="calibre32">
<tr class="calibre31">
<td class="gray"><p class="noindent">0.15</p></td>
<td class="gray"><p class="noindent">1.137</p></td>
<td class="gray"><p class="noindent">8.40020</p></td>
<td class="gray"><p class="noindent">2</p></td></tr>
<tr class="calibre31">
<td class="calibre33"><p class="noindent">0.25</p></td>
<td class="calibre33"><p class="noindent">1.230</p></td>
<td class="calibre33"><p class="noindent">7.83712</p></td>
<td class="calibre33"><p class="noindent">3</p></td>
</tr>
<tr class="calibre31">
<td class="gray"><p class="noindent">0.35</p></td>
<td class="gray"><p class="noindent">2.051</p></td>
<td class="gray"><p class="noindent">8.12444</p></td>
<td class="gray"><p class="noindent">6</p></td></tr>
<tr class="calibre31">
<td class="calibre33"><p class="noindent">0.45</p></td>
<td class="calibre33"><p class="noindent">7.222</p></td>
<td class="calibre33"><p class="noindent">7.46536</p></td>
<td class="calibre33"><p class="noindent">11</p></td>
</tr>
<tr class="calibre31">
<td class="gray"><p class="noindent">0.55</p></td>
<td class="gray"><p class="noindent">17.812</p></td>
<td class="gray"><p class="noindent">7.88648</p></td>
<td class="gray"><p class="noindent">16</p></td>
</tr>
<tr class="calibre31">
<td class="calibre33"><p class="noindent">0.65</p></td>
<td class="calibre33"><p class="noindent">35.269</p></td>
<td class="calibre33"><p class="noindent">7.66808</p></td>
<td class="calibre33"><p class="noindent">24</p></td>
</tr>
<tr class="calibre31">
<td class="gray"><p class="noindent">0.75</p></td>
<td class="gray"><p class="noindent">51.041</p></td>
<td class="gray"><p class="noindent">7.55740</p></td>
<td class="gray"><p class="noindent">33</p></td>
</tr>
<tr class="calibre31">
<td class="calibre33"><p class="noindent">0.85</p></td>
<td class="calibre33"><p class="noindent">72.916</p></td>
<td class="calibre33"><p class="noindent">7.85736</p></td>
<td class="calibre33"><p class="noindent">46</p></td>
</tr>
<tr class="calibre31">
<td class="gray"><p class="noindent">0.95</p></td>
<td class="gray"><p class="noindent">94.761</p></td>
<td class="gray"><p class="noindent">7.72288</p></td>
<td class="gray"><p class="noindent">66</p></td>
</tr>
</tbody>
</table>
</div>
<p class="indent">Furthermore, remember as discussed in <a href="ch03.xhtml#ch03lev1sec2" class="calibre12">Section 3.1.2</a>, the larger <em class="calibre13">n</em> is, the larger we can afford to make <em class="calibre13">p</em>. In the above analysis, we had <em class="calibre13">n</em> = 25000 and <em class="calibre13">p</em> = <em class="calibre13">m</em> and settled on <em class="calibre13">m</em> = 11. But for the full dataset, <em class="calibre13">n</em> = 500000, presumably we should use more than 11 PCs.</p>
<p class="indent">Thus, we may be tempted to run the above code on the full dataset. Yet even for <em class="calibre13">n</em> = 25000, the run time was about half an hour; it would be many hours for the full dataset of more than 500,000 records. So, we may just settle for <em class="calibre13">m</em> = 11 and possibly do a more refined analysis later if time permits.</p>
<h3 class="h2" id="ch04lev4">4.4 The Curse of Dimensionality</h3>
<p class="noindent">The <em class="calibre13">Curse of Dimensionality (CoD)</em> says that ML gets harder and harder as the number of features grows. For instance, mathematical theory shows that in high dimensions, every point is approximately the same distance to every other point. Let’s briefly discuss the intuition underlying this bizarre <span epub:type="pagebreak" id="page_75"/>situation. Clearly, it has implications for k-NN, which relies on distances, and indeed for some, if not all, other ML methods as well.</p>
<p class="indent">To get a rough idea of the CoD, consider data consisting of students’ grades in mathematics, literature, history, geography, and so on. The distance between the data vectors of Students A and B would be the square root of</p>
<p class="center">(math grade<em class="calibre13"><sub class="calibre27">A</sub></em> − math grade<em class="calibre13"><sub class="calibre27">B</sub></em>)<sup class="calibre11">2</sup> + (lit grade<em class="calibre13"><sub class="calibre27">A</sub></em> − lit grade<em class="calibre13"><sub class="calibre27">B</sub></em>)<sup class="calibre11">2</sup> +<br class="calibre1"/>(history grade<em class="calibre13"><sub class="calibre27">A</sub></em> − history grade<em class="calibre13"><sub class="calibre27">B</sub></em>)<sup class="calibre11">2</sup> + (geo grade<em class="calibre13"><sub class="calibre27">A</sub></em> − geo grade<em class="calibre13"><sub class="calibre27">B</sub></em>)<sup class="calibre11">2</sup></p>
<p class="noindent">That expression is a sum, and one can show that sums with a large number of terms (only four here, but we could have many more) have small standard deviations relative to their means. A quantity with small standard deviation is nearly constant, so in high dimensions—that is, in settings with large <em class="calibre13">p</em> (a large number of features)—distances are nearly constant.</p>
<p class="indent">This is why k-NN fares poorly in high dimensions. This issue is not limited to k-NN; much of the ML toolbox has similar problems. The computation of linear/logistic regression (see <a href="ch08.xhtml" class="calibre12">Chapter 8</a>) involves a sum of <em class="calibre13">p</em> terms, and similar computations arise in support vector machines and neural networks.</p>
<p class="indent">In fact, lots of problems arise in high dimensions. Some analysts lump them all together into the CoD. Whatever one counts as CoD, clearly, higher dimensions are a challenge—all the more reason to perform dimension reduction.</p>
<h3 class="h2" id="ch04lev5">4.5 Other Methods of Dimension Reduction</h3>
<p class="noindent">Dimension reduction is one of the most actively researched and debated issues in ML and statistics. While this chapter focused on PCA, a common approach to the problem, there are a number of other approaches.</p>
<h4 class="h3" id="ch04lev5sec1"><em class="calibre22"><strong class="calibre3">4.5.1 Feature Ordering by Conditional Independence</strong></em></h4>
<p class="noindent">One approach that I find useful (and contributed slightly to its development) is <em class="calibre13">Feature Ordering by Conditional Independence (FOCI)</em>. It is based on solid mathematical principles (too complex to explain here) and works quite well, I’ve found.</p>
<p class="indent">The <code>qeML</code> package includes a FOCI wrapper, <code>qeFOCI</code>. Here is the call in its basic form:</p>
<pre class="calibre16">qeFOCI(data,yName)</pre>
<p class="noindent">Since the computational requirement can be large for this method, there are also parallelized options, such as the following:</p>
<pre class="calibre16">qeFOCI(data,yName,parPlat='locThreads')</pre>
<p class="noindent">This will split the computation into chunks, with each of your computer’s cores working on one chunk.</p>
<p class="indent"><span epub:type="pagebreak" id="page_76"/>Let’s try FOCI on the census data from <a href="ch03.xhtml#ch03lev2sec3" class="calibre12">Section 3.2.3</a>:</p>
<pre class="calibre16">&gt; <span class="codestrong">qeFOCI(pef,'wageinc')</span>
$selectedVar
    index   names
 1:    10 wkswrkd
 2:     1     age
 3:     6 occ.102
 4:     2 educ.14
 5:     3 educ.16
 6:     5 occ.101
 7:     4 occ.100
 8:     9   sex.1
 9:     8 occ.140
10:     7 occ.106
<br class="calibre1"/>
$stepT
 [1] 0.1909457 0.2373486 0.2501502 0.2736589 0.2838555 0.2879643 0.2946782
 [8] 0.2978845 0.2985381 0.3004416
<br class="calibre1"/>
attr(,"class")
[1] "foci"</pre>
<p class="indent">Note that <code>qeFOCI()</code> converts R factors to dummy variables. So, we see assessment of, for instance, each of the occupations. The one coded 102 seems to have good predictive power, while occupation 106 perhaps less so.</p>
<p class="indent">The <code>stepT</code> component of the output gives a type of correlation; the more predictors we add, the greater the predictive collective power of the variables. If we wish to be more conservative, we might cut off our choice when this correlation seems to level off, say, after 6 or 7 variables in this case.</p>
<p class="indent">How about the song data? To ease the computational burden, I took a 10 percent subsample of the data and ran with 2 cores. Yet even then, it ran for more than 20 minutes:</p>
<pre class="calibre16">&gt; <span class="codestrong">yr1 &lt;- yr[sample(1:nrow(yr),50000),]</span>
&gt; <span class="codestrong">system.time(z &lt;- qeFOCI(yr1,'V1',numCores=2,parPlat='locThreads'))</span>
[[1]]
 [1]  1 14  2  3  9 10  5  4 21  6 17 13 22
<br class="calibre1"/>
[[2]]
[1]  1 14  3  2 50 16 40 23 21
<br class="calibre1"/>
    user   system  elapsed
  79.873   10.489 1253.839
&gt; <span class="codestrong">z</span>
$selectedVar
   index names
1:     1    V2
2:    14   V15
3:     3    V4
4:     2    V3
5:    50   V51
<br class="calibre1"/>
$stepT
 [1] 0.0496876 0.1006503 0.1306439 0.1547381 0.1695977 0.1687346 0.0000000
 [8] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000
...</pre>
<p class="indent"><span epub:type="pagebreak" id="page_77"/>There were 2 cores used, and each applied FOCI to its chunk of data. The first core chose variables 1, 14, 2, and so on, while the second chose some of the same variables but also different ones; for instance, the second core chose variable 50, but the first did not. We designed the parallel version of the algorithm to take the union of the two sets of variables, so variable 50 does appear in the final list. Only the top 7 were chosen, though, as the correlation did not increase past that point.</p>
<h4 class="h3" id="ch04lev5sec2"><em class="calibre22"><strong class="calibre3">4.5.2 Uniform Manifold Approximation and Projection</strong></em></h4>
<p class="noindent">The Uniform Manifold Approximation and Projection (UMAP) method is similar in usage pattern to PCA in that we find new variables as functions of the original ones and then retain only the top few in terms of predictive ability. The difference, though, is that with UMAP, the new variables are complex nonlinear functions of the old ones.</p>
<p class="indent">The <code>qeML</code> package has a wrapper for UMAP, <code>qeUMAP()</code>. As noted, it is used in a similar manner to <code>qePCA()</code>. We will not pursue this further here in the book, but the reader is urged to give it a try.</p>
<h3 class="h2" id="ch04lev6">4.6 Going Further Computationally</h3>
<p class="noindent">For very large datasets, I highly recommend the <code>data.table</code> package. The <code>bigmemory</code> package can help with memory limitations, though it is for specialists who understand computers at the operating system level. Also, for those who know SQL databases, there are several packages that interface to such data, such as <code>RSQLite</code> and <code>dplyr</code>.</p>
<h3 class="h2" id="ch04lev7">4.7 Conclusions</h3>
<p class="noindent">In this chapter, we’ve developed an understanding of how computation issues interact with the Bias-Variance Trade-off when we work with largescale data. We may, for instance, wish to restrict our number of features well before reaching the point at which adding more variables is statistically unprofitable. Our featured remedy here has been PCA, though we have briefly cited others.<span epub:type="pagebreak" id="page_78"/></p>
</div></body></html>