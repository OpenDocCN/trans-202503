<html><head></head><body>
<h2 class="h2" id="ch07"><span epub:type="pagebreak" id="page_173"/><span class="big"><strong>7</strong></span><br/><strong>WEB AND INTERNET USERS</strong></h2>&#13;
<div class="imagec"><img src="../images/common4.jpg" alt="image"/></div>&#13;
<p class="noindent">One area where Unix really shines is the internet. Whether you want to run a fast server from under your desk or simply surf the web intelligently and efficiently, there’s little you can’t embed in a shell script when it comes to internet interaction.</p>&#13;
<p class="indent">Internet tools are scriptable, even though you might never have thought of them that way. For example, FTP, a program that is perpetually trapped in debug mode, can be scripted in some very interesting ways, as is explored in <a href="ch07.xhtml#ch07lev1sec01">Script #53</a> on <a href="ch07.xhtml#page_174">page 174</a>. Shell scripting can often improve the performance and output of most command line utilities that work with some facet of the internet.</p>&#13;
<p class="indent">The first edition of this book assured readers that the best tool in the internet scripter’s toolbox was <code>lynx</code>; now we recommend using <code>curl</code> instead. Both tools offer a text-only interface to the web, but while <code>lynx</code> tries to offer a browser-like experience, <code>curl</code> is designed specifically for scripts, dumping out the raw HTML source of any page you’d like to examine.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_174"/>For example, the following shows the top seven lines of the source from the home page of <em>Dave on Film</em>, courtesy of <code>curl</code>:</p>&#13;
<pre class="programs">$ <span class="codestrong">curl -s http://www.daveonfilm.com/ | head -7</span>&#13;
&lt;!DOCTYPE html&gt;&#13;
&lt;html lang="en-US"&gt;&#13;
&lt;head&gt;&#13;
&lt;meta charset="UTF-8" /&gt;&#13;
&lt;link rel="profile" href="http://gmpg.org/xfn/11" /&gt;&#13;
&lt;link rel="pingback" href="http://www.daveonfilm.com/xmlrpc.php" /&gt;&#13;
&lt;title&gt;Dave On Film: Smart Movie Reviews from Dave Taylor&lt;/title&gt;</pre>&#13;
<p class="indent">You can accomplish the same result with <code>lynx</code> if <code>curl</code> isn’t available, but if you have both, we recommend <code>curl</code>. That’s what we’ll work with in this chapter.</p>&#13;
<div class="note">&#13;
<p class="notet"><span class="noteg"><strong>WARNING</strong></span></p>&#13;
<p class="notep"><em>One limitation to the website scraper scripts in this chapter is that if the script depends on a website that’s changed its layout or API in the time since this book was written, the script might be broken. But if you can read HTML or JSON (even if you don’t understand it all), you should be able to fix any of these scripts. The problem of tracking other sites is exactly why Extensible Markup Language (XML) was created: it allows site developers to provide the content of a web page separately from the rules for its layout.</em></p>&#13;
</div>&#13;
<h3 class="h3" id="ch07lev1sec01"><strong>#53 Downloading Files via FTP</strong></h3>&#13;
<p class="noindenta">One of the original killer apps of the internet was file transfer, and one of the simplest solutions is FTP, File Transfer Protocol. At a fundamental level, all internet interaction is based on file transfer, whether it’s a web browser requesting an HTML document and its accompanying image files, a chat server relaying lines of discussion back and forth, or an email message traveling from one end of the earth to the other.</p>&#13;
<p class="indent">The original FTP program still lingers on, and while its interface is crude, the program is powerful, capable, and well worth taking advantage of. There are plenty of newer FTP programs around, notably FileZilla (<em><a href="http://filezilla-project.org/">http://filezilla-project.org/</a></em>) and NcFTP (<em><a href="http://www.ncftp.org/">http://www.ncftp.org/</a></em>), plus lots of nice graphical interfaces you can add to FTP to make it more user-friendly. With the help of some shell script wrappers, however, FTP does just fine for uploading and downloading files.</p>&#13;
<p class="indent">For example, a typical use case for FTP is to download files from the internet, which we’ll do with the script in <a href="ch07.xhtml#ch7ex1">Listing 7-1</a>. Quite often, the files will be located on anonymous FTP servers and will have URLs similar to <em>ftp://&lt;someserver&gt;/&lt;path&gt;/&lt;filename&gt;/</em>.</p>&#13;
<h4 class="h4" id="ch07lev2sec01"><span epub:type="pagebreak" id="page_175"/><em><strong>The Code</strong></em></h4>&#13;
<pre class="programs">   #!/bin/bash&#13;
&#13;
   # ftpget--Given an ftp-style URL, unwraps it and tries to obtain the&#13;
   #   file using anonymous ftp&#13;
&#13;
   anonpass="$LOGNAME@$(hostname)"&#13;
&#13;
   if [ $# -ne 1 ] ; then&#13;
     echo "Usage: $0 ftp://..." &gt;&amp;2&#13;
     exit 1&#13;
   fi&#13;
&#13;
   # Typical URL: ftp://ftp.ncftp.com/unixstuff/q2getty.tar.gz&#13;
&#13;
   if [ "$(echo $1 | cut -c1-6)" != "ftp://" ] ; then&#13;
     echo "$0: Malformed url. I need it to start with ftp://" &gt;&amp;2&#13;
     exit 1&#13;
   fi&#13;
&#13;
   server="$(echo $1 | cut -d/ -f3)"&#13;
   filename="$(echo $1 | cut -d/ -f4-)"&#13;
   basefile="$(basename $filename)"&#13;
&#13;
   echo ${0}: Downloading $basefile from server $server&#13;
&#13;
<span class="ent">➊</span> ftp -np &lt;&lt; EOF&#13;
   open $server&#13;
   user ftp $anonpass&#13;
   get "$filename" "$basefile"&#13;
   quit&#13;
   EOF&#13;
&#13;
   if [ $? -eq 0 ] ; then&#13;
     ls -l $basefile&#13;
   fi&#13;
&#13;
   exit 0</pre>&#13;
<p class="listcap"><a id="ch7ex1"/><em>Listing 7-1: The</em> <code><em>ftpget</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec02"><em><strong>How It Works</strong></em></h4>&#13;
<p class="noindenta">The heart of this script is the sequence of commands fed to the FTP program starting at <span class="ent">➊</span>. This illustrates the essence of a batch file: a sequence of instructions that’s fed to a separate program so that the receiving program (in this case FTP) thinks the instructions are being entered by the user. Here we specify the server connection to open, specify the anonymous user <span epub:type="pagebreak" id="page_176"/>(FTP) and whatever default password is specified in the script configuration (typically your email address), and then get the specified file from the FTP site and quit the transfer.</p>&#13;
<h4 class="h4" id="ch07lev2sec03"><em><strong>Running the Script</strong></em></h4>&#13;
<p class="noindenta">This script is straightforward to use: just fully specify an FTP URL, and it’ll download the file to the current working directory, as <a href="ch07.xhtml#ch7ex2">Listing 7-2</a> details.</p>&#13;
<h4 class="h4" id="ch07lev2sec04"><em><strong>The Results</strong></em></h4>&#13;
<pre class="programs">$ <span class="codestrong">ftpget ftp://ftp.ncftp.com/unixstuff/q2getty.tar.gz</span>&#13;
ftpget: Downloading q2getty.tar.gz from server ftp.ncftp.com&#13;
-rw-r--r--  1 taylor  staff  4817 Aug 14  1998 q2getty.tar.gz</pre>&#13;
<p class="listcap"><a id="ch7ex2"/><em>Listing 7-2: Running the</em> <code><em>ftpget</em></code> <em>script</em></p>&#13;
<p class="indent">Some versions of FTP are more verbose than others, and because it’s not too uncommon to find a slight mismatch in the client and server protocol, those verbose versions of FTP can spit out scary-looking errors, like <code>Unimplemented command</code>. You can safely ignore these. For example, <a href="ch07.xhtml#ch7ex3">Listing 7-3</a> shows the same script run on OS X.</p>&#13;
<pre class="programs">$ <span class="codestrong">ftpget ftp://ftp.ncftp.com/ncftp/ncftp-3.1.5-src.tar.bz2</span>&#13;
../Scripts.new/053-ftpget.sh: Downloading q2getty.tar.gz from server ftp.&#13;
ncftp.com&#13;
Connected to ncftp.com.&#13;
220 ncftpd.com NcFTPd Server (licensed copy) ready.&#13;
331 Guest login ok, send your complete e-mail address as password.&#13;
230-You are user #2 of 16 simultaneous users allowed.&#13;
230-&#13;
230 Logged in anonymously.&#13;
Remote system type is UNIX.&#13;
Using binary mode to transfer files.&#13;
local: q2getty.tar.gz remote: unixstuff/q2getty.tar.gz&#13;
227 Entering Passive Mode (209,197,102,38,194,11)&#13;
150 Data connection accepted from 97.124.161.251:57849; transfer starting for&#13;
q2getty.tar.gz (4817 bytes).&#13;
100% |*******************************************************|  4817&#13;
67.41 KiB/s    00:00 ETA&#13;
226 Transfer completed.&#13;
4817 bytes received in 00:00 (63.28 KiB/s)&#13;
221 Goodbye.&#13;
-rw-r--r--  1 taylor  staff  4817 Aug 14  1998 q2getty.tar.gz</pre>&#13;
<p class="listcap"><a id="ch7ex3"/><em>Listing 7-3: Running the</em> <code><em>ftpget</em></code> <em>script on OS X</em></p>&#13;
<p class="indent">If your FTP is excessively verbose and you’re on OS X, you can quiet it down by adding a <code>-V</code> flag to the FTP invocation in the script (that is, instead of FTP <code>-n</code>, use FTP <code>-nV</code>).</p>&#13;
<h4 class="h4" id="ch07lev2sec05"><span epub:type="pagebreak" id="page_177"/><em><strong>Hacking the Script</strong></em></h4>&#13;
<p class="noindenta">This script can be expanded to decompress the downloaded file automatically (see <a href="ch04.xhtml#ch04lev1sec07">Script #33</a> on <a href="ch04.xhtml#page_109">page 109</a> for an example of how to do this) if it has certain file extensions. Many compressed files such as <em>.tar.gz</em> and <em>.tar.bz2</em> can be decompressed by default with the system <code>tar</code> command.</p>&#13;
<p class="indent">You can also tweak this script to make it a simple tool for <em>uploading</em> a specified file to an FTP server. If the server supports anonymous connections (few do nowadays, thanks to script kiddies and other delinquents, but that’s another story), all you really have to do is specify a destination directory on the command line or in the script and change the <code>get</code> to a <code>put</code> in the main script, as shown here:</p>&#13;
<pre class="programs">ftp -np &lt;&lt; EOF&#13;
open $server&#13;
user ftp $anonpass&#13;
cd $destdir&#13;
put "$filename"&#13;
quit&#13;
EOF</pre>&#13;
<p class="indent">To work with a password-protected account, you could have the script prompt for the password interactively by turning off echoing before a <code>read</code> statement and then turning it back on when you’re done:</p>&#13;
<pre class="programs">/bin/echo -n "Password for ${user}: "&#13;
stty -echo&#13;
read password&#13;
stty echo&#13;
echo ""</pre>&#13;
<p class="indent">A smarter way to prompt for a password, however, is to just let the FTP program do the work itself. This will happen as written in our script because if a password is required to gain access to the specified FTP account, the FTP program itself will prompt for it.</p>&#13;
<h3 class="h3" id="ch07lev1sec02"><strong>#54 Extracting URLs from a Web Page</strong></h3>&#13;
<p class="noindenta">A straightforward shell script application of <code>lynx</code> is to extract a list of URLs on a given web page, which can be quite helpful when scraping the internet for links. We said we’d switched from <code>lynx</code> to <code>curl</code> for this edition of the book, but it turns out that <code>lynx</code> is about a hundred times easier to use for this script (see <a href="ch07.xhtml#ch7ex4">Listing 7-4</a>) than <code>curl</code>, because <code>lynx</code> parses HTML automatically whereas <code>curl</code> forces you to parse the HTML yourself.</p>&#13;
<p class="indent">Don’t have <code>lynx</code> on your system? Most Unix systems today have package managers such as <code>yum</code> on Red Hat, <code>apt</code> on Debian, and <code>brew</code> on OS X (though <code>brew</code> is not installed by default) that you can use to install <code>lynx</code>. If you prefer to compile <code>lynx</code> yourself, or just want to download prebuilt binaries, you can download it from <em><a href="http://lynx.browser.org/">http://lynx.browser.org/</a></em>.</p>&#13;
<h4 class="h4" id="ch07lev2sec06"><span epub:type="pagebreak" id="page_178"/><em><strong>The Code</strong></em></h4>&#13;
<pre class="programs">   #!/bin/bash&#13;
&#13;
   # getlinks--Given a URL, returns all of its relative and absolute links.&#13;
   #   Has three options: -d to generate the primary domains of every link,&#13;
   #   -i to list just those links that are internal to the site (that is,&#13;
   #   other pages on the same site), and -x to produce external links only&#13;
   #   (the opposite of -i).&#13;
&#13;
   if [ $# -eq 0 ] ; then&#13;
     echo "Usage: $0 [-d|-i|-x] url" &gt;&amp;2&#13;
     echo "-d=domains only, -i=internal refs only, -x=external only" &gt;&amp;2&#13;
     exit 1&#13;
   fi&#13;
&#13;
   if [ $# -gt 1 ] ; then&#13;
     case "$1" in&#13;
<span class="ent">➊</span>     -d) lastcmd="cut -d/ -f3|sort|uniq"&#13;
           shift&#13;
           ;;&#13;
       -r) basedomain="http://$(echo $2 | cut -d/ -f3)/"&#13;
<span class="ent">➋</span>         lastcmd="grep \"^$basedomain\"|sed \"s|$basedomain||g\"|sort|uniq"&#13;
           shift&#13;
           ;;&#13;
       -a) basedomain="http://$(echo $2 | cut -d/ -f3)/"&#13;
<span class="ent">➌</span>         lastcmd="grep -v \"^$basedomain\"|sort|uniq"&#13;
           shift&#13;
           ;;&#13;
        *) echo "$0: unknown option specified: $1" &gt;&amp;2&#13;
           exit 1&#13;
     esac&#13;
   else&#13;
<span class="ent">➍</span>   lastcmd="sort|uniq"&#13;
   fi&#13;
&#13;
   lynx -dump "$1"|\&#13;
<span class="ent">➎</span>   sed -n '/^References$/,$p'|\&#13;
     grep -E '[[:digit:]]+\.'|\&#13;
     awk '{print $2}'|\&#13;
     cut -d\? -f1|\&#13;
<span class="ent">➏</span>   eval $lastcmd&#13;
&#13;
   exit 0</pre>&#13;
<p class="listcap"><a id="ch7ex4"/><em>Listing 7-4: The</em> <code><em>getlinks</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec07"><em><strong>How It Works</strong></em></h4>&#13;
<p class="noindenta">When displaying a page, <code>lynx</code> shows the text of the page formatted as best it can followed by a list of all hypertext references, or links, found on that page. This script extracts just the links by using a <code>sed</code> invocation to print <span epub:type="pagebreak" id="page_179"/>everything after the <code>"References"</code> string in the web page text <span class="ent">➎</span>. Then the script processes the list of links as needed based on the user-specified flags.</p>&#13;
<p class="indent">One interesting technique demonstrated by this script is the way the variable <code>lastcmd</code> (<span class="ent">➊</span>, <span class="ent">➋</span>, <span class="ent">➌</span>, <span class="ent">➍</span>) is set to filter the list of links that it extracts according to the flags specified by the user. Once <code>lastcmd</code> is set, the amazingly handy <code>eval</code> command <span class="ent">➏</span> is used to force the shell to interpret the content of the variable as if it were a command instead of a variable.</p>&#13;
<h4 class="h4" id="ch07lev2sec08"><em><strong>Running the Script</strong></em></h4>&#13;
<p class="noindenta">By default, this script outputs a list of all links found on the specified web page, not just those that are prefaced with <code>http:</code>. There are three optional command flags that can be specified to change the results, however: <code>-d</code> produces just the domain names of all matching URLs, <code>-r</code> produces a list of just the <em>relative</em> references (that is, those references that are found on the same server as the current page), and <code>-a</code> produces just the <em>absolute</em> references (those URLs that point to a different server).</p>&#13;
<h4 class="h4" id="ch07lev2sec09"><em><strong>The Results</strong></em></h4>&#13;
<p class="noindenta">A simple request is a list of all links on a specified website home page, as <a href="ch07.xhtml#ch7ex5">Listing 7-5</a> shows.</p>&#13;
<pre class="programs">$ <span class="codestrong">getlinks http://www.daveonfilm.com/ | head -10</span>&#13;
http://instagram.com/d1taylor&#13;
http://pinterest.com/d1taylor/&#13;
http://plus.google.com/110193533410016731852&#13;
https://plus.google.com/u/0/110193533410016731852&#13;
https://twitter.com/DaveTaylor&#13;
http://www.amazon.com/Doctor-Who-Shada-Adventures-Douglas/&#13;
http://www.daveonfilm.com/&#13;
http://www.daveonfilm.com/about-me/&#13;
http://www.daveonfilm.com/author/d1taylor/&#13;
http://www.daveonfilm.com/category/film-movie-reviews/</pre>&#13;
<p class="listcap"><a id="ch7ex5"/><em>Listing 7-5: Running the</em> <code><em>getlinks</em></code> <em>script</em></p>&#13;
<p class="indent">Another possibility is to request a list of all domain names referenced at a specific site. This time, let’s first use the standard Unix tool <code>wc</code> to check how many links are found overall:</p>&#13;
<pre class="programs">$ <span class="codestrong">getlinks http://www.amazon.com/ | wc -l</span>&#13;
219</pre>&#13;
<p class="indent">Amazon has 219 links on its home page. Impressive! How many different domains does that represent? Let’s generate a list with the <code>-d</code> flag:</p>&#13;
<pre class="programs">$ <span class="codestrong">getlinks -d http://www.amazon.com/ | head -10</span>&#13;
amazonlocal.com&#13;
aws.amazon.com&#13;
fresh.amazon.com&#13;
kdp.amazon.com&#13;
services.amazon.com&#13;
www.6pm.com&#13;
www.abebooks.com&#13;
www.acx.com&#13;
www.afterschool.com&#13;
www.alexa.com</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_180"/>Amazon doesn’t tend to point outside its own site, but there are some partner links that creep onto the home page. Other sites are different, of course.</p>&#13;
<p class="indent">What if we split the links on the Amazon page into relative and absolute links?</p>&#13;
<pre class="programs">$ <span class="codestrong">getlinks -a http://www.amazon.com/ | wc -l</span>&#13;
51&#13;
$ <span class="codestrong">getlinks -r http://www.amazon.com/ | wc -l</span>&#13;
222</pre>&#13;
<p class="indent">As you might have expected, Amazon has four times more relative links pointing inside its own site than it has absolute links, which would lead to a different website. Gotta keep those customers on your own page!</p>&#13;
<h4 class="h4" id="ch07lev2sec10"><em><strong>Hacking the Script</strong></em></h4>&#13;
<p class="noindenta">You can see where <code>getlinks</code> could be quite useful as a site analysis tool. For a way to enhance the script, stay tuned: <a href="ch09.xhtml#ch09lev1sec01">Script #69</a> on <a href="ch09.xhtml#page_217">page 217</a> complements this script nicely, allowing us to quickly check that all hypertext references on a site are valid.</p>&#13;
<h3 class="h3" id="ch07lev1sec03"><strong>#55 Getting GitHub User Information</strong></h3>&#13;
<p class="noindenta">GitHub has grown to be a huge boon to the open source industry and open collaboration across the world. Many system administrators and developers have visited GitHub to pull down some source code or report an issue to an open source project. Because GitHub is essentially a social platform for developers, getting to know a user’s basic information quickly can be useful. The script in <a href="ch07.xhtml#ch7ex6">Listing 7-6</a> prints some information about a given GitHub user, and it gives a good introduction to the very powerful GitHub API.</p>&#13;
<h4 class="h4" id="ch07lev2sec11"><em><strong>The Code</strong></em></h4>&#13;
<pre class="programs">   #!/bin/bash&#13;
   # githubuser--Given a GitHub username, pulls information about the user&#13;
&#13;
   if [ $# -ne 1 ]; then&#13;
     echo "Usage: $0 &lt;username&gt;"&#13;
     exit 1&#13;
   fi&#13;
&#13;
   # The -s silences curl's normally verbose output.&#13;
<span class="ent">➊</span> curl -s "https://api.github.com/users/$1" | \&#13;
           awk -F'"' '&#13;
               /\"name\":/ {&#13;
                 print $4" is the name of the GitHub user."&#13;
               }&#13;
               /\"followers\":/{&#13;
                 split($3, a, " ")&#13;
                 sub(/,/, "", a[2])&#13;
                 print "They have "a[2]" followers."&#13;
               }&#13;
                 /\"following\":/{&#13;
                 split($3, a, " ")&#13;
                 sub(/,/, "", a[2])&#13;
                 print "They are following "a[2]" other users."&#13;
               }&#13;
               /\"created_at\":/{&#13;
                 print "Their account was created on "$4"."&#13;
               }&#13;
               '&#13;
   exit 0</pre>&#13;
<p class="listcap"><span epub:type="pagebreak" id="page_181"/><a id="ch7ex6"/><em>Listing 7-6: The</em> <code><em>githubuser</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec12"><em><strong>How It Works</strong></em></h4>&#13;
<p class="noindenta">Admittedly, this is almost more of an <code>awk</code> script than a bash script, but sometimes you need the extra horsepower <code>awk</code> provides for parsing (the GitHub API returns JSON). We use <code>curl</code> to ask GitHub for the user <span class="ent">➊</span>, given as the argument of the script, and pipe the JSON to <code>awk</code>. With <code>awk</code>, we specify a field separator of the double quotes character, as this will make parsing the JSON much simpler. Then we match the JSON with a handful of regular expressions in the <code>awk</code> script and print the results in a user-friendly way.</p>&#13;
<h4 class="h4" id="ch07lev2sec13"><em><strong>Running the Script</strong></em></h4>&#13;
<p class="noindenta">The script accepts a single argument: the user to look up on GitHub. If the username provided doesn’t exist, nothing will be printed.</p>&#13;
<h4 class="h4" id="ch07lev2sec14"><em><strong>The Results</strong></em></h4>&#13;
<p class="noindenta">When passed a valid username, the script should print a user-friendly summary of the GitHub user, as <a href="ch07.xhtml#ch7ex7">Listing 7-7</a> shows.</p>&#13;
<pre class="programs">$ <span class="codestrong">githubuser brandonprry</span>&#13;
Brandon Perry is the name of the GitHub user.&#13;
They have 67 followers.&#13;
They are following 0 other users.&#13;
Their account was created on 2010-11-16T02:06:41Z.</pre>&#13;
<p class="listcap"><a id="ch7ex7"/><em>Listing 7-7: Running the</em> <code><em>githubuser</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec15"><span epub:type="pagebreak" id="page_182"/><em><strong>Hacking the Script</strong></em></h4>&#13;
<p class="noindenta">This script has a lot of potential due to the information that can be retrieved from the GitHub API. In this script, we are only printing four values from the JSON returned. Generating a “résumé” for a given user based on the information provided by the API, like those provided by many web services, is just one possibility.</p>&#13;
<h3 class="h3" id="ch07lev1sec04"><strong>#56 ZIP Code Lookup</strong></h3>&#13;
<p class="noindenta">To demonstrate a different technique for scraping the web, this time using <code>curl</code>, let’s create a simple ZIP code lookup tool. Give the script in <a href="ch07.xhtml#ch7ex8">Listing 7-8</a> a ZIP code, and it’ll report the city and state the code belongs to. Easy enough.</p>&#13;
<p class="indent">Your first instinct might be to use the official US Postal Service website, but we’re going to tap into a different site, <em><a href="http://city-data.com/">http://city-data.com/</a></em>, which configures each ZIP code as its own web page so information is far easier to extract.</p>&#13;
<h4 class="h4" id="ch07lev2sec16"><em><strong>The Code</strong></em></h4>&#13;
<pre class="programs">#!/bin/bash&#13;
&#13;
# zipcode--Given a ZIP code, identifies the city and state. Use city-data.com,&#13;
#   which has every ZIP code configured as its own web page.&#13;
&#13;
baseURL="http://www.city-data.com/zips"&#13;
&#13;
/bin/echo -n "ZIP code $1 is in "&#13;
&#13;
curl -s -dump "$baseURL/$1.html" | \&#13;
  grep -i '&lt;title&gt;' | \&#13;
  cut -d\( -f2 | cut -d\) -f1&#13;
&#13;
exit 0</pre>&#13;
<p class="listcap"><a id="ch7ex8"/><em>Listing 7-8: The</em> <code><em>zipcode</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec17"><em><strong>How It Works</strong></em></h4>&#13;
<p class="noindenta">The URLs for ZIP code information pages on <em><a href="http://city-data.com/">http://city-data.com/</a></em> are structured consistently, with the ZIP code itself as the final part of the URL.</p>&#13;
<pre class="programs">http://www.city-data.com/zips/80304.html</pre>&#13;
<p class="indent">This consistency makes it quite easy to create an appropriate URL for a given ZIP code on the fly. The resultant page has the city name in the title, conveniently denoted by open and close parentheses, as follows.</p>&#13;
<pre class="programs"><span epub:type="pagebreak" id="page_183"/>&lt;title&gt;80304 Zip Code (Boulder, Colorado) Profile - homes, apartments,&#13;
schools, population, income, averages, housing, demographics, location,&#13;
statistics, residents and real estate info&lt;/title&gt;</pre>&#13;
<p class="indent">Long, but pretty easy to work with!</p>&#13;
<h4 class="h4" id="ch07lev2sec18"><em><strong>Running the Script</strong></em></h4>&#13;
<p class="noindenta">The standard way to invoke the script is to specify the desired ZIP code on the command line. If it’s valid, the city and state will be displayed, as shown in <a href="ch07.xhtml#ch7ex9">Listing 7-9</a>.</p>&#13;
<h4 class="h4" id="ch07lev2sec19"><em><strong>The Results</strong></em></h4>&#13;
<pre class="programs">$ <span class="codestrong">zipcode 10010</span>&#13;
ZIP code 10010 is in New York, New York&#13;
<span class="codestrong">$ zipcode 30001</span>&#13;
ZIP code 30001 is in &lt;title&gt;Page not found – City-Data.com&lt;/title&gt;&#13;
<span class="codestrong">$ zipcode 50111</span>&#13;
ZIP code 50111 is in Grimes, Iowa</pre>&#13;
<p class="listcap"><a id="ch7ex9"/><em>Listing 7-9: Running the</em> <code><em>zipcode</em></code> <em>script</em></p>&#13;
<p class="indent">Since 30001 isn’t a real ZIP code, the script generates a <code>Page not found</code> error. That’s a bit sloppy, and we can do better.</p>&#13;
<h4 class="h4" id="ch07lev2sec20"><em><strong>Hacking the Script</strong></em></h4>&#13;
<p class="noindenta">The most obvious hack to this script would be to do something in response to errors other than just spew out that ugly <code>&lt;title&gt;Page not found – City-Data.com&lt;/title&gt;</code> sequence. More useful still would be to add a <code>-a</code> flag that tells the script to display more information about the specified region, since <em><a href="http://city-data.com/">http://city-data.com/</a></em> offers quite a bit of information beyond city names—including land area, population demographics, and home prices.</p>&#13;
<h3 class="h3" id="ch07lev1sec05"><strong>#57 Area Code Lookup</strong></h3>&#13;
<p class="noindenta">A variation on the theme of the ZIP code lookup in <a href="ch07.xhtml#ch07lev1sec04">Script #56</a> is an area code lookup. This one turns out to be really simple, because there are some very easy-to-parse web pages with area codes. The page at <em><a href="http://www.bennetyee.org/ucsd-pages/area.html">http://www.bennetyee.org/ucsd-pages/area.html</a></em> is particularly easy to parse, not only because it is in tabular form but also because the author has identified elements with HTML attributes. For example, the line that defines area code 207 reads like so:</p>&#13;
<pre class="programs">&lt;tr&gt;&lt;td align=center&gt;&lt;a name="207"&gt;207&lt;/a&gt;&lt;/td&gt;&lt;td align=center&gt;ME&lt;/td&gt;&lt;td&#13;
align=center&gt;-5&lt;/td&gt;&lt;td&gt;   Maine&lt;/td&gt;&lt;/tr&gt;</pre>&#13;
<p class="indent">We’ll use this site to look up area codes in the script in <a href="ch07.xhtml#ch7ex10">Listing 7-10</a>.</p>&#13;
<h4 class="h4" id="ch07lev2sec21"><span epub:type="pagebreak" id="page_184"/><em><strong>The Code</strong></em></h4>&#13;
<pre class="programs">#!/bin/bash&#13;
&#13;
# areacode--Given a three-digit US telephone area code, identifies the city&#13;
#   and state using the simple tabular data at Bennet Yee's website.&#13;
&#13;
source="http://www.bennetyee.org/ucsd-pages/area.html"&#13;
&#13;
if [ -z "$1" ] ; then&#13;
  echo "usage: areacode &lt;three-digit US telephone area code&gt;"&#13;
  exit 1&#13;
fi&#13;
&#13;
# wc -c returns characters + end of line char, so 3 digits = 4 chars&#13;
if [ "$(echo $1 | wc -c)" -ne 4 ] ; then&#13;
  echo "areacode: wrong length: only works with three-digit US area codes"&#13;
  exit 1&#13;
fi&#13;
&#13;
# Are they all digits?&#13;
if [ ! -z "$(echo $1 | sed 's/[[:digit:]]//g')" ] ; then&#13;
  echo "areacode: not-digits: area codes can only be made up of digits"&#13;
  exit 1&#13;
fi&#13;
&#13;
# Now, finally, let's look up the area code...&#13;
&#13;
result="$(<span class="ent">➊</span>curl -s -dump $source | grep "name=\"$1" | \&#13;
  sed 's/&lt;[^&gt;]*&gt;//g;s/^ //g' | \&#13;
  cut -f2- -d\ | cut -f1 -d\( )"&#13;
&#13;
echo "Area code $1 =$result"&#13;
&#13;
exit 0</pre>&#13;
<p class="listcap"><a id="ch7ex10"/><em>Listing 7-10: The</em> <code><em>areacode</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec22"><em><strong>How It Works</strong></em></h4>&#13;
<p class="noindenta">The code in this shell script is mainly input validation, ensuring the data provided by the user is a valid area code. The core of the script is a <code>curl</code> call <span class="ent">➊</span>, whose output is piped to <code>sed</code> for cleaning up and then trimmed with <code>cut</code> to what we want to display to the user.</p>&#13;
<h4 class="h4" id="ch07lev2sec23"><em><strong>Running the Script</strong></em></h4>&#13;
<p class="noindenta">This script takes a single argument, the area code to look up information for. <a href="ch07.xhtml#ch7ex11">Listing 7-11</a> gives examples of the script in use.</p>&#13;
<h4 class="h4" id="ch07lev2sec24"><span epub:type="pagebreak" id="page_185"/><em><strong>The Results</strong></em></h4>&#13;
<pre class="programs">$ <span class="codestrong">areacode 817</span>&#13;
Area code 817 =  N Cent. Texas: Fort Worth area&#13;
$ <span class="codestrong">areacode 512</span>&#13;
Area code 512 =  S Texas: Austin&#13;
$ <span class="codestrong">areacode 903</span>&#13;
Area code 903 =  NE Texas: Tyler</pre>&#13;
<p class="listcap"><a id="ch7ex11"/><em>Listing 7-11: Testing the</em> <code><em>areacode</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec25"><em><strong>Hacking the Script</strong></em></h4>&#13;
<p class="noindenta">A simple hack would be to invert the search so that you provide a state and city and the script prints all of the area codes for the given city.</p>&#13;
<h3 class="h3" id="ch07lev1sec06"><strong>#58 Keeping Track of the Weather</strong></h3>&#13;
<p class="noindenta">Being inside an office or server room with your nose to a terminal all day sometimes makes you yearn to be outside, especially when the weather is really nice. Weather Underground (<em><a href="http://www.wunderground.com/">http://www.wunderground.com/</a></em>) is a great website, and it actually offers a free API for developers if you sign up for an API key. With the API key, we can write a quick shell script (shown in <a href="ch07.xhtml#ch7ex12">Listing 7-12</a>) to tell us just how nice (or poor) the weather is outside. Then we can decide whether taking a quick walk is really a good idea.</p>&#13;
<h4 class="h4" id="ch07lev2sec26"><em><strong>The Code</strong></em></h4>&#13;
<pre class="programs">   #!/bin/bash&#13;
   # weather--Uses the Wunderground API to get the weather for a given ZIP code&#13;
&#13;
   if [ $# -ne 1 ]; then&#13;
     echo "Usage: $0 &lt;zipcode&gt;"&#13;
     exit 1&#13;
   fi&#13;
&#13;
   apikey="<span class="codeitalic">b03fdsaf3b2e7cd23</span>"   # Not a real API key--you need your own.&#13;
&#13;
<span class="ent">➊</span> weather=`curl -s \&#13;
       "https://api.wunderground.com/api/$apikey/conditions/q/$1.xml"`&#13;
<span class="ent">➋</span> state=`xmllint --xpath \&#13;
       //response/current_observation/display_location/full/text\(\) \&#13;
       &lt;(echo $weather)`&#13;
   zip=`xmllint --xpath \&#13;
       //response/current_observation/display_location/zip/text\(\) \&#13;
       &lt;(echo $weather)`&#13;
   current=`xmllint --xpath \&#13;
       //response/current_observation/temp_f/text\(\) \&#13;
       &lt;(echo $weather)`&#13;
   condition=`xmllint --xpath \&#13;
       //response/current_observation/weather/text\(\) \&#13;
       &lt;(echo $weather)`&#13;
&#13;
   echo $state" ("$zip") : Current temp "$current"F and "$condition" outside."&#13;
&#13;
   exit 0</pre>&#13;
<p class="listcap"><span epub:type="pagebreak" id="page_186"/><a id="ch7ex12"/><em>Listing 7-12: The</em> <code><em>weather</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec27"><em><strong>How It Works</strong></em></h4>&#13;
<p class="noindenta">In this script, we use <code>curl</code> to call the Wunderground API and save the HTTP response data in the <code>weather</code> variable <span class="ent">➊</span>. We then use the <code>xmllint</code> (easily installable with your favorite package manager such as <code>apt</code>, <code>yum</code>, or <code>brew</code>) utility to perform an XPath query on the data returned <span class="ent">➋</span>. We also use an interesting syntax in bash when calling <code>xmllint</code> with the <code>&lt;(echo $weather)</code> at the end. This syntax takes the output of the inner command and passes it to the command as a file descriptor, so the program thinks it’s reading a real file. After gathering all the relevant information from the XML returned, we print a friendly message with general weather stats.</p>&#13;
<h4 class="h4" id="ch07lev2sec28"><em><strong>Running the Script</strong></em></h4>&#13;
<p class="noindenta">When you invoke the script, just specify the desired ZIP code, as <a href="ch07.xhtml#ch7ex13">Listing 7-13</a> shows. Easy enough!</p>&#13;
<h4 class="h4" id="ch07lev2sec29"><em><strong>The Results</strong></em></h4>&#13;
<pre class="programs">$ <span class="codestrong">weather 78727</span>&#13;
Austin, TX (78727) : Current temp 59.0F and Clear outside.&#13;
$ <span class="codestrong">weather 80304</span>&#13;
Boulder, CO (80304) : Current temp 59.2F and Clear outside.&#13;
$ <span class="codestrong">weather 10010</span>&#13;
New York, NY (10010) : Current temp 68.7F and Clear outside.</pre>&#13;
<p class="listcap"><a id="ch7ex13"/><em>Listing 7-13: Testing the</em> <code><em>weather</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec30"><em><strong>Hacking the Script</strong></em></h4>&#13;
<p class="noindenta">We have a secret. This script can actually take more than just ZIP codes. You can also specify regions in the Wunderground API, such as <code>CA/San_Francisco</code> (try it as an argument to the weather script!). However, this format isn’t incredibly user-friendly: it requires underscores instead of spaces and the slash in the middle. Adding the ability to ask for the state abbreviation and the city and then replacing any spaces with underscores if no arguments are passed would be a useful addition. As usual, this script could do with more error-checking code. What happens if you enter a four-digit ZIP code? Or a ZIP code that’s not assigned?</p>&#13;
<h3 class="h3" id="ch07lev1sec07"><span epub:type="pagebreak" id="page_187"/><strong>#59 Digging Up Movie Info from IMDb</strong></h3>&#13;
<p class="noindenta">The script in <a href="ch07.xhtml#ch7ex14">Listing 7-14</a> demonstrates a more sophisticated way to access the internet through <code>lynx</code>, by searching the Internet Movie Database (<em><a href="http://www.imdb.com/">http://www.imdb.com/</a></em>) to find films that match a specified pattern. IMDb assigns every movie, TV series, and even TV episode a unique numeric code; if the user specifies that code, this script will return a synopsis of the film. Otherwise, it will return a list of matching films from a title or partial title.</p>&#13;
<p class="indent">The script accesses different URLs depending on the type of query (numeric ID or file title) and then caches the results so it can dig through the page multiple times to extract different pieces of information. And it uses a lot—a <em>lot</em>!—of calls to <code>sed</code> and <code>grep</code>, as you’ll see.</p>&#13;
<h4 class="h4" id="ch07lev2sec31"><em><strong>The Code</strong></em></h4>&#13;
<pre class="programs">   #!/bin/bash&#13;
   # moviedata--Given a movie or TV title, returns a list of matches. If the user&#13;
   #   specifies an IMDb numeric index number, however, returns the synopsis of&#13;
   #   the film instead. Uses the Internet Movie Database.&#13;
&#13;
   titleurl="http://www.imdb.com/title/tt"&#13;
   imdburl="http://www.imdb.com/find?s=tt&amp;exact=true&amp;ref_=fn_tt_ex&amp;q="&#13;
   tempout="/tmp/moviedata.$$"&#13;
&#13;
<span class="ent">➊</span> summarize_film()&#13;
   {&#13;
     # Produce an attractive synopsis of the film.&#13;
&#13;
     grep "&lt;title&gt;" $tempout | sed 's/&lt;[^&gt;]*&gt;//g;s/(more)//'&#13;
&#13;
     grep --color=never -A2 '&lt;h5&gt;Plot:' $tempout | tail -1 | \&#13;
       cut -d\&lt; -f1 | fmt | sed 's/^/ /'&#13;
&#13;
     exit 0&#13;
   }&#13;
&#13;
   trap "rm -f $tempout" 0 1 15&#13;
&#13;
   if [ $# -eq 0 ] ; then&#13;
     echo "Usage: $0 {movie title | movie ID}" &gt;&amp;2&#13;
     exit 1&#13;
   fi&#13;
&#13;
   #########&#13;
   # Checks whether we're asking for a title by IMDb title number&#13;
&#13;
   nodigits="$(echo $1 | sed 's/[[:digit:]]*//g')"&#13;
&#13;
   if [ $# -eq 1 -a -z "$nodigits" ] ; then&#13;
     lynx -source "$titleurl$1/combined" &gt; $tempout&#13;
     summarize_film&#13;
     exit 0&#13;
   fi&#13;
&#13;
   ##########&#13;
   # It's not an IMDb title number, so let's go with the search...&#13;
&#13;
   fixedname="$(echo $@ | tr ' ' '+')"       # for the URL&#13;
&#13;
   url="$imdburl$fixedname"&#13;
&#13;
<span class="ent">➋</span> lynx -source $imdburl$fixedname &gt; $tempout&#13;
&#13;
   # No results?&#13;
&#13;
<span class="ent">➌</span> fail="$(grep --color=never '&lt;h1 class="findHeader"&gt;No ' $tempout)"&#13;
&#13;
   # If there's more than one matching title...&#13;
&#13;
   if [ ! -z "$fail" ] ; then&#13;
     echo "Failed: no results found for $1"&#13;
     exit 1&#13;
   elif [ ! -z "$(grep '&lt;h1 class="findHeader"&gt;Displaying' $tempout)" ] ; then&#13;
     grep --color=never '/title/tt' $tempout | \&#13;
     sed 's/&lt;/\&#13;
   &lt;/g' | \&#13;
     grep -vE '(.png|.jpg|&gt;[ ]*$)' | \&#13;
     grep -A 1 "a href=" | \&#13;
     grep -v '^--$' | \&#13;
     sed 's/&lt;a href="\/title\/tt//g;s/&lt;\/a&gt; //' | \&#13;
<span class="ent">➍</span>   awk '(NR % 2 == 1) { title=$0 } (NR % 2 == 0) { print title " " $0 }' | \&#13;
     sed 's/\/.*&gt;/: /' | \&#13;
     sort&#13;
   fi&#13;
&#13;
&#13;
   exit 0</pre>&#13;
<p class="listcap"><span epub:type="pagebreak" id="page_188"/><a id="ch7ex14"/><em>Listing 7-14: The</em> <code><em>moviedata</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec32"><em><strong>How It Works</strong></em></h4>&#13;
<p class="noindenta">This script builds a different URL depending on whether the command argument specified is a film title or an IMDb ID number. If the user specifies a title by ID number, the script builds the appropriate URL, downloads it, saves the <code>lynx</code> output to the <code>$tempout</code> file <span class="ent">➋</span>, and finally calls <code>summarize_film()</code> <span class="ent">➊</span>. Not too difficult.</p>&#13;
<p class="indent">But if the user specifies a title, then the script builds a URL for a search query on IMDb and saves the results page to the temp file. If IMDb can’t find a match, then the <code>&lt;h1&gt;</code> tag with <code>class="findHeader"</code> value in the returned HTML will say <code>No results</code>. That’s what the invocation at <span class="ent">➌</span> checks. Then the test is easy: if <code>$fail</code> is not zero length, the script can report that no results were found.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_189"/>If the result <em>is</em> zero length, however, that means that <code>$tempfile</code> now contains one or more successful search results for the user’s pattern. These results can all be extracted by searching for <code>/title/tt</code> as a pattern within the source, but there’s a caveat: IMDb doesn’t make it easy to parse the results because there are multiple matches to any given title link. The rest of that gnarly <code>sed|grep|sed</code> sequence tries to identify and remove the duplicate matches, while still retaining the ones that matter.</p>&#13;
<p class="indent">Further, when IMDb has a match like <code>"Lawrence of Arabia (1962)"</code>, it turns out that the title and year are two different HTML elements on two different lines in the result. Ugh. We need the year, however, to differentiate films with the same title that were released in different years. That’s what the <code>awk</code> statement at <span class="ent">➍</span> does, in a tricky sort of way.</p>&#13;
<p class="indent">If you’re unfamiliar with <code>awk</code>, the general format for an <code>awk</code> script is <code>(<em>condition</em>) { <em>action</em> }</code>. This line saves odd-numbered lines in <code>$title</code> and then, on even-numbered lines (the year and match type data), it outputs both the previous and the current line’s data as one line of output.</p>&#13;
<h4 class="h4" id="ch07lev2sec33"><em><strong>Running the Script</strong></em></h4>&#13;
<p class="noindenta">Though short, this script is quite flexible with input formats, as can be seen in <a href="ch07.xhtml#ch7ex15">Listing 7-15</a>. You can specify a film title in quotes or as separate words, and you can then specify the eight-digit IMDb ID value to select a specific match.</p>&#13;
<h4 class="h4" id="ch07lev2sec34"><em><strong>The Results</strong></em></h4>&#13;
<pre class="programs">$ <span class="codestrong">moviedata lawrence of arabia</span>&#13;
0056172: Lawrence of Arabia (1962)&#13;
0245226: Lawrence of Arabia (1935)&#13;
0390742: Mighty Moments from World History (1985) (TV Series)&#13;
1471868: Mystery Files (2010) (TV Series)&#13;
1471868: Mystery Files (2010) (TV Series)&#13;
1478071: Lawrence of Arabia (1985) (TV Episode)&#13;
1942509: Lawrence of Arabia (TV Episode)&#13;
1952822: Lawrence of Arabia (2011) (TV Episode)&#13;
$ <span class="codestrong">moviedata 0056172</span>&#13;
Lawrence of Arabia (1962)&#13;
    A flamboyant and controversial British military figure and his&#13;
    conflicted loyalties during his World War I service in the Middle East.</pre>&#13;
<p class="listcap"><a id="ch7ex15"/><em>Listing 7-15: Running the</em> <code><em>moviedata</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec35"><em><strong>Hacking the Script</strong></em></h4>&#13;
<p class="noindenta">The most obvious hack to this script would be to get rid of the ugly IMDb movie ID numbers in the output. It would be straightforward to hide the movie IDs (because the IDs as shown are rather unfriendly and prone to mistyping) and have the shell script output a simple menu with unique index values that can then be typed in to select a particular film.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_190"/>In situations where there’s exactly one film matched (try <code>moviedata monsoon wedding</code>), it would be great for the script to recognize that it’s the only match, grab the movie number for the film, and reinvoke itself to get that data. Give it a whirl!</p>&#13;
<p class="indent">A problem with this script, as with most scripts that scrape values from a third-party website, is that if IMDb changes its page layout, the script will break and you’ll need to rebuild the script sequence. It’s a lurking bug but, with a site like IMDb that hasn’t changed in years, probably not a dangerous one.</p>&#13;
<h3 class="h3" id="ch07lev1sec08"><strong>#60 Calculating Currency Values</strong></h3>&#13;
<p class="noindenta">In the first edition of this book, currency conversion was a remarkably difficult task requiring two scripts: one to pull conversion rates from a financial website and save them in a special format and another to use that data to actually do the conversion—say from US dollars to Euros. In the intervening years, however, the web has become quite a bit more sophisticated, and there’s no reason for us to go through tons of work when sites like Google offer simple, script-friendly calculators.</p>&#13;
<p class="indent">For this version of the currency conversion script, shown in <a href="ch07.xhtml#ch7ex16">Listing 7-16</a>, we’re just going to tap into the currency calculator at <em><a href="http://www.google.com/finance/converter">http://www.google.com/finance/converter</a></em>.</p>&#13;
<h4 class="h4" id="ch07lev2sec36"><em><strong>The Code</strong></em></h4>&#13;
<pre class="programs">#!/bin/bash&#13;
&#13;
# convertcurrency--Given an amount and base currency, converts it&#13;
#   to the specified target currency using ISO currency identifiers.&#13;
#   Uses Google's currency converter for the heavy lifting:&#13;
#   http://www.google.com/finance/converter&#13;
&#13;
if [ $# -eq 0 ]; then&#13;
  echo "Usage: $(basename $0) amount currency to currency"&#13;
  echo "Most common currencies are CAD, CNY, EUR, USD, INR, JPY, and MXN"&#13;
  echo "Use \"$(basename $0) list\" for a list of supported currencies."&#13;
fi&#13;
&#13;
if [ $(uname) = "Darwin" ]; then&#13;
  LANG=C   # For an issue on OS X with invalid byte sequences and lynx&#13;
fi&#13;
&#13;
     url="https://www.google.com/finance/converter"&#13;
tempfile="/tmp/converter.$$"&#13;
    lynx=$(which lynx)&#13;
&#13;
# Since this has multiple uses, let's grab this data before anything else.&#13;
&#13;
currencies=$($lynx -source "$url" | grep "option value=" | \&#13;
  cut -d\" -f2- | sed 's/"&gt;/ /' | cut -d\( -f1 | sort | uniq)&#13;
&#13;
########### Deal with all non-conversion requests.&#13;
&#13;
if [ $# -ne 4 ] ; then&#13;
  if [ "$1" = "list" ] ; then&#13;
    # Produce a listing of all currency symbols known by the converter.&#13;
    echo "List of supported currencies:"&#13;
    echo "$currencies"&#13;
  fi&#13;
  exit 0&#13;
fi&#13;
&#13;
########### Now let's do a conversion.&#13;
&#13;
if [ $3 != "to" ] ; then&#13;
  echo "Usage: $(basename $0) value currency TO currency"&#13;
  echo "(use \"$(basename $0) list\" to get a list of all currency values)"&#13;
  exit 0&#13;
fi&#13;
&#13;
amount=$1&#13;
basecurrency="$(echo $2 | tr '[:lower:]' '[:upper:]')"&#13;
targetcurrency="$(echo $4 | tr '[:lower:]' '[:upper:]')"&#13;
&#13;
# And let's do it--finally!&#13;
&#13;
$lynx -source "$url?a=$amount&amp;from=$basecurrency&amp;to=$targetcurrency" | \&#13;
  grep 'id=currency_converter_result' | sed 's/&lt;[^&gt;]*&gt;//g'&#13;
&#13;
exit 0</pre>&#13;
<p class="listcap"><span epub:type="pagebreak" id="page_191"/><a id="ch7ex16"/><em>Listing 7-16: The</em> <code><em>convertcurrency</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec37"><em><strong>How It Works</strong></em></h4>&#13;
<p class="noindenta">The Google Currency Converter has three parameters that are passed via the URL itself: the amount, the original currency, and the currency you want to convert to. You can see this in action in the following request to convert 100 US dollars into Mexican pesos.</p>&#13;
<pre class="programs">https://www.google.com/finance/converter?a=100&amp;from=USD&amp;to=MXN</pre>&#13;
<p class="indent">In the most basic use case, then, the script expects the user to specify each of those three fields as arguments, and then passes it all to Google in the URL.</p>&#13;
<p class="indent">The script also has some usage messages that make it a lot easier to use. To see those, let’s just jump to the demonstration portion, shall we?</p>&#13;
<h4 class="h4" id="ch07lev2sec38"><em><strong>Running the Script</strong></em></h4>&#13;
<p class="noindenta">This script is designed to be easy to use, as <a href="ch07.xhtml#ch7ex17">Listing 7-17</a> details, though a basic knowledge of at least a few countries’ currencies is beneficial.</p>&#13;
<h4 class="h4" id="ch07lev2sec39"><span epub:type="pagebreak" id="page_192"/><em><strong>The Results</strong></em></h4>&#13;
<pre class="programs">$ <span class="codestrong">convertcurrency</span>&#13;
Usage: convert amount currency to currency&#13;
Most common currencies are CAD, CNY, EUR, USD, INR, JPY, and MXN&#13;
Use "convertcurrency list" for a list of supported currencies.&#13;
$ <span class="codestrong">convertcurrency list | head -10</span>&#13;
List of supported currencies:&#13;
&#13;
AED United Arab Emirates <span class="codestrong">Dirham</span>&#13;
AFN Afghan <span class="codestrong">Afghani</span>&#13;
ALL Albanian <span class="codestrong">Lek</span>&#13;
AMD Armenian <span class="codestrong">Dram</span>&#13;
ANG Netherlands Antillean <span class="codestrong">Guilder</span>&#13;
AOA Angolan <span class="codestrong">Kwanza</span>&#13;
ARS Argentine <span class="codestrong">Peso</span>&#13;
AUD Australian <span class="codestrong">Dollar</span>&#13;
AWG Aruban <span class="codestrong">Florin</span>&#13;
$ <span class="codestrong">convertcurrency 75 eur to usd</span>&#13;
75 EUR = 84.5132 USD</pre>&#13;
<p class="listcap"><a id="ch7ex17"/><em>Listing 7-17: Running the</em> <code><em>convertcurrency</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec40"><em><strong>Hacking the Script</strong></em></h4>&#13;
<p class="noindenta">While this web-based calculator is austere and simple to work with, the output could do with some cleaning up. For example, the output in <a href="ch07.xhtml#ch7ex17">Listing 7-17</a> doesn’t entirely make sense because it expresses US dollars with four digits after the decimal point, even though cents only go to two digits. The correct output should be 84.51, or if rounded up, 84.52. That’s something fixable in the script.</p>&#13;
<p class="indent">While you’re at it, validating currency abbreviations would be beneficial. And in a similar vein, changing those abbreviated currency codes to proper currency names would be a nice feature, too, so you’d know that AWG is the Aruban florin or that BTC is Bitcoin.</p>&#13;
<h3 class="h3" id="ch07lev1sec09"><strong>#61 Retrieving Bitcoin Address Information</strong></h3>&#13;
<p class="noindenta">Bitcoin has taken the world by storm, with whole businesses built around the technology of the <em>blockchain</em> (which is the core of how Bitcoin works). For anyone who works with Bitcoin at all, getting useful information about specific Bitcoin addresses can be a major hassle. However, we can easily automate data gathering using a quick shell script, like that in <a href="ch07.xhtml#ch7ex18">Listing 7-18</a>.</p>&#13;
<h4 class="h4" id="ch07lev2sec41"><em><strong>The Code</strong></em></h4>&#13;
<pre class="programs">#!/bin/bash&#13;
# getbtcaddr--Given a Bitcoin address, reports useful information&#13;
&#13;
if [ $# -ne 1 ]; then&#13;
  echo "Usage: $0 &lt;address&gt;"&#13;
  exit 1&#13;
fi&#13;
&#13;
base_url="https://blockchain.info/q/"&#13;
&#13;
balance=$(curl -s $base_url"addressbalance/"$1)&#13;
recv=$(curl -s $base_url"getreceivedbyaddress/"$1)&#13;
sent=$(curl -s $base_url"getsentbyaddress/"$1)&#13;
first_made=$(curl -s $base_url"addressfirstseen/"$1)&#13;
&#13;
echo "Details for address $1"&#13;
echo -e "\tFirst seen: "$(date -d @$first_made)&#13;
echo -e "\tCurrent balance: "$balance&#13;
echo -e "\tSatoshis sent: "$sent&#13;
echo -e "\tSatoshis recv: "$recv</pre>&#13;
<p class="listcap"><span epub:type="pagebreak" id="page_193"/><a id="ch7ex18"/><em>Listing 7-18: The</em> <code><em>getbtcaddr</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec42"><em><strong>How It Works</strong></em></h4>&#13;
<p class="noindenta">This script automates a handful of <code>curl</code> calls to retrieve a few key pieces of information about a given Bitcoin address. The API available on <em><a href="http://blockchain.info/">http://blockchain.info/</a></em> gives us very easy access to all kinds of Bitcoin and block-chain information. In fact, we don’t even need to parse the responses coming back from the API, because it returns only single, simple values. After making calls to retrieve the given address’s balance, how many BTC have been sent and received by it, and when it was made, we print the information to the screen for the user.</p>&#13;
<h4 class="h4" id="ch07lev2sec43"><em><strong>Running the Script</strong></em></h4>&#13;
<p class="noindenta">The script accepts only a single argument, the Bitcoin address we want information about. However, we should mention that a string passed in that is not a real Bitcoin address will simply print all 0s for the sent, received, and current balance values, as well as a creation date in the year 1969. Any nonzero values are in a unit called <em>satoshis</em>, which is the smallest denomination of a Bitcoin (like pennies, but to many more decimal places).</p>&#13;
<h4 class="h4" id="ch07lev2sec44"><em><strong>The Results</strong></em></h4>&#13;
<p class="noindenta">Running the <code>getbtcaddr</code> shell script is simple as it only takes a single argument, the Bitcoin address to request data about, as <a href="ch07.xhtml#ch7ex19">Listing 7-19</a> shows.</p>&#13;
<pre class="programs">$ <span class="codestrong">getbtcaddr 1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa</span>&#13;
Details for address 1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa&#13;
    First seen: Sat Jan 3 12:15:05 CST 2009&#13;
    Current balance: 6554034549&#13;
    Satoshis sent: 0&#13;
    Satoshis recv: 6554034549&#13;
&#13;
$ <span class="codestrong">getbtcaddr 1EzwoHtiXB4iFwedPr49iywjZn2nnekhoj</span>&#13;
Details for address 1EzwoHtiXB4iFwedPr49iywjZn2nnekhoj&#13;
    First seen: Sun Mar 11 11:11:41 CDT 2012&#13;
    Current balance: 2000000&#13;
    Satoshis sent: 716369585974&#13;
    Satoshis recv: 716371585974</pre>&#13;
<p class="listcap"><span epub:type="pagebreak" id="page_194"/><a id="ch7ex19"/><em>Listing 7-19: Running the</em> <code><em>getbtcaddr</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec45"><em><strong>Hacking the Script</strong></em></h4>&#13;
<p class="noindenta">The numbers printed to the screen by default are pretty large and a bit difficult for most people to comprehend. The <code>scriptbc</code> script (<a href="ch01.xhtml#ch01lev1sec10">Script #9</a> on <a href="ch01.xhtml#page_34">page 34</a>) can easily be used to report in more reasonable units, such as whole Bitcoins. Adding a scale argument to the script would be an easy way for the user to get a more readable printout.</p>&#13;
<h3 class="h3" id="ch07lev1sec10"><strong>#62 Tracking Changes on Web Pages</strong></h3>&#13;
<p class="noindenta">Sometimes great inspiration comes from seeing an existing business and saying to yourself, “That doesn’t seem too hard.” The task of tracking changes on a website is a surprisingly simple way of collecting such inspirational material. The script in <a href="ch07.xhtml#ch7ex20">Listing 7-20</a>, <code>changetrack</code>, automates that task. This script has one interesting nuance: when it detects changes to the site, it emails the new web page to the user, rather than just reporting the information on the command line.</p>&#13;
<h4 class="h4" id="ch07lev2sec46"><em><strong>The Code</strong></em></h4>&#13;
<pre class="programs">   #!/bin/bash&#13;
&#13;
   # changetrack--Tracks a given URL and, if it's changed since the last visit,&#13;
   #   emails the new page to the specified address&#13;
&#13;
   sendmail=$(which sendmail)&#13;
   sitearchive="/tmp/changetrack"&#13;
   tmpchanges="$sitearchive/changes.$$"  # Temp file&#13;
   fromaddr="webscraper@intuitive.com"&#13;
   dirperm=755        # read+write+execute for dir owner&#13;
   fileperm=644       # read+write for owner, read only for others&#13;
&#13;
   trap "$(which rm) -f $tmpchanges" 0 1 15  # Remove temp file on exit&#13;
&#13;
   if [ $# -ne 2 ] ; then&#13;
     echo "Usage: $(basename $0) url email" &gt;&amp;2&#13;
     echo "  tip: to have changes displayed on screen, use email addr '-'" &gt;&amp;2&#13;
     exit 1&#13;
   fi&#13;
&#13;
   if [ ! -d $sitearchive ] ; then&#13;
     if ! mkdir $sitearchive ; then&#13;
<span epub:type="pagebreak" id="page_195"/>       echo "$(basename $0) failed: couldn't create $sitearchive." &gt;&amp;2&#13;
       exit 1&#13;
     fi&#13;
     chmod $dirperm $sitearchive&#13;
   fi&#13;
&#13;
   if [ "$(echo $1 | cut -c1-5)" != "http:" ] ; then&#13;
     echo "Please use fully qualified URLs (e.g. start with 'http://')" &gt;&amp;2&#13;
     exit 1&#13;
   fi&#13;
&#13;
   fname="$(echo $1 | sed 's/http:\/\///g' | tr '/?&amp;' '...')"&#13;
   baseurl="$(echo $1 | cut -d/ -f1-3)/"&#13;
&#13;
   # Grab a copy of the web page and put it in an archive file. Note that we&#13;
   #   can track changes by looking just at the content (that is, -dump, not&#13;
   #   -source), so we can skip any HTML parsing....&#13;
&#13;
   lynx -dump "$1" | uniq &gt; $sitearchive/${fname}.new&#13;
   if [ -f "$sitearchive/$fname" ] ; then&#13;
     # We've seen this site before, so compare the two with diff.&#13;
     diff $sitearchive/$fname $sitearchive/${fname}.new &gt; $tmpchanges&#13;
     if [ -s $tmpchanges ] ; then&#13;
       echo "Status: Site $1 has changed since our last check."&#13;
     else&#13;
       echo "Status: No changes for site $1 since last check."&#13;
       rm -f $sitearchive/${fname}.new     # Nothing new...&#13;
       exit 0                              # No change--we're outta here.&#13;
     fi&#13;
   else&#13;
     echo "Status: first visit to $1. Copy archived for future analysis."&#13;
     mv $sitearchive/${fname}.new $sitearchive/$fname&#13;
     chmod $fileperm $sitearchive/$fname&#13;
     exit 0&#13;
   fi&#13;
&#13;
   # If we're here, the site has changed, and we need to send the contents&#13;
   #   of the .new file to the user and replace the original with the .new&#13;
   #   for the next invocation of the script.&#13;
&#13;
   if [ "$2" != "-" ] ; then&#13;
&#13;
   ( echo "Content-type: text/html"&#13;
     echo "From: $fromaddr (Web Site Change Tracker)"&#13;
     echo "Subject: Web Site $1 Has Changed"&#13;
<span class="ent">➊</span>   echo "To: $2"&#13;
     echo ""&#13;
&#13;
<span class="ent">➋</span>   lynx -s -dump $1 | \&#13;
<span class="ent">➌</span>   sed -e "s|src=\"|SRC=\"$baseurl|gi" \&#13;
<span class="ent">➍</span>       -e "s|href=\"|HREF=\"$baseurl|gi" \&#13;
<span class="ent">➎</span>       -e "s|$baseurl\/http:|http:|g"&#13;
   ) | $sendmail -t&#13;
&#13;
   else&#13;
     # Just showing the differences on the screen is ugly. Solution?&#13;
&#13;
     diff $sitearchive/$fname $sitearchive/${fname}.new&#13;
   fi&#13;
&#13;
   # Update the saved snapshot of the website.&#13;
&#13;
   mv $sitearchive/${fname}.new $sitearchive/$fname&#13;
   chmod 755 $sitearchive/$fname&#13;
   exit 0</pre>&#13;
<p class="listcap"><span epub:type="pagebreak" id="page_196"/><a id="ch7ex20"/><em>Listing 7-20: The</em> <code><em>changetrack</em></code> <em>script</em></p>&#13;
<h4 class="h4" id="ch07lev2sec47"><em><strong>How It Works</strong></em></h4>&#13;
<p class="noindenta">Given a URL and a destination email address, this script grabs the web page content and compares it to the content of the site from the previous check. If the site has changed, the new web page is emailed to the specified recipient, with some simple rewrites to try to keep the graphics and <code>href</code> tags working. These HTML rewrites starting at <span class="ent">➋</span> are worth examining.</p>&#13;
<p class="indent">The call to <code>lynx</code> retrieves the source of the specified web page <span class="ent">➋</span>, and then <code>sed</code> performs three different translations. First, <code>SRC="</code> is rewritten as <code>SRC="baseurl/</code> <span class="ent">➌</span> to ensure that any relative pathnames of the form <code>SRC="logo.gif"</code> are rewritten to work properly as full pathnames with the domain name. If the domain name of the site is <em><a href="http://www.intuitive.com/">http://www.intuitive.com/</a></em>, the rewritten HTML would be <code>SRC="http://www.intuitive.com/logo.gif"</code>. Likewise, <code>href</code> attributes are rewritten <span class="ent">➍</span>. Then, to ensure we haven’t broken anything, the third translation pulls the <code>baseurl</code> back <em>out</em> of the HTML source in situations where it’s been erroneously added <span class="ent">➎</span>. For example, <code>HREF="http://www.intuitive.com/http://www.somewhereelse.com/link"</code> is clearly broken and must be fixed for the link to work.</p>&#13;
<p class="indent">Notice also that the recipient address is specified in the <code>echo</code> statement <span class="ent">➊</span> (<code>echo "To: $2"</code>) rather than as an argument to <code>sendmail</code>. This is a simple security trick: by having the address within the <code>sendmail</code> input stream (which <code>sendmail</code> knows to parse for recipients because of the <code>-t</code> flag), there’s no worry about users playing games with addresses like <code>"joe;cat /etc/passwd|mail larry"</code>. This is a good technique to use whenever you invoke <code>sendmail</code> within shell scripts.</p>&#13;
<h4 class="h4" id="ch07lev2sec48"><em><strong>Running the Script</strong></em></h4>&#13;
<p class="noindenta">This script requires two parameters: the URL of the site being tracked (and you’ll need to use a fully qualified URL that begins with <code>http://</code> for it to work properly) and the email address of the person (or comma-separated group of people) who should receive the updated web page, as appropriate. Or, if you’d prefer, just use <code>-</code> (a hyphen) as the email address, and the <code>diff</code> output will instead be displayed on screen.</p>&#13;
<h4 class="h4" id="ch07lev2sec49"><span epub:type="pagebreak" id="page_197"/><em><strong>The Results</strong></em></h4>&#13;
<p class="noindenta">The first time the script sees a web page, the page is automatically mailed to the specified user, as <a href="ch07.xhtml#ch7ex21">Listing 7-21</a> shows.</p>&#13;
<pre class="programs">$ <span class="codestrong">changetrack http://www.intuitive.com/ taylor@intuitive.com</span>&#13;
Status: first visit to http://www.intuitive.com/. Copy archived for future&#13;
analysis.</pre>&#13;
<p class="listcap"><a id="ch7ex21"/><em>Listing 7-21: Running the</em> <code><em>changetrack</em></code> <em>script for the first time</em></p>&#13;
<p class="indent">All subsequent checks on <em><a href="http://www.intuitive.com/">http://www.intuitive.com/</a></em> will produce an email copy of the site only if the page has changed since the last invocation of the script. This change can be as simple as a single typo fix or as complex as a complete redesign. While this script can be used for tracking any website, sites that don’t change frequently will probably work best: if the site is the BBC News home page, checking for changes is a waste of CPU cycles because this site is <em>constantly</em> updated.</p>&#13;
<p class="indent">If a site has not changed when the script is invoked the second time, the script has no output and sends no email to the specified recipient:</p>&#13;
<pre class="programs">$ <span class="codestrong">changetrack http://www.intuitive.com/ taylor@intuitive.com</span>&#13;
$</pre>&#13;
<h4 class="h4" id="ch07lev2sec50"><em><strong>Hacking the Script</strong></em></h4>&#13;
<p class="noindenta">An obvious deficiency in the current script is that it’s hardcoded to look for <em>http://</em> links, which means it will reject any HTTP web pages served over HTTPS with SSL. Updating the script to work with both would require some fancier regular expressions, but is totally possible!</p>&#13;
<p class="indent">Another change to make the script more useful could be to have a granularity option that would allow users to specify that if only one line has changed, the script should not consider the website updated. You could implement this by piping the <code>diff</code> output to <code>wc -l</code> to count lines of output changed. (Keep in mind that <code>diff</code> generally produces <em>three</em> lines of output for each line changed.)</p>&#13;
<p class="indent">This script is also more useful when invoked from a <code>cron</code> job on a daily or weekly basis. We have similar scripts that run every night and send us updated web pages from various sites that we like to track.</p>&#13;
<p class="indent">A particularly interesting possibility is to modify this script to work off a data file of URLs and email addresses, rather than requiring those as input parameters. Drop that modified version of the script into a <code>cron</code> job, write a web-based front end to the utility (similar to the shell scripts in <a href="ch08.xhtml#ch08">Chapter 8</a>), and you’ve just duplicated a function that some companies charge people money to use. No kidding.<span epub:type="pagebreak" id="page_198"/></p>&#13;
</body></html>