<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch18"><span epub:type="pagebreak" id="page_113"/><strong><span class="big">18</span><br/>USING AND FINE-TUNING PRETRAINED TRANSFORMERS</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">What are the different ways to use and fine-tune pretrained large language models?</p>&#13;
<p class="indent">The three most common ways to use and fine-tune pretrained LLMs include a feature-based approach, in-context prompting, and updating a subset of the model parameters. First, most pretrained LLMs or language transformers can be utilized without the need for further fine-tuning. For instance, we can employ a feature-based method to train a new downstream model, such as a linear classifier, using embeddings generated by a pretrained transformer. Second, we can showcase examples of a new task within the input itself, which means we can directly exhibit the expected outcomes without requiring any updates or learning from the model. This concept is also known as <em>prompting</em>. Finally, it’s also possible to fine-tune all or just a small number of parameters to achieve the desired outcomes.</p>&#13;
<p class="indent">The following sections discuss these types of approaches in greater depth.</p>&#13;
<h3 class="h3" id="ch00lev90"><strong>Using Transformers for Classification Tasks</strong></h3>&#13;
<p class="noindent">Let’s start with the conventional methods for utilizing pretrained transformers: training another model on feature embeddings, fine-tuning output <span epub:type="pagebreak" id="page_114"/>layers, and fine-tuning all layers. We’ll discuss these in the context of classification. (We will revisit prompting later in the section “In-Context Learning, Indexing, and Prompt Tuning” on <a href="ch18.xhtml#ch00lev91">page 116</a>.)</p>&#13;
<p class="indent">In the feature-based approach, we load the pretrained model and keep it “frozen,” meaning we do not update any parameters of the pretrained model. Instead, we treat the model as a feature extractor that we apply to our new dataset. We then train a downstream model on these embeddings. This downstream model can be any model we like (random forests, XGBoost, and so on), but linear classifiers typically perform best. This is likely because pretrained transformers like BERT and GPT already extract high-quality, informative features from the input data. These feature embeddings often capture complex relationships and patterns, making it easy for a linear classifier to effectively separate the data into different classes. Furthermore, linear classifiers, such as logistic regression machines and support vector machines, tend to have strong regularization properties. These regularization properties help prevent overfitting when working with high-dimensional feature spaces generated by pretrained transformers. This feature-based approach is the most efficient method since it doesn’t require updating the transformer model at all. Finally, the embeddings can be precomputed for a given training dataset (since they don’t change) when training a classifier for multiple training epochs.</p>&#13;
<p class="indent"><a href="ch18.xhtml#ch18fig1">Figure 18-1</a> illustrates how LLMs are typically created and adopted for downstream tasks using fine-tuning. Here, a pretrained model, trained on a general text corpus, is fine-tuned to perform tasks like German-to-English translation.</p>&#13;
<div class="image"><img id="ch18fig1" src="../images/18fig01.jpg" alt="Image" width="764" height="528"/></div>&#13;
<p class="figcap"><em>Figure 18-1: The general fine-tuning workflow of large language models</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_115"/>The conventional methods for fine-tuning pretrained LLMs include updating only the output layers, a method we’ll refer to as <em>fine-tuning I</em>, and updating all layers, which we’ll call <em>fine-tuning II</em>.</p>&#13;
<p class="indent">Fine-tuning I is similar to the feature-based approach described earlier, but it adds one or more output layers to the LLM itself. The backbone of the LLM remains frozen, and we update only the model parameters in these new layers. Since we don’t need to backpropagate through the whole network, this approach is relatively efficient regarding throughput and memory requirements.</p>&#13;
<p class="indent">In fine-tuning II, we load the model and add one or more output layers, similarly to fine-tuning I. However, instead of backpropagating only through the last layers, we update <em>all</em> layers via backpropagation, making this the most expensive approach. While this method is computationally more expensive than the feature-based approach and fine-tuning I, it typically leads to better modeling or predictive performance. This is especially true for more specialized domain-specific datasets.</p>&#13;
<p class="indent"><a href="ch18.xhtml#ch18fig2">Figure 18-2</a> summarizes the three approaches described in this section so far.</p>&#13;
<div class="image"><img id="ch18fig2" src="../images/18fig02.jpg" alt="Image" width="1107" height="540"/></div>&#13;
<p class="figcap"><em>Figure 18-2: The three conventional approaches for utilizing pretrained LLMs</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_116"/>In addition to the conceptual summary of the three fine-tuning methods described in this section, <a href="ch18.xhtml#ch18fig2">Figure 18-2</a> also provides a rule-of-thumb guideline for these methods regarding training efficiency. Since fine-tuning II involves updating more layers and parameters than fine-tuning I, backpropagation is costlier for fine-tuning II. For similar reasons, fine-tuning II is costlier than a simpler feature-based approach.</p>&#13;
<h3 class="h3" id="ch00lev91"><strong>In-Context Learning, Indexing, and Prompt Tuning</strong></h3>&#13;
<p class="noindent">LLMs like GPT-2 and GPT-3 popularized the concept of <em>in-context learning</em>, often called <em>zero-shot</em> or <em>few-shot learning</em> in this context, which is illustrated in <a href="ch18.xhtml#ch18fig3">Figure 18-3</a>.</p>&#13;
<div class="image"><img id="ch18fig3" src="../images/18fig03.jpg" alt="Image" width="612" height="633"/></div>&#13;
<p class="figcap"><em>Figure 18-3: Prompting an LLM for in-context learning</em></p>&#13;
<p class="indent">As <a href="ch18.xhtml#ch18fig3">Figure 18-3</a> shows, in-context learning aims to provide context or examples of the task within the input or prompt, allowing the model to infer the desired behavior and generate appropriate responses. This approach takes advantage of the model’s ability to learn from vast amounts of data during pretraining, which includes diverse tasks and contexts.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The definition of few-shot learning, considered synonymous with in-context learning-based methods, differs from the conventional approach to few-shot learning discussed in <a href="ch03.xhtml">Chapter 3</a>.</em></p>&#13;
</div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_117"/>For example, suppose we want to use in-context learning for few-shot German–English translation using a large-scale pretrained language model like GPT-3. To do so, we provide a few examples of German–English translations to help the model understand the desired task, as follows:</p>&#13;
<pre class="pre">Translate the following German sentences into English:&#13;
&#13;
Example 1:&#13;
German: "Ich liebe Pfannkuchen."&#13;
English: "I love pancakes."&#13;
&#13;
Example 2:&#13;
German: "Das Wetter ist heute schoen."&#13;
English: "The weather is nice today."&#13;
&#13;
Translate this sentence:&#13;
German: "Wo ist die naechste Bushaltestelle?"</pre>&#13;
<p class="indent">Generally, in-context learning does not perform as well as fine-tuning for certain tasks or specific datasets since it relies on the pretrained model’s ability to generalize from its training data without further adapting its parameters for the particular task at hand.</p>&#13;
<p class="indent">However, in-context learning has its advantages. It can be particularly useful when labeled data for fine-tuning is limited or unavailable. It also enables rapid experimentation with different tasks without fine-tuning the model parameters in cases where we don’t have direct access to the model or where we interact only with the model through a UI or API (for example, ChatGPT).</p>&#13;
<p class="indent">Related to in-context learning is the concept of <em>hard prompt tuning</em>, where <em>hard</em> refers to the non-differentiable nature of the input tokens. Where the previously described fine-tuning methods update the model parameters to better perform the task at hand, hard prompt tuning aims to optimize the prompt itself to achieve better performance. Prompt tuning does not modify the model parameters, but it may involve using a smaller labeled dataset to identify the best prompt formulation for the specific task. For example, to improve the prompts for the previous German–English translation task, we might try the following three prompting variations:</p>&#13;
<ul>&#13;
<li class="noindent"><span class="literal">"Translate the German sentence '{german_sentence}' into English: {english_translation}"</span></li>&#13;
<li class="noindent"><span class="literal">"German: '{german_sentence}' | English: {english_translation}"</span></li>&#13;
<li class="noindent"><span class="literal">"From German to English: '{german_sentence}' -&gt; {english_translation}"</span></li>&#13;
</ul>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_118"/>Prompt tuning is a resource-efficient alternative to parameter fine-tuning. However, its performance is usually not as good as full model fine-tuning, as it does not update the model’s parameters for a specific task, potentially limiting its ability to adapt to task-specific nuances. Furthermore, prompt tuning can be labor intensive since it requires either human involvement comparing the quality of the different prompts or another similar method to do so. This is often known as <em>hard</em> prompting since, again, the input tokens are not differentiable. In addition, other methods exist that propose to use another LLM for automatic prompt generation and evaluation.</p>&#13;
<p class="indent">Yet another way to leverage a purely in-context learning-based approach is <em>indexing</em>, illustrated in <a href="ch18.xhtml#ch18fig4">Figure 18-4</a>.</p>&#13;
<div class="image"><img id="ch18fig4" src="../images/18fig04.jpg" alt="Image" width="777" height="597"/></div>&#13;
<p class="figcap"><em>Figure 18-4: LLM indexing to retrieve information from external documents</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_119"/>In the context of LLMs, we can think of indexing as a workaround based on in-context learning that allows us to turn LLMs into information retrieval systems to extract information from external resources and web-sites. In <a href="ch18.xhtml#ch18fig4">Figure 18-4</a>, an indexing module parses a document or website into smaller chunks, embedded into vectors that can be stored in a vector database. When a user submits a query, the indexing module computes the vector similarity between the embedded query and each vector stored in the database. Finally, the indexing module retrieves the top <em>k</em> most similar embeddings to synthesize the response.</p>&#13;
<h3 class="h3" id="ch00lev92"><strong>Parameter-Efficient Fine-Tuning</strong></h3>&#13;
<p class="noindent">In recent years, many methods have been developed to adapt pretrained transformers more efficiently for new target tasks. These methods are commonly referred to as <em>parameter-efficient fine-tuning</em>, with the most popular methods at the time of writing summarized in <a href="ch18.xhtml#ch18fig5">Figure 18-5</a>.</p>&#13;
<div class="image"><img id="ch18fig5" src="../images/18fig05.jpg" alt="Image" width="582" height="399"/></div>&#13;
<p class="figcap"><em>Figure 18-5: The main categories of parameter-efficient fine-tuning techniques, with popular examples</em></p>&#13;
<p class="indent">In contrast to the hard prompting approach discussed in the previous section, <em>soft prompting</em> strategies optimize embedded versions of the prompts. While in hard prompt tuning we modify the discrete input tokens, in soft prompt tuning we utilize trainable parameter tensors instead.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_120"/>The idea behind soft prompt tuning is to prepend a trainable parameter tensor (the “soft prompt”) to the embedded query tokens. The prepended tensor is then tuned to improve the modeling performance on a target data-set using gradient descent. In Python-like pseudocode, soft prompt tuning can be described as</p>&#13;
<pre class="pre">x = EmbeddingLayer(input_ids)&#13;
x = concatenate([soft_prompt_tensor, x],&#13;
                 dim=seq_len)&#13;
output = model(x)</pre>&#13;
<p class="noindent">where the <span class="literal">soft_prompt_tensor</span> has the same feature dimension as the embedded inputs produced by the embedding layer. Consequently, the modified input matrix has additional rows (as if it extended the original input sequence with additional tokens, making it longer).</p>&#13;
<p class="indent">Another popular prompt tuning method is prefix tuning. <em>Prefix tuning</em> is similar to soft prompt tuning, except that in prefix tuning, we prepend trainable tensors (soft prompts) to each transformer block instead of only the embedded inputs, which can stabilize the training. The implementation of prefix tuning is illustrated in the following pseudocode:</p>&#13;
<pre class="pre">def transformer_block_with_prefix(x):&#13;
 <span class="ent">➊</span> soft_prompt = FullyConnectedLayers(# Prefix&#13;
      soft_prompt)                     # Prefix&#13;
 <span class="ent">➋</span> x = concatenate([soft_prompt, x],  # Prefix&#13;
                     dim=seq_len)      # Prefix&#13;
 <span class="ent">➌</span> residual = x&#13;
    x = SelfAttention(x)&#13;
    x = LayerNorm(x + residual)&#13;
    residual = x&#13;
    x = FullyConnectedLayers(x)&#13;
    x = LayerNorm(x + residual)&#13;
    return x</pre>&#13;
<p class="list" id="ch18lis1"><em>Listing 18-1: A transformer block modified for prefix tuning</em></p>&#13;
<p class="indent">Let’s break <a href="ch18.xhtml#ch18lis1">Listing 18-1</a> into three main parts: implementing the soft prompt, concatenating the soft prompt (prefix) with the input, and implementing the rest of the transformer block.</p>&#13;
<p class="indent">First, the <span class="literal">soft_prompt</span>, a tensor, is processed through a set of fully connected layers <span class="ent">➊</span>. Second, the transformed soft prompt is concatenated with the main input, <span class="literal">x</span> <span class="ent">➋</span>. The dimension along which they are concatenated is denoted by <span class="literal">seq_len</span>, referring to the sequence length dimension. Third, the subsequent lines of code <span class="ent">➌</span> describe the standard operations in a transformer block, including self-attention, layer normalization, and feed-forward neural network layers, wrapped around residual connections.</p>&#13;
<p class="indent">As shown in <a href="ch18.xhtml#ch18lis1">Listing 18-1</a>, prefix tuning modifies a transformer block by adding a trainable soft prompt. <a href="ch18.xhtml#ch18fig6">Figure 18-6</a> further illustrates the difference between a regular transformer block and a prefix tuning transformer block.<span epub:type="pagebreak" id="page_121"/></p>&#13;
<div class="image"><img id="ch18fig6" src="../images/18fig06.jpg" alt="Image" width="819" height="1329"/></div>&#13;
<p class="figcap"><em>Figure 18-6: A regular transformer compared with prefix tuning</em></p>&#13;
<p class="indent">Both soft prompt tuning and prefix tuning are considered parameter efficient since they require training only the prepended parameter tensors and not the LLM parameters themselves.</p>&#13;
<p class="indent"><em>Adapter methods</em> are related to prefix tuning in that they add additional parameters to the transformer layers. In the original adapter method, <span epub:type="pagebreak" id="page_122"/>additional fully connected layers were added after the multihead self-attention and existing fully connected layers in each transformer block, as illustrated in <a href="ch18.xhtml#ch18fig7">Figure 18-7</a>.</p>&#13;
<div class="image"><img id="ch18fig7" src="../images/18fig07.jpg" alt="Image" width="871" height="1318"/></div>&#13;
<p class="figcap"><em>Figure 18-7: Comparison of a regular transformer block (left) and a transformer block with adapter layers</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_123"/>Only the new adapter layers are updated when training the LLM using the original adapter method, while the remaining transformer layers remain frozen. Since the adapter layers are usually small—the first fully connected layer in an adapter block projects its input into a low-dimensional representation, while the second layer projects it back into the original input dimension—this adapter method is usually considered parameter efficient.</p>&#13;
<p class="indent">In pseudocode, the original adapter method can be written as follows:</p>&#13;
<pre class="pre">def transformer_block_with_adapter(x):&#13;
    residual = x&#13;
    x = SelfAttention(x)&#13;
    x = FullyConnectedLayers(x)  # Adapter&#13;
    x = LayerNorm(x + residual)&#13;
    residual = x&#13;
    x = FullyConnectedLayers(x)&#13;
    x = FullyConnectedLayers(x)  # Adapter&#13;
    x = LayerNorm(x + residual)&#13;
    return x</pre>&#13;
<p class="indent"><em>Low-rank adaptation (LoRA)</em>, another popular parameter-efficient fine-tuning method worth considering, refers to reparameterizing pretrained LLM weights using low-rank transformations. LoRA is related to the concept of <em>low-rank transformation</em>, a technique to approximate a high-dimensional matrix or dataset using a lower-dimensional representation. The lower-dimensional representation (or <em>low-rank approximation</em>) is achieved by finding a combination of fewer dimensions that can effectively capture most of the information in the original data. Popular low-rank transformation techniques include principal component analysis and singular vector decomposition.</p>&#13;
<p class="indent">For example, suppose ∆<em>W</em> represents the parameter update for a weight matrix of the LLM with dimension ℝ<em><sup>A×B</sup></em>. We can decompose the weight update matrix into two smaller matrices: ∆<em>W</em> = <em>W<sub>A</sub>W<sub>B</sub></em>, where <em>W<sub>A</sub>∈</em> ℝ<em><sup>A×h</sup></em> and <em>W<sub>A</sub>∈</em> ℝ<em><sup>h×B</sup></em>. Here, we keep the original weight frozen and train only the new matrices <em>W<sub>A</sub></em> and <em>W<sub>B</sub></em>.</p>&#13;
<p class="indent">How is this method parameter efficient if we introduce new weight matrices? These new matrices can be very small. For example, if <em>A</em> = 25 and <em>B</em> = 50, then the size of ∆<em>W</em> is 25 <em>×</em> 50 = 1,250. If <em>h</em> = 5, then <em>W<sub>A</sub></em> has 125 parameters, <em>W<sub>B</sub></em> has 250 parameters, and the two matrices combined have only 125 + 250 = 375 parameters in total.</p>&#13;
<p class="indent">After learning the weight update matrix, we can then write the matrix multiplication of a fully connected layer, as shown in this pseudocode:</p>&#13;
<pre class="pre">def lora_forward_matmul(x):&#13;
    h = x . W  # Regular matrix multiplication&#13;
    h += x . (W_A . W_B) * scalar&#13;
    return h</pre>&#13;
<p class="list" id="ch18lis2"><em>Listing 18-2: Matrix multiplication with LoRA</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_124"/>In <a href="ch18.xhtml#ch18lis2">Listing 18-2</a>, <span class="literal">scalar</span> is a scaling factor that adjusts the magnitude of the combined result (original model output plus low-rank adaptation). This balances the pretrained model’s knowledge and the new task-specific adaptation.</p>&#13;
<p class="indent">According to the original paper introducing the LoRA method, models using LoRA perform slightly better than models using the adapter method across several task-specific benchmarks. Often, LoRA performs even better than models fine-tuned using the fine-tuning II method described earlier.</p>&#13;
<h3 class="h3" id="ch00lev93"><strong>Reinforcement Learning with Human Feedback</strong></h3>&#13;
<p class="noindent">The previous section focused on ways to make fine-tuning more efficient. Switching gears, how can we improve the modeling performance of LLMs via fine-tuning?</p>&#13;
<p class="indent">The conventional way to adapt or fine-tune an LLM for a new target domain or task is to use a supervised approach with labeled target data. For instance, the fine-tuning II approach allows us to adapt a pretrained LLM and fine-tune it on a target task such as sentiment classification, using a dataset that contains texts with sentiment labels like <em>positive</em>, <em>neutral</em>, and <em>negative</em>.</p>&#13;
<p class="indent">Supervised fine-tuning is a foundational step in training an LLM. An additional, more advanced step is <em>reinforcement learning with human feedback (RLHF)</em>, which can be used to further improve the model’s alignment with human preferences. For example, ChatGPT and its predecessor, Instruct-GPT, are two popular examples of pretrained LLMs (GPT-3) fine-tuned using RLHF.</p>&#13;
<p class="indent">In RLHF, a pretrained model is fine-tuned using a combination of supervised learning and reinforcement learning. This approach was popularized by the original ChatGPT model, which was in turn based on Instruct-GPT. Human feedback is collected by having humans rank or rate different model outputs, providing a reward signal. The collected reward labels can be used to train a reward model that is then used to guide the LLMs’ adaptation to human preferences. The reward model is learned via supervised learning, typically using a pretrained LLM as the base model, and is then used to adapt the pretrained LLM to human preferences via additional fine-tuning. The training in this additional fine-tuning stage uses a flavor of reinforcement learning called <em>proximal policy optimization</em>.</p>&#13;
<p class="indent">RLHF uses a reward model instead of training the pretrained model on the human feedback directly because involving humans in the learning process would create a bottleneck since we cannot obtain feedback in real time.</p>&#13;
<h3 class="h3" id="ch00lev94"><strong>Adapting Pretrained Language Models</strong></h3>&#13;
<p class="noindent">While fine-tuning all layers of a pretrained LLM remains the gold standard for adaption to new target tasks, several efficient alternatives exist for leveraging pretrained transformers. For instance, we can effectively apply LLMs <span epub:type="pagebreak" id="page_125"/>to new tasks while minimizing computational costs and resources by utilizing feature-based methods, in-context learning, or parameter-efficient fine-tuning techniques.</p>&#13;
<p class="indent">The three conventional methods—feature-based approach, fine-tuning I, and fine-tuning II—provide different computational efficiency and performance trade-offs. Parameter-efficient fine-tuning methods like soft prompt tuning, prefix tuning, and adapter methods further optimize the adaptation process, reducing the number of parameters to be updated. Meanwhile, RLHF presents an alternative approach to supervised fine-tuning, potentially improving modeling performance.</p>&#13;
<p class="indent">In sum, the versatility and efficiency of pretrained LLMs continue to advance, offering new opportunities and strategies for effectively adapting these models to a wide array of tasks and domains. As research in this area progresses, we can expect further improvements and innovations in using pretrained language models.</p>&#13;
<h3 class="h3" id="ch00lev95"><strong>Exercises</strong></h3>&#13;
<p class="number1"><strong>18-1.</strong> When does it make more sense to use in-context learning rather than fine-tuning, and vice versa?</p>&#13;
<p class="number1"><strong>18-2.</strong> In prefix tuning, adapters, and LoRA, how can we ensure that the model preserves (and does not forget) the original knowledge?</p>&#13;
<h3 class="h3" id="ch00lev96"><strong>References</strong></h3>&#13;
<ul>&#13;
<li class="noindent">The paper introducing the GPT-2 model: Alec Radford et al., “Language Models Are Unsupervised Multitask Learners” (2019), <em><a href="https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe">https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe</a></em>.</li>&#13;
<li class="noindent">The paper introducing the GPT-3 model: Tom B. Brown et al., “Language Models Are Few-Shot Learners” (2020), <em><a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></em>.</li>&#13;
<li class="noindent">The automatic prompt engineering method, which proposes using another LLM for automatic prompt generation and evaluation: Yongchao Zhou et al., “Large Language Models Are Human-Level Prompt Engineers” (2023), <em><a href="https://arxiv.org/abs/2211.01910">https://arxiv.org/abs/2211.01910</a></em>.</li>&#13;
<li class="noindent">LlamaIndex is an example of an indexing approach that leverages in-context learning: <em><a href="https://github.com/jerryjliu/llama_index">https://github.com/jerryjliu/llama_index</a></em>.</li>&#13;
<li class="noindent">DSPy is a popular open source library for retrieval augmentation and indexing: <em><a href="https://github.com/stanfordnlp/dsp">https://github.com/stanfordnlp/dsp</a></em>.</li>&#13;
<li class="noindent">A first instance of soft prompting: Brian Lester, Rami Al-Rfou, and Noah Constant, “The Power of Scale for Parameter-Efficient Prompt Tuning” (2021), <em><a href="https://arxiv.org/abs/2104.08691">https://arxiv.org/abs/2104.08691</a></em>.<span epub:type="pagebreak" id="page_126"/></li>&#13;
<li class="noindent">The paper that first described prefix tuning: Xiang Lisa Li and Percy Liang, “Prefix-Tuning: Optimizing Continuous Prompts for Generation” (2021), <em><a href="https://arxiv.org/abs/2101.00190">https://arxiv.org/abs/2101.00190</a></em>.</li>&#13;
<li class="noindent">The paper introducing the original adapter method: Neil Houlsby et al., “Parameter-Efficient Transfer Learning for NLP” (2019) <em><a href="https://arxiv.org/abs/1902.00751">https://arxiv.org/abs/1902.00751</a></em>.</li>&#13;
<li class="noindent">The paper introducing the LoRA method: Edward J. Hu et al., “LoRA: Low-Rank Adaptation of Large Language Models” (2021), <em><a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a></em>.</li>&#13;
<li class="noindent">A survey of more than 40 research papers covering parameter-efficient fine-tuning methods: Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky, “Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning” (2023), <em><a href="https://arxiv.org/abs/2303.15647">https://arxiv.org/abs/2303.15647</a></em>.</li>&#13;
<li class="noindent">The InstructGPT paper: Long Ouyang et al., “Training Language Models to Follow Instructions with Human Feedback” (2022), <em><a href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a></em>.</li>&#13;
<li class="noindent">Proximal policy optimization, which is used for reinforcement learning with human feedback: John Schulman et al., “Proximal Policy Optimization Algorithms” (2017), <em><a href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a></em>.</li>&#13;
</ul>&#13;
</div>
</div>
</body></html>