<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="263" id="Page_263"/>11</span><br/>
<span class="ChapterTitle">Classifiers</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">In this chapter, we introduce four important classification algorithms, building on the basics of classification that we covered in Chapter 7. We often use these algorithms to help us study and understand our data. In some cases, we can even build a final classification system from them. In other cases, we can use the understanding we gain from these methods to design a deep learning classifier, as discussed in later chapters. </p>
<p>We will usually illustrate our classifiers using 2D data and usually only two classes because that’s easy to draw and understand, but modern classifiers are capable of handling data with any number of dimensions (or features) and huge numbers of classes. Modern libraries let us apply most of these algorithms to our own data with just a handful of lines of code.</p>
<h2 id="h1-500723c11-0001"><span epub:type="pagebreak" title="264" id="Page_264"/> Types of Classifiers</h2>
<p class="BodyFirst">Before we get into specific algorithms, let’s break down the world of classifiers into two main approaches: <em>parametric</em> and <em>nonparametric</em>. </p>
<p>In the parametric approach, we usually think of the algorithm as starting with a preconceived description of the data it’s working with, and it then searches for the best parameters of that description to make it fit. For instance, if we think that our data follows a normal distribution, we can look for the mean and standard deviation that best fit it.</p>
<p>In the nonparametric approach, we let the data lead the way, and we try to come up with some way to represent it only after we’ve analyzed it. For example, we may look at all of the data and attempt to find a boundary that splits it into two or more classes.</p>
<p>In reality, these two approaches are more conceptual than strict. For example, we can argue that simply choosing a particular kind of learning algorithm means that we’re making assumptions about our data. And we can argue that we’re always learning about the data itself by processing it. But these are useful generalizations, and we’ll use them to organize our discussion.</p>
<p>Let’s start by looking at two nonparametric classifiers.</p>
<h2 id="h1-500723c11-0002">k-Nearest Neighbors</h2>
<p class="BodyFirst">We begin with a nonparametric algorithm called <em>k-nearest neighbors</em>, or <em>kNN</em>. As usual, the letter <em>k </em>at the start refers not to a word but to a number. We can pick any integer that’s 1 or larger. Because we set this value before the algorithm runs, it’s a hyperparameter. </p>
<p>In Chapter 7, we saw an algorithm called <em>k-means clustering</em>. Despite the similarity in names, that algorithm and <em>k</em>-nearest neighbor are different techniques. One key difference is that <em>k</em>-means clustering learns from unlabeled data, whereas kNN works with labeled data. In other words, <em>k</em>-means clustering and kNN fall into the classes of unsupervised and supervised learning, respectively.</p>
<p>kNN is fast to train, because all it does is save a copy of every incoming sample into a database. The interesting part comes when training is complete, and a new sample arrives to be classified. The central idea of how kNN classifies a new sample is appealingly geometric, as shown in <a href="#figure11-1" id="figureanchor11-1">Figure 11-1</a>.</p>
<figure>
<img src="Images/F11001.png" alt="F11001" width="839" height="183"/>
<figcaption><p><a id="figure11-1">Figure 11-1</a>: To find the class for a new sample, shown as a star, we find the most popular of its <em>k</em> neighbors. </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="265" id="Page_265"/>In <a href="#figure11-1">Figure 11-1</a>(a) we have a sample point (a star) amid a bunch of other samples that represent three classes (circle, square, and triangle). To determine a class for our new sample, we look at the <em>k</em> nearest samples (or <em>neighbors</em>), and we count up their classes. Whichever class is most populous becomes the new sample’s class. We show which samples are considered for different values of <em>k </em>by a line to each of the <em>k</em> nearest samples. In <a href="#figure11-1">Figure 11-1</a>(b) we’ve set <em>k </em>to 1, meaning we want to use the class of the nearest sample. In this case it’s a red circle, so this new sample is classified as a circle. In <a href="#figure11-1">Figure 11-1</a>(c) we’ve set <em>k </em>to 9, so we look at the nine nearest points. Here we find 3 circles, 4 squares, and 2 triangles. Because there are more squares than any other class, the star is classified as a square. In <a href="#figure11-1">Figure 11-1</a>(d) we’ve set <em>k </em>to 25. Now we have 6 circles, 13 squares, and 6 triangles, so again the star is classified as a square.</p>
<p>To sum this up, kNN accepts a new sample to evaluate, along with a value of <em>k</em>. It then finds the closest <em>k</em> samples to the new sample, and we assign the new sample to the class with the largest number of representatives among the <em>k</em> samples we found. There are various ways to break ties and handle exceptional cases, but that’s the basic idea.</p>
<p>Note that kNN does not create explicit boundaries between groups of points. There’s no notion here of regions or areas that samples belong to. We say that kNN is an <em>on-demand</em>, or <em>lazy</em>, algorithm because it does no processing of the samples during the learning stage. When learning, kNN just stashes the samples in its internal memory and it’s done.</p>
<p>kNN is attractive because it’s simple, and training it is usually exceptionally fast. On the other hand, kNN can require a lot of memory, because (in its basic form) it’s saving all the input samples. Using large amounts of memory can slow down the algorithm. Another problem is that the classification of new points is often slow (compared to other algorithms we’ll see) because of the cost of searching for neighbors. Every time we want to classify a new piece of data, we have to find its <em>k </em>nearest neighbors, which requires work. Of course, there are many ways to enhance the algorithm to speed this up, but it still remains a relatively slow way to classify. For applications where classification speed is important, like real-time systems and websites, the time required by kNN to produce each answer can take it out of the running.</p>
<p>Another problem with this technique is that it depends on having lots of neighbors nearby (after all, if the nearest neighbors are all very far away, then they don’t offer a good proxy for other examples that are like the one we’re trying to classify). This means we need a lot of training data. If we have lots of features (that is, our data has many dimensions) then kNN quickly succumbs to the curse of dimensionality, which we discussed in Chapter 7. As the dimensionality of the space goes up, if we don’t also significantly increase the number of training samples, then the number of samples in any local neighborhood drops, making it harder for kNN to get a good collection of nearby points.</p>
<p>Let’s put kNN to the test. In <a href="#figure11-2" id="figureanchor11-2">Figure 11-2</a> we show a “smile” dataset of 2D data that falls into two classes. In this figure, and the similar ones that follow, the data is made of points. Since points are hard to see, we’re drawing a filled circle around each point as a visual aid.</p>
<span epub:type="pagebreak" title="266" id="Page_266"/><figure>
<img src="Images/F11002.png" alt="F11002" width="694" height="392"/>
<figcaption><p><a id="figure11-2">Figure 11-2</a>: A smile dataset of 2D points. There are two classes, blue and orange.</p></figcaption>
</figure>
<p>Using kNN with different values of <em>k </em>gives us the results in <a href="#figure11-3" id="figureanchor11-3">Figure 11-3</a>.</p>
<figure>
<img src="Images/F11003.png" alt="F11003" width="694" height="513"/>
<figcaption><p><a id="figure11-3">Figure 11-3</a>: Classifying with the points in <a href="#figure11-2">Figure 11-2</a> using kNN for different values of <em>k</em> </p></figcaption>
</figure>
<p>Although kNN doesn’t produce explicit classification boundaries, we can see that when <em>k </em>is small and we compare our input point with only a few neighbors, the space is broken up into sections that share a rather rough border. As <em>k </em>gets larger and we use more neighbors, that border smooths out because we’re getting a better overall picture of the environment around the new sample.</p>
<p><span epub:type="pagebreak" title="267" id="Page_267"/>To make things more interesting, let’s add some noise to our data so that the edges aren’t so easy to find. <a href="#figure11-4" id="figureanchor11-4">Figure 11-4</a> shows a noisy version of <a href="#figure11-2">Figure 11-2</a>.</p>
<figure>
<img src="Images/F11004.png" alt="F11004" width="694" height="450"/>
<figcaption><p><a id="figure11-4">Figure 11-4</a>: A noisy version of the smile dataset from <a href="#figure11-2">Figure 11-2</a></p></figcaption>
</figure>
<p>The results for different values of <em>k </em>are shown in <a href="#figure11-5" id="figureanchor11-5">Figure 11-5</a>.</p>
<figure>
<img src="Images/F11005.png" alt="F11005" width="694" height="513"/>
<figcaption><p><a id="figure11-5">Figure 11-5</a>: Using kNN and <a href="#figure11-4">Figure 11-4</a> to assign a class to points in the plane</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="268" id="Page_268"/>We can see that in the presence of noise, small values of <em>k </em>lead to ragged borders. In this example, we have to get <em>k </em>up to 50 before we see fairly smooth boundaries. Also notice that as <em>k</em> increases, the smile shape contracts, since the perimeter gets eroded by the larger number of background points.</p>
<p>Because kNN doesn’t explicitly represent the boundaries between classes, it can handle any kind of boundaries, or any distribution of classes. To see this, let’s add some eyes to our smile, creating three disconnected sets of the same class. The resulting noisy data is in <a href="#figure11-6" id="figureanchor11-6">Figure 11-6</a>.</p>
<figure>
<img src="Images/F11006.png" alt="F11006" width="694" height="477"/>
<figcaption><p><a id="figure11-6">Figure 11-6</a>: A noisy dataset of a smile and two eyes</p></figcaption>
</figure>
<p>The resulting classifications for different values of <em>k </em>are shown in <a href="#figure11-7" id="figureanchor11-7">Figure 11-7</a>.</p>
<p>In this example, a value of about 20 for <em>k </em>looks best. Too small a value of <em>k</em> can give us ragged edges and noisy results, but too large a value of <em>k</em> can start to erode the features. As is so often the case, finding the best hyperparameter for this algorithm for any given dataset is a matter of repeated experimentation. We can use cross-validation to automatically score the quality of each result, which is particularly useful when there are many dimensions.</p>
<p>kNN is a great nonparametric algorithm: it’s easy to understand and program, and when the dataset isn’t too big, training is extremely fast and classification of new data isn’t too slow. But when the dataset gets large, kNN becomes less appealing: memory requirements go up because every sample is kept, and classification gets slower because the search gets slower. These problems are shared by most nonparametric algorithms.</p>
<span epub:type="pagebreak" title="269" id="Page_269"/><figure>
<img src="Images/F11007.png" alt="F11007" width="694" height="513"/>
<figcaption><p><a id="figure11-7">Figure 11-7</a>: kNN doesn’t create boundaries between clusters of samples, so it works even when a class is broken up into pieces. </p></figcaption>
</figure>
<h2 id="h1-500723c11-0003">Decision Trees</h2>
<p class="BodyFirst">Let’s consider another nonparametric classification method, called <em>decision trees</em>. This algorithm builds a data structure from the points in the sample set, which is then used to classify new points. Let’s begin by taking a look at this structure, and then we’ll see how to build it.</p>
<h3 id="h2-500723c11-0001">Introduction to Trees</h3>
<p class="BodyFirst">We can illustrate the basic idea behind decision trees with the familiar parlor game called 20 Questions. In this game, one player (the chooser) thinks of a specific target object, which is often a person, place, or thing. The other player (the guesser) then asks a series of yes/no questions. If the guesser can correctly identify the target in 20 or fewer questions, they win. One reason the game endures is that it’s fun to narrow down the enormous number of possible people, places, and things to one specific instance with such a small number of simple questions (perhaps surprisingly, with 20 yes/no questions we can only distinguish just over a million distinct targets). </p>
<p>We can draw a typical game of 20 questions in graphical form, as in <a href="#figure11-8" id="figureanchor11-8">Figure 11-8</a>.</p>
<span epub:type="pagebreak" title="270" id="Page_270"/><figure>
<img src="Images/F11008.png" alt="F11008" width="844" height="247"/>
<figcaption><p><a id="figure11-8">Figure 11-8</a>: A tree for playing 20 questions. Note that after each decision there are exactly two choices, one each for “yes” and “no.”</p></figcaption>
</figure>
<p>We call a structure like <a href="#figure11-8">Figure 11-8</a> a <em>tree</em> because it looks something like an upside-down tree. Such trees have a bunch of associated terms that are worth knowing. We say that each splitting point in the tree is a <em>node</em>, and each line connecting nodes is a <em>link</em>, <em>edge</em>, or <em>branch</em>. Following the tree analogy, the node at the top is the <em>root</em>, and the nodes at the bottom are <em>leaves</em>, or <em>terminal nodes</em>. Nodes between the root and the leaves are called <em>internal nodes</em> or <em>decision nodes</em>. </p>
<p>If a tree has a perfectly symmetrical shape, we say the tree is <em>balanced</em>, otherwise it’s <em>unbalanced</em>. In practice, almost all trees are unbalanced when they’re made, but we can run algorithms to make them closer to being balanced if a particular application prefers that. We also say that every node has a <em>depth</em>, which is a number that gives the smallest number of nodes we must go through to reach the root. The root has a depth of 0, the nodes immediately below it have a depth of 1, and so on.</p>
<p><a href="#figure11-9" id="figureanchor11-9">Figure 11-9</a> shows a tree with these labels.</p>
<figure>
<img src="Images/F11009.png" alt="F11009" width="847" height="309"/>
<figcaption><p><a id="figure11-9">Figure 11-9</a>: Some terminology for a tree</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="271" id="Page_271"/>It’s also common to use the terms associated with family trees, though these abstract trees don’t require the union of two nodes to produce children. Every node (except the root) has a node above it. We call this the <em>parent</em> of that node. The nodes immediately below a parent node are its <em>children</em>. We sometimes distinguish between <em>immediate children</em> that are directly connected to a parent, and <em>distant children</em> that are at the same depth as immediate children, but connected to the parent through a sequence of other nodes. If we focus our attention on a specific node, then that node and all of its children taken together are called a <em>subtree</em>. Nodes that share the same immediate parent are called <em>siblings</em>. </p>
<p><a href="#figure11-10" id="figureanchor11-10">Figure 11-10</a> shows some of these ideas graphically.</p>
<figure>
<img src="Images/F11010.png" alt="F11010" width="833" height="338"/>
<figcaption><p><a id="figure11-10">Figure 11-10</a>: Using familiar terms with trees. The green node’s parent is immediately above it, and its children are immediately below.</p></figcaption>
</figure>
<p>With this vocabulary in mind, let’s return to our game of 20 Questions.</p>
<h3 id="h2-500723c11-0002">Using Decision Trees</h3>
<p class="BodyFirst">An interesting quality of the 20 Questions<em> </em>tree shown in <a href="#figure11-8">Figure 11-8</a> is that it’s <em>binary</em>; every parent node has exactly two children: one for yes, one for no. Binary trees are a particularly easy kind of tree for some algorithms to work with. If some nodes have more than two children, we say that the tree overall is <em>bushy</em>. We can always convert a bushy tree to a binary tree if we want. An example of a bushy tree that tries to guess the month of someone’s birthday is shown on the left of <a href="#figure11-11" id="figureanchor11-11">Figure 11-11</a>, and the corresponding binary tree is shown on the right. Because we can easily go back and forth, we usually draw trees in whatever form is most clear and succinct for the discussion at hand.</p>
<p>We can use trees to classify data. When used this way, the trees are called <em>decision trees</em>. The full name of the approach is <em>categorical variable <span epub:type="pagebreak" title="272" id="Page_272"/>decision trees</em>. This is to distinguish it from those times we use decision trees to work with continuous variables, as in regression problems. Those are called <em>continuous variable decision trees</em>.</p>
<figure>
<img src="Images/F11011.png" alt="F11011" width="838" height="426"/>
<figcaption><p><a id="figure11-11">Figure 11-11</a>: Turning a bushy tree (left) into a binary tree (right)</p></figcaption>
</figure>
<p>Let’s stick with the categorical versions here. For simplicity, we’ll just refer to them as decision trees, or simply trees, from now on. An example of such a tree for sorting inputs into different classes is shown in <a href="#figure11-12" id="figureanchor11-12">Figure 11-12</a>.</p>
<p>In <a href="#figure11-12">Figure 11-12</a>, we start with a root node containing samples of different classes, distinguished by their shapes (and colors). To construct the tree, we <em>split</em> the samples at each node into two groups using some kind of test, resulting in a decision. For instance, the test applied at the root node might be, “Is this shape rectangular?” The test for the left child might be, “Is this shape taller than it is wide?” We’ll soon see how to come up with such tests. The goal in this example is to keep splitting each node until we’re left with samples of only one class. At that point, we declare that node to be a leaf, and stop splitting. In <a href="#figure11-12">Figure 11-12</a>, we’ve split our starting data into five classes. It’s essential to remember the test we applied at each node. Now, when a new sample arrives, we can start at the root and apply the root’s test, then the test of the appropriate child, and so on. When we finally reach a leaf, we have determined the class for that sample. </p>
<span epub:type="pagebreak" title="273" id="Page_273"/><figure>
<img src="Images/F11012.png" alt="F11012" width="694" height="706"/>
<figcaption><p><a id="figure11-12">Figure 11-12</a>: A categorical decision tree. Each class is a different shape and color.</p></figcaption>
</figure>
<p>Decision trees don’t start out fully built. Instead, we build the tree based on the samples in the training set. When we reach a leaf node during training, we test to see if the new training sample has the same class as all the other samples in that leaf. If it does, we add the sample to the leaf and we’re done. Otherwise, we come up with decision criteria based on some of the features that let us distinguish between this sample and the previous samples in the node. We then use this test to split<em> </em>the node. The test we come up with gets saved with the node, we create at least two children, and assign each sample to the appropriate child, as in <a href="#figure11-13" id="figureanchor11-13">Figure 11-13</a>.</p>
<p>When we’re done with training, evaluating new samples is easy. We just start at the root and work our way down the tree, following the appropriate branch at each node based on that node’s test with this sample’s features. When we land in a leaf, we report that the sample belongs to the class of the objects in that leaf.</p>
<span epub:type="pagebreak" title="274" id="Page_274"/><figure>
<img src="Images/F11013.png" alt="F11013" width="428" height="478"/>
<figcaption><p><a id="figure11-13">Figure 11-13</a>: Splitting a node by applying a test to its contents </p></figcaption>
</figure>
<p>This is an idealized process. In practice, our tests can be imperfect and our leaves might contain a mixture of objects of different classes if, for efficiency or memory reasons, we chose not to split some leaves any more. For example, if we land in a node that contains samples that are 80 percent from class A and 20 percent from class B, we might report that the new sample has an 80 percent chance of being in A and a 20 percent chance of being in B. If we have to report just one class, we might report that it’s A 80 percent of the time, and B the other 20 percent.</p>
<p>This process of making a decision tree works with just one sample at a time. In the simplest version of this technique, we don’t consider the entire training set at once and try to find, say, the smallest or most balanced tree that classifies the samples. Instead, we consider one sample at a time and split the nodes in the tree as required to handle that sample. Then we do the same for the next sample, and the next, and so on, making decisions in the moment without caring about any other data yet to come. This makes for efficient training.</p>
<p>This algorithm makes decisions based only on the data it has seen before and the data currently under consideration. It doesn’t try to plan or strategize for the future based on what it has seen so far. We call this a <em>greedy</em> algorithm, since it’s focused on maximizing its immediate, short-term gains. </p>
<p>Decisions trees are sometimes preferred in practice over other classifiers because their results are <em>explainable</em>. When the algorithm assigns a class to a sample, we don’t have to unravel some complex mathematical or algorithmic process. Instead, we can fully explain the final result just by identifying each decision along the way. This can be important in real life, too. </p>
<p><span epub:type="pagebreak" title="275" id="Page_275"/>For example, suppose we apply for a loan at a bank, but we’re turned down. When we ask for the reason, the bank can show us each test made along the way. We say that the workings of the algorithm are <em>transparent</em>. Note that this doesn’t mean that they’re fair or reasonable. The bank may have come up with tests that are biased against one or more social groups or that depend on what seem to be irrelevant criteria. Just because they can explain why they made their choice doesn’t make the process or the results satisfactory. Legislators in particular seem to prefer laws that enforce transparency, which is easy to demonstrate, over fairness, which is much harder. Transparency is nice to have, but it doesn’t mean a system is behaving the way we’d like it to.</p>
<p>Decision trees can make bad decisions because they are particularly prone to overfitting. Let’s see why. </p>
<h3 id="h2-500723c11-0003">Overfitting Trees</h3>
<p class="BodyFirst">Let’s begin our discussion of overfitting in the tree-building process by considering a couple of examples.</p>
<p>The data in <a href="#figure11-14" id="figureanchor11-14">Figure 11-14</a> shows a cleanly separated set of data representing two classes. A dataset that roughly follows this kind of geometry is often called a <em>two-moons dataset</em>, presumably because the semicircles reminded someone of crescent moons. </p>
<figure>
<img src="Images/F11014.png" alt="F11014" width="694" height="480"/>
<figcaption><p><a id="figure11-14">Figure 11-14</a>: Our 600 starting data points for building a decision tree, arranged in a two-moons structure. These 2D points represent two classes, blue and orange.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="276" id="Page_276"/>Each step in building a tree potentially involves splitting a leaf, and thus replacing it with a node and two leaves, for a net increase to the tree of one internal node and one leaf. We often think about the size of our tree in terms of the number of leaves it contains. </p>
<p><a href="#figure11-15" id="figureanchor11-15">Figure 11-15</a> shows the process of building a decision tree for this data. </p>
<figure>
<img src="Images/F11015.png" alt="F11015" width="844" height="403"/>
<figcaption><p><a id="figure11-15">Figure 11-15</a>: Building a decision tree for the data in <a href="#figure11-14">Figure 11-14</a>. Notice how the tree starts with big chunks and refines them into smaller and more precise regions.</p></figcaption>
</figure>
<p>In this figure, we drew the dataset over each image just for reference. In this example, each node corresponds to a box. We don’t show it here, but the tree begins with just a single root corresponding to a blue box that covers the entire region. Then the training process receives one of the orange points near the top of the orange curve. This isn’t in the blue class, so we split the root with a horizontal cut into two boxes, shown in the upper left. The next point that comes in is a blue point near the left part of the blue curve. This falls in the orange box, so we split that with a vertical cut into two boxes as shown, giving us a total of three leaves. The rest of the figure shows the evolving tree as more samples arrive.</p>
<p>Notice how the regions gradually refine as the tree grows in response to more training data.</p>
<p>This tree needs only 12 leaves to correctly classify every training sample. The final tree and the original data are shown together in <a href="#figure11-16" id="figureanchor11-16">Figure 11-16</a>. This tree fits the data perfectly.</p>
<p>Note the two horizontal, thin rectangles. They enclose two orange samples at the top of the left side of the arc and manage to slip between the blue points (recall that the samples are points in the center of each circle). This is overfitting, because any future points that fall into those rectangles, despite the fact that they’re both almost completely in a blue region, will be classified as orange.</p>
<span epub:type="pagebreak" title="277" id="Page_277"/><figure>
<img src="Images/F11016.png" alt="F11016" width="670" height="511"/>
<figcaption><p><a id="figure11-16">Figure 11-16</a>: Our final tree with 12 leaves </p></figcaption>
</figure>
<p>Because decision trees are so sensitive to each input sample, they have a profound tendency to overfit. In fact, decision trees almost always overfit, because every training sample can influence the tree’s shape. To see this, take a look at <a href="#figure11-17" id="figureanchor11-17">Figure 11-17</a>. Here we ran the same algorithm as for <a href="#figure11-16">Figure 11-16</a> two times, but in each case, we used a different, randomly chosen 70 percent of the input data.</p>
<figure>
<img src="Images/F11017.png" alt="F11017" width="694" height="262"/>
<figcaption><p><a id="figure11-17">Figure 11-17</a>: Decision trees are very sensitive to their inputs. (Left) We randomly chose 70 percent of the samples from <a href="#figure11-14">Figure 11-14</a> and fit a tree. (Right) The same process, but for a different randomly selected 70 percent of the original samples.</p></figcaption>
</figure>
<p>These two decision trees are similar, but definitely not identical. The tendency of decision trees to overfit is much more pronounced when the data isn’t so easily separated. Let’s look at an example of that now.</p>
<p><span epub:type="pagebreak" title="278" id="Page_278"/><a href="#figure11-18" id="figureanchor11-18">Figure 11-18</a> shows another pair of crescent moons, but this time we added lots of noise to the samples after they had their classes assigned. The two classes no longer have a clean boundary.</p>
<figure>
<img src="Images/F11018.png" alt="F11018" width="694" height="483"/>
<figcaption><p><a id="figure11-18">Figure 11-18</a>: A noisy set of 600 samples for building decision trees</p></figcaption>
</figure>
<p>Fitting a tree to this data starts out with big regions, but it rapidly turns into a complicated set of tiny boxes as the algorithm splits up nodes this way and that to match the noisy data. <a href="#figure11-19" id="figureanchor11-19">Figure 11-19</a> shows the result. In this case, it required 100 leaves for the tree to correctly classify the points.</p>
<figure>
<img src="Images/F11019.png" alt="F11019" width="844" height="403"/>
<figcaption><p><a id="figure11-19">Figure 11-19</a>: The tree-building process. Note that the second row uses large numbers of leaves.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="279" id="Page_279"/><a href="#figure11-20" id="figureanchor11-20">Figure 11-20</a> shows a close-up of the final tree and the original data.</p>
<figure>
<img src="Images/F11020.png" alt="F11020" width="694" height="529"/>
<figcaption><p><a id="figure11-20">Figure 11-20</a>: Our noisy data fit with a tree with 100 leaves. Notice how many little boxes have been used to catch just an odd sample here and there.</p></figcaption>
</figure>
<p>There’s a lot of overfitting here. Though we expect most of the samples in the lower right to be orange and most of those in the upper left to be blue, this tree has carved out a lot of exceptions based on this particular dataset. Future samples that fall into those little boxes are likely to be misclassified.</p>
<p>Let’s repeat our process of building trees using different, random 70 percent selections of the data in <a href="#figure11-18">Figure 11-18</a>. <a href="#figure11-21" id="figureanchor11-21">Figure 11-21</a> shows the results.</p>
<figure>
<img src="Images/F11021.png" alt="F11021" width="694" height="262"/>
<figcaption><p><a id="figure11-21">Figure 11-21</a>: A couple of trees built with different sets of 70 percent of the samples in <a href="#figure11-18">Figure 11-18</a></p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="280" id="Page_280"/>There are similarities, but these trees are significantly different, and lots of little bits exist only to classify a few samples. This is overfitting in action.</p>
<p>Although this may look pretty bad for the decision tree method, in Chapter 12 we’ll see that by combining many simple decision trees into a group, or an <em>ensemble</em>, we can create robust, efficient classifiers that don’t suffer as much from overfitting. </p>
<p>There are a few other ways we can control overfitting. </p>
<p>As we saw in <a href="#figure11-15">Figure 11-15</a> and <a href="#figure11-19">Figure 11-19</a>, the first few steps of the tree’s growth tend to generate big, general shapes. It’s only when the tree gets very deep that we get the tiny boxes that are symptomatic of overfitting. One popular strategy to reducing overfitting is <em>depth limiting: </em>we simply limit the tree’s depth while it’s building. If a node is more than a given number of steps from the root, we just declare it a leaf and don’t split it any more. A different strategy is setting a minimum sample requirement so that we never split a node that has less than a certain number of samples, no matter how mixed they are.</p>
<p>Yet another approach to reducing overfitting is to reduce the size of the tree after it’s made in a process called <em>pruning</em>. This works by removing, or <em>trimming</em>, leaf nodes. We look at each leaf and characterize what would happen to the total error of the tree’s results if we removed that leaf. If the error is acceptable, we simply remove the leaf from the tree. If we remove all the children of a node, then it becomes a leaf itself, and a candidate for further pruning. Pruning a tree can make it shallower, which offers the additional benefit of also making it faster when we classify new data.</p>
<p>Depth limiting, setting minimum sample requirements per node, and pruning all simplify the tree, but because they do so in different ways, they usually give us different results.</p>
<h3 id="h2-500723c11-0004">Splitting Nodes</h3>
<p class="BodyFirst">Before we leave decision trees, let’s return briefly to the node-splitting process, since many machine learning libraries offer us a choice of splitting algorithms to choose from. Here are two questions to ask when we consider a node: First, does it need to be split? Second, how should we split it? Let’s take these in order.</p>
<p>When we ask if a node needs to be split, we usually consider to what extent all the samples in a given node are of the same class. We describe the uniformity of a node’s contents with a number called that node’s <em>purity</em>. If all the samples are in the same class, the node is completely pure. The more samples we have of other classes, the smaller the value of purity becomes. To test if a node needs splitting, we can check the purity against a threshold. If the node is too <em>impure</em>, meaning that the purity is below the threshold, we split it.</p>
<p>Now we can look at how to split the node. If our samples have many features, we can invent lots of different possible splitting tests. We can test the value of just one feature and ignore the others. We can look at groups of features and test on some aggregated values from them. We’re free to <span epub:type="pagebreak" title="281" id="Page_281"/>choose a completely different test at every node based on different features. This gives us a huge variety of possible tests to consider.</p>
<p><a href="#figure11-22" id="figureanchor11-22">Figure 11-22</a> shows a node containing a mix of circles of different sizes and colors. Let’s try to get all the reddish objects in one child and all the bluish ones in another. When we just look at the data (usually the best first step with any new database), it seems like the reddish circles are the biggest. Let’s try using a test based on the radius of each circle. The figure shows the result of splitting on the radius using three different values.</p>
<figure>
<img src="Images/F11022.png" alt="F11022" width="844" height="332"/>
<figcaption><p><a id="figure11-22">Figure 11-22</a>: Splitting a node according to different values of the radii of the circles within it</p></figcaption>
</figure>
<p>In this example, the radius value of 70 produces the purest results, with all the blue objects in one child and all the red ones in the other. If we use this test for this node, we’ll remember which feature we’re splitting on (the radius) and what value to test for (70). </p>
<p>Since we could potentially split the node using a test based on any characteristic of the samples, we need some way to evaluate the results so we can pick the best test. Let’s look at two popular ways to test these results. </p>
<p>Recall from Chapter 6 that <em>entropy</em> is a measure of complexity, or how many bits it takes to communicate some piece of information. The <em>Information Gain (IG)</em> measure uses this idea by comparing the entropy of a node to that of the children produced by each candidate test. </p>
<p>To evaluate a test, IG adds together the entropies of all the new children produced by that test and compares that result to the entropy in the parent cell. The more pure a cell is, the lower its entropy, so if a test makes pure cells, the sum of their entropies is less than the entropy of their parent. After trying different ways to split a node, we choose the split that gives us the biggest reduction in entropy (or the biggest gain in information).</p>
<p>Another popular way to evaluate splitting tests is called the <em>Gini impurity</em>. The math used by this technique is designed to minimize the probability of misclassifying a sample. For example, suppose a leaf has 10 samples of class A and 90 samples of class B. If a new sample ends up at that leaf, and we report that it belongs to class B, there is a 10 percent chance that <span epub:type="pagebreak" title="282" id="Page_282"/>we are wrong. Gini impurity measures those errors at each leaf for multiple candidate split values. It then chooses the split that has the least chance of an erroneous classification.</p>
<p>Some libraries offer other measures for grading the quality of a potential split. As with so many other choices, we usually try out a few options and pick the one that works the best for the specific data we’re working with.</p>
<h2 id="h1-500723c11-0004">Support Vector Machines</h2>
<p class="BodyFirst">Let’s consider our first parametric algorithm: the <em>support vector machine</em> (or <em>SVM</em>). We will use 2D data and just two classes for our illustrations (VanderPlas 2016), but like most machine learning algorithms, the ideas are easily applied to data with any number of dimensions and classes.</p>
<h3 id="h2-500723c11-0005">The Basic Algorithm</h3>
<p class="BodyFirst">Let’s begin with two blobs of points, one for each of two classes, shown in <a href="#figure11-23" id="figureanchor11-23">Figure 11-23</a>.</p>
<figure>
<img src="Images/F11023.png" alt="F11023" width="694" height="477"/>
<figcaption><p><a id="figure11-23">Figure 11-23</a>: Our starting dataset consists of two blobs of 2D samples.</p></figcaption>
</figure>
<p>We want to find a boundary between these clusters. To keep things simple, let’s use a straight line. But which one? A lot of lines split these two groups. Three candidates are shown in <a href="#figure11-24" id="figureanchor11-24">Figure 11-24</a>.</p>
<span epub:type="pagebreak" title="283" id="Page_283"/><figure>
<img src="Images/F11024.png" alt="F11024" width="694" height="477"/>
<figcaption><p><a id="figure11-24">Figure 11-24</a>: Three of the infinite number of lines that can separate our two clusters of samples</p></figcaption>
</figure>
<p>Which of these lines should we pick? One way to think about this is to imagine new data that might come in. Generally speaking, we want to classify any new sample as belonging to the class of the sample that it is nearest. </p>
<p>To evaluate how well any given boundary line achieves this goal, let’s find its distance to the nearest sample of either class. We can use this distance to draw a symmetrical boundary around the line. <a href="#figure11-25" id="figureanchor11-25">Figure 11-25</a> shows the idea for a few different lines.</p>
<figure>
<img src="Images/F11025.png" alt="F11025" width="694" height="145"/>
<figcaption><p><a id="figure11-25">Figure 11-25</a>: We can assign a quality to each line by finding the distance from that line to the nearest data point. </p></figcaption>
</figure>
<p>In <a href="#figure11-25">Figure 11-25</a>, we’ve drawn a circle around the sample that’s closest to the line. In the leftmost figure, many new points that are closer to the lower-right cluster than the upper-left one would be incorrectly classified as part of the upper-left cluster. The same situation holds in the rightmost <span epub:type="pagebreak" title="284" id="Page_284"/>figure. In the center, the line is much better, but it’s now preferring the lower-right cluster a little. Since we want each new point to be assigned to the class of the sample it’s closest to, we’d like our line to go right through the middle of the two clusters. </p>
<p>The algorithm that finds this line is called a <em>support vector machine</em>, or <em>SVM</em> (Steinwart 2008). An SVM finds the line that is farthest from all the points in both clusters. In this context, the word <em>support</em> can be thought of as meaning “nearest,” <em>vector</em> is a synonym for “sample,” and <em>machine</em> is a synonym for “algorithm.” Thus, we can describe SVM as the “nearest sample algorithm.”</p>
<p>The best line for our two clusters in <a href="#figure11-23">Figure 11-23</a>, as calculated by SVM, is shown in <a href="#figure11-26" id="figureanchor11-26">Figure 11-26</a>.</p>
<p>Let’s see how SVM finds this line.</p>
<p>In <a href="#figure11-26">Figure 11-26</a> the circled samples are the nearest samples, or the <em>support vectors</em>. The algorithm’s first job is to locate these circled points. Once it finds them, the algorithm then finds the solid line near the middle of the figure. Of all the lines that separate the two sets of points, this is the line that’s farthest from every sample in each set, because it has the greatest distance to its support vectors. The dashed lines in <a href="#figure11-26">Figure 11-26</a>, like the circles around the support vectors, are just visual aids to help us see that the solid line in the center, found by the SVM, is the one that is as far as possible from all the samples. The distance from the solid line to the dashed lines that pass through the support vectors is called the <em>margin</em>. We can rephrase the idea by saying that the SVM algorithm finds the line with the largest margin.</p>
<figure>
<img src="Images/F11026.png" alt="F11026" width="694" height="477"/>
<figcaption><p><a id="figure11-26">Figure 11-26</a>: The SVM algorithm finds the line that has the greatest distance from all the samples. </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="285" id="Page_285"/>What if the data is noisy, and the blobs overlap, as in <a href="#figure11-27" id="figureanchor11-27">Figure 11-27</a>? Now we can’t create a line surrounded by an empty zone. What’s the best line to draw through these overlapping sets of samples?</p>
<figure>
<img src="Images/F11027.png" alt="F11027" width="694" height="477"/>
<figcaption><p><a id="figure11-27">Figure 11-27</a>: A new set of data where the blobs overlap </p></figcaption>
</figure>
<p>The SVM algorithm gives us control over a parameter that’s conventionally called <em>C</em>. This parameter controls how strict the algorithm is about letting points into the region between the margins. The larger the value of <em>C</em>, the more the algorithm demands an empty zone around the line. The smaller the value of <em>C</em>, the more points can appear in a zone around the line. We frequently need to search for the best value for <em>C</em> using trial and error. In practice, that usually means trying out lots of values and evaluating them with cross-validation.</p>
<p><a href="#figure11-28" id="figureanchor11-28">Figure 11-28</a> shows our overlapping data with a <em>C</em> value of 100,000.</p>
<span epub:type="pagebreak" title="286" id="Page_286"/><figure>
<img src="Images/F11028.png" alt="F11028" width="694" height="477"/>
<figcaption><p><a id="figure11-28">Figure 11-28</a>: The value of <em>C</em> tells SVM how sensitive to be to points that can intrude into the zone around the line that’s fit to the data. Here, <em>C</em> is 100,000.</p></figcaption>
</figure>
<p>Let’s drop <em>C </em>way down to 0.01. <a href="#figure11-29" id="figureanchor11-29">Figure 11-29</a> shows that this lets fewer points into the region around the line.</p>
<figure>
<img src="Images/F11029.png" alt="F11029" width="694" height="477"/>
<figcaption><p><a id="figure11-29">Figure 11-29</a>: Lowering <em>C</em> to 0.01 lets in fewer points compared to <a href="#figure11-28">Figure 11-28</a>. </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="287" id="Page_287"/>The lines in <a href="#figure11-28">Figure 11-28</a> and <a href="#figure11-29">Figure 11-29</a> are different. Which one we prefer depends on what we want from our classifier. If we think the best boundary comes from the details near the zone where the points overlap, we want a small value of <em>C</em> so that we only look at the points near that boundary. If we think the overall shapes of the two collections of points is a better descriptor of that boundary, we want a larger value of <em>C</em> to include more of those farther-away points.</p>
<h3 id="h2-500723c11-0006">The SVM Kernel Trick </h3>
<p class="BodyFirst">A parametric algorithm is limited by the shapes it’s able to find. SVM, for instance, can only find <em>linear</em> shapes, like lines and planes. If we have data that can’t obviously be separated by such a shape, it may seem that SVM won’t be of much use. But there’s a clever trick that can sometimes let us use a linear boundary where it initially appears as if only a curved one could do the job.  </p>
<p>Suppose that we have the data of <a href="#figure11-30" id="figureanchor11-30">Figure 11-30</a>, where there’s a blob of samples of one class surrounded by a ring of samples of another. There’s no way we can draw a straight line to separate these two sets. </p>
<figure>
<img src="Images/F11030.png" alt="F11030" width="542" height="542"/>
<figcaption><p><a id="figure11-30">Figure 11-30</a>: A dataset of two classes</p></figcaption>
</figure>
<p>Here comes the clever part. Suppose we temporarily add a third dimension to each point by elevating it by an amount based on that point’s distance from the center of the square. <a href="#figure11-31" id="figureanchor11-31">Figure 11-31</a> shows the idea.</p>
<span epub:type="pagebreak" title="288" id="Page_288"/><figure>
<img src="Images/F11031.png" alt="F11031" width="694" height="289"/>
<figcaption><p><a id="figure11-31">Figure 11-31</a>: If we push each point in <a href="#figure11-30">Figure 11-30</a> upward by an amount based on its distance from the center of the pink blob, we get two distinct clouds of points, which we can separate with a plane. </p></figcaption>
</figure>
<p>As we can see in <a href="#figure11-31">Figure 11-31</a>, we can now draw a plane (the 2D version of a straight line) between the two sets. In fact, we can use the very same idea of support vectors and margins as we did before to find the plane. <a href="#figure11-32" id="figureanchor11-32">Figure 11-32</a> highlights the support vectors for the plane between the two clusters of points.</p>
<figure>
<img src="Images/F11032.png" alt="F11032" width="558" height="558"/>
<figcaption><p><a id="figure11-32">Figure 11-32</a>: The support vectors for the plane</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="289" id="Page_289"/>Now all points above the plane can be placed into one class and all those below into the other.</p>
<p>If we highlight the support vectors we found from <a href="#figure11-32">Figure 11-32</a> in our original 2D plot, we get <a href="#figure11-33" id="figureanchor11-33">Figure 11-33</a>.</p>
<figure>
<img src="Images/F11033.png" alt="F11033" width="558" height="558"/>
<figcaption><p><a id="figure11-33">Figure 11-33</a>: Looking down on <a href="#figure11-32">Figure 11-32</a></p></figcaption>
</figure>
<p>If we include the boundary created by the plane, we get <a href="#figure11-34" id="figureanchor11-34">Figure 11-34</a>.</p>
<p>In this case we found the right way to modify our data by looking at it and then coming up with a good 3D transformation of the data that let us split it apart. But when the data has many dimensions, we might not be able to visualize it well enough to even guess at a good transformation. Happily, we don’t have to find these transformations manually.</p>
<p>One way to find a good transformation is to try lots and lots of them, and then select the one that works the best. The calculations required make this approach too slow to be practical, but fortunately, we can speed this up in a clever way. This idea focuses on a piece of math called the <em>kernel, </em>which lies at the heart of the SVM algorithm. Mathematicians sometimes honor a particularly clever or neat idea with the complementary term <em>trick. </em>In this case, rewriting the SVM math is called the <em>kernel trick</em> (Bishop 2006). The kernel trick lets the algorithm find the distances between transformed points without actually transforming them, which is a major efficiency boost. The kernel trick is used automatically by all major libraries, so we don’t even have to ask for it (Raschka 2015).</p>
<span epub:type="pagebreak" title="290" id="Page_290"/><figure>
<img src="Images/F11034.png" alt="F11034" width="558" height="558"/>
<figcaption><p><a id="figure11-34">Figure 11-34</a>: The data from <a href="#figure11-30">Figure 11-30</a> with the support vectors, the dashed lines showing the margins, and the boundary created by the plane</p></figcaption>
</figure>
<h2 id="h1-500723c11-0005">Naive Bayes</h2>
<p class="BodyFirst">Let’s look at a parametric classifier that’s often used when we need quick results, even if they’re not the most accurate.</p>
<p>This classifier works quickly because it begins by making assumptions about the data. It is based on Bayes’ Rule, which we looked at in Chapter 4.</p>
<p>Recall that Bayes’ Rule begins with a <em>prior</em>, or a predetermined idea of what the result is likely to be. Normally when we use Bayes’ Rule, we refine the prior by evaluating new pieces of evidence, creating a <em>posterior</em> that then becomes our new prior. But what if we just commit to the prior ahead of time and then see where it leads us?</p>
<p>The <em>naive Bayes</em> classifier takes this approach. It’s called <em>naive</em> because the assumptions we make in our prior are not based on the contents of our data. That is, we make an uninformed, or naive, characterization of our data. We just assume<em> </em>that the data has a certain structure. If we’re right, great, we get good results. The less well the data matches this assumption, the worse the results are. Naive Bayes is popular because this assumption turns out to <span epub:type="pagebreak" title="291" id="Page_291"/>be correct, or nearly correct, often enough that it’s worth taking a look. The interesting thing is that we never check to see if our assumption is justified. We just plow ahead as if we are certain.</p>
<p>In one of the more common forms of naive Bayes, we assume that every feature of our samples follows a Gaussian distribution. Recall from Chapter 2 that this is the famous bell curve: a smooth, symmetrical shape with a central peak. That’s our prior. When we look at a specific feature across all of our samples, we simply try to match that as well as we can with a Gaussian curve.</p>
<p>If our features really do follow Gaussian distributions, then this assumption produces a good fit. The great thing about naive Bayes is that this assumption seems to work well far more frequently than we might expect.</p>
<p>Let’s see it in action, starting with data that does<em> </em>satisfy the prior. <a href="#figure11-35" id="figureanchor11-35">Figure 11-35</a> shows a dataset that was created by drawing samples from two Gaussian distributions.</p>
<figure>
<img src="Images/F11035.png" alt="F11035" width="610" height="419"/>
<figcaption><p><a id="figure11-35">Figure 11-35</a>: A set of 2D data for training with naive Bayes. There are two classes, red and blue.</p></figcaption>
</figure>
<p>When we give this data to a naive Bayes classifier, it assumes that each set of features comes from a Gaussian. That is, it assumes that the x coordinates of the red points follow a Gaussian, and the y coordinates of the red points also follow a Gaussian. It assumes the same thing about the x and y features of the blue points. Then it tries to fit the best four Gaussians it can to that data, creating two 2D hills. The result is shown in <a href="#figure11-36" id="figureanchor11-36">Figure 11-36</a>.</p>
<span epub:type="pagebreak" title="292" id="Page_292"/><figure>
<img src="Images/F11036.png" alt="F11036" width="694" height="264"/>
<figcaption><p><a id="figure11-36">Figure 11-36</a>: Naive Bayes fits a Gaussian to each of the x and y features of each class. Left: The Gaussian for the red class. Right: The Gaussian for the blue class.</p></figcaption>
</figure>
<p>If we overlay the Gaussian blobs and the points and look directly down, as in <a href="#figure11-37" id="figureanchor11-37">Figure 11-37</a>, we can see that they form a very close match. That’s no surprise, because we generated the data in a way that had exactly the distribution the naive Bayes classifier was expecting.</p>
<figure>
<img src="Images/F11037.png" alt="F11037" width="694" height="262"/>
<figcaption><p><a id="figure11-37">Figure 11-37</a>: Our entire training set overlaid on the Gaussians of <a href="#figure11-36">Figure 11-36</a></p></figcaption>
</figure>
<p>To see how well the classifier works in practice, let’s split the training data, putting a randomly selected 70 percent of the points into a training set and the rest into a test set. Let’s train with this new training set and then draw the test set on top of the Gaussians, giving us <a href="#figure11-38" id="figureanchor11-38">Figure 11-38</a>.</p>
<p>In <a href="#figure11-38">Figure 11-38</a>, we drew all the points that were classified as belonging to the first class on the left, and all those in the second class on the right, maintaining their original colors. We can see that all of the test samples were correctly classified.</p>
<span epub:type="pagebreak" title="293" id="Page_293"/><figure>
<img src="Images/F11038.png" alt="F11038" width="694" height="262"/>
<figcaption><p><a id="figure11-38">Figure 11-38</a>: The test data after training with 70 percent of our starting data </p></figcaption>
</figure>
<p>Now let’s try an example where we don’t<em> </em>satisfy the prior that all features of all samples follow Gaussian distributions. <a href="#figure11-39" id="figureanchor11-39">Figure 11-39</a> shows our new starting data of two noisy crescent moons.</p>
<figure>
<img src="Images/F11039.png" alt="F11039" width="694" height="477"/>
<figcaption><p><a id="figure11-39">Figure 11-39</a>: Some noisy crescent moon data</p></figcaption>
</figure>
<p>When we give these samples to the naive Bayes classifier, it assumes (as always) that the red x values, red y values, blue x values, and blue y values all come from Gaussian distributions. It finds the best Gaussians it can, shown in <a href="#figure11-40" id="figureanchor11-40">Figure 11-40</a>.</p>
<span epub:type="pagebreak" title="294" id="Page_294"/><figure>
<img src="Images/F11040.png" alt="F11040" width="694" height="264"/>
<figcaption><p><a id="figure11-40">Figure 11-40</a>: Fitting Gaussians to the crescent-moon data from <a href="#figure11-39">Figure 11-39</a></p></figcaption>
</figure>
<p>Of course, these are not a good match to our data, because they don’t satisfy the assumptions. Overlaying the data on the Gaussians in <a href="#figure11-41" id="figureanchor11-41">Figure 11-41</a> shows that the matches aren’t abysmal, but they’re pretty far off.</p>
<figure>
<img src="Images/F11041.png" alt="F11041" width="694" height="262"/>
<figcaption><p><a id="figure11-41">Figure 11-41</a>: Our training data from <a href="#figure11-39">Figure 11-39</a> overlaid on the Gaussians of <a href="#figure11-40">Figure 11-40</a></p></figcaption>
</figure>
<p>Like before, let’s now split the crescent moons into training and test sets, train on the 70 percent, and look at the predictions. In the left image of <a href="#figure11-42" id="figureanchor11-42">Figure 11-42</a> we can see all the points that we assigned to the red class. As we would hope, this has most of the red points, but some of the points from the upper-left red moon are not classified as red, and some points from the lower-right blue moon are classified as red, because their value from this Gaussian is higher than their value from the other. </p>
<p>On the right of <a href="#figure11-42">Figure 11-42</a>, we can see that the opposite situation holds for the other Gaussian. In other words, we correctly classified lots of the points, but we also misclassified points in each class. </p>
<p>We shouldn’t be too surprised at the misclassifications because our data did not follow the assumptions made by the naive Bayes prior. What is amazing is how well the classifier did. In general, naive Bayes often does a good job on all kinds of data. This is probably because lots of real-world data comes from processes that are well described by Gaussians.</p>
<span epub:type="pagebreak" title="295" id="Page_295"/><figure>
<img src="Images/F11042.png" alt="F11042" width="694" height="262"/>
<figcaption><p><a id="figure11-42">Figure 11-42</a>: Predictions of test data from our naive Bayes classifier trained on the data in <a href="#figure11-39">Figure 11-39</a></p></figcaption>
</figure>
<p>Because naive Bayes is so fast, it’s common to apply it when we’re trying to get a feeling for our data. If it does a great job, we might not have to look at any more complex algorithms.</p>
<h2 id="h1-500723c11-0006">Comparing Classifiers</h2>
<p class="BodyFirst">We looked at four popular classification algorithms in this chapter. Most machine learning libraries offer all of these, along with many others. Very briefly, let’s look at the pros and cons of these four classifiers, starting with the nonparametric algorithms.</p>
<p>The kNN method is flexible. It doesn’t explicitly represent boundaries, so it can handle any kind of complicated structure formed by the class samples in the training data. It’s fast to train, since it typically just saves each training sample. On the other hand, prediction is slow, because the algorithm has to search for the nearest neighbors for every sample we want to classify (there are many efficiency methods that speed up this search, but it still takes time). And because it’s saving every training sample, the algorithm can consume huge gulps of memory. If the training set is larger than the available memory, the operating system typically needs to start saving data on the hard drive (or other external storage), which can slow the algorithm down significantly.</p>
<p>Decision trees are fast to train, and they’re also fast when making predictions. They can handle weird boundaries between classes, though this can require a deeper tree. They have a huge downside due to their appetite for overfitting (though as we mentioned, we will address this issue later by using collections of small trees, so all is not lost). Decision trees have a huge appeal in practice because they are easy to interpret. Sometimes people use decision trees, even when the results are inferior to other classifiers, because their decisions are transparent, or easy to understand. Note that this doesn’t mean that the choices are fair or even correct, just that they’re comprehensible to humans.</p>
<p>Support vector machines are parametric algorithms that can make fast predictions. Once trained, they don’t need much memory, since they <span epub:type="pagebreak" title="296" id="Page_296"/>only store the boundaries between sample regions. And they can use the kernel trick to find classification boundaries that appear much more complicated than the straight lines (and flat planes, and higher-dimensional flat surfaces) that SVM produces. On the other hand, the training time grows with the size of the training set. The quality of the results is sensitive to the parameter <em>C </em>that specifies how many samples are allowed near the boundary. We can use cross-validation to try different values of <em>C </em>and pick out the best one.</p>
<p>Naive Bayes trains quickly and predicts quickly, and it’s not too hard to explain its results (though they’re a bit more abstract than decision trees or kNN results). The method has no parameters that we need to tune. If the classes we’re working with are well separated, then the naive Bayes prior often produces good results. The algorithm works particularly well when our data is Gaussian in nature. It also works well when the data has many features because the classes of such data are often separated in ways that play to the strengths of naive Bayes (VanderPlas 2016). In practice, we often try naive Bayes early in the process of getting to know a dataset, because it’s fast to train and predict and can give us a feeling for the structure of our data. If the prediction quality is poor, we can then turn to a more expensive classifier (that is, one requiring more time or memory). </p>
<p>The algorithms we’ve seen here are frequently used in practice, particularly when we’re first getting to know our data, because they’re usually straightforward to apply and visualize.</p>
<h2 id="h1-500723c11-0007">Summary</h2>
<p class="BodyFirst">In this chapter, we covered two types of classifier. When a classifier has no preconceptions on the structure of the data it’s going to look at, we say it is nonparametric. The <em>k-</em>nearest neighbors algorithm is of this variety, assigning a class to a sample based on its most popular neighbor. Decision trees are also nonparametric, assigning classes based on a series of decisions that are learned from the training data.</p>
<p>On the other hand, parametric classifiers have a preconceived notion of the structure of the data. A basic support vector machine looks for linear shapes, like lines or planes, that separate the training data by class. A naive Bayes classifier presumes that the data has a fixed distribution, usually Gaussian, and then does its best to fit that distribution to each feature in the data.</p>
<p>In the next chapter, we’ll see how to bundle together multiple classifiers to produce ensemble classifiers that outperform their individual components.  </p>
</section>
</div></body></html>