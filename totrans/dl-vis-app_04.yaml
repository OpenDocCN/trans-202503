- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Measuring Performance
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 性能度量
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: As we build systems to predict, classify, and otherwise find patterns in our
    data, we’ll need some way to discuss how well they’re doing their job. We use
    a variety of numerical measurements for just this purpose, which we collectively
    call *performance metrics*. They’ve been designed to enable us to carefully describe
    what the system is doing right, and more importantly, when the system gets the
    wrong answers, specifically how those answers are wrong. These tools are the keys
    to interpreting any system’s results.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建系统来预测、分类以及在数据中发现模式时，我们需要某种方式来讨论它们的表现如何。为此，我们使用了各种数值度量工具，统称为*性能度量标准*。这些工具的设计旨在帮助我们仔细描述系统做得对的地方，更重要的是，当系统得出错误的答案时，具体是哪些地方错了。这些工具是解释任何系统结果的关键。
- en: Our metrics are based on *probability*, or how likely it is that we’ll see different
    types of results. So we’ll begin with a light discussion of probability, focusing
    on just the most important ideas. Then we’ll apply it to build our performance
    metrics.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的度量标准基于*概率*，即我们看到不同类型结果的可能性。因此，我们将从轻松讨论概率开始，专注于最重要的概念。然后，我们将应用这些概念来构建我们的性能度量标准。
- en: 'Probability is an enormous subject, with many deep specialties. Since our focus
    is on using machine learning tools sensibly, we only need command of a few basic
    terms and topics: different kinds of probability, how to measure correctness,
    and a particular way of organizing probabilities called the *confusion matrix*.
    With a command of these basic ideas, we’ll be able to prepare our data to get
    the best performance out of the tools we’ll be using later. Broader and deeper
    discussions on all the topics we’ll cover here, as well as many other topics in
    this field, may be found in many references (Jaynes 2003; Walpole et al. 2011;
    Kunin et al. 2020).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 概率是一个庞大的主题，包含许多深奥的专业领域。由于我们关注的是如何明智地使用机器学习工具，我们只需要掌握一些基本的术语和主题：不同类型的概率、如何衡量正确性，以及一种叫做*混淆矩阵*的概率组织方式。掌握了这些基本概念后，我们将能够准备好数据，以便在后续使用工具时获得最佳性能。有关我们将讨论的所有主题的更广泛和深入的讨论，以及该领域的许多其他主题，可以在许多参考资料中找到（Jaynes
    2003；Walpole 等 2011；Kunin 等 2020）。
- en: Different Types of Probability
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率的不同类型
- en: There are many types of probability. We’ll discuss a few of them here, beginning
    with a metaphor.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 概率有许多种类型。我们将在这里讨论其中的一些，从一个比喻开始。
- en: Dart Throwing
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 掷飞镖
- en: '*Dart throwing*is the classic metaphor for discussing basic probability. The
    fundamental idea is that we’re in a room with a bunch of darts in our hand, facing
    a wall. Instead of hanging a cork target, we’ve painted the wall with some blobs
    of different colors and sizes. We’ll throw our darts at the wall, and we’ll track
    which colored region each one lands in (the background counts as a region as well).
    The idea is illustrated in [Figure 3-1](#figure3-1).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*掷飞镖*是讨论基础概率的经典比喻。基本思想是我们站在一个房间里，手里拿着一堆飞镖，面对着一面墙。我们没有挂一个软木靶，而是用不同颜色和大小的油漆斑块涂满了墙面。我们将飞镖投向墙面，并追踪每一支飞镖落在哪个颜色区域（背景也算作一个区域）。这个概念如[图
    3-1](#figure3-1)所示。'
- en: '![C03f001](Images/C03f001.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![C03f001](Images/C03f001.png)'
- en: 'Figure 3-1: Throwing darts at a wall. The wall is covered in blobs of paint
    of different colors.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-1：向墙上投掷飞镖。墙上覆盖着不同颜色的油漆斑块。
- en: We’re going to assume from now on that our darts will always strike the wall
    somewhere (rather than going into the floor or ceiling, for instance). So the
    probability of each dart striking the wall *somewhere* is 100 percent. We’ll use
    both floating-point (or real) numbers and percentages for probabilities, so a
    probability of 1.0 would be a percentage of 100 percent, a probability of 0.75
    would be a percentage of 75 percent, and so on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在假设我们的飞镖总是会击中墙壁的某个地方（而不是落到地板或天花板上）。因此，每支飞镖击中墙壁*某个地方*的概率是 100%。我们将同时使用浮动点数（或实数）和百分比来表示概率，因此概率为
    1.0 就是 100%，概率为 0.75 就是 75%，以此类推。
- en: Let’s look more closely at our dart-throwing scenario. In the real world, we’re
    more likely to hit the part of the wall that’s directly in front of us, rather
    than, say, something well off to the side. But for the purpose of this discussion,
    we’re going to assume that the probability of our hitting the wall at any point
    is the same *everywhere*. That is, *every point on the wall has the same chance
    of being hit by a dart*. Using the language of Chapter 2, we could also say that
    the probability of striking any given point is given by a uniform distribution.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看我们的投掷飞镖的场景。在现实世界中，我们更可能击中直接在我们面前的墙面部分，而不是击中侧面的一部分。但为了讨论的方便，我们假设在墙面上的任何一点被击中的概率都是相同的*在任何地方都是如此*。也就是说，*墙面上的每一点被飞镖击中的机会相同*。用第二章的术语来说，我们也可以说击中任意一个点的概率遵循均匀分布。
- en: The heart of the rest of the discussion will be based on comparing the areas
    of the various regions, and our chances of striking each of those areas. Remember
    that the background counts as a region (in [Figure 3-1](#figure3-1), it’s the
    white region).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的讨论将重点比较不同区域的面积，以及我们击中这些区域的概率。记住，背景也算作一个区域（在[图3-1](#figure3-1)中，它是白色区域）。
- en: Here’s an example. [Figure 3-2](#figure3-2) shows a red square on the wall.
    When we throw a dart, we know it will hit the wall somewhere, with a probability
    of 1\.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子。[图3-2](#figure3-2)显示了墙面上的一个红色方块。当我们投掷飞镖时，我们知道它会击中墙面上的某个位置，概率为1\。
- en: '![C03f002](Images/C03f002.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![C03f002](Images/C03f002.png)'
- en: 'Figure 3-2: We’re guaranteed to hit the wall. What’s the probability that we’ll
    hit the red square?'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-2：我们保证会击中墙面。我们击中红色方块的概率是多少？
- en: What’s the probability of hitting the red square? In this figure, the square
    covers half of the wall’s total area. Since our rule is that every point on the
    wall has an equal likelihood of being hit, when we throw our dart, we have a 50
    percent chance, or a probability of 0.5, of the dart landing in the red square.
    The probability is just the ratio of the areas. The larger our square, the more
    area it encloses, and so the more likely it is that we’ll land inside of it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，击中红色方块的概率是多少呢？在这个图中，红色方块覆盖了墙面总面积的一半。由于我们的规则是墙面上的每一点被击中的概率相同，所以当我们投掷飞镖时，飞镖落入红色方块的概率是50%，即0.5。概率就是面积的比值。我们的方块越大，所覆盖的面积越多，因此我们落入方块的概率也就越大。
- en: We can illustrate this with a little picture that draws the ratios of the areas.
    [Figure 3-3](#figure3-3) shows the ratio for our square with respect to the wall.
    This kind of diagram, where we draw a “fraction” composed of one shape above the
    other, gives us a visual way to track which areas we’re talking about and get
    an intuitive feel for their relative sizes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个小图示来说明面积的比值。[图3-3](#figure3-3)展示了我们方块与墙面之间的面积比。这个图示通过绘制一个形状在另一个形状上方的“分数”，为我们提供了一种直观的方式，帮助我们跟踪讨论的面积，并对它们的相对大小有一个直观的感受。
- en: '[Figure 3-3](#figure3-3) shows the relative areas accurately, so the area of
    the red square is really half the area of the white box under it. Using the full-size
    shapes can make for awkward diagrams when one of the shapes is much larger than
    the other, so sometimes we’ll scale down regions to make the resulting figure
    fit the page better. That’s okay, because the ratio of the areas won’t change.
    Remember that the purpose of these ratios of shapes is to illustrate the relative
    area of one shape compared to the area of another.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-3](#figure3-3)准确显示了相对面积，因此红色方块的面积实际上是其下方白色方框面积的一半。当其中一个形状远大于另一个形状时，使用全尺寸图形可能会导致图表不够紧凑，因此有时我们会缩小区域，以便让结果图更好地适应页面。这没关系，因为面积比值不会改变。记住，这些形状比值的目的是为了说明一个形状相对于另一个形状的面积。'
- en: '![C03f003](Images/C03f003.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![C03f003](Images/C03f003.png)'
- en: 'Figure 3-3: The probability of hitting the square in [Figure 3-2](#figure3-2)
    is given by the ratio of the area of the red square to the area of the wall, here
    shown as a symbolic fraction.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-3：在[图3-2](#figure3-2)中，击中红色方块的概率由红色方块的面积与墙面面积的比值表示，这里以符号分数的形式展示。
- en: Simple Probability
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简单概率
- en: When we talk about the probability of something happening, we refer to that
    something as an *event*. We often refer to events with capital letters, such as
    A, B, C, and so on. The phrase “the probability of event A happening” simply means
    the probability that A happens. To save some space, rather than write “the probability
    of event A happening,” or more succinctly “the probability of A,” we usually write
    P(A) (some authors use a lowercase p, writing p(A)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论某件事发生的概率时，我们将这件事称为*事件*。我们通常用大写字母表示事件，如 A、B、C 等等。短语“事件 A 发生的概率”只是指 A 发生的概率。为了节省空间，而不是写“事件
    A 发生的概率”或更简洁地写“事件 A 的概率”，我们通常写成 P(A)（一些作者使用小写 p，写作 p(A)）。
- en: Let’s say A is the event in which we throw a dart and hit the red square from
    [Figure 3-2](#figure3-2). We can represent P(A) with a ratio, as we did earlier.
    [Figure 3-4](#figure3-4) shows this graphically.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 A 是我们投掷飞镖并击中[图 3-2](#figure3-2)中的红色方块的事件。我们可以像之前一样用比率表示 P(A)。[图 3-4](#figure3-4)从图形上展示了这一点。
- en: '![C03f004](Images/C03f004.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![C03f004](Images/C03f004.png)'
- en: 'Figure 3-4: We’ll say that hitting the square with our dart is event A. The
    probability of event A occurring is given by the symbolic ratio of areas in [Figure
    3-3](#figure3-3). We write this probability as P(A).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-4：我们假设用飞镖击中方形区域是事件 A。事件 A 发生的概率由[图 3-3](#figure3-3)中的面积比表示。我们将这个概率写作 P(A)。
- en: Here, P(A) is the area of the square divided by the area of the wall, so P(A)
    is 1/2\. This ratio is the probability that, when throwing a dart, we’ll hit the
    square rather than the rest of the wall. We call P(A) a *simple probability*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，P(A) 是方形区域的面积除以墙壁的面积，因此 P(A) 为 1/2。这个比率是投掷飞镖时击中方形区域而不是墙壁其余部分的概率。我们称 P(A)
    为*简单概率*。
- en: Conditional Probability
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 条件概率
- en: Let’s now talk about probabilities involving two events. Either of these events
    might happen, or both of them, or neither of them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论涉及两个事件的概率。这两个事件中的任一个可能发生，或者两个都发生，或者都不发生。
- en: For example, we might ask for the probability that a house contains a piano,
    and the probability that there’s a dog inside. There’s probably no relationship
    between these two qualities (or events). We say that two events that are not related
    to one another in any way are *independent*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能会问房子里有钢琴的概率和房子里有狗的概率。这两个特征（或事件）之间可能没有任何关系。我们称这些彼此无关的事件为*独立事件*。
- en: Many types of events are not independent, but have at least some kind of connection.
    We call these *dependent*. When events are dependent, we might want to find their
    relationship. That is, we’d like to find the probability of one specific event,
    when we already know that another specific event has happened (or is happening).
    For example, suppose we pass a house and hear a dog barking inside. Then we might
    ask, “What is the probability that there’s a dog’s chew toy in the house, *given*
    that we know there’s a dog inside?” In other words, we know that one event has
    happened, and we want to know the probability of the other.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 许多事件类型并非相互独立，而是至少有某种程度的联系。我们称这些为*依赖事件*。当事件是依赖的时，我们可能希望找出它们之间的关系。也就是说，当我们已经知道某个特定事件已经发生（或正在发生）时，我们想要知道另一个特定事件发生的概率。例如，假设我们路过一栋房子，听到里面有狗在叫。然后我们可能会问：“*给定*我们知道房子里有一只狗的情况下，房子里有狗咬玩具的概率是多少？”换句话说，我们知道一个事件已经发生，而我们想要知道另一个事件发生的概率。
- en: Let’s make this a bit more abstract, and discuss two events called A and B.
    Suppose that we know that B has happened, or equivalently, that B is true. Knowing
    this, we can ask what’s the probability that A is *also* true? We write this probability
    as P(A|B). The vertical bar represents the word *given*, so we’d say this out
    loud as “the probability that A is true, given that B is true,” or more simply,
    “the probability of A given B.” This is called the *conditional probability*of
    A given B, since it only applies to the situation, or condition, that B is true.
    We can also talk about P(B|A), which is the probability that B is true, given
    that A is true.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个问题变得更加抽象，讨论两个事件 A 和 B。假设我们知道 B 已经发生，或者等效地，B 为真。知道这一点后，我们可以问 A 也为真的概率是多少？我们将这个概率写作
    P(A|B)。竖线表示“*给定*”，所以我们可以把它读作“在 B 为真的情况下，A 为真的概率”，或者更简单地说，“给定 B 的 A 的概率”。这就是 A
    给定 B 的*条件概率*，因为它只适用于 B 为真的情形或条件。我们还可以讨论 P(B|A)，即给定 A 为真时 B 为真的概率。
- en: We can illustrate this with our picture diagrams. The left diagram in [Figure
    3-5](#figure3-5) shows our wall, with two overlapping blobs labeled A and B. P(A|B)
    is the probability that our dart landed in blob A, given that we already know
    it landed in blob B. In the symbolic ratio on the right of [Figure 3-5](#figure3-5),
    the top shape is the region that is common to both A and B. That is, it’s their
    overlap, or the area where the dart can land in A, given that we know it landed
    in B.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过图示来说明这一点。[图 3-5](#figure3-5)左图展示了我们的墙面，上面有两个重叠的斑块，标记为 A 和 B。P(A|B) 是指在已知飞镖已经落在
    B 区的情况下，它落在 A 区的概率。[图 3-5](#figure3-5)右边的符号比例中，顶部的形状是 A 区和 B 区的公共区域。也就是说，它是它们的重叠部分，或者是给定飞镖已经落在
    B 区时，飞镖可能落在 A 区的区域。
- en: '![C03f005](Images/C03f005.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![C03f005](Images/C03f005.png)'
- en: 'Figure 3-5: Left: The two blobs painted on the wall. Right: The probability
    of being in A given that the dart is already in B is the ratio of the area of
    A overlapping B, divided by the area of B.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-5：左图：墙上涂的两个斑块。右图：已知飞镖已经落在 B 区时，落在 A 区的概率是 A 区与 B 区重叠区域的面积与 B 区面积之比。
- en: P(A|B) is a positive number that we can estimate by using our darts. We can
    estimate P(A|B) by counting all the darts that land in the overlap of A and B,
    and dividing that number by how many land in any part of B.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: P(A|B) 是一个正数，我们可以通过使用飞镖来估算它。我们可以通过计数所有落在 A 区和 B 区重叠部分的飞镖，并将其数量除以落在 B 区任何部分的飞镖数量，来估算
    P(A|B)。
- en: Let’s see this in action. In [Figure 3-6](#figure3-6) we’ve thrown a number
    of darts at the wall containing the blobs of [Figure 3-5](#figure3-5). We placed
    the points to get good coverage over the whole area, with no two points too close
    to one another. Dart tips are too hard to see, so we show the location of each
    dart’s impact by a black circle, where the center of the circle shows where the
    dart struck.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个过程的实际应用。在[图 3-6](#figure3-6)中，我们已经将若干飞镖投向墙面，墙上有[图 3-5](#figure3-5)中所示的斑块。我们把飞镖投放的位置确保能够覆盖整个区域，且没有两个点距离过近。飞镖的尖端很难看到，所以我们用黑色圆圈表示每个飞镖的落点，圆圈的中心表示飞镖的撞击位置。
- en: '![C03f006](Images/C03f006.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![C03f006](Images/C03f006.png)'
- en: 'Figure 3-6: Throwing darts at the wall to find P(A|B). (a) Darts striking the
    wall. (b) All the darts in either A or B. (c) The darts only in B. (d) The darts
    that are in the overlap of A and B.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-6：向墙面投掷飞镖以找到 P(A|B)。 (a) 飞镖击中墙面。(b) 所有落在 A 区或 B 区的飞镖。(c) 仅落在 B 区的飞镖。(d)
    落在 A 区和 B 区重叠部分的飞镖。
- en: In [Figure 3-6](#figure3-6)(a) we show all the darts. In [Figure 3-6](#figure3-6)(b)
    we’ve isolated just the darts that landed in either A or B (remember it’s only
    the center of each black circle that counts). In [Figure 3-6](#figure3-6)(c) we
    see the 66 darts that have landed in region B, and in [Figure 3-6](#figure3-6)(d)
    we see the 23 darts that are in both A and B. The ratio of 23/66 (about 0.35)
    estimates the probability that a dart landing in B will also land in A. So P(A|B)
    is about 0.35\. That is, if a dart lands in B, then about 35 percent of the time,
    it will also be in A.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 3-6](#figure3-6)(a)中，我们展示了所有的飞镖。在[图 3-6](#figure3-6)(b)中，我们仅仅展示了落在 A 区或
    B 区的飞镖（记住，只有每个黑色圆圈的中心才是有效的）。在[图 3-6](#figure3-6)(c)中，我们看到有 66 支飞镖落在 B 区，而在[图 3-6](#figure3-6)(d)中，我们看到有
    23 支飞镖落在 A 区和 B 区的重叠部分。23/66（约 0.35）的比例估算了落在 B 区的飞镖同时落在 A 区的概率。因此，P(A|B) 约为 0.35。也就是说，如果飞镖落在
    B 区，那么大约 35% 的情况下，它也会落在 A 区。
- en: Note that this process doesn’t depend on the absolute area of the colored blobs,
    such as a number in square inches. It’s just the *relative* size of one area with
    respect to another, which is the only measure we really care about (if the wall
    doubled in size and so did the colored regions, the probability of landing in
    each one wouldn’t change).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个过程并不依赖于颜色斑块的绝对面积，比如以平方英寸为单位的数字。它仅仅是一个区域相对于另一个区域的*相对*大小，这才是我们真正关心的唯一度量标准（如果墙面面积加倍，颜色区域也加倍，那么每个区域内落镖的概率并不会改变）。
- en: The bigger the overlap of A and B, the more likely the dart is to land in both.
    If A surrounds B, as in [Figure 3-7](#figure3-7), then we *must* have landed in
    A given that we landed in B. In this case, the overlap of A and B (shown in gray)
    is the region of B itself. Thus the ratio of the overlap’s area to B’s area is
    100 percent, or P(A|B) = 1.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: A 区和 B 区的重叠越大，飞镖落在两者重叠部分的概率就越高。如果 A 区围绕着 B 区，就像在[图 3-7](#figure3-7)中那样，那么只要飞镖落在
    B 区，我们*一定*也会落在 A 区。在这种情况下，A 区和 B 区的重叠部分（以灰色表示）就是 B 区本身。因此，重叠区域面积与 B 区面积之比为 100%，即
    P(A|B) = 1。
- en: '![C03f007](Images/C03f007.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![C03f007](Images/C03f007.png)'
- en: 'Figure 3-7: Left: Two new blobs on the wall. Right: The probability of landing
    in A given that we’re in B is 1, because A encloses B, and thus their overlap
    is the same as B.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-7：左：墙上两个新斑点。右：当我们处于 B 中时，落入 A 的概率为 1，因为 A 包围 B，因此它们的重叠与 B 相同。
- en: On the other hand, if A and B don’t overlap at all, as in [Figure 3-8](#figure3-8),
    then the probability of the dart being in A given that it landed in B is 0 percent,
    or P(A|B) = 0.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果 A 和 B 没有任何重叠，就像 [图 3-8](#figure3-8) 中一样，那么在 B 中落入 A 的概率为 0%，或者 P(A|B)
    = 0。
- en: '![C03f008](Images/C03f008.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![C03f008](Images/C03f008.png)'
- en: 'Figure 3-8: Left: Another two new blobs on the wall. Right: The probability
    of landing in A given that we’re in B is 0 (or, equivalently, 0 percent), because
    there’s no overlap between A and B.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-8：左：墙上另外两个新斑点。右：当我们处于 B 中时，落入 A 的概率为 0（或等效地说，0%），因为 A 和 B 之间没有重叠。
- en: The symbolic ratio in [Figure 3-8](#figure3-8) shows that the area of overlap
    is 0, and 0 divided by anything is still 0.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 3-8](#figure3-8) 中的符号比例显示，重叠区域的面积为 0，而 0 除以任何数仍然是 0。
- en: For fun, let’s flip this around the other way, and ask about P(B|A), or the
    probability that we’re in blob B *given that we’re in blob A*. Using the same
    blobs as in [Figure 3-5](#figure3-5), the result is shown in [Figure 3-9](#figure3-9).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种娱乐，让我们换个角度来看这个问题，问问 P(B|A)，或者我们在 A 中时落入 B 的概率。使用与 [图 3-5](#figure3-5) 中相同的斑点，结果显示在
    [图 3-9](#figure3-9) 中。
- en: '![C03f009](Images/C03f009.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![C03f009](Images/C03f009.png)'
- en: 'Figure 3-9: The conditional probability P(B|A) is the probability we landed
    in B, given that we landed in A.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-9：条件概率 P(B|A) 是我们在 A 中落入 B 的概率。
- en: The logic is the same as before. The area of overlap divided by the area of
    A tells us how much of B appears in A. The more they overlap, the more likely
    it is that a dart landing in A will also land in B. Let’s assign a number to P(B|A).
    Referring back to [Figure 3-6](#figure3-6), we see that 104 darts land in A, and
    23 in B, so P(B|A) is 23/104 or about 0.22.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑与以前相同。重叠区域的面积除以 A 的面积告诉我们 B 出现在 A 中的概率。它们重叠得越多，飞镖落入 A 中并落入 B 的可能性就越大。让我们给
    P(B|A) 赋一个数值。参考 [图 3-6](#figure3-6)，我们看到有 104 枚飞镖落入 A，23 枚落入 B，所以 P(B|A) 是 23/104，约为
    0.22。
- en: Note that the order is important. We can see from [Figure 3-5](#figure3-5) and
    [Figure 3-9](#figure3-9) that P(A|B) does not have the same value as P(B|A). Given
    the sizes of A, B, and their overlap, the chance of landing in A given that we
    landed in B is greater than the chance of landing in B given that we landed in
    A. That is, P(A|B) is about 0.35, but P(B|A) is about 0.22.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意顺序很重要。从 [图 3-5](#figure3-5) 和 [图 3-9](#figure3-9) 可以看出，P(A|B) 的值与 P(B|A) 不同。考虑到
    A、B 及其重叠的大小，落入 A 的概率在已经落入 B 的条件下要大于落入 B 的概率。也就是说，P(A|B) 约为 0.35，而 P(B|A) 约为 0.22。
- en: Joint Probability
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 联合概率
- en: In the last section, we saw a way to express the probability of one event happening,
    given that another event had already occurred. It would also be helpful to know
    the probability of both things happening at once. In the language of our blobs,
    what’s the chance that a dart thrown at the wall will land in *both* blob A and
    blob B? We write the probability of bothA and B happening as P(A,B), where we
    think of the comma as meaning the word *and*. Thus we read P(A,B) out loud as
    “the probability of A and B.”
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了一种表达一个事件发生概率的方式，前提是另一个事件已经发生。知道两件事同时发生的概率也会很有帮助。用我们的斑点语言来说，飞镖投向墙壁时，落入斑点
    A 和斑点 B 的机会有多大？我们将同时发生 A 和 B 的概率写作 P(A,B)，在这里逗号的意思是“和”。因此我们大声读出 P(A,B) 为“A 和 B
    的概率”。
- en: We call P(A,B) the *joint probability*of A and B. Using our blobs, we can find
    this joint probability P(A,B) by comparing the area of the overlap of blobs A
    and B to the area of the wall. After all, we’re asking for the chance that our
    dart lands in both A and B, meaning inside their overlap, compared to the chance
    it could land anywhere on the wall. [Figure 3-10](#figure3-10) shows this idea.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称 P(A,B) 为斑点 A 和斑点 B 的*联合概率*。利用我们的斑点，我们可以通过比较斑点 A 和斑点 B 的重叠区域与墙壁面积来找到这个联合概率
    P(A,B)。毕竟，我们正在询问飞镖落入 A 和 B 的概率，即落入它们的重叠区域，相比于它可能落入墙壁的任何地方。参见 [图 3-10](#figure3-10)。
- en: '![C03f010](Images/C03f010.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![C03f010](Images/C03f010.png)'
- en: 'Figure 3-10: The probability that both A and B will occur is called their joint
    probability, written P(A,B).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-10：A 和 B 同时发生的概率称为它们的联合概率，记作 P(A,B)。
- en: There’s another way to look at the joint probability that’s a little more subtle,
    but powerful. It’s so useful that it will lead to the heart of Chapter 4\. This
    alternative view of the joint probability combines a simple probability with a
    conditional probability.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种看待联合概率的方法，它稍微复杂一些，但非常强大。它非常有用，将引领我们进入第4章的核心内容。这种联合概率的替代视角将简单概率与条件概率结合起来。
- en: 'Suppose we know the simple probability of hitting B, or P(B). And suppose we
    also know the conditional probability P(A|B), or the probability of hitting A,
    knowing that we hit B. We can combine these into a chain of reasoning: given the
    probability of hitting B, we’ll combine that with the probability of hitting A
    given that we hit B, to get the probability of hitting both A and B at the same
    time.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们知道击中B的简单概率，或者P(B)。假设我们还知道条件概率P(A|B)，即在已知击中B的情况下，击中A的概率。我们可以将这些结合起来形成一条推理链：在知道击中B的概率后，我们将其与已知击中B时击中A的概率结合，以得到同时击中A和B的概率。
- en: Let’s see the chain of reasoning with an example. Suppose that blob B covers
    half of the wall, so P(B) = 1/2\. Further, suppose that blob A covers a third
    of blob B, so P(A|B) = 1/3\. Then half of our darts thrown at the wall will land
    in B, and a third of those will fall in A. Since half of the darts fall in B,
    and a third of those will also fall in A, the total number that land in both B
    and in A is 1/2 × 1/3, or 1/6\.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来看一下推理链。假设B区域覆盖了墙的一半，所以P(B) = 1/2。进一步假设A区域覆盖了B的一三分之一，所以P(A|B) = 1/3。那么我们投掷到墙上的一半飞镖将落在B区域，其中三分之一的飞镖将落在A区域。由于一半的飞镖落在B区域，而其中三分之一也将落在A区域，所以同时落在B和A的飞镖总数是1/2
    × 1/3，即1/6。
- en: 'This example shows us the general rule: to find P(A,B) we multiply P(A|B) and
    P(B). This is really quite remarkable: we just found the joint probability P(A,B)
    using only the conditional probability P(A|B) and the simple probability P(B)!
    We write this as P(A,B) = P(A|B) × P(B). In practice, we usually leave off the
    explicit multiplication sign, writing just P(A,B) = P(A|B) P(B).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子向我们展示了一个通用规则：要找P(A,B)，我们将P(A|B)与P(B)相乘。这真的非常了不起：我们仅用条件概率P(A|B)和简单概率P(B)就找到了联合概率P(A,B)！我们将其写作P(A,B)
    = P(A|B) × P(B)。实际上，我们通常省略显式的乘法符号，直接写作P(A,B) = P(A|B) P(B)。
- en: '[Figure 3-11](#figure3-11) shows what we just did using our little area diagrams.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-11](#figure3-11)展示了我们刚刚使用小面积图示所做的事情。'
- en: '![C03f011](Images/C03f011.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![C03f011](Images/C03f011.png)'
- en: 'Figure 3-11: Another way to think about the joint probability P(A,B)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-11：另一种思考联合概率P(A,B)的方法
- en: Consider the right side of [Figure 3-11](#figure3-11) and think of the little
    symbolic ratios as actual fractions. Then the green blobs of area B cancel each
    other, and we’re left with the gray area over the square, showing that the left
    and right sides of our little equation are, indeed, equal.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑[图 3-11](#figure3-11)的右侧，并将这些小的符号比率看作实际的分数。然后，绿色的B区域相互抵消，剩下的就是覆盖在正方形上的灰色区域，这表明我们的小方程的左右两边确实是相等的。
- en: We can do this the other way around, too, using event A rather than B. We start
    with P(B|A) to learn the probability of landing in B given that we landed in A,
    and then we multiply that by the probability of landing in A, or P(A). The result
    is P(A,B) = P(B|A) P(A). Graphically, this follows the same pattern as [Figure
    3-11](#figure3-11), only now it’s the A blobs that cancel each other.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以反过来做，使用事件A而不是B。我们从P(B|A)开始，学习在已知落在A的情况下落在B的概率，然后将其与落在A的概率P(A)相乘。结果是P(A,B)
    = P(B|A) P(A)。在图形上，这遵循与[图 3-11](#figure3-11)相同的模式，只是这次是A区域相互抵消。
- en: In symbols, P(B,A) = P(A,B), since both refer to the probability of landing
    in A *and* B simultaneously. Unlike conditional probability, in joint probability,
    the order of naming A and B doesn’t matter.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 用符号表示，P(B,A) = P(A,B)，因为这两者都指的是同时落在A *和* B的概率。与条件概率不同，在联合概率中，A和B的顺序不重要。
- en: These ideas can be a little challenging to get used to, but mastering them will
    pay off in Chapter 4\. It may help to make up a few little scenarios and play
    with them, imagining different blobs and how they overlap, or even thinking of
    A and B as actual situations. For instance, imagine an ice cream shop where people
    can buy different flavors of ice cream, in either a waffle cone or cup. We might
    say V is true if someone orders vanilla ice cream, and W is true if a person orders
    their ice cream in a waffle cone. Then P(V) is how likely a random customer will
    order vanilla, and P(W) is how likely an independently chosen customer will ask
    for a waffle cone. P(V|W) tells us how likely it is that someone who got a waffle
    cone ordered vanilla, and P(W|V) tells us how likely it is that someone who ordered
    vanilla got it in a waffle cone. And P(V,W) tells us how likely it is that a randomly
    chosen customer got vanilla ice cream in a waffle cone.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念可能有些难以理解，但掌握它们将在第4章派上用场。你可以尝试构造几个小情境并加以练习，想象不同的区域及它们的重叠，或者甚至把A和B看作实际的情境。例如，假设有一家冰激凌店，顾客可以购买不同口味的冰激凌，且可以选择用华夫饼干筒或杯子装。我们可以说如果某人点了香草冰激凌，那么V为真；如果某人选择了华夫饼干筒，那么W为真。这样，P(V)表示随机顾客点香草冰激凌的可能性，P(W)表示随机顾客选择华夫饼干筒的可能性。P(V|W)告诉我们在选择了华夫饼干筒的人中，有多大可能性点了香草口味，而P(W|V)则告诉我们在选择了香草冰激凌的人中，有多大可能性选择了华夫饼干筒。而P(V,W)告诉我们随机顾客点香草冰激凌并选择华夫饼干筒的可能性。
- en: Marginal Probability
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 边际概率
- en: Another term used for simple probability is *marginal probability*, and understanding
    where this term comes from will help us understand how we can calculate simple
    probabilities for multiple events.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用于简单概率的术语是*marginal probability*（边际概率），理解这个术语的来源将帮助我们理解如何计算多个事件的简单概率。
- en: Let’s start with the word *marginal*, which can seem pretty strange in this
    context. After all, what does a margin have to do with probability? The legend
    behind the word *marginal* is that it comes from books that contained tables of
    precomputed probabilities. The idea is that we (or the printer) would sum up the
    totals in each row of these tables, and write those totals in the margin of the
    page (Glen 2014).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从词语*marginal*开始，它在这个背景下可能看起来有些陌生。毕竟，边缘和概率有什么关系呢？“边际”一词的由来是它源自于包含预计算概率表格的书籍。其背后的理念是我们（或印刷工）会将表格中每一行的总和算出，然后把这些总和写在页面的边缘（Glen
    2014）。
- en: Let’s illustrate this idea by returning to our ice cream shop. In [Figure 3-12](#figure3-12)
    we show some recent purchases made by our customers. Our shop is brand new and
    serves only vanilla and chocolate, in either a waffle cone or cup. Based on the
    purchases of the 150 people who came in yesterday, we can ask the probability
    of someone buying a cup versus a waffle cone, or vanilla versus chocolate. We
    find those values by adding up the numbers in each row or column (giving us the
    number in the margin) and dividing by the total number of customers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回到我们的冰激凌店来说明这个概念。在[图3-12](#figure3-12)中，我们展示了顾客近期的购买情况。我们的店铺刚开张，出售香草和巧克力冰激凌，且只能选择华夫饼干筒或杯子装。根据昨天进店的150位顾客的购买情况，我们可以问一下顾客购买杯子装与华夫饼干筒的概率，或者香草与巧克力的概率。我们通过将每一行或每一列的数值加起来（得到边缘的数字），然后除以总顾客数，来计算这些概率。
- en: '![C03f012](Images/C03f012.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![C03f012](Images/C03f012.png)'
- en: 'Figure 3-12: Finding marginal probabilities for 150 recent visitors at an ice
    cream shop. The values in the green boxes (located in the margins of the grid)
    are the marginal probabilities.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-12：计算150名冰激凌店近期顾客的边际概率。绿色框中的数值（位于网格的边缘）是边际概率。
- en: Note that the probabilities of someone buying a cup *or* waffle cone add up
    to 1, since every customer buys one or the other. Similarly, everyone buys either
    vanilla orchocolate, so those probabilities also add up to 1\. In general, all
    the probabilities for the various outcomes of any event will always add up to
    1, because it’s 100 percent sure that *one* of those choices will occur.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，顾客购买杯子*或*华夫饼干筒的概率之和为1，因为每个顾客要么买杯子，要么买华夫饼干筒。同样，所有人都购买香草或巧克力冰激凌，所以这些概率的和也为1。一般来说，任何事件的不同结果的概率之和总是为1，因为总有一个选择是100%发生的。
- en: Measuring Correctness
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量正确性
- en: 'Let’s now move from probability to our first performance measure: given an
    imperfect algorithm, how likely is it to produce the correct answer? This is a
    key question in machine learning, because we will almost always work with systems
    that fall short of being perfectly accurate. So it’s important to understand what
    kinds of errors they make.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从概率转到第一个性能度量：给定一个不完美的算法，它产生正确答案的可能性有多大？这是机器学习中的关键问题，因为我们几乎总是会与那些不完全准确的系统打交道。所以，理解它们会犯哪些错误是非常重要的。
- en: 'Let’s consider a simple classifier with just two classes. We can ask it for
    the probability that a piece of data is in some specific class (the two classes
    are then *in category* and *out of category*). For instance, we might ask for
    the probability that a photograph is of a dog, or the probability that a hurricane
    will hit land, or how likely it is that our high-tech enclosures are strong enough
    to hold our genetically engineered super-dinosaurs (spoiler: not very).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个只有两个类别的简单分类器。我们可以询问它某个数据点属于某个特定类别的概率（这两个类别分别是*在类别内*和*不在类别内*）。例如，我们可能会询问一张照片是狗的概率，或飓风登陆的概率，或我们的高科技围栏是否足够坚固以容纳我们的基因工程超级恐龙的概率（剧透：不太可能）。
- en: Naturally, we’d like our classifier to make accurate decisions. The trick is
    to define what we mean by *accurate*. Just counting the number of incorrect results
    is the easiest way to measure something that we might call accuracy, but it’s
    not very illuminating. The reason is that there is more than one way to be wrong.
    If we want to use our mistakes to improve our performance, then we need to identify
    the different ways our predictions can be wrong and consider how much trouble
    each kind of error causes us. This kind of analysis applies far beyond just machine
    learning. The following ideas can help diagnose and solve all kinds of problems
    where we’re making decisions on the basis of labels we’ve assigned.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，我们希望我们的分类器做出准确的决策。诀窍在于定义我们所说的*准确*是什么。仅仅统计错误结果的数量是衡量准确度的最简单方法，但它并不十分有启发性。原因在于，错误的表现形式不止一种。如果我们希望通过错误来改进我们的表现，那么我们需要识别预测出错的不同方式，并考虑每种错误类型带来的困扰。这种分析不仅仅适用于机器学习。以下的思想可以帮助诊断和解决各种基于标签做出决策的问题。
- en: Before we dig in, we’ll note that some of the terms we’ll be using here, such
    as *precision*, *recall*,and *accuracy*, are used casually in popular and informal
    writing. But in technical discussions (like in this book), these words have precise
    definitions and have specific meanings. Unfortunately, not all authors use the
    same definitions for these terms, which can cause all kinds of confusion. In this
    book, we’ll stick to the way they’re usually used when discussing probability
    and machine learning, and we’ll define them carefully when we come to them later
    in this chapter. But be aware that these terms appear in lots of places with different
    meanings or are just left as vague concepts. It’s unfortunate when words get overloaded
    this way, but it happens.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨之前，我们需要注意，像*精度*、*召回率*和*准确性*等术语在流行和非正式写作中使用得比较随意。但在技术讨论中（比如在本书中），这些词有精确的定义和特定的含义。不幸的是，并不是所有的作者对这些术语的定义都是一致的，这可能会引起各种混淆。在本书中，我们将坚持通常在讨论概率和机器学习时的用法，并将在本章后续部分详细定义它们。但请注意，这些术语在许多地方的含义不同，或只是作为模糊概念使用。当这些词语被过度加载时，虽然很遗憾，但它是常见的现象。
- en: Classifying Samples
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 样本分类
- en: 'Let’s narrow our language to the task at hand. We want to know if a given piece
    of data, or sample, is, or isn’t, in a given class. For now, think of this in
    yes/no question form: Is this sample in the class? There are no “maybe” answers
    allowed.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将语言集中于当前的任务。我们想知道给定的一个数据点或样本是否属于某个特定类别。现在，可以将其视为一个是/否问题：这个样本属于该类别吗？不允许有“可能”这样的回答。
- en: If the answer is “yes,” we call the sample *positive*. If the answer is “no,”
    we call the sample *negative*. We’ll discuss accuracy by comparing the answers
    we get from our classifier against the real, or correct, labels that we’ve assigned
    beforehand. The choice of positive or negative that we’ve manually assigned to
    the sample is called its *ground truth*or *actual value*. We’ll say that the value
    that comes back from our classifier is the *predicted value*. In a perfect world,
    the predicted value would always match the ground truth. In the real world, there
    are often errors, and our goal here is to characterize those errors.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果答案是“是”，我们称该样本为*正面*。如果答案是“否”，我们称该样本为*负面*。我们将通过将分类器的答案与我们提前分配的真实或正确标签进行比较，来讨论准确性。我们手动分配给样本的正面或负面标签称为其*真实值*或*实际值*。我们将分类器返回的值称为*预测值*。在理想情况下，预测值将始终与真实值匹配。在现实世界中，通常会发生错误，我们的目标是描述这些错误。
- en: We’ll illustrate our discussion with two-dimensional (2D) data. That is, every
    sample, or data point, has two values. These might be a person’s height and weight,
    or a weather measurement of humidity and wind speed, or a musical note’s frequency
    and volume. Then we can plot each piece of data on a 2D grid with the X axis corresponding
    to one measurement and the Y axis to the other.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过二维（2D）数据来说明我们的讨论。也就是说，每个样本或数据点都有两个值。这些值可能是一个人的身高和体重，或者是天气的湿度和风速，或者是音乐音符的频率和音量。然后，我们可以将每个数据点绘制在一个二维坐标系中，X轴对应一个测量值，Y轴对应另一个测量值。
- en: Our samples will each belong to one of two classes. Let’s call them *positive*
    and *negative*. To identify a sample’s correct classification, or its ground truth,
    we’ll use color and shape cues, as in [Figure 3-13](#figure3-13).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的样本将属于两个类别之一。我们称它们为*正面*和*负面*。为了确定样本的正确分类或真实值，我们将使用颜色和形状提示，如[图 3-13](#figure3-13)所示。
- en: '![C03f013](Images/C03f013.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![C03f013](Images/C03f013.png)'
- en: 'Figure 3-13: Two-dimensional data belonging to two different classes'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-13：属于两个不同类别的二维数据
- en: We’ll show the results of our predictions by drawing a *boundary*, or curve,
    through the collection of points. The boundary may be smooth or twisty. We can
    think of it as a kind of summary of the classifier’s decision-making process.
    All points in one side of the curve will be predicted to be of one class, while
    all those on the other side will be predicted to be in the other class. In the
    right diagram of [Figure 3-13](#figure3-13), the classifier has done a perfect
    job of predicting the ground truth of each sample. That’s a rare thing.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过绘制一条*边界*或曲线来展示预测结果，穿过一组数据点。边界可以是平滑的，也可以是弯曲的。我们可以将其视为分类器决策过程的一种总结。曲线一侧的所有点将被预测为一个类别，而另一侧的所有点则被预测为另一个类别。在[图
    3-13](#figure3-13)的右侧图中，分类器已完美地预测了每个样本的真实情况。这是非常罕见的情况。
- en: We sometimes say that the boundary has a positive side and a negative side.
    This matches up to our class if we think of the classifier as answering the question,
    “Does this sample belong to the class?” If the answer is positive, then the prediction
    is “yes,” otherwise the prediction is “no.” It’s often helpful to color in the
    regions on either side of the boundary, as we’ve done in [Figure 3-13](#figure3-13),
    to make it easy to see which side holds the predictions of *positive*, and which
    holds the predictions of *negative*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有时会说边界有正面和负面两侧。如果我们将分类器视为回答“这个样本属于该类别吗？”这个问题，这与我们课堂上所学的内容相符。如果答案是肯定的，那么预测就是“是”，否则预测就是“否”。通常，将边界两侧的区域上色，如我们在[图
    3-13](#figure3-13)中所做的那样，会有助于清晰地看到哪一侧表示*正面*预测，哪一侧表示*负面*预测。
- en: For our dataset, we’ll use a set of 20 samples, shown in [Figure 3-14](#figure3-14).
    The samples with a ground truth (or manual label) of *positive* are shown as green
    circles, while those with a ground truth (or manual label) of *negative* are drawn
    as red squares. So the color and shape of each sample corresponds to its ground
    truth, and the background color shows the value assigned by the classifier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的数据集，我们将使用一组20个样本，如[图 3-14](#figure3-14)所示。具有*正面*真实值（或手动标签）的样本显示为绿色圆圈，而具有*负面*真实值（或手动标签）的样本则显示为红色方块。因此，每个样本的颜色和形状对应于其真实值，而背景颜色显示分类器分配的值。
- en: '![C03f014](Images/C03f014.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![C03f014](Images/C03f014.png)'
- en: 'Figure 3-14: Left: The classifier’s curve does an okay job of classifying the
    data, but it makes some mistakes. Right: A schematic version of the same diagram.
    The curved boundary reminds us that the actual boundary is rarely a straight line.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-14：左侧：分类器的曲线在分类数据方面做得还可以，但它犯了一些错误。右侧：相同图表的示意图。曲线边界提醒我们，实际边界很少是直线。
- en: The job of the classifier is to try to find a boundary so that all the positive
    samples land on one side, and all the negative samples land on the other. To see
    how well the classifier’s prediction of each sample matches its ground truth,
    we can just look to see if that sample ended up on the correct side of the classifier’s
    boundary curve. That curve splits the space into two regions. We’ve used light
    green to show the positive region, and light red for negative, so every point
    in the light-green region is predicted, or classified, as positive, and every
    point in the light-red region is classified as negative.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的任务是尽量找到一个边界，使得所有正样本都位于一侧，所有负样本都位于另一侧。为了查看分类器对每个样本的预测与实际情况的匹配程度，我们可以直接观察该样本是否落在分类器边界曲线的正确一侧。那条曲线将空间分为两个区域。我们使用浅绿色表示正区域，浅红色表示负区域，因此在浅绿色区域内的每个点都被预测或分类为正类，而在浅红色区域内的每个点都被分类为负类。
- en: In a perfect world, all the green circles (the ones with a positive ground truth)
    would be on the green side of the boundary curve (showing that the classifier
    predicted them as positive), and all the red squares would be on the red side.
    But as we can see in the figure, this classifier has made some mistakes. On the
    left of [Figure 3-14](#figure3-14) we plotted each piece of data using its two
    values, along with the boundary curve (and regions) that characterize the classifier’s
    decisions. But we don’t really care in this discussion about the specific locations
    of the points or the shape of the curve. Our interest is in how many points were
    correctly and incorrectly classified and thus landed on the right and wrong side
    of the boundary. So in the figure on the right, we’ve cleaned up the geometry
    to make it easier to count the samples at a glance.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的情况下，所有绿色圆圈（具有正实际标签的那些）都会位于边界曲线的绿色一侧（显示分类器将它们预测为正类），所有红色方块都会位于红色一侧。但正如我们在图中所看到的，这个分类器确实犯了一些错误。在[图3-14](#figure3-14)的左侧，我们用每个数据的两个值以及描述分类器决策的边界曲线（和区域）绘制了每一条数据。但在这个讨论中，我们并不关心点的位置或曲线的形状。我们的兴趣在于有多少点被正确或错误地分类，因此落在了边界的正确或错误一侧。因此，在右侧的图中，我们简化了几何结构，以便一目了然地计算样本数量。
- en: This diagram represents what typically happens when we run a classifier on a
    real dataset. Some data is classified correctly, and some isn’t. If our classifier
    isn’t performing well enough for us, we’ll need to take some sort of action—perhaps
    by modifying the classifier or even throwing it out and making a new one—so it’s
    important to be able to usefully characterize how well it’s doing.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该图表示我们在真实数据集上运行分类器时通常发生的情况。一些数据被正确分类，而一些则没有。如果我们的分类器表现得不够好，我们需要采取一些措施——可能是修改分类器，甚至丢弃它并重新创建一个新的——因此，能够有效地描述其表现非常重要。
- en: Let’s find some ways to do that. We’d like to characterize the errors in [Figure
    3-14](#figure3-14) in a way that tells us something about the nature of the classifier’s
    performance, or how well its predictions matched our given labels. It would be
    nice to know something more than just “right” and “wrong”—we’d like to know the
    nature of the mistakes, because some mistakes might matter to us a lot, while
    others might not matter much at all.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找些方法来做到这一点。我们希望能够描述[图3-14](#figure3-14)中的错误，以一种能够告诉我们分类器表现的性质，或者它的预测与给定标签的匹配程度的方式。我们希望了解的不仅仅是“对”或“错”——我们希望了解错误的性质，因为有些错误对我们来说可能非常重要，而另一些错误可能根本不重要。
- en: The Confusion Matrix
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: To characterize the classifier’s answers, we can make a little table that has
    two columns, one for each predicted class, and two rows, one for each actual,
    or ground truth, class. That gives us a 2 by 2 grid, referred to as a *confusion
    matrix*. The name refers to how the grid, or matrix, shows us the ways in which
    our classifier was mistaken, or confused, about its predictions. The classifier’s
    output is repeated in [Figure 3-15](#figure3-15), along with its confusion matrix.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述分类器的预测结果，我们可以制作一个小表格，表格有两列，每一列代表一个预测类别，和两行，每一行代表一个实际类别，或者称为真实类别。这就形成了一个2×2的网格，称为*混淆矩阵*。这个名称反映了矩阵如何展示分类器在预测时出错或混淆的情况。分类器的输出在[图3-15](#figure3-15)中得以展示，同时也展示了它的混淆矩阵。
- en: '![C03f015](Images/C03f015.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![C03f015](Images/C03f015.png)'
- en: 'Figure 3-15: We can summarize what went where in [Figure 3-14](#figure3-14)
    (repeated here on the left, with labels) into a confusion matrix, which tells
    us how many samples landed in each of the four classes.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-15：我们可以将[图3-14](#figure3-14)（此处左侧重复，带有标签）中的内容汇总到一个混淆矩阵中，这个矩阵告诉我们有多少样本落入了四个类别中的每一个。
- en: As [Figure 3-15](#figure3-15) shows, each of the four cells in the table has
    a conventional name, which describes a specific combination of the predicted and
    actual values. The six positive green circles were correctly predicted as positive,
    so they go into the *true positive*category. In other words, they were predicted
    to be positive, and they actually were positive, so the prediction of positive
    was correct, or true. The four green circles that were incorrectly classified
    as negative go into the *false negative*category, because they were incorrectly,
    or falsely, labeled as negative. The eight red negative squares were correctly
    classified as negative, so they all go into the *true negative*category. Finally,
    the two red squares that were incorrectly predicted to be positive go into *false
    positive*, because they were incorrectly, or falsely, predicted to be positive.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[图3-15](#figure3-15)所示，表格中的每一个单元格都有一个传统名称，描述了预测值和实际值的特定组合。六个绿色的正圆圈被正确预测为正，因此它们进入*真正类*。换句话说，它们被预测为正，实际上也确实是正，因此正的预测是正确的，或者说是真的。四个错误分类为负的绿色圆圈进入*假负类*，因为它们被错误地或虚假地标记为负。八个红色负方块被正确分类为负，因此它们都进入*真负类*。最后，两个被错误预测为正的红色方块进入*假正类*，因为它们被错误地或虚假地预测为正。
- en: We can write this more concisely using two-letter abbreviations for the four
    classes and a number describing how many samples fell into each category. [Figure
    3-16](#figure3-16) shows the form that the confusion matrix is usually shown in.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用四个类别的两字母缩写和描述每个类别中有多少样本的数字来更简洁地表达这一点。[图3-16](#figure3-16)展示了混淆矩阵通常的形式。
- en: Unfortunately, there is no universal agreement on where the various labels go
    in confusion matrix diagrams. Some authors put predictions on the left and actual
    values on top, and some place positive and negative in the opposite locations
    than shown here. When we encounter a confusion matrix, it’s important to look
    at the labels and make sure we know what each box represents.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于混淆矩阵图中的各个标签的位置，并没有统一的约定。一些作者将预测值放在左侧，将实际值放在顶部，而有些则将正负类别的位置与这里所示的相反。当我们遇到混淆矩阵时，重要的是查看标签，确保我们知道每个单元格代表什么。
- en: '![C03f016](Images/C03f016.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![C03f016](Images/C03f016.png)'
- en: 'Figure 3-16: The confusion matrix of [Figure 3-15](#figure3-15) in numerical
    form'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-16：[图3-15](#figure3-15)的混淆矩阵的数字表示形式
- en: Characterizing Incorrect Predictions
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 描述错误预测
- en: We mentioned earlier that some errors might matter more to us than others. Let’s
    see why that might be.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，有些错误可能对我们更重要。让我们来看看为什么会这样。
- en: Suppose that we work for a company that makes toy figurines in the likeness
    of popular TV characters. Our toys are a hit right now, so our production line
    is running at full capacity. Our job is to take the manufactured figurines, box
    them, and ship them off to retail stores.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在一个公司工作，这家公司生产以流行电视角色为原型的玩具人物模型。我们的玩具现在非常受欢迎，所以生产线正处于满负荷运转。我们的工作是将制造出来的模型人物包装好，并送往零售商店。
- en: Suddenly, one day we’re told that our company has lost the rights to sell a
    particular character named Glasses McGlassface. If we accidentally ship any of
    those figurines, we’ll get sued, so it’s important to make sure that none of them
    leave our factory. Unfortunately, the machines are still cranking them out, and
    if we stop the production line to update the machines, we’ll fall way behind on
    our orders. We decide the better approach is to keep making the forbidden figurines,
    but spot them after they’ve been made and throw them into a bin for recycling.
    So our goal is to identify each Glasses McGlassface and throw it in the bin, making
    sure none of them get out the door.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 突然有一天，我们被告知，公司失去了销售名为眼镜麦克眼镜脸的角色的权利。如果我们不小心发货了这些洋娃娃，我们会被起诉，因此确保这些娃娃不离开我们的工厂非常重要。不幸的是，机器仍在生产这些娃娃，如果我们停止生产线来更新机器，我们的订单会落后很多。我们决定更好的做法是继续生产这些禁止的洋娃娃，但在它们被制造出来之后，再把它们挑出来放进回收箱。所以我们的目标是识别每一个眼镜麦克眼镜脸并将其丢进回收箱，确保它们不会从工厂流出。
- en: '[Figure 3-17](#figure3-17) shows the situation.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-17](#figure3-17)展示了这一情况。'
- en: We need to work fast, so we might make some mistakes. In [Figure 3-17](#figure3-17)
    we see one figurine that we incorrectly recycled. That is, when answering the
    question, “Is this Glasses McGlassface?” we incorrectly said, “yes.” Using our
    language from the last section, this doll is a false positive. How big a problem
    is that?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要快速工作，因此可能会犯一些错误。在[图3-17](#figure3-17)中，我们看到一个被错误回收的洋娃娃。也就是说，在回答“这是眼镜麦克眼镜脸吗？”这个问题时，我们错误地说了“是的”。按照我们上一节的说法，这个娃娃是一个假阳性。这会是一个多大的问题呢？
- en: In this case, it’s not a big deal (as long as we don’t do it too often). Our
    goal is to make sure that every Glasses McGlassface is correctly identified and
    removed. Missing even one would cost us a lot. But a false positive costs us only
    a little, since we’ll melt down the plastic and reuse it. So in this situation,
    false positives, while not desirable, are tolerable.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这并不是一个大问题（只要我们不要做得太频繁）。我们的目标是确保每个眼镜麦克眼镜脸都能被正确识别和移除。即使错过一个也会给我们带来很大的损失。但一个假阳性仅仅会给我们带来一点损失，因为我们会将塑料融化并重新使用。所以在这种情况下，假阳性虽然不理想，但可以容忍。
- en: '![C03f017](Images/C03f017.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![C03f017](Images/C03f017.png)'
- en: 'Figure 3-17: Glasses McGlassface is the first character on the top row. We
    want to remove any doll that could be that character. Our selections are in the
    middle row.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-17：眼镜麦克眼镜脸是第一排的第一个角色。我们想要移除所有可能是该角色的洋娃娃。我们的选择在第二排。
- en: Suppose we’ve later noticed that some figurines are not having their eyes painted
    on properly. Giving a child a toy without eyes could be traumatic, so we definitely
    want to catch them all. As before, we’ll look at every toy, this time asking,
    “Are the eyes present?” If not, we throw the figurine into a bin for recycling.
    [Figure 3-18](#figure3-18) shows the idea.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们后来发现一些洋娃娃的眼睛没有画好。给孩子一个没有眼睛的玩具可能会造成创伤，因此我们绝对想要抓住所有这种情况。像之前一样，我们会检查每一个玩具，这次问的是：“眼睛在吗？”如果没有，就将洋娃娃丢进回收箱。[图3-18](#figure3-18)展示了这个思路。
- en: '![C03f018](Images/C03f018.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![C03f018](Images/C03f018.png)'
- en: 'Figure 3-18: A new group of toys. Now we’re looking for any with mispainted
    eyes. Our selections are in the bottom row.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-18：一组新的玩具。现在我们在寻找眼睛画错的玩具。我们的选择在最下面一排。
- en: 'Here we have a false negative: the doll has its eyes painted in, but we said
    it didn’t. In this situation, a few false negatives aren’t so bad. As long as
    we’re sure to remove every doll that is missing its eyes, it’s okay if we remove
    a few with their eyes present.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们遇到了一个假阴性：洋娃娃的眼睛已经画上了，但我们说它没有眼睛。在这种情况下，少数假阴性并不是什么大问题。只要我们确保移除所有没有眼睛的洋娃娃，即使移除了一些眼睛存在的娃娃也是可以接受的。
- en: To sum up, true positives and true negatives are the easy cases to understand.
    How we should respond to false positives and false negatives is dependent on our
    situation and our goals. It’s important to know what our question is, and what
    our policy is, so we can work out how we want to respond to these different types
    of errors.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，真正的正例和真正的负例是容易理解的情况。我们如何应对假阳性和假阴性，取决于我们的情况和目标。了解我们的问题是什么，政策是什么，这样我们才能确定如何应对这些不同类型的错误。
- en: Measuring Correct and Incorrect
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 衡量正确与错误
- en: Let’s return to our overview of true and false positives and negatives, as summarized
    in a confusion matrix. Looking at a confusion matrix can be, well, confusing,
    so people have created a variety of terms to help us talk about how well our classifier
    is performing.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到关于真阳性、假阳性和假阴性的概述，正如混淆矩阵中总结的那样。查看混淆矩阵可能会令人困惑，因此人们创造了各种术语来帮助我们讨论我们的分类器性能如何。
- en: We’ll illustrate these terms using a medical diagnosis scenario, where *positive*
    means someone has a particular condition, and *negative* means they’re healthy.
    Suppose that we’re public health workers who have come to a town that’s experiencing
    an outbreak of a terrible but completely imaginary disease called *morbus pollicus*
    *(MP)*. Anyone who has MP needs to have their thumbs surgically removed right
    away, or the disease will kill them within hours. It’s therefore critical that
    we correctly diagnose everyone with MP. But we definitely don’t want to make any
    incorrect diagnoses that lead to removing anyone’s thumbs if their life is notin
    danger—thumbs are important!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个医疗诊断场景来说明这些术语，其中*阳性*表示某人患有特定的疾病，*阴性*表示他们健康。假设我们是公共卫生工作者，来到了一个正经历着一场可怕但完全虚构的疾病叫做*拇指病*（*MP*）的爆发。任何患有MP的人都需要立刻接受拇指切除手术，否则疾病会在数小时内致命。因此，正确诊断所有患有MP的人至关重要。但我们绝对不希望做出错误诊断，导致切除任何人的拇指，特别是当他们的生命不在危险中时——拇指是很重要的！
- en: 'Let’s imagine that we have a laboratory test for detecting MP. The lab test
    is flawless, so it always gives us the correct answer: a positive diagnosis means
    the person has MP, and a negative diagnosis means they do not. Using this test,
    we’ve checked every person in town, and we now know whether or not they have MP.
    But our lab test is slow, and expensive. We’re worried about future outbreaks,
    so based on what we’ve just learned, we develop a fast, cheap, and portable field
    test that will predict immediately if someone does or does not have MP.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一种用于检测MP的实验室测试。这个实验室测试是完美无缺的，因此它总是能给出正确的答案：阳性诊断意味着此人有MP，阴性诊断意味着没有。使用这个测试，我们已经检查了城镇中的每个人，现在知道他们是否患有MP。但我们的实验室测试速度慢且费用高昂。我们担心未来的疫情，因此根据我们刚刚学到的情况，我们开发了一种快速、便宜且便于携带的现场测试，它可以立即预测某人是否患有MP。
- en: Unfortunately, our field test is not perfectly reliable, and sometimes makes
    incorrect diagnoses. Although we know our field test is flawed, when we’re in
    the middle of an outbreak it may be the only tool we have. So we want to characterize
    how often the field test is correct and how often it’s wrong, and when it’s wrong,
    we want to characterize the ways it’s wrong.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们的现场测试并不完全可靠，有时会做出错误的诊断。尽管我们知道我们的现场测试有缺陷，但在爆发期间，它可能是我们唯一的工具。因此，我们希望了解现场测试的正确性以及错误的频率，并且在出现错误时，我们希望了解错误的具体方式。
- en: 'To work this out, we need data. We’ve just heard of another town where a few
    people have reported MP. We’ll check every person in town with both tests: our
    perfect (but slow and expensive) lab test, and our imperfect (but quick and cheap)
    field test. In other words, the lab test gives us the ground truth for each person,
    and the field test gives us a prediction. The lab test is too expensive to always
    run both tests on every person, but we can afford it this once.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这个，我们需要数据。我们刚刚听说另一个城镇有几个人报告了MP。我们将用两种测试检查镇上的每个人：我们的完美（但慢且昂贵）实验室测试，和我们不完美（但快速且便宜）现场测试。换句话说，实验室测试为每个人提供了真实的结果，而现场测试则提供了预测。实验室测试太贵，不能总是对每个人都进行这两项测试，但我们这一次可以负担得起。
- en: 'By comparing the field test predictions with the lab test label, we’ll know
    all four quadrants of the confusion matrix for our field test:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将现场测试的预测与实验室测试的标签进行比较，我们将知道现场测试的混淆矩阵的四个象限：
- en: '**True Positive:** the person has MP, and our field test correctly says that
    they have it.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**真阳性**：此人有MP，且我们的现场测试正确地诊断出他们有。'
- en: '**True Negative:** the person does *not* have MP, and our field test agrees.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**真阴性**：此人没有MP，且我们的现场测试一致。'
- en: '**False Positive:** the person does *not* have MP, but our field test says
    that they do.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**假阳性**：此人没有MP，但我们的现场测试说他们有。'
- en: '**False Negative:** the person has MP, but our field test says they don’t.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**假阴性**：此人有MP，但我们的现场测试说他们没有。'
- en: Both true positive and true negative are correct answers, while false negative
    and false positive are incorrect. A false positive means we’d operate without
    cause, and a false negative would leave someone at risk of dying.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 真阳性和真阴性都是正确答案，而假阴性和假阳性是错误的。假阳性意味着我们会在没有理由的情况下操作，而假阴性则会让某人面临死亡风险。
- en: If we build a confusion matrix for our field test by attaching numbers to each
    of the four cells, we can use those values to determine how well our field test
    is performing. We will be able to characterize its performance with a few well-known
    statistics. The *accuracy* will tell us how often the field test gives us a correct
    answer, the *precision* will tell us something about false positives, and the
    *recall* will tell us about false negatives. These values are the standard way
    that people talk about the quality of a test like this, so let’s look at those
    values now. Then we’ll come back to our confusion matrix for the field test, compute
    these values, and see how they help us interpret the test’s predictions.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过为混淆矩阵的四个单元格附加数字来构建我们的实地测试，我们可以利用这些值来判断我们的实地测试表现如何。我们将能够通过一些常见的统计数据来描述其表现。*准确率*将告诉我们实地测试给出正确答案的频率，*精度*将告诉我们关于假阳性的一些信息，而*召回率*将告诉我们关于假阴性的一些信息。这些值是人们讨论类似测试质量的标准方式，所以让我们现在来看一下这些值。然后我们将回到实地测试的混淆矩阵，计算这些值，并看看它们如何帮助我们解读测试的预测。
- en: Accuracy
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确率
- en: 'Each of the terms we’ll discuss in this section is built from the four values
    in the confusion matrix. To make things a bit easier to discuss, we’ll use the
    common abbreviations: TP for true positive, FP for false positive, TN for true
    negative, and FN for false negative.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一节中讨论的每个术语都来源于混淆矩阵中的四个值。为了更方便讨论，我们将使用常见的缩写：TP表示真阳性，FP表示假阳性，TN表示真阴性，FN表示假阴性。
- en: Our first term to characterize the quality of a classifier is *accuracy*. The
    accuracy of the predictions made for any collection of samples is a number from
    0 to 1\. It’s a measure of the percentage of samples that were assigned to the
    correct category. So it’s just the sum of the two “correct” values, TP and TN,
    divided by the total number of samples measured. [Figure 3-19](#figure3-19) shows
    the idea graphically. In this figure, as in the ones to come, the samples we’re
    counting for any given computation will be shown, and the samples that don’t contribute
    to that value will be omitted.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来描述分类器质量的第一个术语是*准确率*。对于任何样本集合，预测的准确率是一个从0到1的数值。它是正确分类的样本所占的百分比。因此，它就是两个“正确”值（真阳性TP和真阴性TN）的总和，除以测量的样本总数。[图3-19](#figure3-19)以图形方式展示了这个概念。在这个图中，和接下来的图一样，我们将展示用于任何给定计算的样本，且那些不参与该值计算的样本会被省略。
- en: We want the accuracy to be 1.0, but usually it will be less than that. In [Figure
    3-19](#figure3-19), we have an accuracy of 0.7, or 70 percent, which isn’t great.
    The accuracy doesn’t tell us in what way the predictions are wrong, but it does
    give us a broad feeling for how much of the time we get the right result. Accuracy
    is a rough measurement.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望准确率是1.0，但通常它会小于1.0。在[图3-19](#figure3-19)中，我们的准确率是0.7，或者说是70%，这并不是很高。准确率并不能告诉我们预测错误的方式，但它确实给了我们一个关于正确结果的广泛感知。准确率是一个粗略的度量。
- en: Let’s now look at two other measures that provide more specific characterizations
    of our predictions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看两个其他的衡量标准，它们可以更具体地描述我们的预测。
- en: '![C03f019](Images/C03f019.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![C03f019](Images/C03f019.png)'
- en: 'Figure 3-19: Accuracy is a number from 0 to 1 that tells us how often our prediction
    is correct.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-19：准确率是一个从0到1的数值，告诉我们我们的预测有多准确。
- en: Precision
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精度
- en: '*Precision*(also called *positive predictive value*) tells us the percentage
    of our samples that were properly labeled positive, relative to all the samples
    we labeled as positive. Numerically, it’s the value of TP relative to TP + FP.
    In other words, precision tells us what percentage of our positive predictions
    were correct.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*精度*（也叫做*正预测值*）告诉我们，在我们标记为正类的所有样本中，实际上正确标记为正类的样本所占的百分比。从数值上讲，它是TP与TP + FP的比值。换句话说，精度告诉我们在所有正类预测中，有多少是正确的。'
- en: If the precision is 1.0, then every sample that really is positive was correctly
    predicted as positive. As the percentage falls, it carries with it our confidence
    in these predictions. For example, if the precision is 0.8, then we can only be
    80 percent sure that any given sample that’s labeled positive has the correct
    label. [Figure 3-20](#figure3-20) shows the idea visually.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果精确度为 1.0，那么每一个真实为正的样本都被正确预测为正。当精确度下降时，它也意味着我们对这些预测的信心降低。例如，如果精确度为 0.8，那么我们只能有
    80% 的信心，认为任何标记为正的样本是正确的。[图 3-20](#figure3-20) 以图形化的方式展示了这一概念。
- en: '![C03f020](Images/C03f020.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![C03f020](Images/C03f020.png)'
- en: 'Figure 3-20: The value of precision is the total number of positive samples
    that really are positive, divided by the total number of samples that we labeled
    as positive.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-20：精确度的值是实际为正的正样本的总数，除以我们标记为正的样本总数。
- en: 'When the precision is less than 1.0, it means we labeled some samples as positive
    when we shouldn’t have. In our healthcare example from before with our imaginary
    disease, a precision value of less than 1.0 means that we’d perform some unnecessary
    operations. An important quality of precision is that it doesn’t tell us if we
    actually found all the positive objects: that is, all the people who had MP. Precision
    ignores all samples except those labeled as positive.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当精确度小于 1.0 时，意味着我们错误地将一些样本标记为正。以之前的医疗案例为例，在我们假设的疾病中，精确度小于 1.0 意味着我们会进行一些不必要的手术。精确度的一个重要特性是，它并不告诉我们是否找到了所有的正样本，也就是说，它不关心那些真实为正的样本是否被完全找出。精确度只关注被标记为正的样本。
- en: Recall
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 召回率
- en: Our third measure is *recall* (also called *sensitivity*, *hit rate*, or *true
    positive rate*). This tells us the percentage of the samples we correctly predicted
    to be positive, relative to all the samples that really were positive. That is,
    it tells us the percentage of positive samples that we correctly predicted.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第三个衡量指标是*召回率*（也叫*敏感性*、*命中率*或*真正率*）。它告诉我们正确预测为正的样本占所有真实为正样本的百分比。也就是说，它告诉我们我们正确预测的正样本所占的百分比。
- en: When recall is 1.0, then we correctly predicted every positive event. The more
    that recall drops below that number, the more positive events we missed. [Figure
    3-21](#figure3-21) shows this idea visually.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当召回率为 1.0 时，意味着我们正确预测了每个正事件。召回率越低，错过的正事件就越多。[图 3-21](#figure3-21) 以图形化的方式展示了这一概念。
- en: '![C03f021](Images/C03f021.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![C03f021](Images/C03f021.png)'
- en: 'Figure 3-21: The value of recall is the total number of correctly labeled positive
    samples, divided by the total number of samples that should have been labeled
    as positive.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-21：召回率的值是正确标记为正的样本的总数，除以应该标记为正的样本总数。
- en: When recall is less than 1.0, it means that we missed some positive answers.
    In our healthcare example, it means we would misdiagnose some people with MP as
    not having the disease. The result is that we wouldn’t operate on those people,
    even though they’re infected and in danger.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当召回率小于 1.0 时，意味着我们错过了一些正答案。在我们的医疗案例中，这意味着我们会误诊一些患有 MP 的人没有得病。结果是我们不会对这些人进行手术，即使他们已经感染且处于危险之中。
- en: Precision-Recall Tradeoff
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精确度-召回率权衡
- en: 'When we’re categorizing data into two classes, and we can’t eliminate false
    positives and false negatives, there’s a tradeoff between precision and recall:
    as one goes up, the other goes down. That’s because as we reduce the number of
    false positives (and therefore increase precision), we necessarily also increase
    the number of false negatives (and therefore reduce recall), and vice versa. Let’s
    see how this comes about.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将数据分类为两类，且无法消除假阳性和假阴性时，精确度与召回率之间存在权衡：当一个增加时，另一个就会下降。这是因为，当我们减少假阳性的数量（因此增加精确度）时，必然会增加假阴性的数量（从而降低召回率），反之亦然。让我们看看这是如何发生的。
- en: '[Figure 3-22](#figure3-22) shows 20 pieces of data. They start out as negatives
    (red squares) at the far left, and gradually become positives (green circles)
    as we move right. We’ll draw a boundary line vertically somewhere, predicting
    everything to its left as negative, and everything to its right as positive. We
    want all the red squares to be predicted as negative, and all the green circles
    to be positive. Because they’re mixed up, there’s no boundary that separates the
    two groups perfectly.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-22](#figure3-22)显示了20个数据点。它们最初位于最左侧时是负样本（红色方块），随着我们向右移动，逐渐变为正样本（绿色圆圈）。我们会在某个位置垂直画一条边界线，预测它左侧的所有数据为负，右侧的所有数据为正。我们希望所有的红色方块都被预测为负，所有的绿色圆圈都被预测为正。由于它们是混合在一起的，因此没有一条边界线能够完美地分开这两组数据。'
- en: '![C03f022](Images/C03f022.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![C03f022](Images/C03f022.png)'
- en: 'Figure 3-22: As we move the boundary line to the right, from top to bottom,
    we decrease the number of false positives (red squares with a heavy border), but
    increase the number of false negatives (green circles with a heavy border).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-22：当我们将边界线向右移动，从上到下时，我们减少了假阳性（带有粗边框的红色方块）的数量，但增加了假阴性（带有粗边框的绿色圆圈）的数量。
- en: In the top row of [Figure 3-22](#figure3-22), the boundary is near the left
    end. All the green circles are correctly marked positive, but many of the red
    squares are false positives (shown with a thick outline). As we move the boundary
    to the right in lower rows, we reduce the number of false positives, but we increase
    the number of false negatives, because now we’re predicting more green circles
    to be negative.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 3-22](#figure3-22)的顶行中，边界线靠近最左端。所有的绿色圆圈都被正确标记为正样本，但许多红色方块是假阳性（用粗线条显示）。随着我们将边界线向右移动到下方的行，我们减少了假阳性的数量，但增加了假阴性的数量，因为现在我们预测更多的绿色圆圈为负样本。
- en: Let’s increase the dataset size to 5,000 elements. The data will be like [Figure
    3-22](#figure3-22), so each entry will be positive with a probability given by
    its distance from the left end. The leftmost graph of [Figure 3-23](#figure3-23)
    shows the number of true positives and true negatives as we move the decision
    boundary from the far left to the far right. The middle graph shows the number
    of false positives and false negatives, and the rightmost graph shows the resulting
    accuracy.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将数据集的大小增加到5000个元素。数据将类似于[图 3-22](#figure3-22)，因此每个数据项的正样本概率由其距离最左侧的距离决定。[图
    3-23](#figure3-23)最左侧的图显示了我们将决策边界从最左端移动到最右端时，真实正例和真实负例的数量。中间图显示了假阳性和假阴性的数量，最右侧的图则显示了最终的准确率。
- en: '![C03f023](Images/C03f023.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![C03f023](Images/C03f023.png)'
- en: 'Figure 3-23: Left: The number of true positives and true negatives as we move
    the boundary. Middle: The number of false positives and false negatives. Right:
    The accuracy.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-23：左图：我们移动边界时，真实正例和真实负例的数量。中图：假阳性和假阴性的数量。右图：准确率。
- en: To find precision and recall, we’ll gather together TP and FP in the left graph
    of [Figure 3-24](#figure3-24), and TP and FN in the middle. At the right, we show
    the result of combining these pairs with TP, following the earlier definitions
    to compute the precision and recall for each position of the boundary.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算精度和召回率，我们会将[图 3-24](#figure3-24)左侧图中的TP和FP，以及中间图中的TP和FN收集在一起。右侧图显示了将这些对与TP结合后的结果，并根据之前的定义计算每个边界位置的精度和召回率。
- en: '![C03f024](Images/C03f024.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![C03f024](Images/C03f024.png)'
- en: 'Figure 3-24: TP and FP, TP and FN, and precision and recall as we move the
    boundary from far left to right'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-24：随着边界从最左端向最右端移动，TP 和 FP，TP 和 FN，以及精度和召回率。
- en: Notice that as we increase precision, we decrease recall, and vice versa. That’s
    the precision-recall tradeoff.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，随着精度的提高，我们的召回率会降低，反之亦然。这就是精度-召回率的权衡。
- en: In this example, the precision follows a straight line, whereas the recall is
    a curve. To get a feeling for why, consider that the sum TP + FP of the curves
    shown at the left of [Figure 3-24](#figure3-24) would be a diagonal line from
    northwest to southeast, whereas the sum TP + FN of the curves in the middle of
    the figure would be a horizontal line. Dividing the TP curve by these two differently
    oriented lines gives us the different shapes of the precision and recall curves.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，精度呈直线变化，而召回率则呈曲线变化。为了理解其中的原因，考虑[图 3-24](#figure3-24)左侧曲线的TP+FP总和将是从左上到右下的对角线，而中间部分的TP+FN总和将是一个水平线。将TP曲线除以这两条方向不同的线，便得到了精度和召回率曲线的不同形状。
- en: 'For other kinds of datasets, all of these curves would look different, but
    the precision-recall tradeoff would remain: the better the precision, the worse
    the recall, and vice versa.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他类型的数据集，这些曲线看起来会不同，但精度和召回率之间的权衡将保持不变：精度越好，召回率越差，反之亦然。
- en: Misleading Measures
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 误导性度量
- en: Accuracy is a common measure, but in machine learning precision and recall appear
    more frequently because they’re useful for characterizing the performance of a
    classifier and comparing it against others. But both precision and recall can
    be misleading if taken all by themselves, because extreme conditions can give
    us a great value for either measure, whereas overall performance is lousy.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是一个常见的度量标准，但在机器学习中，精度和召回率出现得更多，因为它们有助于表征分类器的性能并与其他分类器进行比较。但是，如果单独考虑精度和召回率，它们可能会具有误导性，因为极端条件可能会给我们带来这两个度量的高值，而总体性能却很差。
- en: These misleading results can come from many sources. Perhaps the most common,
    and difficult to catch, is when we’re not careful enough about what we ask the
    computer to do for us. For example, our organization might want us to produce
    a classifier that delivers extremely high precision or recall. That may sound
    desirable, but let’s see why it could be a mistake.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这些误导性的结果可能来自许多来源。也许最常见且最难发现的是，当我们对计算机应该为我们做什么不够小心时。例如，我们的组织可能希望我们创建一个具有极高精度或召回率的分类器。听起来可能很理想，但让我们来看看为什么这可能是一个错误。
- en: To see the problem, consider what might happen if we ask for one of the two
    extremes of *perfect precision*and *perfect recall*. We’ll invent lousy boundary
    curves to demonstrate the issues, but keep in mind that these can come out naturally
    from an algorithm tasked to produce perfect precision or recall.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要看清问题，可以考虑如果我们要求达到*完美精度*和*完美召回率*这两个极端，可能会发生什么。我们将设计糟糕的边界曲线来展示这些问题，但请记住，这些问题可能自然地出现在一个被要求达到完美精度或召回率的算法中。
- en: One way to create a boundary curve with perfect precision is to look through
    all of the samples and find the one we are most certain is really true. Then we
    draw the curve so that the point we selected is the only positive sample, and
    everything else is negative. [Figure 3-25](#figure3-25) shows the idea.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一条具有完美精度的边界曲线的一种方法是，查看所有样本，找出我们最确定的正例。然后我们画出这条曲线，使得我们选择的点是唯一的正样本，其他所有的都是负样本。[图
    3-25](#figure3-25)展示了这个想法。
- en: '![C03f025](Images/C03f025.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![C03f025](Images/C03f025.png)'
- en: 'Figure 3-25: Left: This boundary curve gives us a perfect score for precision.
    Right: A schematic version of the figure on the left.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-25：左图：这条边界曲线给了我们完美的精度得分。右图：左图的示意版本。
- en: How does this give us perfect precision? Remember that precision is the number
    of true positives (here only 1) divided by the total number of points labeled
    positive (again, just 1). So we get the fraction 1/1, or 1, which is a perfect
    score. But the accuracy and recall are both pretty awful because we’ve also created
    lots of false negatives, as shown in [Figure 3-26](#figure3-26).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这如何给我们完美的精度呢？记住，精度是正确正例的数量（这里只有 1 个）除以所有被标记为正例的点的总数（同样，只有 1 个）。所以我们得到的比值是 1/1，或者
    1，这是一个完美的分数。但准确率和召回率都非常糟糕，因为我们也创建了大量的假负例，正如[图 3-26](#figure3-26)所示。
- en: '![C03f026](Images/C03f026.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![C03f026](Images/C03f026.png)'
- en: 'Figure 3-26: These figures all share the same boundary curve, which has labeled
    exactly one green circle as positive, and all the others as negative.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-26：这些图形都共享相同的边界曲线，该曲线将一个绿色圆圈标记为正例，其他所有的都标记为负例。
- en: Let’s do a similar trick with recall. To create a boundary curve with perfect
    recall is even easier. All we have to do is label everything as positive. [Figure
    3-27](#figure3-27) shows the idea.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用召回率做一个类似的操作。创建一条具有完美召回率的边界曲线甚至更容易。我们所要做的就是将所有样本都标记为正例。[图 3-27](#figure3-27)展示了这个想法。
- en: '![C03f027](Images/C03f027.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![C03f027](Images/C03f027.png)'
- en: 'Figure 3-27: Left: This boundary curve gives us a perfect recall score. Right:
    A schematic version of the figure on the left.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-27：左图：这条边界曲线给了我们完美的召回率得分。右图：左图的示意版本。
- en: We get perfect recall from this because recall is the number of correctly labeled
    true points (here, all 10 of them) divided by the total number of true points
    (again, 10). So 10/10 is 1, or a perfect score for recall. But of course, accuracy
    and precision are both poor, because every negative sample is now a false positive,
    as shown in [Figure 3-28](#figure3-28).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从中得到了完美的召回率，因为召回率是正确标记的真实点的数量（这里是所有 10 个）除以真实点的总数（同样是 10）。所以 10/10 等于 1，或者召回率的完美分数。但当然，准确度和精确度都很差，因为每个负样本现在都是假阳性，正如
    [图 3-28](#figure3-28) 所示。
- en: '![C03f028](Images/C03f028.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![C03f028](Images/C03f028.png)'
- en: 'Figure 3-28: All of these figures share the same boundary curve. With this
    curve, every point is predicted to be positive. We get a perfect recall, because
    every positive point is correctly labeled. Unfortunately, accuracy and precision
    both have very low scores.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-28：所有这些图形共享相同的边界曲线。通过这条曲线，所有点都被预测为正类。我们获得了完美的召回率，因为每个正类点都被正确标记。不幸的是，准确度和精确度都得到了很低的分数。
- en: The moral of Figures 3-26 and 3-28 is that asking for perfect precision or perfect
    recall is unlikely to give us what we really want, which is perfect correctness.
    We want accuracy, precision, and recall to all be near 1, but if we’re not careful,
    we can get a great score for just one of these measures by picking an extreme
    solution that performs poorly when we look at the results in just about any other
    way.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-26 和 3-28 的教训是，要求完美的精确度或完美的召回率不太可能带来我们真正想要的结果，即完美的正确性。我们希望准确度、精确度和召回率都接近
    1，但如果不小心，我们可能会通过选择一个极端的解决方案来获得其中一个度量的高分，而这种解决方案在任何其他方式下查看结果时表现较差。
- en: f1 Score
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: f1 分数
- en: Looking at both precision and recall is informative, but they can be combined
    with a bit of mathematics into a single measure called the *f1 score*. This is
    a special type of “average” called a *harmonic mean*. It lets us look at a single
    number that combines both precision and recall (the formula appears later on in
    the last lines of [Figure 3-30](#figure3-30) and [Figure 3-32](#figure3-32)).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 同时考虑精确度和召回率是有意义的，但它们可以通过一些数学方法结合成一个单一的度量，称为 *f1 分数*。这是一种特殊类型的“平均数”，叫做 *调和均值*。它使我们能够查看一个结合了精确度和召回率的单一数字（公式稍后出现在
    [图 3-30](#figure3-30) 和 [图 3-32](#figure3-32) 的最后几行）。
- en: '[Figure 3-29](#figure3-29) shows the f1 score visually.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-29](#figure3-29) 直观地展示了 f1 分数。'
- en: Generally speaking, the f1 score will be low when either precision or recall
    is low and will approach 1 when both measures also approach 1\.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，当精确度或召回率较低时，f1 分数会较低，当两者都接近 1 时，f1 分数也会接近 1。
- en: '![C03f029](Images/C03f029.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![C03f029](Images/C03f029.png)'
- en: 'Figure 3-29: The f1 score is 0 when either precision or recall is also 0, and
    1 when both are 1\. In between it slowly rises as both measures increase.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-29：当精确度或召回率为 0 时，f1 分数为 0，当两者都为 1 时，f1 分数为 1。在两者之间，随着精确度和召回率的增加，f1 分数逐渐上升。
- en: When a system is working well, sometimes people just cite the f1 score as a
    shorthand way of showing that both precision and recall are high.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个系统表现良好时，有时人们会引用 f1 分数，作为展示精确度和召回率都很高的一种简便方式。
- en: About These Terms
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于这些术语
- en: The terms *accuracy*, *precision*, and *recall* may not seem obviously connected
    to what they measure. Let’s make those connections, which can help us remember
    what these terms mean.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*准确度*、*精确度* 和 *召回率* 这些术语可能看起来与它们所衡量的内容并不明显相关。让我们建立这些联系，这有助于我们记住这些术语的含义。'
- en: '*Accuracy* tells us what percentage of the samples we predicted correctly.
    If we predicted every label perfectly, accuracy would be 1\. As the percentage
    of mistakes increases, accuracy drops toward 0\. To characterize our mistakes,
    we want to know our rate of false positives and false negatives. This is what
    precision and recall are for.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*准确度* 告诉我们我们正确预测的样本百分比。如果我们完全正确地预测了每个标签，准确度将为 1。当错误的百分比增加时，准确度会下降到接近 0。为了描述我们的错误，我们需要了解假阳性和假阴性的比率。这就是精确度和召回率的作用所在。'
- en: '*Precision* reveals our percentage of false positives, or how many samples
    we incorrectly predicted to be positive. So this measures the specificity, or
    precision, of our positive prediction. The larger the value of precision, the
    more confidence we have that a positive prediction is accurate. In terms of our
    medical example, if our test has high precision, then it’s likely that a positive
    diagnosis means that person really has MP. But precision doesn’t tell us how many
    infected people we improperly declared to be disease-free.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*精确度*揭示了我们假正例的百分比，或者说我们错误预测为正类的样本数量。所以这衡量了我们正类预测的特异性或精度。精确度的值越大，我们对正类预测准确性的信心就越强。在我们的医学例子中，如果我们的测试具有高精度，那么正向诊断很可能意味着那个人确实患有
    MP。但精确度并不能告诉我们有多少感染者被错误地诊断为健康。'
- en: '*Recall* reveals our percentage of false negatives. If we think of our system
    as finding, or recalling, just the positives from a set of data, this tells us
    how well we’ve done. The better our recall, the more confidence we have that we
    correctly retrieved all the positive samples. In our medical example, if our test
    has high recall, then we can feel confident that we’ve identified everyone with
    MP. But recall doesn’t tell us how many healthy people we incorrectly identified
    as having MP.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率*揭示了我们假负例的百分比。如果我们将我们的系统视为仅从一组数据中寻找或召回正类样本，那么它告诉我们我们做得如何。召回率越好，我们就越有信心自己正确地检索到了所有正类样本。在我们的医学例子中，如果我们的测试具有高召回率，那么我们可以确信自己已经识别出所有患有
    MP 的人。但召回率并不能告诉我们有多少健康人被错误地识别为患有 MP。'
- en: Other Measures
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他度量
- en: We’ve seen the measures of accuracy, recall, precision, and f1\. There are lots
    of other terms that are sometimes used in discussions of probability and machine
    learning (Wikipedia 2020). We won’t encounter most of these terms in this book,
    but we’ll summarize them here to provide a one-stop reference that gathers all
    the definitions in one place.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了准确率、召回率、精确度和 f1 的度量。在概率和机器学习的讨论中，有很多其他术语有时也会被使用（维基百科 2020）。虽然我们在本书中不会遇到这些术语中的大部分，但我们在这里总结它们，提供一个方便的参考，集中了所有定义。
- en: '[Figure 3-30](#figure3-30) provides this summary. Don’t bother memorizing any
    unfamiliar terms and their meanings. The purpose of this table is to offer a convenient
    place to look these things up when needed.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-30](#figure3-30)提供了这个总结。不要费心去记忆任何不熟悉的术语及其含义。这个表格的目的是提供一个方便的地方，在需要时查找这些内容。'
- en: '![C03f030](Images/C03f030.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![C03f030](Images/C03f030.png)'
- en: 'Figure 3-30: Common confidence terms derived from the confusion matrix'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-30：来自混淆矩阵的常见置信度术语
- en: This table is a lot to take in. We provide an alternative that presents the
    terms graphically, using our distribution of samples from [Figure 3-14](#figure3-14),
    repeated here as [Figure 3-31](#figure3-31).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格的信息量很大。我们提供了一个替代方案，通过图示展示这些术语，使用来自[图 3-14](#figure3-14)的样本分布，这里重复呈现为[图 3-31](#figure3-31)。
- en: '![C03f031](Images/C03f031.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![C03f031](Images/C03f031.png)'
- en: 'Figure 3-31: The data from [Figure 3-14](#figure3-14) labeled with the four
    classes of True Positive, False Positive, False Negative, and True Negative'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-31：[图 3-14](#figure3-14)中的数据，标记了四个类别：真正例、假正例、假负例和真负例
- en: Reading from top to bottom, we have six positive points correctly labeled (TP
    = 6), two negative points incorrectly labeled (FP = 2), four positive points incorrectly
    labeled (FN = 4), and eight negative points correctly labeled (TN = 8).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 从上到下阅读，我们有六个正类点被正确标记（TP = 6），两个负类点被错误标记（FP = 2），四个正类点被错误标记（FN = 4），以及八个负类点被正确标记（TN
    = 8）。
- en: With these points, we can illustrate the measures of [Figure 3-30](#figure3-30)
    by combining these four numbers, or their pictures, in different ways. [Figure
    3-32](#figure3-32) shows how we’d compute the measures using just the relevant
    pieces of the data.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些要点，我们可以通过将这四个数字或它们的图示以不同的方式组合起来，来说明[图 3-30](#figure3-30)中的度量。[图 3-32](#figure3-32)展示了我们如何仅使用相关数据片段来计算这些度量。
- en: Constructing a Confusion Matrix Correctly
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正确构造混淆矩阵
- en: Understanding a test (or classifier) from its statistical measures can be difficult.
    There’s a lot to take in, and keeping everything straight and organized can be
    a challenge. It’s important to rise to this challenge, because most real-world
    tests (in every field) are imperfect, as are most machine-learning systems. In
    general, they need to be understood in terms of their statistical performances.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 从统计度量的角度理解一个测试（或分类器）可能会很困难。信息量大，要保持条理清晰并组织好可能是一个挑战。接受这一挑战很重要，因为大多数现实世界的测试（各个领域的测试）都是不完美的，许多机器学习系统也是如此。一般来说，它们需要从统计性能的角度来理解。
- en: The confusion matrix is a simple but powerful way to simplify and summarize
    our understanding. But we have to build and interpret it carefully, or we can
    too easily come to the wrong conclusions. To wrap up this chapter, let’s look
    more closely at how to properly build and interpret a confusion matrix.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是一个简单但强大的工具，能够简化并总结我们的理解。但我们必须小心构建和解释它，否则很容易得出错误结论。为了总结本章内容，让我们更深入地看看如何正确构建和解释混淆矩阵。
- en: The plan will be to return to our imaginary disease of MP, attach some numbers
    to our confusion matrix, and ask some questions about the quality of our fast,
    but inaccurate, field test. Recall that we earlier said that we’d measure everyone
    in a town with our slow and expensive, but perfectly accurate, lab test (giving
    us the *ground truth*), as well as our faster, cheaper, and imperfect field test
    (giving us *predictions*).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计划是回到假设的麻疹疫情，给我们的混淆矩阵附上一些数字，并提出一些关于我们快速但不准确的实地测试质量的问题。回想一下，我们之前说过，我们会用慢速且昂贵，但完全准确的实验室测试来测量一个城镇的每个人（这将给我们提供*真相*），同时也用更快、更便宜且不完美的实地测试（这将给我们提供*预测*）。
- en: '![C03f032](Images/C03f032.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![C03f032](Images/C03f032.png)'
- en: 'Figure 3-32: Our statistical measures of [Figure 3-29](#figure3-29) in visual
    form using the data of [Figure 3-30](#figure3-30)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-32：我们使用[图 3-29](#figure3-29)的数据，将统计测量以可视化形式展示，并结合[图 3-30](#figure3-30)的数据。
- en: 'Let’s suppose that these measurements show that the field test has a high true
    positive rate: we found that 99 percent of the time, someone with MP is correctly
    diagnosed. Since the TP rate is 0.99, the false negative (FN) rate, which contains
    all of the people with MP who we did *not* correctly diagnose, is 1 – 0.99 = 0.01\.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这些测量结果显示，实地测试具有较高的真实阳性率：我们发现，99% 的情况下，麻疹患者被正确诊断。由于真实阳性率（TP）为 0.99，假阴性率（FN），即所有我们没有正确诊断的麻疹患者，便是
    1 – 0.99 = 0.01。
- en: The test does a bit worse for people who *don’t* have MP. We’ll suppose that
    the true negative (TN) rate is 0.98, so 98 times out of 100 when we predict that
    someone is not infected, they really aren’t. But this means that the false positive
    (FP) rate is 1 – 0.98 = 0.02, so 2 people in 100 who don’t have MP will get an
    incorrect positive diagnosis.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些*没有*感染麻疹的人，测试表现稍差。我们假设真实的阴性率（TN）为 0.98，即在 100 次预测没有感染的情况下，有 98 次预测是准确的。但这意味着假阳性（FP）率为
    1 – 0.98 = 0.02，因此每 100 个没有麻疹的人中，会有 2 个被错误诊断为阳性。
- en: Let’s suppose that we’ve just heard of an outbreak of MP in a new town of 10,000
    people. From experience, given the amount of time that has passed, we expect that
    1 percent of the population is already infected*. This is essential information*.
    We’re not testing people blindly. We *already know* that there’s only a 1 in 100
    chance that someone has MP. It will be essential for us to include this information
    to correctly understand the results from our field test.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们刚刚听说一个新城镇（人口 10,000）的麻疹疫情爆发。从经验来看，考虑到已经过去的时间，我们预计有 1% 的人群已经被感染*。这是关键信息*。我们并不是盲目地测试人群，我们*已经知道*，一个人感染麻疹的几率只有
    1/100。为了正确理解我们的实地测试结果，包含这些信息至关重要。
- en: So we pack up our gear and head into town at top speed.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们收拾好装备，以最快的速度赶往市区。
- en: There’s no time to send our results to the big and slow lab, so we get everyone
    to come down to city hall to get tested with our field test. Suppose someone comes
    up positive. What should they do? How likely is it that they have MP? Suppose
    instead the test is negative. What should those people do? How likely is it that
    they *don’t* have it?
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 没有时间将我们的结果送往大型而缓慢的实验室，所以我们让大家来市政厅参加我们的实地测试。假设某人测试结果为阳性。他们应该怎么办？他们感染麻疹的可能性有多大？假设测试结果为阴性。那么这些人应该怎么办？他们*没有*感染麻疹的可能性有多大？
- en: We can answer these questions by building a confusion matrix. If we jump into
    it, we might build a confusion matrix just by popping the values above into their
    corresponding boxes, as in [Figure 3-33](#figure3-33). But this is *not* the way
    to go! This matrix is incompleteand will lead us to the wrong answersto our questions.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过构建混淆矩阵来回答这些问题。如果我们直接进入，就可能通过将上面的值填入相应的框中来构建混淆矩阵，就像在 [图 3-33](#figure3-33)
    中那样。但这*不是*正确的方法！这个矩阵是不完整的，将导致我们得出错误的答案。
- en: '![C03f033](Images/C03f033.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![C03f033](Images/C03f033.png)'
- en: 'Figure 3-33: This is not the confusion matrix we’re looking for. Left: The
    matrix using our measured values. Right: Multiplying each value by 100 to show
    them as percentages.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-33：这不是我们想要的混淆矩阵。左：使用我们测得的值的矩阵。右：将每个值乘以 100，显示为百分比。
- en: 'The problem is that we’re ignoring a critical piece of information: only 1
    percent of the people in town will have MP right now. The chart in [Figure 3-33](#figure3-33)
    doesn’t include that knowledge and therefore isn’t telling us what we need to
    know.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于我们忽略了一个关键的信息：现在只有 1% 的镇上居民会感染 MP。图表 [图 3-33](#figure3-33) 没有包括这一信息，因此它没有告诉我们我们需要知道的内容。
- en: In [Figure 3-34](#figure3-34), we work out the proper matrix by considering
    the 10,000 people in town and analyzing what we expect from the test by using
    our knowledge of the infection rate and the test’s measured performance.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 3-34](#figure3-34) 中，我们通过考虑镇上 10,000 人的情况，并利用我们对感染率和测试性能的了解，来计算出正确的矩阵。
- en: '![C03f034](Images/C03f034.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![C03f034](Images/C03f034.png)'
- en: 'Figure 3-34: Working out the populations we expect from our infection rate
    and our test'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-34：根据我们的感染率和测试预期的群体分布
- en: '[Figure 3-34](#figure3-34) forms the heart of the correct process, so let’s
    walk through it. We start at the left with 10,000 people in town. Our essential
    starting information is that we already know from prior experience that 1 person
    out of 100, or 1 percent of the population, will be infected with MP. That’s shown
    in the upper path, where we show the 1 percent of 10,000, or 100, people who have
    MP. Our test will correctly come up positive for 99 of them, and negative for
    only 1\. Returning to our starting population, on the lower path we follow the
    99 percent, or 9,900, people who are notinfected. Our test will correctly identify
    98 percent of them, or 9,702 people, as being negative. The remaining 2 percent
    of those 9,900, or 198 people, will get an incorrect positive result.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-34](#figure3-34) 是正确过程的核心部分，让我们一起走一遍。我们从镇上的 10,000 人开始。我们最基本的起始信息是，根据以前的经验，我们已经知道
    100 个人中就有 1 个人，或者说 1% 的人口会感染 MP。这个信息显示在上方路径中，我们显示了 10,000 中的 1%，即 100 人感染了 MP。我们的测试将正确地显示其中
    99 个人为阳性，仅有 1 个人为阴性。回到我们最初的总人口，沿下方路径我们跟踪那 99% 的人，即 9,900 人，他们没有感染。我们的测试将正确地识别
    98% 的人，即 9,702 个人为阴性。剩下的 2%（198 人）将获得错误的阳性结果。'
- en: '[Figure 3-34](#figure3-34) tells us the values that we *should* use to populate
    our confusion matrix, because they incorporate our knowledge of the 1 percent
    infection rate. From our 10,000 tests, we’ll expect (on average) 99 true positives,
    1 false negative, 9,702 true negatives, and 198 false positives. These values
    give us the proper confusion matrix in [Figure 3-35](#figure3-35).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-34](#figure3-34) 告诉我们应当使用的值来填充混淆矩阵，因为它们融入了我们对 1% 感染率的知识。从我们的 10,000 次测试中，我们预期（平均而言）会有
    99 个真正阳性、1 个假阴性、9,702 个真正阴性和 198 个假阳性。这些值给我们提供了 [图 3-35](#figure3-35) 中的正确混淆矩阵。'
- en: '![c03f035](Images/c03f035.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![c03f035](Images/c03f035.png)'
- en: 'Figure 3-35: The proper confusion matrix for our MP test, incorporating our
    knowledge of the 1 percent infection rate'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-35：我们的 MP 测试的正确混淆矩阵，结合了我们对 1% 感染率的知识
- en: 'Compared to [Figure 3-33](#figure3-33), the TN rate has changed by a lot! Instead
    of a TN value of 98, we have 9,702\. The value for FP has also gone through a
    huge change, from 2 to 198\. This is important: 198 healthy people are going to
    get back results saying that they’re infected.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与 [图 3-33](#figure3-33) 相比，TN 率变化非常大！TN 值从 98 变为 9,702\. 假阳性（FP）值也发生了巨大的变化，从
    2 变为 198\. 这是很重要的：198 名健康人将收到显示他们感染的结果。
- en: Now that we have the right matrix, we’re ready to answer our questions. Suppose
    someone gets a positive test result. What’s the chance that they really do have
    MP? In statistical terms, what’s the conditional probability that someone has
    MP, given that the test says they do? More simply, what percentage of the positive
    results we get back are true positives? That’s just what precision measures. In
    this case, the precision is 99 / (99 + 198), or 0.33, or 33 percent.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了正确的矩阵，我们准备回答我们的问题。假设某人得到了阳性测试结果。那么他们真正患有 MP 的概率是多少？用统计学的术语来说，就是给定测试显示为阳性，某人患有
    MP 的条件概率是什么？更简单地说，返回的阳性结果中有多少比例是真阳性？这正是精确度衡量的内容。在这种情况下，精确度为 99 / (99 + 198)，即
    0.33，或者 33%。
- en: Wait a second. Something seems strange. Our test has a 99 percent probability
    of correctly diagnosing MP, yet 2/3 of the times when it gives us a positive result,
    that person does *not* have the disease. More than half of our positive results
    are wrong!
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 等一下，似乎有什么地方不对。我们的测试有 99% 的概率正确诊断 MP，但在 2/3 的情况下，当它给出阳性结果时，那个病人*并没有*患有该病。我们的大部分阳性结果都是错误的！
- en: That definitely seems weird.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实看起来很奇怪。
- en: And that’s why we’re going through this example. Understanding probabilities
    can be tricky. Here we have a test with a 99 percent true positive rate, which
    sounds pretty great. Yet the majority of our positive diagnoses are wrong.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们为什么要通过这个例子的原因。理解概率可能是很棘手的。这里我们有一个 99% 正确诊断 MP 的测试，听起来相当不错。然而，大多数我们的阳性诊断是错误的。
- en: This surprising result comes about because even though the chance of missing
    an infected person is very small, there’s a huge number of healthy people being
    tested. So we get a whole lot of those rare incorrect positive diagnoses, and
    they add up fast. The result is that if someone gets a positive result, we should
    *not* operate right away. We should instead interpret this result as a signal
    to do the more expensive and accurate test.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这个令人惊讶的结果出现是因为尽管错过感染者的概率很小，但被测试的健康人数量巨大。因此，我们得到了大量这些罕见的错误阳性诊断，并且它们迅速累积。结果是，如果某人得到阳性结果，我们*不应该*立即进行手术。我们应该将此结果解读为信号，提示我们进行更昂贵且更准确的测试。
- en: Let’s look at these numbers using our region diagrams. We’ll have to distort
    the sizes of the areas in [Figure 3-36](#figure3-36) in order to make a diagram
    that we can interpret.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们的区域图来查看这些数字。我们需要扭曲[图 3-36](#figure3-36)中区域的大小，以便制作一个我们能够解读的图表。
- en: '![C03f036](Images/C03f036.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![C03f036](Images/C03f036.png)'
- en: 'Figure 3-36: Left: The population contains 100 people with MP, and 9,900 without.
    Middle and Right: The results of our test. The sizes of the shapes are not to
    scale.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-36：左侧：该人群包含 100 个患有 MP 的人，以及 9,900 个没有患病的人。中间和右侧：我们的测试结果。图形的大小不按比例。
- en: We saw earlier that the precision tells us the chance that someone who is diagnosed
    as positive really does have MP. This is illustrated at the far left of [Figure
    3-37](#figure3-37). We can see that the field test incorrectly labels people without
    MP as positive, giving us a precision of 0.33\. That tells us to be suspicious
    of positive results, because 1 – 0.33 ≈ 0.66, or 66 percent, of those results
    will be wrong.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，精确度告诉我们，被诊断为阳性的人真正患有 MP 的概率。这在[图 3-37](#figure3-37)的最左侧进行了说明。我们可以看到，现场测试错误地将没有
    MP 的人标记为阳性，导致精确度为 0.33。这告诉我们应该对阳性结果保持怀疑，因为 1 – 0.33 ≈ 0.66，或者 66% 的阳性结果是错误的。
- en: What if someone gets a negative result? Are they really clear? That’s the ratio
    of true negatives to the total number of negatives, or TN / (TN + FN), which [Figure
    3-29](#figure3-29) gives the name of *negative predictive value*. In this case
    it’s 9,702 / (9,702 + 1). That’s well over 0.999, or 99.9 percent. So if someone
    gets back a negative result, there’s only about 1 chance in 10,000 that the test
    was wrong and they do have MP. We can tell them that, and let them decide if they
    want the slower, more expensive test.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某人得到了阴性结果，情况如何？他们真的是清楚的吗？这就是正确阴性与总阴性数之比，即 TN / (TN + FN)，这就是[图 3-29](#figure3-29)所定义的*负预测值*。在这种情况下，它是
    9,702 / (9,702 + 1)，大约为 0.999，或 99.9%。所以，如果某人得到了阴性结果，那么他们的测试结果错误的概率只有大约 1/10,000，他们确实患有
    MP。我们可以告诉他们这一点，让他们决定是否需要更慢、更昂贵的测试。
- en: '![C03f037](Images/C03f037.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![C03f037](Images/C03f037.png)'
- en: 'Figure 3-37: Four statistics describing our test for MP based on the results
    of [Figure 3-36](#figure3-36). Precision: What percentage of our positives are
    accurate? Recall: What is our percentage of finding all the positives? Negative
    Predictive Value: What percentage of our negatives are accurate? Specificity:
    What is our percentage of finding all the negatives? As before, the region sizes
    are not to scale.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-37：描述我们测试MP的四个统计数据，基于[图 3-36](#figure3-36)的结果。精准度：我们的阳性诊断中有多少百分比是准确的？召回率：我们找到所有阳性的百分比是多少？阴性预测值：我们的阴性诊断中有多少百分比是准确的？特异性：我们找到所有阴性的百分比是多少？如之前所述，区域大小并不按比例绘制。
- en: We’ve found that the chance that a positive result means that someone actually
    does have MP is only about 33 percent. On the other hand, a negative result is
    99.9 percent sure to be really negative.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，阳性结果意味着某人实际上确实患有MP的概率只有大约33%。另一方面，阴性结果有99.9%的可能性确实是阴性。
- en: '[Figure 3-37](#figure3-37) shows a couple of other measurements. The recall
    tells us the percentage of people that are properly diagnosed as positive. Since
    we only missed one person out of 100, that value is 99 percent. The specificity
    tells us the percentage of people that are properly diagnosed as negative. Since
    we gave 198 incorrect negative diagnoses, that result is a little less than 1\.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-37](#figure3-37)展示了其他几个测量值。召回率告诉我们被正确诊断为阳性的人群百分比。由于我们只漏掉了100人中的一个，因此该值为99%。特异性告诉我们被正确诊断为阴性的人群百分比。由于我们错误地给出了198个阴性诊断，因此该结果略低于1\。'
- en: To summarize, out of 10,000 people in this town with a 1 percent infection rate,
    our test will only miss 1 case of MP. But we’ll get nearly 200 incorrect positive
    diagnoses (that is, false positives), which can unduly scare and worry people.
    Some might even have the surgery right away, rather than wait for the slower test.
    In our desire to correctly find every person with MP, our test may have become
    overly zealous in telling people they’re infected.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在这个城镇中，1%感染率的1万名居民中，我们的测试只会漏掉1例MP。但我们将得到近200个错误的阳性诊断（即假阳性），这可能会不当地吓到和担心人们。有些人甚至可能会立刻进行手术，而不是等待更慢的测试。在我们希望正确找到每一个患有MP的人时，我们的测试可能在告知人们他们感染了病毒时过于激进。
- en: As we saw earlier, if we wanted to make a test that would never miss any person
    with MP, we could simply label everyone as positive, but that’s not useful. The
    goal in real situations with imperfect systems is to balance the false negatives
    and false positives in a way that serves our purposes, while keeping those errors
    in mind.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，如果我们想要做一个永远不会漏掉任何患有MP的人的测试，我们可以简单地将每个人都标记为阳性，但那样并没有意义。在现实情况下，面对不完美的系统，目标是平衡假阴性和假阳性的比例，以便达到我们的目的，同时考虑到这些错误。
- en: Our example of MP was imaginary, but the real world is full of situations where
    people are making important decisions based on incorrect confusion matrices or
    poorly constructed questions. And some of those decisions are related to real
    and very serious health issues.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的MP示例是虚构的，但现实世界充满了人们基于错误的混淆矩阵或构造不当的问题做出重要决策的情况。那些决策中有些与真实且非常严重的健康问题相关。
- en: For example, many women have had needless mastectomies because their surgeons
    misunderstood the probabilities from a breast exam and gave their patients bad
    counseling (Levitin 2016). Recommending someone undergo an unnecessary surgery
    is a dangerous mistake. Men were also operated on without cause, because many
    were given bad advice based on their doctors misunderstanding the statistics of
    using elevated PSA levels as evidence for prostate cancer (Kirby 2011).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，许多女性因其外科医生误解了乳腺检查的概率并给出了错误的建议，导致她们接受了不必要的乳房切除术（Levitin 2016）。推荐某人接受不必要的手术是一个危险的错误。男性也因没有必要的原因进行了手术，因为许多人根据医生误解了使用升高的PSA水平作为前列腺癌证据的统计学，得到了错误的建议（Kirby
    2011）。
- en: Probability and statistics can be subtle. It’s essential that we go slow, think
    things through, and make sure that we’re interpreting our data correctly.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 概率和统计学可能是微妙的。我们必须慢慢来，仔细思考，并确保我们正确地解读数据。
- en: Now we know that we shouldn’t be fooled by hearing that some test is “99 percent
    accurate,” or even that it “correctly identifies 99 percent of the positive cases.”
    In our town where only 1 percent of the people are infected, using a test with
    an impressive 99 percent true positive rate, anyone with a positive diagnosis
    is more than likely to *not* really have the disease.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道，听到某个测试“99%准确”或“正确识别了99%的阳性病例”时，我们不应轻易被误导。在我们这个只有1%人群被感染的小镇上，使用一个令人印象深刻的99%真正阳性率的测试，任何被诊断为阳性的人很可能*并没有*真正患病。
- en: The moral is that statistical claims in any situation, from advertising to science,
    need to be looked at closely and placed into context. Often, terms like “precision”
    and “accuracy” are used colloquially or casually, which, at best, makes them difficult
    to interpret. Even when these terms are used in their technical sense, bare claims
    of accuracy and related measures can easily be misleading and can lead to poor
    decisions.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这个道理是，任何情况下的统计声明，从广告到科学，都需要仔细审视并放入上下文中。通常，像“精确度”和“准确度”这样的术语被随意或口语化地使用，最好是使它们难以解读。即使这些术语在技术意义上使用，单纯的准确度和相关指标的声明也很容易引起误导，并可能导致错误的决策。
- en: When it comes to probability, don’t trust your gut. There are surprises and
    counterintuitive results that lie in wait all over the place. Go slow, gather
    all the data, and think it through.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及概率时，不要仅凭直觉。到处都有意想不到的惊讶和反直觉的结果。慢慢来，收集所有数据，仔细思考。
- en: Summary
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: We’ve seen a lot in this chapter! We covered some of the most important ideas
    in probability. We saw a term for how likely it is for some event A to happen,
    P(A); or for some event A to happen given that some other event B already happened,
    P(A|B); or for events A and B to happen together, P(A,B).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一章中学到了很多！我们涵盖了一些概率中的最重要的概念。我们看到了一个术语，用来表示某个事件A发生的可能性，P(A)；或者在某个事件B已经发生的条件下，事件A发生的可能性，P(A|B)；又或者是事件A和B同时发生的可能性，P(A,B)。
- en: We then looked at a few performance measures that let us characterize how well
    a test is able to properly identify the positive and negative samples in a dataset.
    We saw that we can use these measures to help us interpret the results of any
    decision-making process. We organized those terms into a confusion matrix, which
    helps us make sense of all that information.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看了一些性能指标，帮助我们表征一个测试在正确识别数据集中的正负样本方面的能力。我们看到，我们可以使用这些指标来帮助我们解释任何决策过程的结果。我们将这些术语组织成一个混淆矩阵，这有助于我们理解所有这些信息。
- en: We saw that statistics can be misleading. If we’re not careful, we can create
    tests (or classifiers) that seem to do a great job according to one set of measurements,
    but are lousy in other ways. It’s important to go slow, consider all the data,
    think things through, and use language carefully when working with probability.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了统计数据可能会引起误导。如果我们不小心，我们可能会创建出一些测试（或分类器），它们根据一组度量看起来做得很好，但在其他方面却很糟糕。重要的是要慢慢来，考虑所有数据，仔细思考，并且在处理概率时小心使用语言。
- en: In Chapter 4, we’ll apply some of these ideas to a method of reasoning about
    probabilities that is widely used in machine learning. This will give us another
    tool to help us later in designing learning algorithms that will learn and be
    able to usefully perform our desired tasks.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们将把这些概念应用到一种广泛用于机器学习中的推理方法。这将为我们提供另一个工具，帮助我们在设计学习算法时，能够有效地完成我们期望的任务。
