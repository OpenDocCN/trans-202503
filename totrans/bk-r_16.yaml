- en: '**13**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ELEMENTARY STATISTICS**'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Statistics is the practice of turning *data* into *information* to identify
    trends and understand features of populations. This chapter will cover some basic
    definitions and use R to demonstrate their application.
  prefs: []
  type: TYPE_NORMAL
- en: '**13.1 Describing Raw Data**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Often, the first thing statistical analysts are faced with is raw data—in other
    words, the records or observations that make up a sample. Depending on the nature
    of the intended analysis, these data could be stored in a specialized R object,
    often a data frame ([Chapter 5](ch05.xhtml#ch05)), possibly read in from an external
    file using techniques from [Chapter 8](ch08.xhtml#ch08). Before you can begin
    summarizing or modeling your data, however, it is important to clearly identify
    your available variables.
  prefs: []
  type: TYPE_NORMAL
- en: A *variable* is a characteristic of an individual in a population, the value
    of which can differ between entities within that population. For example, in [Section
    5.2](ch05.xhtml#ch05lev1sec20), you experimented with an illustrative data frame
    `mydata`. You recorded the age, sex, and humor level for a sample of people. These
    characteristics are your variables; the values measured will differ between the
    individuals.
  prefs: []
  type: TYPE_NORMAL
- en: Variables can take on a number of forms, determined by the nature of the values
    they may take. Before jumping into R, you’ll look at some standard ways in which
    variables are described.
  prefs: []
  type: TYPE_NORMAL
- en: '***13.1.1 Numeric Variables***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A *numeric* variable is one whose observations are naturally recorded as numbers.
    There are two types of numeric variables: continuous and discrete.'
  prefs: []
  type: TYPE_NORMAL
- en: A *continuous* variable can be recorded as any value in some interval, up to
    any number of decimals (which technically gives an infinite number of possible
    values, even if the continuum is restricted in range). For example, if you were
    observing rainfall amount, a value of 15 mm would make sense, but so would a value
    of 15.42135 mm. Any degree of measurement precision gives a valid observation.
  prefs: []
  type: TYPE_NORMAL
- en: A *discrete* variable, on the other hand, may take on only distinct numeric
    values—and if the range is restricted, then the number of possible values is finite.
    For example, if you were observing the number of heads in 20 flips of a coin,
    only whole numbers would make sense. It would not make sense to observe 15.42135
    heads; the possible outcomes are restricted to the integers from 0 to 20 (inclusive).
  prefs: []
  type: TYPE_NORMAL
- en: '***13.1.2 Categorical Variables***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Though numeric observations are common for many variables, it’s also important
    to consider *categorical* variables. Like some discrete variables, categorical
    variables may take only one of a finite number of possibilities. Unlike discrete
    variables, however, categorical observations are not always recorded as numeric
    values.
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of categorical variables. Those that cannot be logically
    ranked are called *nominal*. A good example of a categorical-nominal variable
    is sex. In most data sets, it has two fixed possible values, male and female,
    and the order of these categories is irrelevant. Categorical variables that can
    be naturally ranked are called *ordinal*. An example of a categorical-ordinal
    variable would be the dose of a drug, with the possible values low, medium, and
    high. These values can be ordered in either increasing or decreasing amounts,
    and the ordering might be relevant to the research.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Some statistical texts blur the definitions of discrete and categorical variables
    or even use them interchangeably. While this practice is not necessarily incorrect,
    I prefer to keep the definitions separate, for clarity. That is, I’ll say “discrete”
    when referring to a naturally numeric variable that cannot be expressed on a continuous
    scale (such as a count), and I’ll say “categorical” when the possible outcomes
    for a given individual are not necessarily numeric and the number of possible
    values is always finite.*'
  prefs: []
  type: TYPE_NORMAL
- en: Once you know what to look for, identifying the types of variables in a given
    data set is straightforward. Take the data frame `chickwts`, which is available
    in the automatically loaded `datasets` package. At the prompt, directly entering
    the following gives you the first five records of this data set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'R’s help file (`?chickwts`) describes these data as comprising the weights
    of 71 chicks (in grams) after six weeks, based on the type of food provided to
    them. Now let’s take a look at the two columns in their entirety as vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`weight` is a numeric measurement that can fall anywhere on a continuum, so
    this is a numeric-continuous variable. The fact that the chick weights appear
    to have been rounded or recorded to the nearest gram does not affect this definition
    because in reality the weights can be any figure (within reason). `feed` is clearly
    a categorical variable because it has only six possible outcomes, which aren’t
    numeric. The absence of any natural or easily identifiable ordering leads to the
    conclusion that `feed` is a categorical-nominal variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '***13.1.3 Univariate and Multivariate Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When discussing or analyzing data related to only one dimension, you’re dealing
    with *univariate* data. For example, the `weight` variable in the earlier example
    is univariate since each measurement can be expressed with one component—a single
    number.
  prefs: []
  type: TYPE_NORMAL
- en: When it’s necessary to consider data with respect to variables that exist in
    more than one dimension (in other words, with more than one component or measurement
    associated with each observation), your data are considered *multivariate*. Multivariate
    measurements are arguably most relevant when the individual components aren’t
    as useful when considered on their own (in other words, as univariate quantities)
    in any given statistical analysis.
  prefs: []
  type: TYPE_NORMAL
- en: An ideal example is that of spatial coordinates, which must be considered in
    terms of at least two components—a horizontal *x*-coordinate and a vertical *y*-coordinate.
    The univariate data alone—for example, the *x*-axis values only—aren’t especially
    useful. Consider the `quakes` data set (like `chickwts`, this is automatically
    available through the `datasets` package), which contains observations on 1,000
    seismic events recorded off the coast of Fiji. If you look at the first five records
    and read the descriptions in the help file `?quakes`, you quickly get a good understanding
    of what’s presented.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The columns `lat` and `long` provide the latitude and longitude of the event,
    `depth` provides the depth of the event (in kilometers), `mag` provides the magnitude
    on the Richter scale, and `stations` provides the number of observation stations
    that detected the event. If you’re interested in the spatial dispersion of these
    earthquakes, then examining only the latitude or the longitude is rather uninformative.
    The location of each event is described with two components: a latitude *and*
    a longitude value. You can easily plot these 1,000 events; [Figure 13-1](ch13.xhtml#ch13fig1)
    shows the result of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f13-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-1: Plotting the spatial locations of earthquakes using a bivariate
    (multivariate with two components) variable*'
  prefs: []
  type: TYPE_NORMAL
- en: '***13.1.4 Parameter or Statistic?***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As already noted, statistics as a discipline is concerned with understanding
    features of an overall *population*, defined as the entire collection of individuals
    or entities of interest. The characteristics of that population are referred to
    as *parameters*. Because researchers are rarely able to access relevant data on
    every single member of the population of interest, they typically collect a *sample*
    of entities to represent the population and record relevant data from these entities.
    They may then estimate the parameters of interest using the sample data—and those
    estimates are the *statistics*.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you were interested in the average age of women in the United
    States who own cats, the population of interest would be all women residing in
    the United States who own at least one cat. The parameter of interest is the true
    mean age of women in the United States who own at least one cat. Of course, obtaining
    the age of every single female American with a cat would be a difficult feat.
    A more feasible approach would be to randomly identify a smaller number of cat-owning
    American women and take data from them—this is your sample, and the mean age of
    the women in the sample is your statistic.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the key difference between a statistic and a parameter is whether the
    characteristic refers to the sample you drew your data from or the wider population.
    [Figure 13-2](ch13.xhtml#ch13fig2) illustrates this, with the mean *μ* of a measure
    for individuals in a population as the parameter and with the mean *x̄* of a sample
    of individuals taken from that population as the statistic.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f13-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-2: A conceptualization of statistical practice to illustrate the
    definitions of* parameter *and* statistic*, using the mean as an example*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 13.1**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each of the following, identify the type of variable described: numeric-continuous,
    numeric-discrete, categorical-nominal, or categorical-ordinal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of blemishes on the hood of a car coming off a production line
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A survey question that asks the participant to select from Strongly agree, Agree,
    Neutral, Disagree, and Strongly disagree
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The noise level (in decibels) at a concert
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The noise level out of three possible choices: high, medium, low'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A choice of primary color
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The distance between a cat and a mouse
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For each of the following, identify whether the quantity discussed is a population
    parameter or a sample statistic. If the latter, also identify what the corresponding
    population parameter is.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The percentage of 50 New Zealanders who own a gaming console
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The average number of blemishes found on the hoods of three cars in the No Dodgy
    Carz yard
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The proportion of domestic cats in the United States that wear a collar
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The average number of times per day a vending machine is used in a year
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The average number of times per day a vending machine is used in a year, based
    on data collected on three distinct days in that year
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**13.2 Summary Statistics**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you’ve learned the basic terminology, you’re ready to calculate some
    statistics with R. In this section, you’ll look at the most common types of statistics
    used to summarize the different types of variables I’ve discussed.
  prefs: []
  type: TYPE_NORMAL
- en: '***13.2.1 Centrality: Mean, Median, Mode***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Measures of centrality* are commonly used to explain large collections of
    data by describing where numeric observations are centered. One of the most common
    measures of centrality is of course the arithmetic *mean*. It’s considered to
    be the central “balance point” of a collection of observations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a set of *n* numeric measurements labeled *x* = {*x*[1], *x*[2], . . .
    , *x*[n]}, you find the sample mean *x̄* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e13-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, for example, if you observe the data 2,4.4,3,3,2,2.2,2,4, the mean is calculated
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0267-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The *median* is the “middle magnitude” of your observations, so if you place
    your observations in order from smallest to largest, you can find the median by
    either taking the middle value (if there’s an odd number of observations) or finding
    the mean of the two middle values (if there’s an even number of observations).
    Using the notation for *n* measurements labeled *x* = {*x*[1], *x*[2], . . . ,
    *x*[n]}, you find the sample median ![image](../images/mx.jpg) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: • Sort the observations from smallest to largest to give the “order statistics”
    ![image](../images/f0267-02.jpg),![image](../images/f0267-02a.jpg),...,![image](../images/f0267-03.jpg),
    where ![image](../images/f0267-04.jpg) denotes the *t*th smallest observation,
    regardless of observation number *i*, *j*, *k*, . . . .
  prefs: []
  type: TYPE_NORMAL
- en: '• Then, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e13-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the same data, sorting them from smallest to largest yields 2, 2, 2, 2.2,
    3, 3, 4, 4.4\. With *n* = 8 observations, you have *n*/2 = 4\. The median is therefore
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0267-03a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The *mode* is simply the “most common” observation. This statistic is more often
    used with numeric-discrete data than with numeric-continuous, though it is used
    with reference to *intervals* of the latter (commonly when discussing probability
    density functions—see [Chapters 15](ch15.xhtml#ch15) and [16](ch16.xhtml#ch16)).
    It’s possible for a collection of *n* numeric measurements *x*[1], *x*[2], . .
    . , *x[n]* to have no mode (where each observation is unique) or to have more
    than one mode (where more than one particular value occurs the largest number
    of times). To find the mode ![image](../images/dx.jpg), simply tabulate the frequency
    of each measurement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again using the eight observations from the example, you can see the frequencies
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Observation** | 2 | 2.2 | 3 | 4 | 4.4 |'
  prefs: []
  type: TYPE_TB
- en: '| **Frequency** | 3 | 1 | 2 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: The value 2 occurs three times, which is more frequent than any other value,
    so the single mode for these data is the value 2.
  prefs: []
  type: TYPE_NORMAL
- en: In R, it’s easy to compute the arithmetic mean and the median with built-in
    functions of the same names. First, store the eight observations as the numeric
    vector `xdata`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then compute the statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Finding a mode is perhaps most easily achieved by using R’s `table` function,
    which gives you the frequencies you need.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Though this clearly shows the mode for a small data set, it’s good practice
    to write code that can automatically identify the most frequent observations for
    any `table`. The `min` and `max` functions will report the smallest and largest
    values, with `range` returning both in a vector of length 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: When applied to a `table`, these commands operate on the reported frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Finally, therefore, you can construct a logical flag vector to get the mode
    from `table`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, 2 is the value and 3 is the frequency of that value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s return to the `chickwts` data set explored earlier in [Section 13.1.2](ch13.xhtml#ch13lev2sec113).
    The mean and median weights of the chicks are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also look at the `quakes` data set explored in [Section 13.1.3](ch13.xhtml#ch13lev2sec114).
    The most common magnitude of earthquake in the data set is identified with the
    following, which indicates that there were 107 occurrences of a 4.5 magnitude
    event:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Several methods are available to compute medians, though the impact on results
    is usually negligible for most practical purposes. Here I’ve simply used the default
    “sample” version used by R.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the functions R uses to compute statistics from a numeric structure
    will not run if the data set includes missing or undefined values (`NA`s or `NaN`s).
    Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: To prevent unintended `NaN`s or forgotten `NA`s being ignored without the user’s
    knowledge, R does not by default ignore these special values when running functions
    such as `mean`—and therefore will not return the intended numeric results. You
    can, however, set an optional argument `na.rm` to `TRUE`, which will force the
    function to operate only on the numeric values that are present.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You should use this argument only if you’re aware there might be missing values
    and that the result will be computed based on only those values that *have* been
    observed. Functions that I’ve discussed already such as `sum`, `prod`, `mean`,
    `median`, `max`, `min`, and `range`—essentially anything that calculates a numeric
    statistic based on a numeric vector—all have the `na.rm` argument available to
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, in calculating simple summary statistics, it’s useful to remind yourself
    of the `tapply` function (see [Section 10.2.3](ch10.xhtml#ch10lev2sec94)), used
    to compute statistics grouped by a specific categorical variable. Suppose, for
    example, you wanted to find the mean weight of the chicks grouped by feed type.
    One solution would be to use the `mean` function on each specific subset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This is cumbersome and lengthy. Using `tapply`, however, you can calculate the
    same values by category using just one line of code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, the first argument is the numeric vector upon which to operate, the `INDEX`
    argument specifies the grouping variable, and the `FUN` argument gives the name
    of the function to be performed on the data in the first argument as per the subsets
    defined by `INDEX`. Like other functions you’ve seen that request the user to
    specify *another* function to govern operations, `tapply` includes an ellipsis
    (see [Sections 9.2.5](ch09.xhtml#ch09lev2sec86) and [11.2.4](ch11.xhtml#ch11lev2sec102))
    to allow the user to supply further arguments directly to `FUN` if required.
  prefs: []
  type: TYPE_NORMAL
- en: '***13.2.2 Counts, Percentages, and Proportions***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, you’ll look at the summary of data that aren’t necessarily
    numeric. It makes little sense, for example, to ask R to compute the mean of a
    categorical variable, but it is sometimes useful to count the number of observations
    that fall within each category—these *counts* or *frequencies* represent the most
    elementary summary statistic of categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This uses the same count summary that was necessary for the mode calculation
    in [Section 13.2.1](ch13.xhtml#ch13lev2sec116), so again you can use the `table`
    command to obtain frequencies. Recall there are six feed types making up the diet
    of the chicks in the `chickwts` data frame. Getting these factor-level counts
    is as straightforward as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can gather more information from these counts by identifying the *proportion*
    of observations that fall into each category. This will give you comparable measures
    across multiple data sets. Proportions represent the fraction of observations
    in each category, usually expressed as a decimal (floating-point) number between
    0 and 1 (inclusive). To calculate proportions, you only need to modify the previous
    count function by dividing the count (or frequency) by the overall sample size
    (obtained here by using `nrow` on the appropriate data frame object; see [Section
    5.2](ch05.xhtml#ch05lev1sec20)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, you needn’t do everything associated with counts via `table`. A
    simple `sum` of an appropriate logical flag vector can be just as useful—recall
    that `TRUE`s are automatically treated as `1` and `FALSE`s as `0` in any arithmetic
    treatment of logical structures in R (refer to [Section 4.1.4](ch04.xhtml#ch04lev2sec40)).
    Such a `sum` will provide you with the desired frequency, but to get a proportion,
    you still need to divide by the total sample size. Furthermore, this is actually
    equivalent to finding the `mean` of a logical flag vector. For example, to find
    the proportion of chicks fed soybean, note that the following two calculations
    give identical results of around 0.197:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use this approach to calculate the proportion of entities in combined
    groups, achieved easily through logical operators (see [Section 4.1.3](ch04.xhtml#ch04lev2sec39)).
    The proportion of chicks fed either soybean *or* horsebean is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Yet again, the `tapply` function can prove useful. This time, to get the proportions
    of chicks on each diet, you’ll define the `FUN` argument to be an anonymous function
    (refer to [Section 11.3.2](ch11.xhtml#ch11lev2sec104)) that performs the required
    calculation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The disposable function here is defined with a dummy argument `x`, which you’re
    using to represent the vector of weights in each feed group to which `FUN` applies.
    Finding the desired proportion is therefore a case of dividing the number of observations
    in `x` by the total number of observations.
  prefs: []
  type: TYPE_NORMAL
- en: The last function to note is the `round` function, which rounds numeric data
    output to a certain number of decimal places. You need only supply to `round`
    your numeric vector (or matrix or any other appropriate data structure) and however
    many decimal places (as the argument `digits`) you want your figures rounded to.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This provides output that’s easier to read at a glance. If you set `digits=0`
    (the default), output is rounded to the nearest integer.
  prefs: []
  type: TYPE_NORMAL
- en: Before the next exercise, it’s worth briefly remarking on the relationship between
    a proportion and a percentage. The two represent the same thing. The only difference
    is the scale; the *percentage* is merely the proportion multiplied by 100\. The
    percentage of chicks on a soybean diet is therefore approximately 19.7 percent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Since proportions always lie in the interval [0,1], percentages always lie within
    [0,100].
  prefs: []
  type: TYPE_NORMAL
- en: Most statisticians use proportions over percentages because of the role proportions
    play in the direct representation of *probabilities* (discussed in [Chapter 15](ch15.xhtml#ch15)).
    However, there are situations in which percentages are preferred, such as basic
    data summaries or in the definition of *percentiles*, which will be detailed in
    [Section 13.2.3](ch13.xhtml#ch13lev2sec118).
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 13.2**'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain, rounded to two decimal places, the proportion of seismic events in the
    `quakes` data frame that occurred at a depth of 300 km or deeper.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remaining with the `quakes` data set, calculate the mean and median magnitudes
    of the events that occurred at a depth of 300 km or deeper.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the `chickwts` data set, write a `for` loop that gives you the mean weight
    of chicks for each feed type—the same as the results given by the `tapply` function
    in [Section 13.2.1](ch13.xhtml#ch13lev2sec116). Display the results rounded to
    one decimal place and, when printing, ensure each mean is labeled with the appropriate
    feed type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another ready-to-use data set (in the automatically loaded `datasets` package)
    is `InsectSprays`. It contains data on the number of insects found on various
    agricultural units, as well as the type of insect spray that was used on each
    unit. Ensure you can access the data frame at the prompt; then study the help
    file `?InsectSprays` to get an idea of R’s representation of the two variables.
  prefs: []
  type: TYPE_NORMAL
- en: Identify the two variable types in `InsectSprays` (as per the definitions in
    [Section 13.1.1](ch13.xhtml#ch13lev2sec112) and [Section 13.1.2](ch13.xhtml#ch13lev2sec113)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the modes of the distribution of insect counts, regardless of spray
    type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `tapply` to report the total insect counts by each spray type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the same kind of `for` loop as in (c), compute the percentage of agricultural
    units in each spray type group that had at least five bugs on them. When printing
    to the screen, round the percentages to the nearest whole number.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Obtain the same numeric results as in (g), with rounding, but use `tapply` and
    a disposable function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***13.2.3 Quantiles, Percentiles, and the Five-Number Summary***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s return, once more, to thinking about raw numeric observations. An understanding
    of how observations are *distributed* is an important statistical concept, and
    this will form a key feature of discussions in [Chapter 15](ch15.xhtml#ch15) onward.
  prefs: []
  type: TYPE_NORMAL
- en: You can gain more insight into the distribution of a set of observations by
    examining quantiles. A *quantile* is a value computed from a collection of numeric
    measurements that indicates an observation’s rank when compared to all the other
    present observations. For example, the median ([Section 13.2.1](ch13.xhtml#ch13lev2sec116))
    is itself a quantile—it gives you a value below which half of the measurements
    lie—it’s the 0.5th quantile. Alternatively, quantiles can be expressed as a *percentile*—this
    is identical but on a “percent scale” of 0 to 100\. In other words, the *p*th
    quantile is equivalent to the 100 × *p*th percentile. The median, therefore, is
    the 50th percentile.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of different algorithms that can be used to compute quantiles
    and percentiles. They all work by sorting the observations from smallest to largest
    and using some form of weighted average to find the numeric value that corresponds
    to *p*, but results may vary slightly in other statistical software.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtaining quantiles and percentiles in R is done with the `quantile` function.
    Using the eight observations stored as the vector `xdata`, the 0.8th quantile
    (or 80th percentile) is confirmed as 3.6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `quantile` takes the data vector of interest as its first argument,
    followed by a numeric value supplied to `prob`, giving the quantile of interest.
    In fact, `prob` can take a numeric vector of quantile values. This is convenient
    when multiple quantiles are desired.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, you’ve used `quantile` to obtain what’s called the *five-number summary*
    of `xdata`, comprised of the 0th percentile (the minimum), the 25th percentile,
    the 50th percentile, the 75th percentile, and the 100th percentile (the maximum).
    The 0.25th quantile is referred to as the *first* or *lower quartile*, and the
    0.75th quantile is referred to as the *third* or *upper quartile*. Also note that
    the 0.5th quantile of `xdata` is equivalent to the median (2.6, calculated in
    [Section 13.2.1](ch13.xhtml#ch13lev2sec116) using `median`). The median is the
    second quartile, with the maximum value being the fourth quartile.
  prefs: []
  type: TYPE_NORMAL
- en: There are ways to obtain the five-number summary other than using `quantile`;
    when applied to a numeric vector, the `summary` function also provides these statistics,
    along with the mean, automatically.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: To look at some examples using real data, let’s compute the lower and upper
    quartiles of the weights of the chicks in the `chickwts`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that 25 percent of the weights lie at or below 204.5 grams and
    that 75 percent of the weights lie at or below 323.5 grams.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s also compute the five-number summary (along with the mean) of the magnitude
    of the seismic events off the coast of Fiji that occurred at a depth of less than
    400 km, using the `quakes` data frame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This begins to highlight how useful quantiles are for interpreting the distribution
    of numeric measurements. From these results, you can see that most of the magnitudes
    of events at a depth of less than 400 km lie around 4.6, the median, and the first
    and third quartiles are just 4.4 and 4.9, respectively. But you can also see that
    the maximum value is much further away from the upper quartile than the minimum
    is from the lower quartile, suggesting a *skewed* distribution, one that stretches
    more positively (in other words, to the right) from its center than negatively
    (in other words, to the left). This notion is also supported by the fact that
    the mean is greater than the median—the mean is being “dragged upward” by the
    larger values.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll explore this further in [Chapter 14](ch14.xhtml#ch14) when you investigate
    data sets using basic statistical plots, and some of the associated terminology
    will be formalized in [Chapter 15](ch15.xhtml#ch15).
  prefs: []
  type: TYPE_NORMAL
- en: '***13.2.4 Spread: Variance, Standard Deviation, and the Interquartile Range***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The measures of centrality explored in [Section 13.2.1](ch13.xhtml#ch13lev2sec116)
    offer a good indication of where your numeric measurements are massed, but the
    mean, median, and mode do nothing to describe how *dispersed* your data are. For
    this, measures of *spread* are needed.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to your vector of eight hypothetical observations, given again here,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'you’ll also look at another eight observations stored as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Although these are two different collections of numbers, note that they have
    an identical arithmetic mean.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s plot these two data vectors side by side, each one on a horizontal
    line, by executing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: You saw how to use these base R graphics functions in [Chapter 7](ch07.xhtml#ch07),
    though it should be explained that because some of the observations in `xdata`
    and in `ydata` occur more than once, you can randomly alter them slightly to prevent
    overplotting, which aids in the visual interpretation. This step is known as *jittering*
    and is achieved by passing the numeric vector of interest to the `jitter` function
    prior to plotting with `points`. Additionally, note that you can use `yaxt="n"`
    in any call to `plot` to suppress the *y*-axis; similarly, `bty="n"` removes the
    typical box that’s placed around a plot (you’ll focus more on this type of plot
    customization in [Chapter 23](ch23.xhtml#ch23)).
  prefs: []
  type: TYPE_NORMAL
- en: The result, shown in [Figure 13-3](ch13.xhtml#ch13fig3), provides you with valuable
    information. Though the mean is the same for both `xdata` and `ydata`, you can
    easily see that the observations in `ydata` are more “spread out” around the measure
    of centrality than the observations in `xdata`. To quantify spread, you use values
    such as the variance, the standard deviation, and the interquartile range.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f13-03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-3: Comparing two hypothetical data vectors that share an identical
    arithmetic mean (marked by the vertical dotted line) but have different magnitudes
    of spread. Identical observations are jittered slightly.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample *variance* measures the degree of the spread of numeric observations
    around their arithmetic mean. The variance is a particular representation of the
    *average squared distance* of each observation when compared to the mean. For
    a set of *n* numeric measurements labeled *x* = {*x*[1], *x*[2], . . . , *x*[n]},
    the sample variance ![image](../images/common-05.jpg) is given by the following,
    where *x̄* is the sample mean described in [Equation (13.1)](ch13.xhtml#ch13eq1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e13-3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, if you take the eight illustrative observations 2, 4.4, 3, 3,
    2, 2.2, 2, 4, their sample variance is as follows when rounded to three decimal
    places (some terms are hidden with ... for readability):'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0277-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The *standard deviation* is simply the square root of the variance. Since the
    variance is a representation of the average squared distance, the standard deviation
    provides a value interpretable with respect to the scale of the original observations.
    With the same notation for a sample of *n* observations, the sample standard deviation
    *s* is found by taking the square root of [Equation (13.3)](ch13.xhtml#ch13eq3).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e13-4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, based on the sample variance calculated earlier, the standard
    deviation of the eight hypothetical observations is as follows (to three decimal
    places):'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0278-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, a rough way to interpret this is that 0.953 represents the average distance
    of each observation from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the variance and standard deviation, the *interquartile range (IQR)*
    is not computed with respect to the sample mean. The IQR measures the width of
    the “middle 50 percent” of the data, that is, the range of values that lie within
    a 25 percent quartile on either side of the median. As such, the IQR is computed
    as the difference between the upper and lower quartiles of your data. Formally,
    where *Q[x]* ( · ) denotes the quantile function (as defined in [Section 13.2.3](ch13.xhtml#ch13lev2sec118)),
    the IQR is given as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e13-5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The direct R commands for computing these measures of spread are `var` (variance),
    `sd` (standard deviation), and `IQR` (interquartile range).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: You can confirm the relationship between the sample variance and standard deviation
    using the square root function `sqrt` on the result from `var`, and you can reproduce
    the IQR by calculating the difference between the third and first quartiles.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Note that `as.numeric` (see [Section 6.2.4](ch06.xhtml#ch06lev2sec62)) strips
    away the percentile annotations (that label the results by default) from the returned
    object of `quantile`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, do the same with the `ydata` observations that had the same arithmetic
    mean as `xdata`. The calculations give you the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '`ydata` is on the same scale as `xdata`, so the results confirm what you can
    see in [Figure 13-3](ch13.xhtml#ch13fig3)—that the observations in the former
    are more spread out than in the latter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For two quick final examples, let’s return again to the `chickwts` and `quakes`
    data sets. In [Section 13.2.1](ch13.xhtml#ch13lev2sec116), you saw that the mean
    weight of all the chicks is 261.3099 grams. You can now find that the standard
    deviation of the weights is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Informally, this implies that the weight of each chick is, on average, around
    78.1 grams away from the mean weight (technically, though, remember it is merely
    the square root of a function of the squared distances—see the following note).
  prefs: []
  type: TYPE_NORMAL
- en: In [Section 13.2.3](ch13.xhtml#ch13lev2sec118), you used `summary` to obtain
    the five-number summary of the magnitudes of some of the earthquakes in the `quakes`
    data set. Looking at the first and third quartiles in these earlier results (4.4
    and 4.9, respectively), you can quickly determine that the IQR of this subset
    of the events is 0.5\. This can be confirmed using `IQR`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This gives you the width, in units of the Richter scale, of the middle 50 percent
    of the observations.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The definition of the variance (and hence the standard deviation) here has
    referred exclusively to the “sample estimator,” the default in R, which uses the
    divisor of n* − *1 in the formula. This is the formula used when the observations
    at hand represent a sample of an assumed larger population. In these cases, use
    of the divisor n* − *1 is more accurate, providing what’s known as an* unbiased
    *estimate of the true population value. Thus, you aren’t exactly calculating the
    “average squared distance,” though it can loosely be thought of as such and does
    indeed approach this as the sample size n increases.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 13.3**'
  prefs: []
  type: TYPE_NORMAL
- en: Using the `chickwts` data frame, compute the 10th, 30th, and 90th percentiles
    of all the chick weights and then use `tapply` to determine which feed type is
    associated with the highest sample variance of weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Turn to the seismic event data in `quakes` and complete the following tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the IQR of the recorded depths.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the five-number summary of all magnitudes of seismic events that occur
    at a depth of 400 km *or deeper*. Compare this to the summary values found in
    [Section 13.2.3](ch13.xhtml#ch13lev2sec118) of those events occurring at less
    than 400 km and briefly comment on what you notice.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use your knowledge of `cut` ([Section 4.3.3](ch04.xhtml#ch04lev2sec48)) to
    create a new factor vector called `depthcat` that identifies four evenly spaced
    categories of `quakes$depth` so that when you use `levels(depthcat)`, it gives
    the following:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Find the sample mean and standard deviation of the magnitudes of the events
    associated with each category of depth according to `depthcat`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `tapply` to compute the 0.8th quantile of the magnitudes of the seismic
    events in `quakes`, split by `depthcat`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '***13.2.5 Covariance and Correlation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When analyzing data, it’s often useful to be able to investigate the *relationship*
    between two numeric variables to assess trends. For example, you might expect
    height and weight observations to have a noticeable positive relationship—taller
    people tend to weigh more. Conversely, you might imagine that handspan and length
    of hair would have less of an association. One of the simplest and most common
    ways such associations are quantified and compared is through the idea of correlation,
    for which you need the covariance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *covariance* expresses how much two numeric variables “change together”
    and the nature of that relationship, whether it is positive or negative. Suppose
    for *n* individuals you have a sample of observations for two variables, labeled
    *x* = {*x*[1], *x*[2], . . . , *x*[n]} and *y* = {*y*[1], *y*[2], . . . , *y*[n]},
    where *x[i]* corresponds to *y[i]* for *i* = 1, . . . , *n*. The sample covariance
    *r[xy]* is computed with the following, where x̄ and ![image](../images/y2.jpg)
    represent the respective sample means of both sets of observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e13-6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When you get a positive result for *r*[xy], it shows that there is a positive
    linear relationship—as *x* increases, *y* increases. When you get a negative result,
    it shows a negative linear relationship—as *x* increases, *y* decreases, and vice
    versa. When *r[xy]* = 0, this indicates that there is no linear relationship between
    the values of *x* and *y*. It is useful to note that the order of the variables
    in the formula itself doesn’t matter; in other words, *r[xy]* ≡ *r*[yx].
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate, let’s use the original eight illustrative observations, which
    I’ll denote here with *x* = {2,4.4,3,3,2,2.2,2,4}, and the additional eight observations
    denoted with *y* = {1,4.4,1,3,2,2.2,2,7}. Remember that both *x* and *y* have
    sample means of 2.825\. The sample covariance of these two sets of observations
    is as follows (rounded to three decimal places):'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0281-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The figure is a positive number, so this suggests there is a positive relationship
    based on the observations in *x* and *y*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Correlation* allows you to interpret the covariance further by identifying
    both the direction and the strength of any association. There are different types
    of correlation coefficients, but the most common of these is *Pearson’s product-moment
    correlation coefficient*, the default implemented by R (this is the estimator
    I will use in this chapter). Pearson’s sample correlation coefficient *ρ*[xy]
    is computed by dividing the sample covariance by the product of the standard deviation
    of each data set. Formally, where *r[xy]* corresponds to [Equation (13.6)](ch13.xhtml#ch13eq6)
    and *s[x]* and *s[y]* to [Equation (13.4)](ch13.xhtml#ch13eq4),'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e13-7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which ensures that −1 ≤ *ρ*[xy] ≤ 1.
  prefs: []
  type: TYPE_NORMAL
- en: When *ρ*[xy] = −1, a perfect negative linear relationship exists. Any result
    less than zero shows a negative relationship, and the relationship gets weaker
    the nearer to zero the coefficient gets, until *ρ*[xy] = 0, showing no relationship
    at all. As the coefficient increases above zero, a positive relationship is shown,
    until *ρ*[xy] = 1, which is a perfect positive linear relationship.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you take the standard deviations already computed for *x* and *y* in [Section
    13.2.4](ch13.xhtml#ch13lev2sec119) (*s*[x] = 0.953 and *s[y]* = 2.013 to three
    decimal places), you find the following to three decimal places:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0281-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*ρ*[xy] is positive just like *r*[xy]; the value of 0.771 indicates a moderate-to-strong
    positive association between the observations in *x* and *y*. Again, *ρ*[xy] ≡
    *ρ*[yx].'
  prefs: []
  type: TYPE_NORMAL
- en: The R commands `cov` and `cor` are used for the sample covariance and correlation;
    you need only to supply the two corresponding vectors of data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'You can plot these bivariate observations as a coordinate-based plot (a *scatterplot*—see
    more examples in [Section 14.4](ch14.xhtml#ch14lev1sec47)). Executing the following
    gives you [Figure 13-4](ch13.xhtml#ch13fig4):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f13-04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-4: Plotting the* `xdata` *and* `ydata` *observations as bivariate
    data points to illustrate the interpretation of the correlation coefficient*'
  prefs: []
  type: TYPE_NORMAL
- en: As discussed earlier, the correlation coefficient estimates the nature of the
    *linear* relationship between two sets of observations, so if you look at the
    pattern formed by the points in [Figure 13-4](ch13.xhtml#ch13fig4) and imagine
    drawing a perfectly straight line that best represents all the points, you can
    determine the strength of the linear association by how close those points are
    to your line. Points closer to a perfect straight line will have a value of *ρ*[xy]
    closer to either −1 or 1\. The direction is determined by how the line is sloped—an
    increasing trend, with the line sloping upward toward the right, indicates positive
    correlation; a negative trend would be shown by the line sloping downward toward
    the right. Considering this, you can see that the estimated correlation coefficient
    for the data plotted in [Figure 13-4](ch13.xhtml#ch13fig4) makes sense according
    to the previous calculations. The points do appear to increase together as a rough
    straight line in terms of the values in `xdata` and `ydata`, but this linear association
    is by no means perfect. How you can compute the “ideal” or “best” straight line
    to fit such data is discussed in [Chapter 20](ch20.xhtml#ch20).
  prefs: []
  type: TYPE_NORMAL
- en: To aid your understanding of the idea of correlation, [Figure 13-5](ch13.xhtml#ch13fig5)
    displays different scatterplots, each showing 100 points. These observations have
    been randomly and artificially generated to follow preset “true” values of *ρ*[xy],
    labeled above each plot.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f13-05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-5: Artificial* x *and* y *observations, generated to illustrate
    a given value of the correlation coefficient*'
  prefs: []
  type: TYPE_NORMAL
- en: The first row of scatterplots shows negatively correlated data; the second shows
    positively correlated data. These match what you would expect to see—the direction
    of the line shows the negative or positive correlation of the trend, and the extremity
    of the coefficient corresponds to the closeness to a “perfect line.”
  prefs: []
  type: TYPE_NORMAL
- en: The third and final row shows data sets generated with a correlation coefficient
    set to zero, implying no linear relationship between the observations in *x* and
    *y*. The middle and rightmost plots are particularly important because they highlight
    the fact that Pearson’s correlation coefficient identifies only “straight-line”
    relationships; these last two plots clearly show some kind of trend or pattern,
    but this particular statistic cannot be used to detect such a trend.
  prefs: []
  type: TYPE_NORMAL
- en: 'To wrap up this section, look again at the `quakes` data. Two of the variables
    are `mag` (the magnitude of each event) and `stations` (the number of stations
    that reported detection of the event). A plot of `stations` on the *y*-axis against
    `mag` on the *x*-axis can be produced with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 13-6](ch13.xhtml#ch13fig6) shows this image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f13-06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-6: Plotting the number of stations reporting the event (* y*) and
    the magnitude (* x*) of each event in the* `quakes` *data frame*'
  prefs: []
  type: TYPE_NORMAL
- en: You can see by the vertical patterning that the magnitudes appear to have been
    recorded to a certain specific level of precision (this is owed to the difficulty
    associated with measuring earthquake magnitudes exactly). Nevertheless, a positive
    relationship (more stations tend to detect events of higher magnitude) is clearly
    visible in the scatterplot, a feature that is confirmed by a positive covariance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As you might expect from examining the pattern, Pearson’s correlation coefficient
    confirms that the linear association is quite strong.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*It is important to remember that* correlation does not imply causation*. When
    you detect a high correlative effect between two variables, this does not mean
    that one* causes *the other. Causation is difficult to prove in even the most
    controlled situations. Correlation merely allows you to measure* association.'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, there are other representations of correlation that can
    be used; *rank* coefficients, such as Spearman’s and Kendall’s correlation coefficients,
    differ from Pearson’s estimate in that they do not require the relationship to
    be linear. These are also available through the `cor` function by accessing the
    optional `method` argument (see `?cor` for details). Pearson’s correlation coefficient
    is the most commonly used, however, and is related to linear regression methods,
    which you’ll start to examine in [Chapter 20](ch20.xhtml#ch20).
  prefs: []
  type: TYPE_NORMAL
- en: '***13.2.6 Outliers***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An *outlier* is an observation that does not appear to “fit” with the rest of
    the data. It is a noticeably extreme value when compared with the bulk of the
    data, in other words, an anomaly. In some cases, you might suspect that such an
    extreme observation has not actually come from the same mechanism that generated
    the other observations, but there is no hard-and-fast numeric rule as to what
    constitutes an outlier. For example, consider the 10 hypothetical data points
    in `foo`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Using skills from [Chapter 7](ch07.xhtml#ch07) (and from creating [Figure 13-3](ch13.xhtml#ch13fig3)),
    you can plot `foo` on a line as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The result is given on the left of [Figure 13-7](ch13.xhtml#ch13fig7).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f13-07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-7: Illustrating the definition of outliers for univariate (left)
    and bivariate (right) data. Should you include such values in your statistical
    analysis? The answer can be difficult to determine.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'From this plot, you see that most of the observations are centered around zero,
    but one value is way out at 6\. To give a bivariate example, I’ll use two further
    vectors, `bar` and `baz`, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: I’ll plot these data using the following code; the result is on the right of
    [Figure 13-7](ch13.xhtml#ch13fig7).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: It’s important to identify outliers because of the potential impact they can
    have on any statistical calculations or model fitting. For this reason, many researchers
    will try to identify possible outliers before computing results by conducting
    an “exploratory” analysis of their data using basic summary statistics and data
    visualization tools (like those you’ll look at in [Chapter 14](ch14.xhtml#ch14)).
  prefs: []
  type: TYPE_NORMAL
- en: Outliers can occur naturally, where the outlier is a “true” or accurate observation
    recorded from the population, or unnaturally, where something has “contaminated”
    that particular contribution to the sample, such as incorrectly inputting data.
    As such, it is common to omit any outliers occurring through unnatural sources
    prior to analysis, but in practice this is not always easy because the cause of
    an outlier can be difficult to determine. In some cases, researchers conduct their
    analysis both ways—presenting results including and excluding any perceived outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, if you return to the example shown on the left in [Figure
    13-7](ch13.xhtml#ch13fig7), you can see that when you include all observations,
    you get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'However, when the possible outlier of 6 (the 10th observation) is deleted,
    you get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This highlights the impact a single extreme observation can have. Without any
    additional information about the sample, it would be difficult to say whether
    it’s sensible to exclude the outlier 6\. The same kind of effect is noticeable
    if you compute, say, the correlation coefficient of `bar` with `baz`, shown on
    the right in [Figure 13-7](ch13.xhtml#ch13fig7) (again, it’s the 10th observation
    that is the possible outlier).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: You see the correlation becomes much stronger without that outlier. Again, knowing
    whether to delete the outlier can be hard to correctly gauge in practice. At this
    stage, it’s important simply to be aware of the impact outliers can have on an
    analysis and to perform at least a cursory inspection of the raw data before beginning
    more rigorous statistical investigations.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The extent of the effect that extreme observations have on your data analysis
    depends not only on their extremity but on the statistics you intend to calculate.
    The sample mean, for example, is highly sensitive to outliers and will differ
    greatly when including or excluding them, so any statistic that depends on the
    mean, like the variance or covariance, will be affected too. Quantiles and related
    statistics, such as the median or IQR, are relatively unaffected by outliers.
    In statistical parlance this property is referred to as* robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 13.4**'
  prefs: []
  type: TYPE_NORMAL
- en: In [Exercise 7.1](ch07.xhtml#ch7exc1) (b) on [page 139](ch07.xhtml#page_139),
    you plotted height against weight measurements. Compute the correlation coefficient
    based on the observed data of these two variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another of R’s built-in, ready-to-use data sets is `mtcars`, containing a number
    of descriptive details on performance aspects of 32 automobiles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure you can access this data frame by entering `mtcars` at the prompt. Then
    inspect its help file to get an idea of the types of data present.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Two of the variables describe a vehicle’s horsepower and shortest time taken
    to travel a quarter-mile distance. Using base R graphics, plot these two data
    vectors with horsepower on the *x*-axis and compute the correlation coefficient.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify the variable in `mtcars` that corresponds to transmission type. Use
    your knowledge of factors in R to create a new factor from this variable called
    `tranfac`, where manual cars should be labeled `"manual"` and automatic cars `"auto"`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, use `qplot` from `ggplot2` in conjunction with `tranfac` to produce the
    same scatterplot as in (ii) so that you’re able to visually differentiate between
    manual and automatic cars.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, compute separate correlation coefficients for horsepower and quarter-mile
    time based on the transmission of the vehicles and, comparing these estimates
    with the overall value from (ii), briefly comment on what you note.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Return to `chickwts` to complete the following tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Produce a plot like the left panel of [Figure 13-7](ch13.xhtml#ch13fig7), based
    on the weights of chicks on the sunflower diet only. Note that one of the sunflower-fed
    chicks has a far lower weight than the others.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the standard deviation and IQR of the weights of the sunflower-fed chicks.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, suppose you’re told that the lowest weight of the sunflower-fed chicks
    was caused by a certain illness, irrelevant to your research. Delete this observation
    and recalculate the standard deviation and IQR of the remaining sunflower chicks.
    Briefly comment on the difference in calculated values.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Important Code in This Chapter**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| **Function/operator** | **Brief description** | **First occurrence** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `mean` | Arithmetic mean | [Section 13.2.1](ch13.xhtml#ch13lev2sec116), [p.
    268](ch13.xhtml#page_268) |'
  prefs: []
  type: TYPE_TB
- en: '| `median` | Median | [Section 13.2.1](ch13.xhtml#ch13lev2sec116), [p. 268](ch13.xhtml#page_268)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `table` | Tabulate frequencies | [Section 13.2.1](ch13.xhtml#ch13lev2sec116),
    [p. 268](ch13.xhtml#page_268) |'
  prefs: []
  type: TYPE_TB
- en: '| `min, max, range` | Minimum and maximum | [Section 13.2.1](ch13.xhtml#ch13lev2sec116),
    [p. 268](ch13.xhtml#page_268) |'
  prefs: []
  type: TYPE_TB
- en: '| `round` | Round numeric values | [Section 13.2.2](ch13.xhtml#ch13lev2sec117),
    [p. 272](ch13.xhtml#page_272) |'
  prefs: []
  type: TYPE_TB
- en: '| `quantile` | Quantiles/percentiles | [Section 13.2.3](ch13.xhtml#ch13lev2sec118),
    [p. 274](ch13.xhtml#page_274) |'
  prefs: []
  type: TYPE_TB
- en: '| `summary` | Five-number summary | [Section 13.2.3](ch13.xhtml#ch13lev2sec118),
    [p. 275](ch13.xhtml#page_275) |'
  prefs: []
  type: TYPE_TB
- en: '| `jitter` | Jitter points in plotting | [Section 13.2.4](ch13.xhtml#ch13lev2sec119),
    [p. 276](ch13.xhtml#page_276) |'
  prefs: []
  type: TYPE_TB
- en: '| `var, sd` | Variance, standard deviation | [Section 13.2.4](ch13.xhtml#ch13lev2sec119),
    [p. 278](ch13.xhtml#page_278) |'
  prefs: []
  type: TYPE_TB
- en: '| `IQR` | Interquartile range | [Section 13.2.4](ch13.xhtml#ch13lev2sec119),
    [p. 278](ch13.xhtml#page_278) |'
  prefs: []
  type: TYPE_TB
- en: '| `cov, cor` | Covariance, correlation | [Section 13.2.5](ch13.xhtml#ch13lev2sec120),
    [p. 281](ch13.xhtml#page_281) |'
  prefs: []
  type: TYPE_TB
