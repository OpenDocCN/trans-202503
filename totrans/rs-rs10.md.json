["```\nenum Ordering {\n    Relaxed,\n    Release,\n    Acquire,\n    AcqRel,\n    SeqCst\n}\n```", "```\nstatic X: AtomicBool = AtomicBool::new(false);\nstatic Y: AtomicBool = AtomicBool::new(false);\n\nlet t1 = spawn(|| {\n  1 let r1 = Y.load(Ordering::Relaxed);\n  2 X.store(r1, Ordering::Relaxed);\n});\nlet t2 = spawn(|| {\n  3 let r2 = X.load(Ordering::Relaxed);\n  4 Y.store(true, Ordering::Relaxed)\n});\n```", "```````````` The relaxed memory ordering allows this execution because it imposes no additional constraints on concurrent execution. That is, under relaxed memory ordering, the compiler must ensure only that execution dependencies on any given thread are respected (just as if atomics weren’t involved); it need not make any promises about the interleaving of concurrent operations. Reordering 3 and 4 is permitted for a single-threaded execution, so it is permitted under relaxed ordering as well.    In some cases, this kind of reordering is fine. For example, if you have a counter that just keeps track of metrics, it doesn’t really matter when exactly it executes relative to other instructions, and `Ordering::Relaxed` is fine. In other cases, this could be disastrous: say, if your program uses `r2` to figure out if security protections have already been set up, and thus ends up erroneously believing that they already have been.    You don’t generally notice this reordering when writing code that doesn’t make fancy use of atomics—the CPU has to promise that there is no observable difference between the code as written and what each thread actually executes, so everything seems like it runs in order just as you wrote it. This is referred to as respecting program order or evaluation order; the terms are synonyms.    #### Acquire/Release Ordering    At the next step up in the memory ordering hierarchy, we have `Ordering::Acquire`, `Ordering::Release`, and `Ordering::AcqRel` (acquire plus release). At a high level, these establish an execution dependency between a store in one thread and a load in another and then restrict how operations can be reordered with respect to that load and store. Crucially, these dependencies not only establish a relationship between a store and a load of a single value, but also put ordering constraints on *other* loads and stores in the threads involved. This is because every execution must respect the program order; if a load in thread B has a dependency on some store in thread A (the store in A must execute before the load in B), then any read or write in B after that load must also happen after that store in A.    Concretely, these memory orderings place the following restrictions on execution:    1.  Loads and stores cannot be moved forward past a store with `Ordering::Release`. 2.  Loads and stores cannot be moved back before a load with `Ordering::Acquire`. 3.  An `Ordering::Acquire` load of a variable must see all stores that happened before an `Ordering::Release` store that stored what the load loaded.    To see how these memory orderings change things, [Listing 10-3](#listing10-3) shows [Listing 10-2](#listing10-2) again but with the memory ordering swapped out for `Acquire` and `Release`.    ``` static X: AtomicBool = AtomicBool::new(false); static Y: AtomicBool = AtomicBool::new(false);  let t1 = spawn(|| {     let r1 = Y.load(Ordering::Acquire);     X.store(r1, Ordering::Release); }); let t2 = spawn(|| {   1 let r2 = X.load(Ordering::Acquire);   2 Y.store(true, Ordering::Release) }); ```    Listing 10-3: [Listing 10-2](#listing10-2) with `Acquire/Release` `memory ordering`   ``````````` These additional restrictions mean that it is no longer possible for `t2` to see `r2 = true`. To see why, consider the primary cause of the weird outcome in [Listing 10-2](#listing10-2): the reordering of 1 and 2. The very first restriction, on stores with `Ordering::Release`, dictates that we cannot move 1 below 2, so we’re all good!    But these rules are useful beyond this simple example. For example, imagine that you implement a mutual exclusion lock. You want to make sure that any loads and stores a thread runs while it holds the lock are executed only while it’s actually holding the lock, and visible to any thread that takes the lock later. This is exactly what `Release` and `Acquire` enable you to do. By performing a `Release` store to release the lock and an `Acquire` load to acquire the lock, you can guarantee that the loads and stores in the critical section are never moved to before the lock was actually acquired or to after the lock was released!   `````````` #### Sequentially Consistent Ordering    Sequentially consistent ordering (`Ordering::SeqCst`) is the strongest memory ordering we have access to. Its exact guarantees are somewhat hard to nail down, but very broadly, it requires not only that each thread sees results consistent with `Acquire/Release```` , but also that all threads see the *same* ordering as one another. This is best seen by way of contrast with the behavior of `Acquire` and `Release`. Specifically, `Acquire/Release` `` ordering does *not* guarantee that if two threads A and B atomically load values written by two other threads X and Y, A and B will see a consistent pattern of when X wrote relative to Y. That’s fairly abstract, so consider the example in [Listing 10-4](#listing10-4), which shows a case where `Acquire/Release` `ordering can produce unexpected results. Afterwards, we’ll see how sequentially consistent ordering avoids that particular unexpected outcome.` `` ```   ````````` ```````` ``````` ``` static X: AtomicBool = AtomicBool::new(false); static Y: AtomicBool = AtomicBool::new(false); static Z: AtomicI32 = AtomicI32::new(0);  let t1 = spawn(|| {     X.store(true, Ordering::Release); }); let t2 = spawn(|| {     Y.store(true, Ordering::Release); }); let t3 = spawn(|| {     while (!X.load(Ordering::Acquire)) {}   1 if (Y.load(Ordering::Acquire)) {         Z.fetch_add(1, Ordering::Relaxed); } }); let t4 = spawn(|| {     while (!Y.load(Ordering::Acquire)) {}   2 if (X.load(Ordering::Acquire)) {         Z.fetch_add(1, Ordering::Relaxed); } }); ```    Listing 10-4: Weird results with `Acquire/Release` `ordering`   `````` The two threads `t1` and `t2` set `X` and `Y` to `true`, respectively. Thread `t3` waits for `X` to be `true`; once `X` is `true`, it checks if `Y` is `true` and, if so, adds 1 to `Z`. Thread `t4` instead waits for `Y` to become `true`, and then checks if `X` is `true` and, if so, adds 1 to `Z`. At this point the question is: what are the possible values for `Z` after all the threads terminate? Before I show you the answer, try to work your way through it given the definitions of `Release` and `Acquire` ordering in the previous section.    First, let’s recap the conditions under which `Z` is incremented. Thread `t3` increments `Z` if it sees that `Y` is `true` after it observes that `X` is `true`, which can happen only if `t2` runs before `t3` evaluates the load at 1. Conversely, thread `t4` increments `Z` if it sees that `X` is `true` after it observes that `Y` is `true`, so only if `t1` runs before `t4` evaluates the load at 2. To simplify the explanation, let’s assume for now that each thread runs to completion once it runs.    Logically, then, `Z` can be incremented twice if the threads run in the order 1, 2, 3, 4—both `X` and `Y` are set to `true`, and then `t3` and `t4` run to find that their conditions for incrementing `Z` are met. Similarly, `Z` can trivially be incremented just once if the threads run in the order 1, 3, 2, 4\\. This satisfies `t4`’s condition for incrementing `Z`, but not `t3`’s. Getting `Z` to be `0`, however, *seems* impossible: if we want to prevent `t3` from incrementing `Z`, `t2` has to run after `t3`. Since `t3` runs only after `t1`, that implies that `t2` runs after `t1`. However, `t4` won’t run until after `t2` has run, so `t1` must have run and set `X` to `true` by the time `t4` runs, and so `t4` will increment `Z`.    Our inability to get `Z` to be `0` stems mostly from our human inclination for linear explanations; this happened, then this happened, then this happened. Computers aren’t limited in the same way and have no need to box all events into a single global order. There’s nothing in the rules for `Release` and `Acquire` that says that `t3` must observe the same execution order for `t1` and `t2` as `t4` observes. As far as the computer is concerned, it’s fine to let `t3` observe `t1` as having executed first, while having `t4` observe `t2` as having executed first. With that in mind, an execution in which `t3` observes that `Y` is `false` after it observes that `X` is `true` (implying that `t2` runs after `t1`), while in the same execution `t4` observes that `X` is `false` after it observes that `Y` is `true` (implying that `t2` runs before `t1`), is completely reasonable, even if that seems outrageous to us mere humans.    As we discussed earlier, `Acquire/Release` ``requires only that an `Ordering::Acquire` load of a variable must see all stores that happened before an `Ordering::Release` store that stored what the load loaded. In the ordering just discussed, the computer *did* uphold that property: `t3` sees `X == true`, and indeed sees all stores by `t1` prior to it setting `X = true`—there are none. It also sees `Y == false`, which was stored by the main thread at program startup, so there aren’t any relevant stores to be concerned with. Similarly, `t4` sees `Y = true` and also sees all stores by `t2` prior to setting `Y = true`—again, there are none. It also sees `X == false`, which was stored by the main thread and has no preceding store. No rules are broken, yet it just seems wrong somehow.``   ````` Our intuitive expectation was that we could put the threads in some global order to make sense of what every thread saw and did, but that was not the case for `Acquire/Release` `ordering in this example. To achieve something closer to that intuitive expectation, we need sequential consistency. Sequential consistency requires all the threads taking part in an atomic operation to coordinate to ensure that what each thread observes corresponds to (or at least appears to correspond to) *some* single, common execution order. This makes it easier to reason about but also makes it costly.`   ````Atomic loads and stores marked with `Ordering::SeqCst` instruct the compiler to take any extra precautions (such as using special CPU instructions) needed to guarantee sequential consistency for those loads and stores. The exact formalism around this is fairly convoluted, but sequential consistency essentially ensures that if you looked at all the related `SeqCst` operations from across all your threads, you could put the thread executions in *some* order so that the values that were loaded and stored would all match up.    If we replaced all the memory ordering arguments in [Listing 10-4](#listing10-4) with `SeqCst`, `Z` could not possibly be `0` after all the threads have exited, just as we originally expected. Under sequential consistency, it must be possible to say either that `t1` definitely ran before `t2` or that `t2` definitely ran before `t1`, so the execution where `t3` and `t4` see different orders is not allowed, and thus `Z` cannot be `0`.    ### Compare and Exchange    In addition to `load` and `store`, all of Rust’s atomic types provide a method called `compare_exchange`. This method is used to atomically *and conditionally* replace a value. You provide `compare_exchange` with the last value you observed for an atomic variable and the new value you want to replace the original value with, and it will replace the value only if it is still the same as it was when you last observed it. To see why this is important, take a look at the (broken) implementation of a mutual exclusion lock in [Listing 10-5](#listing10-5). This implementation keeps track of whether the lock is held in the static atomic variable `LOCK`. We use the Boolean value `true` to represent that the lock is held. To acquire the lock, a thread waits for `LOCK` to be `false`, then sets it to `true` again; it then enters its critical section and sets `LOCK` to `false` to release the lock when its work (`f`) is done.    ``` static LOCK: AtomicBool = AtomicBool::new(false);  fn mutex(f: impl FnOnce()) {     // Wait for the lock to become free (false).     while LOCK.load(Ordering::Acquire)       { /* .. TODO: avoid spinning .. */ }     // Store the fact that we hold the lock.     LOCK.store(true, Ordering::Release);     // Call f while holding the lock.     f();     // Release the lock.     LOCK.store(false, Ordering::Release); } ```    Listing 10-5: An incorrect implementation of a mutual exclusion lock    This mostly works, but it has a terrible flaw—two threads might both see `LOCK == false` at the same time and both leave the `while` loop. Then they both set `LOCK` to `true` and both enter the critical section, which is exactly what the `mutex` function was supposed to prevent!    The issue in [Listing 10-5](#listing10-5) is that there is a gap between when we load the current value of the atomic variable and when we subsequently update it, during which another thread might get to run and read or touch its value. It is exactly this problem that `compare_exchange` solves—it swaps out the value behind the atomic variable *only* if its value still matches the previous read, and otherwise notifies you that the value has changed. [Listing 10-6](#listing10-6) shows the corrected implementation using `compare_exchange`.    ``` static LOCK: AtomicBool = AtomicBool::new(false);  fn mutex(f: impl FnOnce()) {     // Wait for the lock to become free (false).     loop {       let take = LOCK.compare_exchange(           false,           true,           Ordering::AcqRel,           Ordering::Relaxed       );       match take {         Ok(false) => break,         Ok(true) | Err(false) => unreachable!(),  Err(true) => { /* .. TODO: avoid spinning .. */ }       }     }     // Call f while holding the lock.     f();     // Release the lock.     LOCK.store(false, Ordering::Release); } ```    Listing 10-6: A corrected implementation of a mutual exclusion lock    This time around, we use `compare_exchange` in the loop, and it takes care of both checking that the lock is currently not held and storing `true` to take the lock as appropriate. This happens through the first and second arguments to `compare_exchange`, respectively: in this case, `false` and then `true`. You can read the invocation as “Store `true` only if the current value is `false`.” The `compare_exchange` method returns a `Result` that indicates either that the value was successfully updated (`Ok`) or that it could not be updated (`Err`). In either case, it also returns the current value. This isn’t too useful with an `AtomicBool` since we know what the value must be if the operation failed, but for something like an `AtomicI32`, the updated current value will let you quickly recompute what to store and then try again without having to do another load.    Unlike simple loads and stores, `compare_exchange` takes *two* `Ordering` arguments. The first is the “success ordering,” and it dictates what memory ordering should be used for the load and store that the `compare_exchange` represents in the case that the value was successfully updated. The second is the “failure ordering,” and it dictates the memory ordering for the load if the loaded value does not match the expected current value. These two orderings are kept separate so that the developer can give the CPU leeway to improve execution performance by reordering loads and stores on failure when appropriate, but still get the correct ordering on success. In this case, it’s okay to reorder loads and stores across failed iterations of the lock acquisition loop, but it’s *not* okay to reorder loads and stores inside the critical section in such a way that they end up outside of it.    Even though its interface is simple, `compare_exchange` is a very powerful synchronization primitive—so much so that it’s been theoretically proven that you can build all other distributed consensus primitives using only `compare_exchange`! For that reason, it is the workhorse of many, if not most, synchronization constructs when you really dig into the implementation details.    Be aware, though, that a `compare_exchange` requires that a single CPU has exclusive access to the underlying value, and it is therefore a form of mutual exclusion at the hardware level. This in turn means that `compare_exchange` can quickly become a scalability bottleneck: only one CPU can make progress at a time, so there’s a portion of your code that will not scale with the number of cores. In fact, it’s probably worse than that—the CPUs have to coordinate to ensure that only one CPU succeeds at a `compare_exchange` for a variable at a time (take a look at the MESI protocol if you’re curious about how that works), and that coordination grows quadratically more costly the more CPUs are involved!    ### The Fetch Methods    Fetch methods (`fetch_add`, `fetch_sub`, `fetch_and`, and the like) are designed to allow more efficient execution of atomic operations that commute—that is, operations that have meaningful semantics regardless of the order they execute in. The motivation for this is that the `compare_exchange` method is powerful, but also costly—if two threads both want to update a single atomic variable, one will succeed, while the other will fail and have to retry. If many threads are involved, they all have to mediate sequential access to the underlying value, and there will be plenty of spinning while threads retry on failure.    For simple operations that commute, rather than fail and retry just because another thread modified the value, we can tell the CPU what operation to perform on the atomic variable. It’ll then perform that operation on whatever the current value happens to be when the CPU eventually gets exclusive access. Think of an `AtomicUsize` that counts the number of operations a pool of threads has completed. If two threads both complete a job at the same time, it doesn’t matter which one updates the counter first as long as both their increments are counted.    The fetch methods implement these kinds of commutative operations. They perform a read *and* a store operation in a single step and guarantee that the store operation was performed on the atomic variable when it held exactly the value returned by the method. As an example, `AtomicUsize::fetch_add(1, Ordering::Relaxed)` never fails—it always adds 1 to the current value of the `AtomicUsize`, no matter what it is, and returns the value of the `AtomicUsize` precisely when this thread’s 1 was added.    The fetch methods tend to be more efficient than `compare_exchange` because they don’t require threads to fail and retry when multiple threads contend for access to a variable. Some hardware architectures even have specialized fetch method implementations that scale much better as the number of involved CPUs grows. Nevertheless, if enough threads try to operate on the same atomic variable, those operations will begin to slow down and exhibit sublinear scaling due to the coordination required. In general, the best way to significantly improve the performance of a concurrent algorithm is to split contended variables into more atomic variables that are each less contended, rather than switching from `compare_exchange` to a fetch method.    ## Sane Concurrency    Writing correct and performant concurrent code is harder than writing sequential code; you have to consider not only possible execution interleavings but also how your code interacts with the compiler, the CPU, and the memory subsystem. With such a wide array of footguns at your disposal, it’s easy to want to throw your hands in the air and just give up on concurrency altogether. In this section we’ll explore some techniques and tools that can help ensure that you write correct concurrent code without (as much) fear.    ### Start Simple    It is a fact of life that simple, straightforward, easy-to-follow code is more likely to be correct. This principle also applies to concurrent code—always start with the simplest concurrent design you can think of, then measure, and only if measurement reveals a performance problem should you optimize your algorithm.    To follow this tip in practice, start out with concurrency patterns that do not require intricate use of atomics or lots of fine-grained locks. Begin with multiple threads that run sequential code and communicate over channels, or that cooperate through locks, and then benchmark the resulting performance with the workload you care about. You’re much less likely to make mistakes this way than by implementing fancy lockless algorithms or by splitting your locks into a thousand pieces to avoid false sharing. For many use cases, these designs are plenty fast enough; it turns out a lot of time and effort has gone into making channels and locks perform well! And if the simple approach is fast enough for your use case, why introduce more complex and error-prone code?    If your benchmarks indicate a performance problem, then figure out exactly which part of your system scales poorly. Focus on fixing that bottleneck in isolation where you can, and try to do so with small adjustments where possible. Maybe it’s enough to split a lock in two rather than move to a concurrent hash table, or to introduce another thread and a channel rather than implement a lock-free work stealing queue. If so, do that.    Even when you do have to work directly with atomics and the like, keep things simple until there’s a proven need to optimize—use `Ordering::SeqCst` and `compare_exchange` at first, and then iterate if you find concrete evidence that those are becoming bottlenecks that must be taken care of.    ### Write Stress Tests    As the author, you have a lot of insight into where bugs in your code may hide, without necessarily knowing what those bugs are (yet, anyway). Writing stress tests is a good way to shake out some of the hidden bugs. Stress tests don’t necessarily perform a complex sequence of steps but instead have lots of threads doing relatively simple operations in parallel.    For example, if you were writing a concurrent hash map, one stress test might be to have *N* threads insert or update keys and *M* threads read keys in such a way that those *M*+*N* threads are likely to often choose the same keys. Such a test doesn’t test for a particular outcome or value but instead tries to trigger many possible interleavings of operations in the hopes that buggy interleavings might reveal themselves.    Stress tests resemble fuzz tests in many ways; whereas fuzzing generates many random inputs to a given function, the stress test instead generates many random thread and memory access schedules. Just like fuzzers, stress tests are therefore only as good as the assertions in your code; they can’t tell you about a bug that doesn’t manifest in some easy-to-spot way like an assertion failure or some other kind of panic. For that reason, it’s a good idea to litter your low-level concurrency code with assertions, or `debug_assert_*` if you’re worried about runtime cost in particularly hot loops.    ### Use Concurrency Testing Tools    The primary challenge in writing concurrent code is to handle all the possible ways the execution of different threads can interleave. As we saw in the `Ordering::SeqCst` example in [Listing 10-4](#listing10-4), it’s not just the thread scheduling that matters, but also which memory values are possible for a given thread to observe at any given point in time. Writing tests that execute every possible legal execution is not only tedious but also difficult—you need very low-level control over which threads execute when and what values their reads return, which the operating system likely doesn’t provide.    #### Model Checking with Loom    Luckily, a tool already exists that can simplify this execution exploration for you in the form of the `loom` crate. Given the relative release cycles of this book and that of a Rust crate, I won’t give any examples of how to use Loom here, as they’d likely be out of date by the time you read this book, but I will give an overview of what it does.    Loom expects you to write dedicated test cases in the form of closures that you pass into a Loom model. The model keeps track of all cross-thread interactions and tries to intelligently explore all possible iterations of those interactions by executing the test case closure multiple times. To detect and control thread interactions, Loom provides replacement types for all the types in the standard library that allow threads to coordinate with one another; that includes most types under `std::sync` and `std::thread` as well as `UnsafeCell` and a few others. Loom expects your application to use those replacement types whenever you run the Loom tests. The replacement types tie into the Loom executor and perform a dual function: they act as rescheduling points so that Loom can choose which operation to run next after each possible thread interaction point, and they inform Loom of new possible interleavings to consider. Essentially, Loom builds up a tree of all the possible future executions for each point at which multiple execution interleavings are possible and then tries to execute all of them, one after the other.    Loom attempts to fully explore all possible executions of the test cases you provide it with, which means it can find bugs that occur only in extremely rare executions that stress testing would not find in a hundred years. While that’s great for smaller test cases, it’s generally not feasible to apply that kind of rigorous testing to larger test cases that test more involved sequences of operations or require many threads to run at once. Loom would simply take too long to get decent coverage of the code. In practice, you may therefore want to tell Loom to consider only a subset of the possible executions, which Loom’s documentation has more details on.    Like with stress tests, Loom can catch only bugs that manifest as panics, so that’s yet another reason to spend some time placing strategic assertions in your concurrent code! In many cases, it may even be worthwhile to add additional state tracking and bookkeeping instructions to your concurrent code to give you better assertions.    #### Runtime Checking with ThreadSanitizer    For larger test cases, your best bet is to run the test through a couple of iterations under Google’s excellent `ThreadSanitizer`, also known as TSan. TSan automatically augments your code by placing extra bookkeeping instructions prior to every memory access. Then, as your code runs, those bookkeeping instructions update and check a special state machine that flags any concurrent memory operations that indicate a problematic race condition. For example, if thread B writes to some atomic value X, but has not synchronized (lots of hand waving here) with the thread that wrote the previous value of X that indicates a write/write race, which is nearly always a bug.    Since TSan only observes your code running and does not execute it over and over again like Loom, it generally only adds a constant-factor overhead to the runtime of your program. While that factor can be significant (5–15 times at the time of writing), it’s still small enough that you can execute even most complex test cases in a reasonable amount of time.    At the time of writing, to use TSan you need to use a nightly version of the Rust compiler and pass in the `-Zsanitizer=thread` command-line argument (or set it in `RUSTFLAGS`), though hopefully in time this will be a standard supported option. Other sanitizers are also available that check things like out-of-bounds memory accesses, use-after-free, memory leaks, and reads of uninitialized memory, and you may want to run your concurrent test suite through those too!    ## Summary    In this chapter, we first covered common correctness and performance pitfalls in concurrent Rust, and some of the high-level concurrency patterns that successful concurrent applications tend to use to work around them. We also explored how asynchronous Rust enables concurrency without parallelism, and how to explicitly introduce parallelism in asynchronous Rust code. We then dove deeper into Rust’s many different lower-level concurrency primitives, including how they work, how they differ, and what they’re all for. Finally, we explored techniques for writing better concurrent code and looked at tools like Loom and TSan that can help you vet that code. In the next chapter we’ll continue our journey through the lower levels of Rust by digging into foreign function interfaces, which allow Rust code to link directly against code written in other languages.```` ````` `````` ``````` ```````` ````````` `````````` ``````````` ````````````"]