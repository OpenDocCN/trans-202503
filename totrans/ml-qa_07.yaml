- en: '**6'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**6'
- en: REDUCING OVERFITTING WITH MODEL MODIFICATIONS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过模型修改减少过拟合**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: Suppose we train a neural network classifier in a supervised fashion and already
    employ various dataset-related techniques to mitigate overfitting. How can we
    change the model or make modifications to the training loop to further reduce
    the effect of overfitting?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们以监督方式训练一个神经网络分类器，并且已经采用了各种与数据集相关的技术来减轻过拟合。我们如何改变模型或修改训练循环以进一步减少过拟合的影响？
- en: The most successful approaches against overfitting include regularization techniques
    like dropout and weight decay. As a rule of thumb, models with a larger number
    of parameters require more training data to generalize well. Hence, decreasing
    the model size and capacity can sometimes also help reduce overfitting. Lastly,
    building ensemble models is among the most effective ways to combat overfitting,
    but it comes with increased computational expense.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗过拟合的最成功的方法包括正则化技术，如丢弃法和权重衰减。通常而言，参数较多的模型需要更多的训练数据才能很好地进行泛化。因此，减小模型的规模和容量有时也有助于减少过拟合。最后，构建集成模型是对抗过拟合最有效的方法之一，但它伴随了更高的计算开销。
- en: This chapter outlines the key ideas and techniques for several categories of
    reducing overfitting with model modifications and then compares them to one another.
    It concludes by discussing how to choose between all types of overfitting reduction
    methods, including those discussed in the previous chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了几种通过修改模型来减少过拟合的关键思想和技术，并将它们进行比较。最后，讨论了如何在所有过拟合减少方法中做出选择，包括前一章讨论的那些方法。
- en: '**Common Methods**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**常见方法**'
- en: 'The various model- and training-related techniques to reduce overfitting can
    be grouped into three broad categories: (1) adding regularization, (2) choosing
    smaller models, and (3) building ensemble models.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少过拟合，诸多模型和训练相关的技术可以分为三大类：（1）添加正则化，（2）选择更小的模型，以及（3）构建集成模型。
- en: '***Regularization***'
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***正则化***'
- en: We can interpret regularization as a penalty against complexity. Classic regularization
    techniques for neural networks include *L*[2] regularization and the related weight
    decay method. We implement *L*[2] regularization by adding a penalty term to the
    loss function that is minimized during training. This added term represents the
    size of the weights, such as the squared sum of the weights. The following formula
    shows an *L*[2] regularized loss
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将正则化解释为对复杂度的惩罚。经典的神经网络正则化技术包括 *L*[2] 正则化和相关的权重衰减方法。我们通过向损失函数中添加一个惩罚项来实现
    *L*[2] 正则化，该项在训练过程中被最小化。这个添加的项代表了权重的大小，比如权重的平方和。以下公式展示了一个 *L*[2] 正则化的损失函数。
- en: '![Image](../images/f0030-01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0030-01.jpg)'
- en: where *λ* is a hyperparameter that controls the regularization strength.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *λ* 是一个超参数，用于控制正则化的强度。
- en: During backpropagation, the optimizer minimizes the modified loss, now including
    the additional penalty term, which leads to smaller model weights and can improve
    generalization to unseen data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播过程中，优化器最小化修改后的损失函数，损失函数现在包括了额外的惩罚项，这会导致模型权重变小，从而提高模型对未见数据的泛化能力。
- en: Weight decay is similar to *L*[2] regularization but is applied to the optimizer
    directly rather than modifying the loss function. Since weight decay has the same
    effect as *L*[2] regularization, the two methods are often used synonymously,
    but there may be subtle differences depending on the implementation details and
    optimizer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 权重衰减类似于 *L*[2] 正则化，但它直接应用于优化器，而不是修改损失函数。由于权重衰减与 *L*[2] 正则化具有相同的效果，这两种方法通常可以互换使用，但根据实现细节和优化器的不同，可能会有细微差异。
- en: 'Many other techniques have regularizing effects. For brevity’s sake, we’ll
    discuss just two more widely used methods: dropout and early stopping.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他技术具有正则化效果。为了简洁起见，我们只讨论两种更常用的方法：丢弃法（dropout）和提前停止（early stopping）。
- en: Dropout reduces overfitting by randomly setting some of the activations of the
    hidden units to zero during training. Consequently, the neural network cannot
    rely on particular neurons to be activated. Instead, it learns to use a larger
    number of neurons and multiple independent representations of the same data, which
    helps to reduce overfitting.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 丢弃法通过在训练过程中随机将一些隐藏单元的激活值设为零来减少过拟合。因此，神经网络不能依赖于某些特定的神经元被激活。相反，它学会使用更多的神经元和多种独立的数据表示，这有助于减少过拟合。
- en: In early stopping, we monitor the model’s performance on a validation set during
    training and stop the training process when the performance on the validation
    set begins to decline, as illustrated in [Figure 6-1](ch06.xhtml#ch6fig1).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在早停法中，我们在训练过程中监控模型在验证集上的表现，并在验证集上的表现开始下降时停止训练过程，如[图 6-1](ch06.xhtml#ch6fig1)所示。
- en: '![Image](../images/06fig01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/06fig01.jpg)'
- en: '*Figure 6-1: Early stopping*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-1：早停法*'
- en: In [Figure 6-1](ch06.xhtml#ch6fig1), we can see that the validation accuracy
    increases as the training and validation accuracy gap closes. The point where
    the training and validation accuracy is closest is the point with the least amount
    of over-fitting, which is usually a good point for early stopping.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 6-1](ch06.xhtml#ch6fig1)中，我们可以看到，随着训练和验证准确率差距的缩小，验证准确率逐渐提高。训练和验证准确率最接近的那个点通常是过拟合最少的点，通常是早停的最佳时机。
- en: '***Smaller Models***'
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***更小的模型***'
- en: Classic bias-variance theory suggests that reducing model size can reduce overfitting.
    The intuition behind this theory is that, as a general rule of thumb, the smaller
    the number of model parameters, the smaller its capacity to memorize or overfit
    to noise in the data. The following paragraphs discuss methods to reduce the model
    size, including pruning, which removes parameters from a model, and knowledge
    distillation, which transfers knowledge to a smaller model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的偏差-方差理论表明，减少模型大小可以降低过拟合。该理论的直觉是，作为一个经验法则，模型参数越小，它对数据中的噪声进行记忆或过拟合的能力就越小。接下来的段落讨论了减少模型大小的方法，包括剪枝（从模型中移除参数）和知识蒸馏（将知识转移到较小的模型中）。
- en: Besides reducing the number of layers and shrinking the layers’ widths as a
    hyperparameter tuning procedure, another approach to obtaining smaller models
    is *iterative pruning*, in which we train a large model to achieve good performance
    on the original dataset. We then iteratively remove parameters of the model, retraining
    it on the dataset such that it maintains the same predictive performance as the
    original model. (The lottery ticket hypothesis, discussed in [Chapter 4](ch04.xhtml),
    uses iterative pruning.)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了作为超参数调优过程中的减少层数和缩小层宽度外，获取更小模型的另一种方法是*迭代剪枝*，即我们训练一个大型模型，使其在原始数据集上表现良好。然后我们反复去除模型的参数，重新训练它，使得它能够保持与原始模型相同的预测性能。（[第4章](ch04.xhtml)中讨论的“彩票票据假说”使用了迭代剪枝。）
- en: Another common approach to obtaining smaller models is *knowledge distillation*.
    The general idea behind this approach is to transfer knowledge from a large, more
    complex model (the *teacher*) to a smaller model (the *student*). Ideally, the
    student achieves the same predictive accuracy as the teacher, but it does so more
    efficiently due to the smaller size. As a nice side effect, the smaller student
    may overfit less than the larger teacher model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的获取较小模型的方法是*知识蒸馏*。这种方法的基本思想是将来自一个大型、复杂模型（*教师*）的知识转移到一个较小的模型（*学生*）中。理想情况下，学生模型能够达到与教师模型相同的预测准确性，但由于模型较小，它能更高效地完成任务。作为一个附带的好处，较小的学生模型可能比较大的教师模型更少发生过拟合。
- en: '[Figure 6-2](ch06.xhtml#ch6fig2) diagrams the original knowledge distillation
    process. Here, the teacher is first trained in a regular supervised fashion to
    classify the examples in the dataset well, using a conventional cross-entropy
    loss between the predicted scores and ground truth class labels. While the smaller
    student network is trained on the same dataset, the training objective is to minimize
    both (a) the cross entropy between the outputs and the class labels and (b) the
    difference between its outputs and the teacher outputs (measured using *Kullback–Leibler*
    divergence, which quantifies the difference between two probability distributions
    by calculating how much one distribution diverges from the other in terms of information
    content).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-2](ch06.xhtml#ch6fig2)展示了原始的知识蒸馏过程。在这个过程中，教师模型首先通过常规的监督学习方式进行训练，以良好地分类数据集中的示例，使用常规的交叉熵损失来计算预测得分与真实类别标签之间的差异。与此同时，较小的学生网络在同一数据集上进行训练，训练目标是最小化（a）输出与类别标签之间的交叉熵损失，以及（b）其输出与教师输出之间的差异（使用*Kullback–Leibler*散度来衡量，Kullback–Leibler散度量化了两个概率分布之间的差异，计算一个分布在信息内容上与另一个分布的偏离程度）。'
- en: '![Image](../images/06fig02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/06fig02.jpg)'
- en: '*Figure 6-2: The original knowledge distillation process*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-2：原始的知识蒸馏过程*'
- en: By minimizing the Kullback–Leibler divergence—the difference between the teacher
    and student score distributions—the student learns to mimic the teacher while
    being smaller and more efficient.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化Kullback–Leibler散度——教师和学生得分分布之间的差异——学生在变得更小、更高效的同时学会模仿教师。
- en: '***Caveats with Smaller Models***'
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***较小模型的警告***'
- en: While pruning and knowledge distillation can also enhance a model’s generalization
    performance, these techniques are not primary or effective ways of reducing overfitting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管修剪和知识蒸馏也可以增强模型的泛化性能，但这些技术并不是减少过拟合的主要或有效方法。
- en: Early research results indicate that pruning and knowledge distillation can
    improve the generalization performance, presumably due to smaller model sizes.
    However, counterintuitively, recent research studying phenomena like double descent
    and grokking also showed that larger, overparameterized models have improved generalization
    performance if they are trained beyond the point of overfitting. *Double descent*
    refers to the phenomenon in which models with either a small or an extremely large
    number of parameters have good generalization performance, while models with a
    number of parameters equal to the number of training data points have poor generalization
    performance. *Grokking* reveals that as the size of a dataset decreases, the need
    for optimization increases, and generalization performance can improve well past
    the point of overfitting.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的研究结果表明，修剪和知识蒸馏能够提高泛化性能，可能是由于模型变得更小。然而，令人反直觉的是，最近的研究研究了诸如双重下降和“grokking”现象，发现较大、过度参数化的模型如果训练超越过拟合点，也能表现出更好的泛化性能。*双重下降*是指在模型的参数数量较少或极多时，具有较好的泛化性能，而在模型的参数数量等于训练数据点数时，泛化性能较差。*Grokking*揭示了随着数据集大小的减少，优化的需求增加，且泛化性能能够在超越过拟合点后继续改善。
- en: How can we reconcile the observation that pruned models can exhibit better generalization
    performance with contradictory observations from studies of double descent and
    grokking? Researchers recently showed that the improved training process partly
    explains the reduction of overfitting due to pruning. Pruning involves more extended
    training periods and a replay of learning rate schedules that may be partly responsible
    for the improved generalization performance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何调和修剪后的模型在泛化性能上可能更好的观察结果与来自双重下降和“grokking”现象研究的矛盾观察？最近的研究表明，改进的训练过程在一定程度上解释了由于修剪导致的过拟合减少。修剪涉及更长时间的训练周期和学习率调度的重复，这可能在一定程度上促进了泛化性能的提升。
- en: Pruning and knowledge distillation remain excellent ways to improve the computational
    efficiency of a model. However, while they can also enhance a model’s generalization
    performance, these techniques are not primary or effective ways of reducing overfitting.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪和知识蒸馏仍然是提高模型计算效率的优秀方法。然而，尽管它们也可以增强模型的泛化性能，但这些技术并不是减少过拟合的主要或有效方法。
- en: '***Ensemble Methods***'
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***集成方法***'
- en: Ensemble methods combine predictions from multiple models to improve the overall
    prediction performance. However, the downside of using multiple models is an increased
    computational cost.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法结合多个模型的预测，以提高整体预测性能。然而，使用多个模型的缺点是增加了计算成本。
- en: We can think of ensemble methods as asking a committee of experts to weigh in
    on a decision and then combining their judgments in some way to make a final decision.
    Members in a committee often have different backgrounds and experiences. While
    they tend to agree on basic decisions, they can overrule bad decisions by majority
    rule. This doesn’t mean that the majority of experts is always right, but there
    is a good chance that the majority of the committee is more often right, on average,
    than every single member.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将集成方法看作是请一组专家委员会对某个决策进行评估，然后通过某种方式将他们的判断结合起来，做出最终决策。委员会成员通常具有不同的背景和经验。虽然他们在基本决策上往往意见一致，但他们可以通过多数规则推翻错误的决策。这并不意味着大多数专家总是正确的，但大多数委员会成员的判断，平均而言，比每一个单独的成员更有可能是正确的。
- en: The most basic example of an ensemble method is majority voting. Here, we train
    *k* different classifiers and collect the predicted class label from each of these
    *k* models for a given input. We then return the most frequent class label as
    the final prediction. (Ties are usually resolved using a confidence score, randomly
    picking a label, or picking the class label with the lowest index.)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法最基本的例子是多数投票法。在这里，我们训练*k*个不同的分类器，并从这些*k*个模型中收集每个模型对于给定输入的预测类别标签。然后，我们返回出现频率最高的类别标签作为最终预测。（对于平局，通常通过置信度得分、随机选择标签或选择具有最低索引的类别标签来解决。）
- en: Ensemble methods are more prevalent in classical machine learning than deep
    learning because it is more computationally expensive to employ multiple models
    than to rely on a single one. In other words, deep neural networks require significant
    computational resources, making them less suitable for ensemble methods.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法在经典机器学习中比在深度学习中更为常见，因为使用多个模型比依赖单个模型更为计算密集。换句话说，深度神经网络需要大量计算资源，使得它们不太适合用于集成方法。
- en: 'Random forests and gradient boosting are popular examples of ensemble methods.
    However, by using majority voting or stacking, for example, we can combine any
    group of models: an ensemble may consist of a support vector machine, a multilayer
    perceptron, and a nearest-neighbor classifier. Here, stacking (also known as *stacked
    generalization*) is a more advanced variant of majority voting that involves training
    a new model to combine the predictions of several other models rather than obtaining
    the label by majority vote.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林和梯度提升是集成方法的常见示例。然而，通过使用多数投票或堆叠等方法，我们可以将任何一组模型结合在一起：一个集成可能由支持向量机、多层感知机和最近邻分类器组成。在这里，堆叠（也称为*堆叠泛化*）是多数投票的一种更高级的变体，涉及训练一个新的模型来结合多个其他模型的预测，而不是通过多数投票获得标签。
- en: A popular industry technique is to build models from *k-fold cross-validation*,
    a model evaluation technique in which we train and evaluate a model on *k* training
    folds. We then compute the average performance metric across all *k* iterations
    to estimate the overall performance measure of the model. After evaluation, we
    can either train the model on the entire training dataset or combine the individual
    models as an ensemble, as shown in [Figure 6-3](ch06.xhtml#ch6fig3).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的行业技术是使用*k*-折交叉验证来构建模型，这是一种模型评估技术，在这种方法中，我们在*k*个训练折上训练和评估模型。然后，我们计算所有*k*次迭代的平均性能指标，以估计模型的整体性能度量。评估后，我们可以选择在整个训练数据集上训练模型，或者将单个模型作为集成进行组合，如[图6-3](ch06.xhtml#ch6fig3)所示。
- en: '![Image](../images/06fig03.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/06fig03.jpg)'
- en: '*Figure 6-3:* k*-fold cross-validation for creating model ensembles*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-3：* k*-折交叉验证用于创建模型集成*'
- en: As shown in [Figure 6-3](ch06.xhtml#ch6fig3), the *k*-fold ensemble approach
    trains each of the *k* models on the respective *k* – 1 training folds in each
    round. After evaluating the models on the validation folds, we can combine them
    into a majority vote classifier or build an ensemble using stacking, a technique
    that combines multiple classification or regression models via a meta-model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6-3](ch06.xhtml#ch6fig3)所示，*k*-折集成方法在每一轮中将每个*k*模型在相应的*k* – 1个训练折上进行训练。在对模型进行验证集折评估后，我们可以将它们组合成多数投票分类器，或者使用堆叠方法构建一个集成，堆叠方法是通过元模型结合多个分类或回归模型的技术。
- en: While the ensemble approach can potentially reduce overfitting and improve robustness,
    this approach is not always suitable. For instance, potential downsides include
    managing and deploying an ensemble of models, which can be more complex and computationally
    expensive than using a single model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管集成方法有可能减少过拟合并提高鲁棒性，但这种方法并不总是适用。例如，潜在的缺点包括管理和部署一个集成模型，这比使用单个模型更为复杂且计算开销更大。
- en: '**Other Methods**'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**其他方法**'
- en: So far, this book has covered some of the most prominent techniques to reduce
    overfitting. [Chapter 5](ch05.xhtml) covered techniques that aim to reduce over-fitting
    from a data perspective. Additional techniques for reducing overfitting with model
    modifications include skip-connections (found in residual networks, for example),
    look-ahead optimizers, stochastic weight averaging, multitask learning, and snapshot
    ensembles.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书已经介绍了一些最突出减少过拟合的技术。[第5章](ch05.xhtml)介绍了从数据角度减少过拟合的技术。通过对模型的修改来减少过拟合的其他技术包括跳跃连接（例如，残差网络中的跳跃连接）、前瞻优化器、随机权重平均、多任务学习和快照集成。
- en: While they are not originally designed to reduce overfitting, layer input normalization
    techniques such as batch normalization (BatchNorm) and layer normalization (LayerNorm)
    can stabilize training and often have a regularizing effect that reduces overfitting.
    Weight normalization, which normalizes the model weights instead of layer inputs,
    could also lead to better generalization performance. However, this effect is
    less direct since weight normalization (WeightNorm) doesn’t explicitly act as
    a regularizer like weight decay does.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它们最初并非为减少过拟合而设计，但诸如批量归一化（BatchNorm）和层归一化（LayerNorm）等层输入归一化技术可以稳定训练，并且通常具有正则化效果，能够减少过拟合。权重归一化通过对模型权重进行归一化，而不是层输入，可能也会带来更好的泛化性能。然而，这一效果并不如权重衰减那样直接，因为权重归一化（WeightNorm）并不像权重衰减那样明确地充当正则化器。
- en: '**Choosing a Regularization Technique**'
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**选择正则化技术**'
- en: Improving data quality is an essential first step in reducing overfitting. However,
    for recent deep neural networks with large numbers of parameters, we need to do
    more to achieve an acceptable level of overfitting. Therefore, data augmentation
    and pretraining, along with established techniques such as dropout and weight
    decay, remain crucial overfitting reduction methods.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 提高数据质量是减少过拟合的一个重要步骤。然而，对于具有大量参数的现代深度神经网络，我们需要做更多的工作才能实现可接受的过拟合水平。因此，数据增强和预训练，以及诸如
    dropout 和权重衰减等已建立的技术，仍然是重要的过拟合减少方法。
- en: In practice, we can and should use multiple methods at once to reduce overfitting
    for an additive effect. To achieve the best results, treat selecting these techniques
    as a hyperparameter optimization problem.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们可以并且应该同时使用多种方法来减少过拟合，以实现加成效果。为了获得最佳结果，将选择这些技术视为超参数优化问题。
- en: '**Exercises**'
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '**6-1.** Suppose we’re using early stopping as a mechanism to reduce over-fitting—in
    particular, a modern early-stopping variant that creates checkpoints of the best
    model (for instance, the model with the highest validation accuracy) during training
    so that we can load it after the training has completed. This mechanism can be
    enabled in most modern deep learning frameworks. However, a colleague recommends
    tuning the number of training epochs instead. What are some of the advantages
    and disadvantages of each approach?'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**6-1.** 假设我们使用早停作为减少过拟合的机制——特别是使用一种现代的早停变体，在训练过程中创建最佳模型的检查点（例如，具有最高验证准确度的模型），以便在训练完成后加载它。大多数现代深度学习框架中都可以启用此机制。然而，一位同事建议调整训练周期数。每种方法的优缺点是什么？'
- en: '**6-2.** Ensemble models have been established as a reliable and successful
    method for decreasing overfitting and enhancing the reliability of predictive
    modeling efforts. However, there’s always a trade-off. What are some of the drawbacks
    associated with ensemble techniques?'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**6-2.** 集成模型已被证明是减少过拟合并增强预测建模可靠性的可靠且成功的方法。然而，这总是存在一个权衡。与集成技术相关的一些缺点是什么？'
- en: '**References**'
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'For more on the distinction between *L*[2] regularization and weight decay:
    Guodong Zhang et al., “Three Mechanisms of Weight Decay Regularization” (2018),
    *[https://arxiv.org/abs/1810.12281](https://arxiv.org/abs/1810.12281)*.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想了解更多关于 *L*[2] 正则化与权重衰减之间的区别，请参阅：Guodong Zhang 等人，“权重衰减正则化的三种机制”（2018），*[https://arxiv.org/abs/1810.12281](https://arxiv.org/abs/1810.12281)*。
- en: 'Research results indicate that pruning and knowledge distillation can improve
    generalization performance, presumably due to smaller model sizes: Geoffrey Hinton,
    Oriol Vinyals, and Jeff Dean, “Distilling the Knowledge in a Neural Network” (2015),
    *[https://arxiv.org/abs/1503.02531](https://arxiv.org/abs/1503.02531)*.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究结果表明，剪枝和知识蒸馏可以改善泛化性能，这可能是由于模型规模的减小：Geoffrey Hinton, Oriol Vinyals, 和 Jeff
    Dean，“神经网络中的知识蒸馏”（2015），*[https://arxiv.org/abs/1503.02531](https://arxiv.org/abs/1503.02531)*。
- en: 'Classic bias-variance theory suggests that reducing model size can reduce overfitting:
    Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie, “Model Selection and
    Bias-Variance Tradeoff,” Chapter 2.9, in *The Elements of Statistical Learning*
    (Springer, 2009).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经典的偏差-方差理论表明，减小模型规模可以减少过拟合：Jerome H. Friedman, Robert Tibshirani, 和 Trevor Hastie,
    “模型选择与偏差-方差权衡，”*统计学习的元素*（Springer, 2009）第2.9章。
- en: 'The lottery ticket hypothesis applies knowledge distillation to find smaller
    networks with the same predictive performance as the original one: Jonathan Frankle
    and Michael Carbin, “The Lottery Ticket Hypothesis: Finding Sparse, Trainable
    Neural Networks” (2018), *[https://arxiv.org/abs/1803.03635](https://arxiv.org/abs/1803.03635)*.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彩票票假设通过知识蒸馏找到具有与原始网络相同预测性能的更小网络：Jonathan Frankle 和 Michael Carbin，“彩票票假设：找到稀疏、可训练的神经网络”（2018年），
    *[https://arxiv.org/abs/1803.03635](https://arxiv.org/abs/1803.03635)*。
- en: 'For more on double descent: *[https://en.wikipedia.org/wiki/Double_descent](https://en.wikipedia.org/wiki/Double_descent)*.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于双重下降的更多信息： *[https://en.wikipedia.org/wiki/Double_descent](https://en.wikipedia.org/wiki/Double_descent)*。
- en: 'The phenomenon of grokking indicates that generalization performance can improve
    well past the point of overfitting: Alethea Power et al., “Grokking: Generalization
    Beyond Overfitting on Small Algorithmic Datasets” (2022), *[https://arxiv.org/abs/2201.02177](https://arxiv.org/abs/2201.02177)*.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grokking 现象表明，泛化性能可以远远超过过拟合的点：Alethea Power 等人，“Grokking：在小型算法数据集上超越过拟合的泛化”（2022年），
    *[https://arxiv.org/abs/2201.02177](https://arxiv.org/abs/2201.02177)*。
- en: 'Recent research shows that the improved training process partly explains the
    reduction of overfitting due to pruning: Tian Jin et al., “Pruning’s Effect on
    Generalization Through the Lens of Training and Regularization” (2022), *[https://arxiv.org/abs/2210.13738](https://arxiv.org/abs/2210.13738)*.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近的研究表明，改进的训练过程在一定程度上解释了由于修剪导致的过拟合减少：Tian Jin 等人，“从训练和正则化的角度看修剪对泛化的影响”（2022年），
    *[https://arxiv.org/abs/2210.13738](https://arxiv.org/abs/2210.13738)*。
- en: 'Dropout was previously discussed as a regularization technique, but it can
    also be considered an ensemble method that approximates a weighted geometric mean
    of multiple networks: Pierre Baldi and Peter J. Sadowski, “Understanding Dropout”
    (2013), *[https://proceedings.neurips.cc/paper/2013/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html](https://proceedings.neurips.cc/paper/2013/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html)*.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout 曾被讨论作为一种正则化技术，但它也可以被视为一种集成方法，近似多个网络的加权几何平均值：Pierre Baldi 和 Peter J.
    Sadowski，“理解Dropout”（2013年）， *[https://proceedings.neurips.cc/paper/2013/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html](https://proceedings.neurips.cc/paper/2013/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html)*。
- en: 'Regularization cocktails need to be tuned on a per-dataset basis: Arlind Kadra
    et al., “Well-Tuned Simple Nets Excel on Tabular Datasets” (2021), *[https://arxiv.org/abs/2106.11189](https://arxiv.org/abs/2106.11189)*.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化组合需要根据每个数据集进行调整：Arlind Kadra 等人，“调优得当的简单网络在表格数据集上的表现优秀”（2021年）， *[https://arxiv.org/abs/2106.11189](https://arxiv.org/abs/2106.11189)*。
