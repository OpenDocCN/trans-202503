<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch22"><span epub:type="pagebreak" id="page_147"/><strong><span class="big">22</span><br/>SPEEDING UP INFERENCE</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">What are techniques to speed up model inference through optimization without changing the model architecture or sacrificing accuracy?</p>&#13;
<p class="indent">In machine learning and AI, <em>model inference</em> refers to making predictions or generating outputs using a trained model. The main general techniques for improving model performance during inference include parallelization, vectorization, loop tiling, operator fusion, and quantization, which are discussed in detail in the following sections.</p>&#13;
<h3 class="h3" id="ch00lev108"><strong>Parallelization</strong></h3>&#13;
<p class="noindent">One common way to achieve better parallelization during inference is to run the model on a batch of samples rather than on a single sample at a time. This is sometimes also referred to as <em>batched inference</em> and assumes that we are receiving multiple input samples or user inputs simultaneously or within a short time window, as illustrated in <a href="ch22.xhtml#ch22fig1">Figure 22-1</a>.<span epub:type="pagebreak" id="page_148"/></p>&#13;
<div class="image"><img id="ch22fig1" src="../images/22fig01.jpg" alt="Image" width="692" height="438"/></div>&#13;
<p class="figcap"><em>Figure 22-1: Sequential inference and batched inference</em></p>&#13;
<p class="indent"><a href="ch22.xhtml#ch22fig1">Figure 22-1</a> shows sequential inference processing one item at a time, which creates a bottleneck if there are several samples waiting to be classified. In batched inference, the model processes all four samples at the same time.</p>&#13;
<h3 class="h3" id="ch00lev109"><strong>Vectorization</strong></h3>&#13;
<p class="noindent"><em>Vectorization</em> refers to performing operations on entire data structures, such as arrays (tensors) or matrices, in a single step rather than using iterative constructs like <code>for</code> loops. Using vectorization, multiple operations from the loop are performed simultaneously using single instruction, multiple data (SIMD) processing, which is available on most modern CPUs.</p>&#13;
<p class="indent">This approach takes advantage of the low-level optimizations in many computing systems and often results in significant speedups. For example, it might rely on BLAS.</p>&#13;
<p class="indent"><em>BLAS</em> (which is short for <em>Basic Linear Algebra Subprograms</em>) is a specification that prescribes a set of low-level routines for performing common linear algebra operations such as vector addition, scalar multiplication, dot products, matrix multiplication, and others. Many array and deep learning libraries like NumPy and PyTorch use BLAS under the hood.</p>&#13;
<p class="indent">To illustrate vectorization with an example, suppose we wanted to compute the dot product between two vectors. The non-vectorized way of doing this would be to use a <code>for</code> loop, iterating over each element of the array one by one. However, this can be quite slow, especially for large arrays. With vectorization, you can perform the dot product operation on the entire array at once, as shown in <a href="ch22.xhtml#ch22fig2">Figure 22-2</a>.<span epub:type="pagebreak" id="page_149"/></p>&#13;
<div class="image"><img id="ch22fig2" src="../images/22fig02.jpg" alt="Image" width="579" height="487"/></div>&#13;
<p class="figcap"><em>Figure 22-2: A classic</em> <span class="codeitalic1">for</span> <em>loop versus a vectorized dot product computation in Python</em></p>&#13;
<p class="indent">In the context of linear algebra or deep learning frameworks like TensorFlow and PyTorch, vectorization is typically done automatically. This is because these frameworks are designed to work with multidimensional arrays (also known as <em>tensors</em>), and their operations are inherently vectorized. This means that when you perform functions using these frameworks, you automatically leverage the power of vectorization, resulting in faster and more efficient computations.</p>&#13;
<h3 class="h3" id="ch00lev110"><strong>Loop Tiling</strong></h3>&#13;
<p class="noindent"><em>Loop tiling</em> (also often referred to as <em>loop nest optimization</em>) is an advanced optimization technique to enhance data locality by breaking down a loop’s iteration space into smaller chunks or “tiles.” This ensures that once data is loaded into cache, all possible computations are performed on it before the cache is cleared.</p>&#13;
<p class="indent"><a href="ch22.xhtml#ch22fig3">Figure 22-3</a> illustrates the concept of loop tiling for accessing elements in a two-dimensional array. In a regular <code>for</code> loop, we iterate over columns and rows one element at a time, whereas in loop tiling, we subdivide the array into smaller tiles.<span epub:type="pagebreak" id="page_150"/></p>&#13;
<div class="image"><img id="ch22fig3" src="../images/22fig03.jpg" alt="Image" width="711" height="383"/></div>&#13;
<p class="figcap"><em>Figure 22-3: Loop tiling in a two-dimensional array</em></p>&#13;
<p class="indent">Note that in languages such as Python, we don’t usually perform loop tiling, because Python and many other high-level languages do not allow control over cache memory like lower-level languages such as C and C++ do. These kinds of optimizations are often handled by underlying libraries like NumPy and PyTorch when performing operations on large arrays.</p>&#13;
<h3 class="h3" id="ch00lev111"><strong>Operator Fusion</strong></h3>&#13;
<p class="noindent"><em>Operator fusion</em>, sometimes called <em>loop fusion</em>, is an optimization technique that combines multiple loops into a single loop. This is illustrated in <a href="ch22.xhtml#ch22fig4">Figure 22-4</a>, where two separate loops to calculate the sum and the product of an array of numbers are fused into a single loop.</p>&#13;
<div class="image"><img id="ch22fig4" src="../images/22fig04.jpg" alt="Image" width="848" height="403"/></div>&#13;
<p class="figcap"><em>Figure 22-4: Fusing two</em> <span class="codeitalic1">for</span> <em>loops (left) into one (right)</em></p>&#13;
<p class="indent">Operator fusion can improve the performance of a model by reducing the overhead of loop control, decreasing memory access times by improving cache performance, and possibly enabling further optimizations through vectorization.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_151"/>You might think this behavior of vectorization would be incompatible with loop tiling, in which we break a <code>for</code> loop into multiple loops. However, these techniques are actually complementary, used for different optimizations, and applicable in different situations. Operator fusion is about reducing the total number of loop iterations and improving data locality when the entire data fits into cache. Loop tiling is about improving cache utilization when dealing with larger multidimensional arrays that do not fit into cache.</p>&#13;
<p class="indent">Related to operator fusion is the concept of <em>reparameterization</em>, which can often also be used to simplify multiple operations into one. Popular examples include training a network with multibranch architectures that are reparameterized into single-stream architectures during inference. This reparameterization approach differs from traditional operator fusion in that it does not merge multiple operations into a single operation. Instead, it rearranges the operations in the network to create a more efficient architecture for inference. In the so-called RepVGG architecture, for example, each branch during training consists of a series of convolutions. Once training is complete, the model is reparameterized into a single sequence of convolutions.</p>&#13;
<h3 class="h3" id="ch00lev112"><strong>Quantization</strong></h3>&#13;
<p class="noindent"><em>Quantization</em> reduces the computational and storage requirements of machine learning models, particularly deep neural networks. This technique involves converting the floating-point numbers (technically discrete but representing continuous values within a specific range) for implementing weights and biases in a trained neural network to more discrete, lower-precision representations such as integers. Using less precision reduces the model size and makes it quicker to execute, which can lead to significant improvements in speed and hardware efficiency during inference.</p>&#13;
<p class="indent">In the realm of deep learning, it has become increasingly common to quantize trained models down to 8-bit and 4-bit integers. These techniques are especially prevalent in the deployment of large language models.</p>&#13;
<p class="indent">There are two main categories of quantization. In <em>post-training quantization</em>, the model is first trained normally with full-precision weights, which are then quantized after training. <em>Quantization-aware training</em>, on the other hand, introduces the quantization step during the training process. This allows the model to learn to compensate for the effects of quantization, which can help maintain the model’s accuracy.</p>&#13;
<p class="indent">However, it’s important to note that quantization can occasionally lead to a reduction in model accuracy. Since this chapter focuses on techniques to speed up model inference <em>without</em> sacrificing accuracy, quantization is not as good a fit for this chapter as the previous categories.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>Other techniques to improve inference speeds include knowledge distillation and pruning, discussed in <a href="ch06.xhtml">Chapter 6</a>. However, these techniques affect the model architecture, resulting in smaller models, so they are out of scope for this chapter’s question.</em></p>&#13;
</div>&#13;
<h3 class="h3" id="ch00lev113"><span epub:type="pagebreak" id="page_152"/><strong>Exercises</strong></h3>&#13;
<p class="number1"><strong>22-1.</strong> <a href="ch07.xhtml">Chapter 7</a> covered several multi-GPU training paradigms to speed up model training. Using multiple GPUs can, in theory, also speed up model inference. However, in reality, this approach is often not the most efficient or most practical option. Why is that?</p>&#13;
<p class="number1"><strong>22-2.</strong> Vectorization and loop tiling are two strategies for optimizing operations that involve accessing array elements. What would be the ideal situation in which to use each?</p>&#13;
<h3 class="h3" id="ch00lev114"><strong>References</strong></h3>&#13;
<ul>&#13;
<li class="noindent">The official BLAS website: <em><a href="https://www.netlib.org/blas/">https://www.netlib.org/blas/</a></em>.</li>&#13;
<li class="noindent">The paper that proposed loop tiling: Michael Wolfe, “More Iteration Space Tiling” (1989), <em><a href="https://dl.acm.org/doi/abs/10.1145/76263.76337">https://dl.acm.org/doi/abs/10.1145/76263.76337</a></em>.</li>&#13;
<li class="noindent">RepVGG CNN architecture merging operations in inference mode: Xiaohan Ding et al., “RepVGG: Making VGG-style ConvNets Great Again” (2021), <em><a href="https://arxiv.org/abs/2101.03697">https://arxiv.org/abs/2101.03697</a></em>.</li>&#13;
<li class="noindent">A new method for quantizing the weights in large language models down to 8-bit integer representations: Tim Dettmers et al., “LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale” (2022), <em><a href="https://arxiv.org/abs/2208.07339">https://arxiv.org/abs/2208.07339</a></em>.</li>&#13;
<li class="noindent">A new method for quantizing the weights in LLMs farther down to 4-bit integers: Elias Frantar et al., “GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers” (2022), <em><a href="https://arxiv.org/abs/2210.17323">https://arxiv.org/abs/2210.17323</a></em>.</li>&#13;
</ul>&#13;
</div>
</div>
</body></html>