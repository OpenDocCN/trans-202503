<html><head></head><body>
<h2 class="h2" id="ch2"><span epub:type="pagebreak" id="page_21"/><span class="big">2</span><br/>RANDOMNESS</h2>
<div class="image9"><img src="../images/common01.jpg" alt="image"/></div>
<p class="noindent">Randomness is found everywhere in cryptography: in the generation of secret keys, in encryption schemes, and even in the attacks on cryptosystems. Without randomness, cryptography would be impossible because all operations would become predictable, and therefore insecure.</p>
<p class="indent">This chapter introduces you to the concept of randomness in the context of cryptography and its applications. We discuss pseudorandom number generators and how operating systems can produce reliable randomness, and we conclude with real examples showing how flawed randomness can impact security.</p>
<h3 class="h3" id="lev1sec14"><span epub:type="pagebreak" id="page_22"/>Random or Non-Random?</h3>
<p class="noindent">You’ve probably already heard the phrase “random bits,” but strictly speaking there is no such thing as a series of random bits. What is random is actually the algorithm or process that produces a series of random bits; therefore, when we say “random bits,” we actually mean randomly generated bits.</p>
<p class="indent">What do random bits look like? For example, to most people, the 8-bit string 11010110 is more random than 00000000, although both have the same chance of being generated (namely, 1/256). The value 11010110 looks more random than 00000000 because it has the signs typical of a randomly generated value. That is, 11010110 has no obvious pattern.</p>
<p class="indent">When we see the string 11010110, our brain registers that it has about as many zeros (three) as it does ones (five), just like 55 other 8-bit strings (11111000, 11110100, 11110010, and so on), but only one 8-bit string has eight zeros. Because the pattern three-zeros-and-five-ones is more likely to occur than the pattern eight-zeros, we identify 11010110 as random and 00000000 as non-random, and if a program produces the bits 11010110, you may think that it’s random, even if it’s not. Conversely, if a randomized program produces 00000000, you’ll probably doubt that it’s random.</p>
<p class="indentb">This example illustrates two types of errors people often make when identifying randomness:</p>
<p class="hang"><strong>Mistaking non-randomness for randomness</strong> Thinking that an object was randomly generated simply because it <em>looks</em> random.</p>
<p class="hang"><strong>Mistaking randomness for non-randomness</strong> Thinking that patterns appearing by chance are there for a reason other than chance.</p>
<p class="indentt">The distinction between random-looking and actually random is crucial. Indeed, in crypto, non-randomness is often synonymous with insecurity.</p>
<h3 class="h3" id="lev1sec15">Randomness as a Probability Distribution</h3>
<p class="noindent">Any randomized process is characterized by a <em>probability distribution</em>, which gives all there is to know about the randomness of the process. A probability distribution, or simply distribution, lists the outcomes of a randomized process where each outcome is assigned a <em>probability</em>.</p>
<p class="indent">A probability measures the likelihood of an event occurring. It’s expressed as a real number between 0 and 1 where a probability 0 means impossible and a probability of 1 means certain. For example, when tossing a two-sided coin, each side has a probability of landing face up of 1/2, and we usually assume that landing on the edge of the coin has probability zero.</p>
<p class="indent">A probability distribution must include all possible outcomes, such that the sum of all probabilities is 1. Specifically, if there are <em>N</em> possible events, there are <em>N</em> probabilities <em>p</em><sub>1</sub>, <em>p</em><sub>2</sub>, … , <em>p</em><sub><em>N</em></sub> with <em>p</em><sub>1</sub> + <em>p</em><sub>2</sub> + … + <em>p</em><sub><em>N</em></sub> = 1. In the case of the coin toss, the distribution is 1/2 for heads and 1/2 for tails. The sum of both probabilities is equal to 1/2 + 1/2 = 1, because the coin will fall on one of its two faces.</p>
<p class="indent"><span epub:type="pagebreak" id="page_23"/>A <em>uniform distribution</em> occurs when all probabilities in the distribution are equal, meaning that all outcomes are equally likely to occur. If there are <em>N</em> events, then each event has probability 1/<em>N</em>. For example, if a 128-bit key is picked uniformly at random—that is, according to a uniform distribution—then each of the 2<sup>128</sup> possible keys should have a probability of 1/2<sup>128</sup>.</p>
<p class="indent">In contrast, when a distribution is <em>non-uniform</em>, probabilities aren’t all equal. A coin toss with a non-uniform distribution is said to be biased, and may yield heads with probability 1/4 and tails with probability 3/4, for example.</p>
<h3 class="h3" id="lev1sec16">Entropy: A Measure of Uncertainty</h3>
<p class="noindent"><em>Entropy</em> is the measure of uncertainty, or disorder in a system. You might think of entropy as the amount of surprise found in the result of a randomized process: the higher the entropy, the less the certainty found in the result.</p>
<p class="indent">We can compute the entropy of a probability distribution. If your distribution consists of probabilities <em>p</em><sub>1</sub>, <em>p</em><sub>2</sub>, … , <em>p</em><sub><em>N</em></sub>, then its entropy is the negative sum of all probabilities multiplied by their logarithm, as shown in this expression:</p>
<p class="center">−<em>p</em><sub>1</sub> × log(<em>p</em><sub>1</sub>) − <em>p</em><sub>2</sub> × log(<em>p</em><sub>2</sub>) − … − <em>p<sub>N</sub></em> × log(<em>p<sub>N</sub></em>)</p>
<p class="indent">Here the function <em>log</em> is the <em>binary logarithm</em>, or logarithm in base two. Unlike the natural logarithm, the binary logarithm expresses the information in bits and yields integer values when probabilities are powers of two. For example, log(1/2) = –1, log(1/4) = –2, and more generally log(1/2<sup><em>n</em></sup>) = –<em>n</em>. (That’s why we actually take the <em>negative sum</em>, in order to end up with a positive number.) Random 128-bit keys produced using a uniform distribution therefore have the following entropy:</p>
<p class="center">2<sup>128</sup> × (−2<sup>−128</sup> × log(2<sup>−128</sup>)) = −log(2<sup>−128</sup>) = 128 bits</p>
<p class="indent">If you replace 128 by any integer <em>n</em> you will find that the entropy of a uniformly distributed <em>n</em>-bit string will be <em>n</em> bits.</p>
<p class="indent">Entropy is maximized when the distribution is uniform because a uniform distribution maximizes uncertainty: no outcome is more likely than the others. Therefore, <em>n</em>-bit values can’t have more than <em>n</em> bits of entropy.</p>
<p class="indent">By the same token, when the distribution is not uniform, entropy is lower. Consider the coin toss example. The entropy of a fair toss is the following:</p>
<p class="center">−(1/2) × log (1/2) − (1/2) × log (1/2) = 1/2 + 1/2 = 1 bit</p>
<p class="indent">What if one side of the coin has a higher probability of landing face up than the other? Say heads has a probability of 1/4 and tails 3/4 (remember that the sum of all probabilities should be 1).</p>
<p class="indentb"><span epub:type="pagebreak" id="page_24"/>The entropy of such a biased toss is this:</p>
<p class="center">−(3/4) × log(3/4) − (1/4) × log(1/4) ≈ −(3/4) × (−0.415) − (1/4) × (−2) ≈ 0.81 bit</p>
<p class="indentt">The fact that 0.81 is less than the 1-bit entropy of a fair toss tells us that the more biased the coin, the less uniform the distribution and the lower the entropy. Taking this example further, if heads has a probability of 1/10, the entropy is 0.469; if the probability drops to 1/100, the entropy drops to 0.081.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>Entropy can also be viewed as a measure of information. For example, the result of a fair coin toss gives you exactly one bit of information—heads or tails—and you’re unable to predict the result of the toss in advance. In the case of the unfair coin toss, you know in advance that tails is more probable, so you can usually predict the outcome of the toss. The result of the coin toss gives you the information needed to predict the result with certainty.</em></p>
</div>
<h3 class="h3" id="lev1sec17">Random Number Generators (RNGs) and Pseudorandom Number Generators (PRNGs)</h3>
<p class="noindentb">Cryptosystems need randomness to be secure and therefore need a component from which to get their randomness. The job of this component is to return random bits when requested to do so. How is this randomness generation done? You’ll need two things:</p>
<ul>
<li class="noindent">A source of uncertainty, or <em>source of entropy</em>, provided by random number generators (RNGs).</li>
<li class="noindent">A cryptographic algorithm to produce high-quality random bits from the source of entropy. This is found in pseudorandom number generators (PRNGs).</li>
</ul>
<p class="indentt">Using RNGs and PRNGs is the key to making cryptography practical and secure. Let’s briefly look at how RNGs work before exploring PRNGs in depth.</p>
<p class="indent">Randomness comes from the environment, which is analog, chaotic, uncertain, and hence unpredictable. Randomness can’t be generated by computer-based algorithms alone. In cryptography, randomness usually comes from <em>random number generators (RNGs)</em>, which are software or hardware components that leverage entropy in the analog world to produce unpredictable bits in a digital system. For example, an RNG might directly sample bits from measurements of temperature, acoustic noise, air turbulence, or electrical static. Unfortunately, such analog entropy sources aren’t always available, and their entropy is often difficult to estimate.</p>
<p class="indent">RNGs can also harvest the entropy in a running operating system by drawing from attached sensors, I/O devices, network or disk activity, system logs, running processes, and user activities such as key presses and mouse <span epub:type="pagebreak" id="page_25"/>movement. Such system- and human-generated activities can be a good source of entropy, but they can be fragile and manipulated by an attacker. Also, they’re slow to yield random bits.</p>
<p class="indent"><em>Quantum random number generators (QRNGs)</em> are a type of RNG that relies on the randomness arising from quantum mechanical phenomena such as radioactive decay, vacuum fluctuations, and observing photons’ polarization. These can provide <em>real</em> randomness, rather than just apparent randomness. However, in practice, QRNGs may be biased and don’t produce bits quickly; like the previously cited entropy sources, they need an additional component to produce reliably at high speed.</p>
<p class="indent"><em>Pseudorandom number generators (PRNGs)</em> address the challenge we face in generating randomness by reliably producing many artificial random bits from a few true random bits. For example, an RNG that translates mouse movements to random bits would stop working if you stop moving the mouse, whereas a PRNG always returns pseudorandom bits when requested to do so.</p>
<p class="indent">PRNGs rely on RNGs but behave differently: RNGs produce true random bits relatively slowly from analog sources, in a nondeterministic way, and with no guarantee of high entropy. In contrast, PRNGs produce random-looking bits quickly from digital sources, in a deterministic way, and with maximum entropy. Essentially, PRNGs transform a few unreliable random bits into a long stream of reliable pseudorandom bits suitable for crypto applications, as shown in <a href="ch02.xhtml#ch2fig1">Figure 2-1</a>.</p>
<div class="image"><img src="../images/f02-01.jpg" alt="image"/></div>
<p class="figcap"><a id="ch2fig1"/><em>Figure 2-1: RNGs produce few unreliable bits from analog sources, whereas PRNGs expand those bits to a long stream of reliable bits.</em></p>
<h4 class="h4" id="lev2sec22"><em>How PRNGs Work</em></h4>
<p class="noindent">A PRNG receives random bits from an RNG at regular intervals and uses them to update the contents of a large memory buffer, called the <em>entropy pool</em>. The entropy pool is the PRNG’s source of entropy, just like the physical environment is to an RNG. When the PRNG updates the entropy pool, it mixes the pool’s bits together to help remove any statistical bias.</p>
<p class="indent">In order to generate pseudorandom bits, the PRNG runs a deterministic random bit generator (DRBG) algorithm that expands some bits from the entropy pool into a much longer sequence. As its name suggests, a DRBG is deterministic, not randomized: given one input you will always get the same output. The PRNG ensures that its DRBG never receives the same input twice, in order to generate unique pseudorandom sequences.</p>
<p class="indentb">In the course of its work, the PRNG performs three operations, as follows:</p>
<p class="hang"><strong><em>init()</em></strong> Initializes the entropy pool and the internal state of the PRNG</p>
<p class="hang"><strong><em>refresh(R)</em></strong> Updates the entropy pool using some data, <em>R</em>, usually sourced from an RNG</p>
<p class="hang"><strong><em>next(N)</em></strong> Returns <em>N</em> pseudorandom bits and updates the entropy pool</p>
<p class="indentt"><span epub:type="pagebreak" id="page_26"/>The <em>init</em> operation resets the PRNG to a fresh state, reinitializes the entropy pool to some default value, and initializes any variables or memory buffers used by the PRNG to carry out the <em>refresh</em> and <em>next</em> operations.</p>
<p class="indent">The <em>refresh</em> operation is often called <em>reseeding</em>, and its argument <em>R</em> is called a <em>seed</em>. When no RNG is available, seeds may be unique values hardcoded in a system. The <em>refresh</em> operation is typically called by the operating system, whereas <em>next</em> is typically called or requested by applications. The <em>next</em> operation runs the DRBG and modifies the entropy pool to ensure that the next call will yield different pseudorandom bits.</p>
<h4 class="h4" id="lev2sec23"><em>Security Concerns</em></h4>
<p class="noindent">Let’s talk briefly about the way that PRNGs address some high-level security concerns. Specifically, PRNGs should guarantee <em>backtracking resistance</em> and <em>prediction resistance</em>. Backtracking resistance (also called <em>forward secrecy</em>) means that previously generated bits are impossible to recover, whereas prediction resistance (<em>backward secrecy</em>) means that future bits should be impossible to predict.</p>
<p class="indent">In order to achieve backtracking resistance, the PRNG should ensure that the transformations performed when updating the state through the <em>refresh</em> and <em>next</em> operations are irreversible so that if an attacker compromises the system and obtains the entropy pool’s value, they can’t determine the previous values of the pool or the previously generated bits. To achieve prediction resistance, the PRNG should call <em>refresh</em> regularly with <em>R</em> values that are unknown to an attacker and that are difficult to guess, thus preventing an attacker from determining future values of the entropy pool, even if the whole pool is compromised. (Even if the list of <em>R</em> values used were known, you’d need to know the order in which <em>refresh</em> and <em>next</em> calls were made in order to reconstruct the pool.)</p>
<h4 class="h4" id="lev2sec24"><em>The PRNG Fortuna</em></h4>
<p class="noindent"><em>Fortuna</em> is a PRNG construction used in Windows originally designed in 2003 by Niels Ferguson and Bruce Schneier. Fortuna superseded <em>Yarrow</em>, a 1998 design by Kelsey and Schneier now used in the macOS and iOS operating systems. I won’t provide the Fortuna specification here or show you how to implement it, but I will try to explain how it works. You’ll find a complete description of Fortuna in <a href="ch09.xhtml#ch9">Chapter 9</a> of <em>Cryptography Engineering</em> by Ferguson, Schneier, and Kohno (Wiley, 2010).</p>
<p class="indentb">Fortuna’s internal memory includes the following:</p>
<ul>
<li class="noindent">Thirty-two entropy pools, <em>P</em><sub>1</sub>, <em>P</em><sub>2</sub>, … , <em>P</em><sub>32</sub>, such that <em>P</em><sub><em>i</em></sub> is used every 2<sup><em>i</em></sup> reseeds.</li>
<li class="noindent">A key, <em>K</em>, and a counter, <em>C</em> (both 16 bytes). These form the internal state of Fortuna’s DRBG.</li>
</ul>
<p class="indentb"><span epub:type="pagebreak" id="page_27"/>In simplest terms, Fortuna works like this:</p>
<ul>
<li class="noindent"><em>init</em>() sets <em>K</em> and <em>C</em> to zero and empties the 32 entropy pools <em>P</em><sub><em>i</em></sub>, where <em>i</em> = 1 … 32.</li>
<li class="noindent"><em>refresh</em>(<em>R</em>) appends the data, <em>R</em>, to one of the entropy pools. The system chooses the RNGs used to produce <em>R</em> values, and it should call <em>refresh</em> regularly.</li>
<li class="noindent"><em>next</em>(<em>N</em>) updates <em>K</em> using data from one or more entropy pools, where the choice of the entropy pools depends mainly on how many updates of <em>K</em> have already been done. The <em>N</em> bits requested are then produced by encrypting <em>C</em> using <em>K</em> as a key. If encrypting <em>C</em> is not enough, Fortuna encrypts <em>C</em> + 1, then <em>C</em> + 2, and so on, to get enough bits.</li>
</ul>
<p class="indent">Although Fortuna’s operations look fairly simple, implementing them correctly is hard. For one thing, you need to get all the details of the algorithm right—namely, how entropy pools are chosen, the type of cipher to be used in <em>next</em>, how to behave when no entropy is received, and so on. Although the specs define most of the details, they don’t include a comprehensive test suite to check that an implementation is correct, which makes it difficult to ensure that your implementation of Fortuna will behave as expected.</p>
<p class="indent">Even if Fortuna is correctly implemented, security failures may occur for reasons other than the use of an incorrect algorithm. For example, Fortuna might not notice if the RNGs fail to produce enough random bits, and as a result Fortuna will produce lower-quality pseudorandom bits, or it may stop delivering pseudorandom bits altogether.</p>
<p class="indent">Another risk inherent in Fortuna implementations lies in the possibility of exposing associated <em>seed files</em> to attackers. The data in Fortuna seed files is used to feed entropy to Fortuna through <em>refresh</em> calls when an RNG is not immediately available, such as immediately after a system reboot and before the system’s RNGs have recorded any unpredictable events. However, if an identical seed file is used twice, then Fortuna will produce the same bit sequence twice. Seed files should therefore be erased after being used to ensure that they aren’t reused.</p>
<p class="indent">Finally, if two Fortuna instances are in the same state because they are sharing a seed file (meaning they are sharing the same data in the entropy pools, including the same <em>C</em> and <em>K</em>), then the <em>next</em> operation will return the same bits in both instances.</p>
<h4 class="h4" id="lev2sec25"><em>Cryptographic vs. Non-Cryptographic PRNGs</em></h4>
<p class="noindent">There are both cryptographic and non-cryptographic PRNGs. Non-crypto PRNGs are designed to produce uniform distributions for applications such as scientific simulations or video games. However, you should never use non-crypto PRNGs in crypto applications, because they’re insecure—they’re only concerned with the quality of the bits’ probability distribution and not with their predictability. Crypto PRNGs, on the other hand, are unpredictable, because they’re also concerned with the strength of the underlying <em>operations</em> used to deliver well-distributed bits.</p>
<p class="indent"><span epub:type="pagebreak" id="page_28"/>Unfortunately, most PRNGs exposed by programming languages, such as libc’s <span class="literal">rand</span> and <span class="literal">drand48</span>, PHP’s <span class="literal">rand</span> and <span class="literal">mt_rand</span>, Python’s <span class="literal">random</span> module, Ruby’s <span class="literal">Random</span> class, and so on, are non-cryptographic. Defaulting to a non-crypto PRNG is a recipe for disaster because it often ends up being used in crypto applications, so be sure to use only crypto PRNGs in crypto applications.</p>
<h5 class="h5">A Popular Non-Crypto PRNG: Mersenne Twister</h5>
<p class="noindent">The <em>Mersenne Twister</em> (MT) algorithm is a non-cryptographic PRNG used (at the time of this writing) in PHP, Python, R, Ruby, and many other systems. MT will generate uniformly distributed random bits without statistical bias, but it’s predictable: given a few bits produced by MT, it’s easy enough to tell which bits will follow.</p>
<p class="indentb">Let’s look under the hood to see what makes the Mersenne Twister insecure. The MT algorithm is much simpler than that of crypto PRNGs: its internal state is an array, <em>S</em>, consisting of 624 32-bit words. This array is initially set to <em>S</em><sub>1</sub>, <em>S</em><sub>2</sub>, … , <em>S</em><sub>624</sub> and evolves to <em>S</em><sub>2</sub>, … , <em>S</em><sub>625</sub>, then <em>S</em><sub>3</sub>, … , <em>S</em><sub>626</sub>, and so on, according to this equation:</p>
<p class="center"><em>S</em><sub><em>k</em> + 624</sub> = <em>S</em><sub><em>k</em> + 397</sub> ⊕ <strong>A</strong>((<em>S<sub>k</sub></em> ∧ 0x80000000) ∨ (<em>S</em><sub><em>k</em> + 1</sub> ∧ 0x7fffffff))</p>
<p class="indent">Here, ⊕ denotes the bitwise XOR (<span class="literal">^</span> in the C programming language), ∧ denotes the bitwise AND (&amp; in C), ∨ denotes the bitwise OR (<span class="literal">|</span> in C), and <strong>A</strong> is a function that transforms some 32-bit word, <em>x</em>, to (<em>x</em> &gt;&gt; 1), if <em>x</em>’s most significant bit is 0, or to (<em>x</em> &gt;&gt; 1) ⊕ 0x9908b0df otherwise.</p>
<p class="indent">Notice in this equation that bits of <em>S</em> interact with each other only through XORs. The operators ∧ and ∨ never combine two bits of <em>S</em> together, but just bits of <em>S</em> with bits from the constants 0x80000000 and 0x7fffffff. This way, any bit from <em>S</em><sub>625</sub> can be expressed as an XOR of bits from <em>S</em><sub>398</sub>, <em>S</em><sub>1</sub>, and <em>S</em><sub>2</sub>, and any bit from any future state can be expressed as an XOR combination of bits from the initial state <em>S</em><sub>1</sub>, … , <em>S</em><sub>624</sub>. (When you express, say, <em>S</em><sub>228 + 624</sub> = <em>S</em><sub>852</sub> as a function of <em>S</em><sub>625</sub>, <em>S</em><sub>228</sub>, and <em>S</em><sub>229</sub>, you can in turn replace <em>S</em><sub>625</sub> by its expression in terms of <em>S</em><sub>398</sub>, <em>S</em><sub>1</sub>, and <em>S</em><sub>2</sub>.)</p>
<p class="indent">Because there are exactly 624 × 32 = 19,968 bits in the initial state (or 624 32-bit words), any output bit can be expressed as an equation with at most 19,969 terms (19,968 bits plus one constant bit). That’s just about 2.5 kilobytes of data. The converse is also true: bits from the initial state can be expressed as an XOR of output bits.</p>
<h5 class="h5">Linearity Insecurity</h5>
<p class="noindent">We call an XOR combination of bits a <em>linear combination</em>. For example, if <em>X</em>, <em>Y</em>, and <em>Z</em> are bits, then the expression <em>X</em> ⊕ <em>Y</em> ⊕ <em>Z</em> is a linear combination, whereas (<em>X</em> ∧ <em>Y</em>) ⊕ <em>Z</em> is not because there’s an AND (∧). If you flip a bit of <em>X</em> in <em>X</em> ⊕ <em>Y</em> ⊕ <em>Z</em>, then the result changes as well, regardless of the value of the <em>Y</em> and <em>Z</em>. In contrast, if you flip a bit of <em>X</em> in (<em>X</em> ∧ <em>Y</em>) ⊕ <em>Z</em>, the result changes only if <em>Y</em>’s bit at the same position is 1. The upshot is that linear combinations are predictable, because you don’t need to know the value of the bits in order to predict how a change in their value will affect the result.</p>
<p class="indent"><span epub:type="pagebreak" id="page_29"/>For comparison, if the MT algorithm were cryptographically strong, its equations would be <em>nonlinear</em> and would involve not only single bits but also AND-combinations (<em>products</em>) of bits, such as <em>S</em><sub>1</sub><em>S</em><sub>15</sub><em>S</em><sub>182</sub> or <em>S</em><sub>17</sub><em>S</em><sub>256</sub><em>S</em><sub>257</sub><em>S</em><sub>354</sub><em>S</em><sub>498</sub><em>S</em><sub>601</sub>. Although linear combinations of those bits include at most 624 variables, nonlinear combinations allow for up to 2<sup>624</sup> variables. It would be impossible to solve, let alone write down the whole of these equations. (Note that 2<sup>305</sup>, a much smaller number, is the estimated information capacity of the observable universe.)</p>
<p class="indent">The key here is that linear transformations lead to short equations (comparable in size to the number of variables), which are easy to solve, whereas nonlinear transformations give rise to equations of exponential size, which are practically unsolvable. The game of cryptographers is thus to design PRNG algorithms that emulate such complex nonlinear transformations using only a small number of simple operations.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>Linearity is just one of many security criteria. Although necessary, nonlinearity alone does not make a PRNG cryptographically secure.</em></p>
</div>
<h4 class="h4" id="lev2sec26"><em>The Uselessness of Statistical Tests</em></h4>
<p class="noindent">Statistical test suites like TestU01, Diehard, or the National Institute of Standards and Technology (NIST) test suite are one way to test the quality of pseudorandom bits. These tests take a sample of pseudorandom bits produced by a PRNG (say, one megabyte worth), compute some statistics on the distribution of certain patterns in the bits, and compare the results with the typical results obtained for a perfect, uniform distribution. For example, some tests count the number of 1 bits versus the number of 0 bits, or the distribution of 8-bit patterns. But statistical tests are largely irrelevant to cryptographic security, and it’s possible to design a cryptographically weak PRNG that will fool any statistical test.</p>
<p class="indent">When you run statistical tests on randomly generated data, you will usually see a bunch of statistical indicators as a result. These are typically <em>p</em>-values, a common statistical indicator. These results aren’t always easy to interpret, because they’re rarely as simple as passed or failed. If your first results seem abnormal, don’t worry: they may be the result of some accidental deviation, or you may be testing too few samples. To ensure that the results you see are normal, compare them with those obtained for some reliable sample of identical size; for example, one generated with the OpenSSL toolkit using the following command:</p>
<p class="programs">$ <span class="codestrong">openssl rand</span> <span class="codestrong"><em>&lt;number of bytes&gt;</em></span> <span class="codestrong">-out</span> <span class="codestrong"><em>&lt;output file&gt;</em></span></p>
<h3 class="h3" id="lev1sec18">Real-World PRNGs</h3>
<p class="noindent">Let’s turn our attention to how to implement PRNGs in the real world. You’ll find crypto PRNGs in the operating systems (OSs) of most platforms, from desktops and laptops to embedded systems such as routers and set-top <span epub:type="pagebreak" id="page_30"/>boxes, as well as virtual machines, mobile phones, and so on. Most of these PRNGs are software based, but some are pure hardware. Those PRNGs are used by applications running on the OS, and sometimes other PRNGs running on top of cryptographic libraries or applications.</p>
<p class="indent">Next we’ll look at the most widely deployed PRNGs: the one for Linux, Android, and many other Unix-based systems; the one in Windows; and the one in recent Intel microprocessors, which is hardware based.</p>
<h4 class="h4" id="lev2sec27"><em>Generating Random Bits in Unix-Based Systems</em></h4>
<p class="noindent">The device file <em>/dev/urandom</em> is the userland interface to the crypto PRNG of common *nix systems, and it’s what you will typically use to generate reliable random bits. Because it’s a device file, requesting random bits from <em>/dev/urandom</em> is done by reading it as a file. For example, the following command uses <em>/dev/urandom</em> to write 10MB of random bits to a file:</p>
<p class="programs">$ <span class="codestrong">dd if=/dev/urandom of=</span><span class="codestrong"><em>&lt;output file&gt;</em></span> <span class="codestrong">bs=1M count=10</span></p>
<h5 class="h5">The Wrong Way to Use /dev/urandom</h5>
<p class="noindent">You could write a naive and insecure C program like the one shown in <a href="ch02.xhtml#ch2list1">Listing 2-1</a> to read random bits, and hope for the best, but that would be a bad idea.</p>
<p class="programs">int random_bytes_insecure(void *buf, size_t len)<br/>{<br/>    int fd = open("/dev/urandom", O_RDONLY);<br/>    read(fd, buf, len);<br/>    close(fd);<br/>    return 0;<br/>}</p>
<p class="figcap"><a id="ch2list1"/><em>Listing 2-1: Insecure use of /dev/urandom</em></p>
<p class="indent">This code is insecure; it doesn’t even check the return values of <span class="literal">open()</span> and <span class="literal">read()</span>, which means your expected random buffer could end up filled with zeroes, or left unchanged.</p>
<h5 class="h5">A Safer Way to Use /dev/urandom</h5>
<p class="noindent"><a href="ch02.xhtml#ch2list2">Listing 2-2</a>, copied from LibreSSL, shows a safer way to use <em>/dev/urandom</em>.</p>
<p class="programs">int random_bytes_safer(void *buf, size_t len)<br/>{<br/>    struct stat st;<br/>    size_t i;<br/>    int fd, cnt, flags;<br/>    int save_errno = errno;<br/><br/>start:<br/>    flags = O_RDONLY;<br/>#ifdef O_NOFOLLOW<br/>    flags |= O_NOFOLLOW;<br/>#endif<br/>#ifdef O_CLOEXEC<br/>    flags |= O_CLOEXEC;<br/>#endif<br/>    fd = <span class="ent">❶</span>open("/dev/urandom", flags, 0);<br/>    if (fd == -1) {<br/>        if (errno == EINTR)<br/>            goto start;<br/>        goto nodevrandom;<br/>    }<br/>#ifndef O_CLOEXEC<br/>    fcntl(fd, F_SETFD, fcntl(fd, F_GETFD) | FD_CLOEXEC);<br/>#endif<br/>    /* Lightly verify that the device node looks sane */<br/>    if (fstat(fd, &amp;st) == -1 || !S_ISCHR(st.st_mode)) {<br/>        close(fd);<br/>        goto nodevrandom;<br/>    }<br/>    if (ioctl(fd, RNDGETENTCNT, &amp;cnt) == -1) {<br/>        close(fd);<br/>        goto nodevrandom;<br/>    }<br/>    for (i = 0; i &lt; len; ) {<br/>        size_t wanted = len - i;<br/>        ssize_t ret = <span class="ent">❷</span>read(fd, (char *)buf + i, wanted);<br/>        if (ret == -1) {<br/>            if (errno == EAGAIN || errno == EINTR)<br/>                continue;<br/>            close(fd);<br/>            goto nodevrandom;<br/>        }<br/>        i += ret;<br/>    }<br/>    close(fd);<br/>    if (gotdata(buf, len) == 0) {<br/>        errno = save_errno;<br/>        return 0;                		/* satisfied */<br/>    }<br/>nodevrandom:<br/>    errno = EIO;<br/>    return -1;<br/>}</p>
<p class="figcap"><span epub:type="pagebreak" id="page_31"/><a id="ch2list2"/><em>Listing 2-2: Safe use of /dev/urandom</em></p>
<p class="indent">Unlike <a href="ch02.xhtml#ch2list1">Listing 2-1</a>, <a href="ch02.xhtml#ch2list2">Listing 2-2</a> makes several sanity checks. Compare, for example, the call to <span class="literal">open()</span> at <span class="ent">❶</span> and the call to <span class="literal">read()</span> at <span class="ent">❷</span> with those in <a href="ch02.xhtml#ch2list1">Listing 2-1</a>: you’ll notice that the safer code checks the return values of those functions, and upon failure closes the file descriptor and returns –1.</p>
<h5 class="h5"><span epub:type="pagebreak" id="page_32"/>Differences Between /dev/urandom and /dev/random on Linux</h5>
<p class="noindent">Different Unix versions use different PRNGs. The Linux PRNG, defined in <em>drivers/char/random.c</em> in the Linux kernel, mainly uses the hash function SHA-1 to turn raw entropy bits into reliable pseudorandom bits. The PRNG harvests entropy from various sources (including the keyboard, mouse, disk, and interrupt timings) and has a primary entropy pool of 512 bytes, as well as a non-blocking pool for <em>/dev/urandom</em> and a blocking pool for <em>/dev/random</em>.</p>
<p class="indent">What’s the difference between <em>/dev/urandom</em> and <em>/dev/random</em>? The short story is that <em>/dev/random</em> attempts to estimate the amount of entropy and refuses to return bits if the level of entropy is too low. Although this may sound like a good idea, it’s not. For one thing, entropy estimators are notoriously unreliable and can be fooled by attackers (which is one reason why Fortuna ditched Yarrow’s entropy estimation). Furthermore, <em>/dev/random</em> runs out of estimated entropy pretty quickly, which can produce a denial-of-service condition, slowing applications that are forced to wait for more entropy. The upshot is that in practice, <em>/dev/random</em> is no better than <em>/dev/urandom</em> and creates more problems than it solves.</p>
<h5 class="h5">Estimating the Entropy of /dev/random</h5>
<p class="noindent">You can observe how <em>/dev/random</em>’s entropy estimate evolves by reading its current value in bits in <em>/proc/sys/kernel/random/entropy_avail</em> on Linux. For example, the shell script shown in <a href="ch02.xhtml#ch2list3">Listing 2-3</a> first minimizes the entropy estimate by reading 4KB from <em>/dev/random</em>, waits until it reaches an estimate of 128 bits, reads 64 bits from <em>/dev/random</em>, and then shows the new estimate. When running the script, notice how user activity accelerates entropy recovery (bytes read are printed to stdout encoded in base64).</p>
<p class="programs">#!/bin/sh<br/>ESTIMATE=/proc/sys/kernel/random/entropy_avail<br/>timeout 3s dd if=/dev/random bs=4k count=1 2&gt; /dev/null | base64<br/>ent=`cat $ESTIMATE`<br/>while [ $ent -lt 128 ]<br/>do<br/>    sleep 3<br/>    ent=`cat $ESTIMATE`<br/>    echo $ent<br/>done<br/>dd if=/dev/random bs=8 count=1 2&gt; /dev/null | base64<br/>cat $ESTIMATE</p>
<p class="figcap"><a id="ch2list3"/><em>Listing 2-3: A script showing the evolution of</em> /dev/urandom’s entropy estimate</p>
<p class="indent">A sample run of <a href="ch02.xhtml#ch2list3">Listing 2-3</a> gave the output shown in <a href="ch02.xhtml#ch2list4">Listing 2-4</a>. (Guess when I started randomly moving the mouse and hitting the keyboard to gather entropy.)</p>
<p class="programs">xFNX/f2R87/zrrNJ6Ibr5R1L913tl+F4GNzKb60BC+qQnHQcyA==<br/>2<br/>18<br/>19<br/>27<br/>28<br/>72<br/>124<br/>193<br/>jq8XWCt8<br/>129</p>
<p class="figcap"><span epub:type="pagebreak" id="page_33"/><a id="ch2list4"/><em>Listing 2-4: A sample execution of the entropy estimate evolution script in <a href="ch02.xhtml#ch2list3">Listing 2-3</a></em></p>
<p class="indent">As you can see in <a href="ch02.xhtml#ch2list4">Listing 2-4</a>, we have 193 − 64 = 129 bits of entropy left in the pool, as per <em>/dev/random</em>’s estimator. Does it make sense to consider a PRNG as having <em>N</em> less entropy bits just because <em>N</em> bits were just read from the PRNG? (Spoiler: it does not.)</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>Like</em> /dev/random, <em>Linux’s <span class="literalital">getrandom()</span> system call blocks if it hasn’t gathered enough initial entropy. However, unlike</em> /dev/random, <em>it won’t attempt to estimate the entropy in the system and will never block after its initialization stage. And that’s fine. (You can force <span class="literalital">getrandom()</span> to use</em> /dev/random <em>and to block by tweaking its flags, but I don’t see why you’d want to do that.)</em></p>
</div>
<h4 class="h4" id="lev2sec28"><em>The CryptGenRandom() Function in Windows</em></h4>
<p class="noindent">In Windows, the legacy userland interface to the system’s PRNG is the <span class="literal">CryptGenRandom()</span> function from the Cryptography application programming interface (API). The <span class="literal">CryptGenRandom()</span> function has been replaced in recent Windows versions with the <span class="literal">BcryptGenRandom()</span> function in the Cryptography API: Next Generation (CNG) API. The Windows PRNG takes entropy from the kernel mode driver <em>cng.sys</em> (formerly <em>ksecdd.sys</em>), whose entropy collector is loosely based on Fortuna. As is usually the case in Windows, the process is complicated.</p>
<p class="indent"><a href="ch02.xhtml#ch2list5">Listing 2-5</a> shows a typical C++ invocation of <span class="literal">CryptGenRandom()</span> with the required checks.</p>
<p class="programs">int random_bytes(unsigned char *out, size_t outlen)<br/>{<br/>    static HCRYPTPROV handle = 0; /* only freed when the program ends */<br/>    if(!handle) {<br/>        if(!CryptAcquireContext(&amp;handle, 0, 0, PROV_RSA_FULL,<br/>                            CRYPT_VERIFYCONTEXT | CRYPT_SILENT)) {<br/>            return -1;<br/>        }<br/>    }<br/>    while(outlen &gt; 0) {<br/>        const DWORD len = outlen &gt; 1048576UL ? 1048576UL : outlen;<br/>        if(!CryptGenRandom(handle, len, out)) {<br/>            return -2;<br/>        }<br/>        out    += len;<br/>        outlen -= len;<br/>    }<br/>    return 0;<br/>}</p>
<p class="figcap"><span epub:type="pagebreak" id="page_34"/><a id="ch2list5"/><em>Listing 2-5: Using the Windows <span class="literalcaption">CryptGenRandom()</span> PRNG interface</em></p>
<p class="indent">Notice in <a href="ch02.xhtml#ch2list5">Listing 2-5</a> that prior to calling the actual PRNG, you need to declare a <em>cryptographic service provider</em> (<span class="literal">HCRYPTPROV</span>) and then acquire a <em>cryptographic context</em> with <span class="literal">CryptAcquireContext()</span>, which increases the chances of things going wrong. For instance, the final version of the TrueCrypt encryption software was found to call <span class="literal">CryptAcquireContext()</span> in a way that could silently fail, leading to suboptimal randomness without notifying the user. Fortunately, the newer <span class="literal">BCryptGenRandom()</span> interface for Windows is much simpler and doesn’t require the code to explicitly open a handle (or at least makes it much easier to use without a handle).</p>
<h4 class="h4" id="lev2sec29"><em>A Hardware-Based PRNG: RDRAND in Intel Microprocessors</em></h4>
<p class="noindent">We’ve discussed only software PRNGs so far, so let’s have a look at a hardware one. The <em>Intel Digital Random Number Generator</em> is a hardware PRNG introduced in 2012 in Intel’s Ivy Bridge microarchitecture, and it’s based on NIST’s SP 800-90 guidelines with the Advanced Encryption Standard (AES) in CTR_DRBG mode. Intel’s PRNG is accessed through the <span class="literal">RDRAND</span> assembly instruction, which offers an interface independent of the operating system and is in principle faster than software PRNGs.</p>
<p class="indent">Whereas software PRNGs try to collect entropy from unpredictable sources, <span class="literal">RDRAND</span> has a single entropy source that provides a serial stream of entropy data as zeroes and ones. In hardware engineering terms, this entropy source is a dual differential jamb latch with feedback; essentially, a small hardware circuit that jumps between two states (0 or 1) depending on thermal noise fluctuations, at a frequency of 800 MHz. This kind of thing is usually pretty reliable.</p>
<p class="indent">The <span class="literal">RDRAND</span> assembly instruction takes as an argument a register of 16, 32, or 64 bits and then writes a random value. When invoked, <span class="literal">RDRAND</span> sets the carry flag to 1 if the data set in the destination register is a valid random value, and to 0 otherwise, which means you should be sure to check the <span class="literal">CF</span> flag if you write assembly code directly. Note that the C intrinsics available in common compilers don’t check the <span class="literal">CF</span> flag but do return its value.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>Intel’s PRNG framework provides an assembly instruction other than</em> <span class="literalital">RDRAND</span><em>: the</em> <span class="literalital">RDSEED</span> <em>assembly instruction returns random bits directly from the entropy source, after some conditioning or cryptographic processing. It’s intended to be able to seed other PRNGs.</em></p>
</div>
<p class="indent">Intel’s PRNG is only partially documented, but it’s built on known standards, and has been audited by the well-regarded company Cryptography Research (see their report titled “Analysis of Intel’s Ivy Bridge Digital Random Number Generator”). Nonetheless, there have been some concerns about its security, especially following Snowden’s revelations about <span epub:type="pagebreak" id="page_35"/>cryptographic backdoors, and PRNGs are indeed the perfect target for sabotage. If you’re concerned but still wish to use <span class="literal">RDRAND</span> or <span class="literal">RDSEED</span>, just mix them with other entropy sources. Doing so will prevent effective exploitation of a hypothetical backdoor in Intel’s hardware or in the associated microcode in all but the most far-fetched scenarios.</p>
<h3 class="h3" id="lev1sec19">How Things Can Go Wrong</h3>
<p class="noindent">To conclude, I’ll present a few examples of randomness failures. There are countless examples to choose from, but I’ve chosen four that are simple enough to understand and illustrate different problems.</p>
<h4 class="h4" id="lev2sec30"><em>Poor Entropy Sources</em></h4>
<p class="noindent">In 1996, the SSL implementation of the Netscape browser was computing 128-bit PRNG seeds according to the pseudocode shown in <a href="ch02.xhtml#ch2list6">Listing 2-6</a>, copied from Goldberg and Wagner’s page at <em><a href="http://www.cs.berkeley.edu/~daw/papers/ddj-netscape.html">http://www.cs.berkeley.edu/~daw/papers/ddj-netscape.html</a></em>.</p>
<p class="programs">   global variable seed;<br/><br/>   RNG_CreateContext()<br/>       (seconds, microseconds) = time of day; /* Time elapsed since 1970 */<br/>       pid = process ID;  ppid = parent process ID;<br/>       a = mklcpr(microseconds);<br/>    <span class="ent">➊</span> b = mklcpr(pid + seconds + (ppid &lt;&lt; 12));<br/>      seed = MD5(a, b); /* Derivation of a 128-bit value using the hash MD5 */<br/><br/>   mklcpr(x) /* not cryptographically significant; shown for completeness */<br/>       return ((0xDEECE66D * x + 0x2BBB62DC) &gt;&gt; 1);<br/><br/>   MD5() /* a very good standard mixing function, source omitted */</p>
<p class="figcap"><a id="ch2list6"/><em>Listing 2-6: Pseudocode of the Netscape browser’s generation of 128-bit PRNG seeds</em></p>
<p class="indent">The problem here is that the PIDs and microseconds are guessable values. Assuming that you can guess the value of <span class="literal">seconds</span>, <span class="literal">microseconds</span> has only 10<sup>6</sup> possible values and thus an entropy of log(10<sup>6</sup>), or about 20 bits. The process ID (PID) and parent process ID (PPID) are 15-bit values, so you’d expect 15 + 15 = 30 additional entropy bits. But if you look at how <span class="literal">b</span> is computed at <span class="ent">❶</span>, you’ll see that the overlap of three bits yields an entropy of only about 15 + 12 = 27 bits, for a total entropy of only 47 bits, whereas a 128-bit seed should have 128 bits of entropy.</p>
<h4 class="h4" id="lev2sec31"><em>Insufficient Entropy at Boot Time</em></h4>
<p class="noindent">In 2012, researchers scanned the whole internet and harvested public keys from TLS certificates and SSH hosts. They found that a handful of systems had identical public keys, and in some cases very similar keys (namely, RSA <span epub:type="pagebreak" id="page_36"/>keys with shared prime factors): in short, two numbers, <em>n</em> = <em>pq</em> and <em>n′</em> = <em>p′q′</em>, with <em>p</em> = <em>p′</em>, whereas normally all <em>p</em>s and <em>q</em>s should be different in distinct modulus values.</p>
<p class="indent">After further investigation, it turned out that many devices generated their public key early, at first boot, before having collected enough entropy, despite using an otherwise decent PRNG (typically <em>/dev/urandom</em>). PRNGs in different systems ended up producing identical random bits due to a same base entropy source (for example, a hardcoded seed).</p>
<p class="indent">At a high level, the presence of identical keys is due to key-generation schemes like the following, in pseudocode:</p>
<p class="programs">prng.seed(seed)<br/>p = prng.generate_random_prime()<br/>q = prng.generate_random_prime()<br/>n = p*q</p>
<p class="indent">If two systems run this code given an identical seed, they’ll produce the same <em>p</em>, the same <em>q</em>, and therefore the same <em>n</em>.</p>
<p class="indent">The presence of shared primes in different keys is due to key-generation schemes where additional entropy is injected during the process, as shown here:</p>
<p class="programs">prng.seed(seed)<br/>p = prng.generate_random_prime()<br/>prng.add_entropy()<br/>q = prng.generate_random_prime()<br/>n = p*q</p>
<p class="indent">If two systems run this code with the same seed, they’ll produce the same <em>p</em>, but the injection of entropy through <span class="literal">prng.add_entropy()</span> will ensure distinct <em>q</em>s.</p>
<p class="indent">The problem with shared prime factors is that given <em>n</em> = <em>pq</em> and <em>n′</em> = <em>pq′</em>, it’s trivial to recover the shared <em>p</em> by computing the <em>greatest common divisor</em> (GCD) of <em>n</em> and <em>n′</em>. For the details, see the paper “Mining Your Ps and Qs” by Heninger, Durumeric, Wustrow, and Halderman, available at <em><a href="https://factorable.net/">https://factorable.net/</a></em>.</p>
<h4 class="h4" id="lev2sec32"><em>Non-cryptographic PRNG</em></h4>
<p class="noindent">Earlier we discussed the difference between crypto and non-crypto PRNGs and why the latter should never be used for crypto applications. Alas, many systems overlook that detail, so I thought I should give you at least one such example.</p>
<p class="noindent">The popular MediaWiki application runs on Wikipedia and many other wikis. It uses randomness to generate things like security tokens and temporary passwords, which of course should be unpredictable. Unfortunately, a now obsolete version of MediaWiki used a non-crypto PRNG, the Mersenne Twister, to generate these tokens and passwords. Here’s a snippet from the <span epub:type="pagebreak" id="page_37"/>vulnerable MediaWiki source code. Look for the function called to get a random bit, and be sure to read the comments.</p>
<p class="programs">        /**<br/>         * Generate a hex-y looking random token for various uses.<br/>         * Could be made more cryptographically sure if someone cares.<br/>         * @return string<br/>         */<br/>function generateToken( $salt = '' ) {<br/>    $token = dechex(mt_rand()).dechex(mt_rand());<br/>    return md5( $token . $salt );<br/>}</p>
<p class="indent">Did you notice <span class="literal">mt_rand()</span> in the preceding code? Here, <span class="literal">mt</span> stands for Mersenne Twister, the non-crypto PRNG discussed earlier. In 2012, researchers showed how to exploit the predictability of Mersenne Twister to predict future tokens and temporary passwords, given a couple of security tokens. MediaWiki was patched in order to use a crypto PRNG.</p>
<h4 class="h4" id="lev2sec33"><em>Sampling Bug with Strong Randomness</em></h4>
<p class="noindent">The next bug shows how even a strong crypto PRNG with sufficient entropy can produce a biased distribution. The chat program Cryptocat was designed to offer secure communication. It used a function that attempted to create a uniformly distributed string of decimal digits—namely, numbers in the range 0 through 9. However, just taking random bytes modulo 10 doesn’t yield a uniform distribution, because when taking all numbers between 0 and 255 and reducing them modulo 10, you don’t get an equal number of values in 0 to 9.</p>
<p class="indent">Cryptocat did the following to address that problem and obtain a uniform distribution:</p>
<p class="programs">Cryptocat.random = function() {<br/>    var x, o = '';<br/>    while (o.length &lt; 16) {<br/>         x = state.getBytes(1);<br/>         if (x[0] &lt;= 250) {<br/>             o += x[0] % 10;<br/>         }<br/>     }<br/>    return parseFloat('0.' + o)<br/>}</p>
<p class="indent">And that was almost perfect. By taking only the numbers up to a multiple of 10 and discarding others, you’d expect a uniform distribution of the digits 0 through 9. Unfortunately, there was an off-by-one error in the <span class="literal">if</span> condition. I’ll leave the details to you as an exercise. You should find that the values generated had an entropy of 45 instead of approximately 53 bits (hint: <span class="literal">&lt;=</span> should have been <span class="literal">&lt;</span> instead).</p>
<h3 class="h3" id="lev1sec20"><span epub:type="pagebreak" id="page_38"/>Further Reading</h3>
<p class="noindent">I’ve just scratched the surface of randomness in cryptography in this chapter. There is much more to learn about the theory of randomness, including topics such as different entropy notions, randomness extractors, and even the power of randomization and derandomization in complexity theory. To learn more about PRNGs and their security, read the classic 1998 paper “Cryptanalytic Attacks on Pseudorandom Number Generators” by Kelsey, Schneier, Wagner, and Hall. Then look at the implementation of PRNGs in your favorite applications and try to find their weaknesses. (Search online for “random generator bug” to find plenty of examples.)</p>
<p class="indent">We’re not done with randomness, though. We’ll encounter it again and again throughout this book, and you’ll discover the many ways it helps to construct secure systems.</p>
</body></html>