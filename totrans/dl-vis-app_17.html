<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="351" id="Page_351"/>14</span><br/>
<span class="ChapterTitle">Backpropagation</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">As we’ve seen, a neural network is just a collection of neurons, each doing its own little calculation and then passing on its results to other neurons. How can we train such a thing to produce the results we want? And how can we do it efficiently?</p>
<p>The answer is <em>backpropagation</em>, or simply <em>backprop</em>. Without backprop, we wouldn’t have today’s widespread use of deep learning because we wouldn’t be able to train big networks in reasonable amounts of time. Every modern deep learning library provides a stable and efficient implementation of backprop. Even though most people will never implement backprop, it’s important to understand the algorithm because so much of deep learning depends on it.</p>
<p>Most introductions to backprop are presented mathematically, as a collection of equations with associated discussion (Fullér 2010). As usual, we skip the mathematics here and focus instead on the concepts. The middle of this chapter, where we discuss the core of backprop, is the most detailed part of this book. You might want to read it lightly the first time to get the big <span epub:type="pagebreak" title="352" id="Page_352"/>picture for what’s going on and how the pieces fit together. Then, if you like, you can come back and take it more slowly, following the individual steps.</p>
<h2 id="h1-500723c14-0001">A High-Level Overview of Training </h2>
<p class="BodyFirst">Networks learn by minimizing their mistakes. The process begins with a number called a <em>cost</em>, <em>loss</em>, or <em>penalty</em>, for each mistake. During training, the network reduces the cost, resulting in outputs closer to what we want. </p>
<h3 id="h2-500723c14-0001">Punishing Error</h3>
<p class="BodyFirst">Suppose we have a classifier that identifies each input as one of five classes, numbered 1 to 5. The class that has the largest value is the network’s prediction for each input’s class. Our classifier is brand-new and has had no training, so all of the weights have small random values. <a href="#figure14-1" id="figureanchor14-1">Figure 14-1</a> shows the network classifying its first input sample.</p>
<figure>
<img src="Images/F14001.png" alt="F14001" width="483" height="340"/>
<figcaption><p><a id="figure14-1">Figure 14-1</a>: A neural network processing a sample and assigning it to class 1. We want it to be assigned to class 3.</p></figcaption>
</figure>
<p>In this example, the network has decided that the sample belongs to class 1 because the largest output, 0.35, is from output 1 (we’re assuming we have a softmax layer at the end of the network, so the outputs add up to 1). Unfortunately, the sample was labeled as belonging to class 3. We shouldn’t have expected the right answer. The network can easily have thousands, or even millions, of weights, and they currently all have their initial, random values. Therefore, the outputs are just random values as well. Even if the network had predicted class 3 for this sample, it would have been pure luck. </p>
<p>When a prediction doesn’t match that sample’s label, we can come up with a single number to tell us just how wrong this answer is. For example, if class 3 were to get almost the largest score, we say the network would be more correct (or less wrong) relative to assigning class 3 the smallest score. We call this number describing the mismatch between the label and the prediction the <em>error score</em>, or <em>error</em>, or sometimes the <em>penalty</em>, or <em>loss</em> (if the word <em/><span epub:type="pagebreak" title="353" id="Page_353"/>loss seems like a strange synonym for “error,” it may help to think of it as describing how much information is “lost” because we have a wrong answer). </p>
<p>The error (or loss) is a floating-point number that can take on any value, though often we set things up so that it’s always positive. The larger the error, the more “wrong” our network’s prediction is for the label of this input. An error of zero means that the network predicted the sample’s label correctly. In a perfect world, we’d get the error down to zero for every sample in the training set. In practice, we usually settle for getting as close as we can.</p>
<p>Although in this chapter we focus on reducing the error on specific samples (or groups of samples), our overall goal is to minimize the total error for the entire training set, which is usually just the sum of the individual errors.</p>
<p>The way we choose to determine the error gives us tremendous flexibility in guiding the network’s learning process. The thinking can seem a little backward, however, because the error tells the network what <em>not </em>to do. It’s like the apocryphal quote about sculpting: to carve an elephant, you simply start with a block of stone and chip away everything that doesn’t look like an elephant (Quote Investigator 2020).</p>
<p>In our case, we start with an initialized network and then use the error term to chip away all the behavior we don’t want. In other words, we don’t really teach the network to find the correct answers. Instead, we penalize incorrect answers by assigning them a positive amount of error. The only way the network can reduce the overall error is to avoid incorrect answers, so that’s what it learns to do. This is a powerful idea: to get the behavior we want, we penalize the behavior we don’t want.</p>
<p>If we want to penalize several things at once, we compute a <em>value</em>, or <em>term</em>, for each one and add them up to get the total error. For instance, we might want our classifier to predict the correct class <em>and</em> assign it a score that is at least twice as large as the score for the next-closest class. We can compute numbers representing both desires and use their sum as our error term. The only way for the network to drive the error down to zero (or as close to zero as it can get) is to change its weights to achieve both goals.</p>
<p>A popular error term comes from the observation that learning is often the most efficient when the weights in the network are all in a small range, such as [–1, 1]. To enforce this, we can include an error term that has a large value when the weights get too far from this range. This is called <em>regularization</em>. In order to minimize the error, the network learns to keep the weights small.</p>
<p>All of this raises the natural question of how on earth the network is able to accomplish the goal of minimizing the error. That’s the point of this chapter.</p>
<p>To keep things simple, we’ll use an error measure with just one term that punishes a mismatch between the network’s prediction and the label. Everything we see in the rest of this chapter works identically when there are more terms in the error. Our first algorithm for teaching the network is just a thought experiment since it would be absurdly slow on today’s computers. But the ideas resulting from this experiment form the conceptual basis for the more efficient techniques we discuss later in this chapter.</p>
<h3 id="h2-500723c14-0002"><span epub:type="pagebreak" title="354" id="Page_354"/>A Slow Way to Learn</h3>
<p class="BodyFirst">Let’s stick with our running example of a classifier trained with supervised learning. We’ll give the network a sample and compare the system’s prediction with the sample’s label. If the network gets it right and predicts the correct label, we won’t change anything and we’ll move on to the next sample (as the proverb goes, “If it ain’t broke, don’t fix it” [Seung 2005]). But if the result for a particular sample is incorrect, we’ll try to improve things. </p>
<p>Let’s make this improvement in a simple way. We’ll pick one weight at random from the whole network and <em>freeze</em> all the other values so they can’t change. We already know the error associated with the weight’s current value, so we create a small random value centered around zero, which we’ll call <em>m,</em> add that to that weight, and reevaluate the same sample again. This change to one weight causes a ripple effect through the rest of the network, as every neuron that depends on a computation involving that neuron’s output also changes. The result is a new set of predictions, and thus a new error for that sample.  </p>
<p>If the new error is less than the previous error, then we’ve made things better and we keep this change. If the results didn’t get better, then we need to undo the change. Now we pick another weight at random, modify it by another random amount, reevaluate the network to see if we want to keep that change, pick another weight, modify it, and so on, again and again. </p>
<p>We can continue nudging weights until the results improve by a certain amount, we decide we’ve tried enough times, or we decide to stop for any other reason. At this point, we select the next sample and tune lots of weights again. When we’ve used all the samples in our training set, we just go through them all again (maybe in a different order), over and over. The idea is that each little improvement brings us closer to a network that accurately predicts the label for every sample.  </p>
<p>With this technique, we expect the network to slowly improve, though there may be setbacks along the way. For example, later samples may cause changes that ruin the improvements we just made for earlier samples. </p>
<p>Given enough time and resources, we expect that the network will eventually improve to the point where it’s predicting every sample as well as it can. The important word in that last sentence is <em>eventually</em>. As in, “The water will boil, eventually,” or “The Andromeda galaxy will collide with our Milky Way galaxy, eventually” (NASA 2012). Although the concepts are right, this technique is definitely not practical. Modern networks can have millions of weights. Trying to find the best values for all those weights with this algorithm is just not realistic.</p>
<p>Our goal in the rest of this chapter is to take this rough idea and restructure it into a vastly more practical algorithm.</p>
<p>Before we move on, it’s worth noting that because we’re focusing on weights, we’re automatically adjusting the influence of each neuron’s bias, thanks to the bias trick<b> </b>we saw in Chapter 13. That means we don’t have to think about the bias terms, which makes everything simpler.</p>
<p>Let’s now consider how we might improve our incredibly slow weight-changing algorithm.</p>
<h3 id="h2-500723c14-0003"><span epub:type="pagebreak" title="355" id="Page_355"/>Gradient Descent</h3>
<p class="BodyFirst">The algorithm of the last section improved our network, but at a glacial pace. One big source of inefficiency was that half of our adjustments to the weights were in the wrong direction: we added a value when we should have subtracted it, and vice versa. That’s why we had to undo our changes when the error went up. Another problem is that we tuned each weight one by one, which required us to evaluate an immense number of samples. Let’s solve these problems.</p>
<p>We can double our training speed if we know beforehand whether we want to nudge each weight in a positive or negative direction. We can get exactly that information from the gradient<b> </b>of the error with respect to that weight. Recall that we met the gradient in Chapter 5, where it told us how the height of a surface changes as each of its parameters changes. Let’s narrow that down for the present case. </p>
<p>As before, we’re going to freeze the whole network except for one weight. If we plot the value of that weight on a horizontal axis, we can plot the network’s error for that weight vertically. The errors, taken together, form a curve called the <em>error curve</em>. In this situation, we can find the gradient (or derivative) of the error at any particular value of the weight by finding the slope of the error curve above that weight.</p>
<p>If the gradient directly above the weight is positive (that is, the line goes up as we move to the right), then increasing the value of the weight (moving it to the right) causes the error to go up. Similarly, and more usefully for us, decreasing the value of the weight (moving it to the left) causes the error to go down. If the slope of the error is negative, the situations are reversed.</p>
<p><a href="#figure14-2" id="figureanchor14-2">Figure 14-2</a> shows two examples.</p>
<figure>
<img src="Images/F14002.png" alt="F14002" width="844" height="274"/>
<figcaption><p><a id="figure14-2">Figure 14-2</a>: The gradient tells us what happens to the error (the black curves) if we make a weight smaller or larger, for two different error curves. Each figure shows the gradient at two weights.</p></figcaption>
</figure>
<p>The error curve for every weight in the network is different because every weight has a different effect on the final error. But if we can find the gradient for a specific weight, we’ve solved the problem of guessing whether it needs to increase or decrease in order to reduce the error. If we can find the gradients for all the weights, we can adjust them all at once, rather than one by one. If we can adjust every weight simultaneously, using its own <span epub:type="pagebreak" title="356" id="Page_356"/>specific gradient to tell us whether to make it bigger or smaller, we have an efficient way to improve our network.</p>
<p>This is just what we do. Because we use the gradient to move each weight to produce a lower value on the error curve, we call the algorithm <em>gradient descent</em>.</p>
<p>Before we dig into gradient descent, notice that this algorithm makes the assumption that tweaking all the weights independently and simultaneously after we evaluate an incorrect sample leads to a reduction in the error, not just for that sample, but for the entire training set, and by extension, all data that we see after the network is released. This is a bold assumption because we’ve already noted how changing one weight can cause ripple effects through the rest of the network. Changing the output of one neuron changes the inputs, and thus the outputs, of all neurons that use that value, which in turn changes their gradients. If we’re unlucky, some weights that had a positive gradient might now have a negative gradient, or vice versa. That means if we stick with the gradients we computed, changing those weights makes the error bigger, not smaller. To control this problem, we usually make small changes to every weight in the hopes that any such mistakes won’t drown out our improvements.</p>
<h2 id="h1-500723c14-0002">Getting Started</h2>
<p class="BodyFirst">Let’s reduce the overall error by adjusting the network’s weights in two steps. In the first step, called <em>backpropagation </em>or <em>backprop, </em>we visit each neuron where we calculate and store a number that is related to the network’s error. Once we have this value for every neuron, we use it to update every weight coming into that neuron. This second step is called the <em>update</em> <em>step</em>, or the<em> optimization step</em>. It’s not typically considered part of backpropagation, but sometimes people casually roll the two steps together and call the whole thing backpropagation. This chapter focuses on just the first step. Chapter 15 focuses on optimization.</p>
<p>In this discussion, we’re going to ignore activation functions. Their nonlinear nature is essential to making neural networks work, but that same nature introduces a lot of detail that isn’t relevant to understanding the essence of backprop. Despite this simplification for the point of a clearer discussion, activation functions are definitely accounted for in any implementation of backprop.  </p>
<p>With this simplification in place, we can make an important observation: When any neuron output in our network changes, the final output error changes by a proportional amount.</p>
<p>Let’s unpack that. We only care about two types of values in a neural network: weights (which we can set and change as we please), and neuron outputs (which are computed automatically and are beyond our direct control). Except for the very first layer, a neuron’s input values are each the output of a previous neuron times the weight of the edge that output travels on. Each neuron’s output is just the sum of all of these weighted inputs. Without an activation function, every change in a neuron’s output <span epub:type="pagebreak" title="357" id="Page_357"/>is proportional to the changes in its inputs, or the weights on those inputs. If the inputs themselves are constant, the only way for a neuron’s output to change (and thus affect the final error) is if the weights on its inputs change.</p>
<p>Imagine we’re looking at a neuron whose output has just changed. What happens to the network’s error as a result? Without activation functions, the only operations in our network are multiplication and addition. If we write down the math (which we won’t do), it turns out that the change in the final error is always proportional to the change in the neuron’s output. </p>
<p>The connection between any change<em> </em>in the neuron’s output and the resulting change<em> </em>in the final error is just the neuron’s change multiplied by some number. This number goes by various names, but the most popular is the lowercase Greek letter <em>δ</em> (delta), though sometimes the uppercase version, Δ, is used. Mathematicians often use the delta character to mean “change” of some sort, so this was a natural (if terse) choice of name. </p>
<p>Every neuron has a delta, or <em>δ</em>, associated with it as a result of evaluating the current network with the current sample. This is a real number that can be big or small, positive or negative. Assuming the network’s input doesn’t change and the rest of the network is frozen, if a neuron’s output changes by a particular amount, we can multiply that change by the neuron’s delta to see how the entire network’s output will change.</p>
<p>To illustrate the idea, let’s focus just on one neuron’s output for a moment. Let’s add some arbitrary number to its output just before that value emerges. <a href="#figure14-3" id="figureanchor14-3">Figure 14-3</a> shows the idea graphically, where we use the letter <em>m </em>(for “modification”) for this extra value.</p>
<figure>
<img src="Images/F14003.png" alt="F14003" width="694" height="373"/>
<figcaption><p><a id="figure14-3">Figure 14-3</a>: Computing the change in the network’s final error due to a change in a neuron’s output</p></figcaption>
</figure>
<p>Because the output will change by <em>m</em>, we know the change in the final error is <em>m</em> times the neuron’s <em>δ</em>.</p>
<p>In <a href="#figure14-3">Figure 14-3</a>, we changed the output directly by placing the value <em>m </em>inside the neuron. Alternatively, we can cause a change in the output by <span epub:type="pagebreak" title="358" id="Page_358"/>changing one of the inputs. Let’s change the value that’s coming in from any other neuron. The same logic holds as for <a href="#figure14-3">Figure 14-3</a> and is shown in <a href="#figure14-4" id="figureanchor14-4">Figure 14-4</a>. We can instead add <em>m </em>to the value coming in from neurons A or C if we prefer; all that matters is that the output of D changes by <em>m</em>. Since we’re still just changing the output by <em>m</em>, we find the change in the final error by multiplying it by the same value of <em>δ</em> as in <a href="#figure14-3">Figure 14-3</a>.</p>
<figure>
<img src="Images/F14004.png" alt="F14004" width="689" height="436"/>
<figcaption><p><a id="figure14-4">Figure 14-4</a>: A variation of <a href="#figure14-3">Figure 14-3</a>, where we add <em>m</em> to the output of B (after it has been multiplied by the weight BD)</p></figcaption>
</figure>
<p><a href="#figure14-3">Figure 14-3</a> and <a href="#figure14-4">Figure 14-4</a> illustrate that the change in the network’s final output can be predicted from either a change to any neuron’s output or any weight in the network. </p>
<p>We can use the delta associated with each neuron to tell us whether each of its incoming weights should be nudged in the positive or negative direction.</p>
<p>Let’s walk through an example. </p>
<p>This is where things start to get detailed. The basic idea is that the error will give us a gradient for every weight, and then we can use this gradient to adjust each weight by a little bit so that the overall error decreases. The mechanics for this aren’t super complicated, but there are some new ideas, some new names, and a bunch of details to keep straight. If it feels like too much to take in, you might want to skim this part of the chapter on first reading (up to, say, the section “Backprop on a Larger Network”), and return here later for a more complete understanding of the process. </p>
<h2 id="h1-500723c14-0003">Backprop on a Tiny Neural Network</h2>
<p class="BodyFirst">To get a handle on backprop, we’ll use a tiny network that classifies 2D points into two categories, which we’ll call class 1 and class 2. If the points <span epub:type="pagebreak" title="359" id="Page_359"/>can be separated by a straight line, then we can do this job with just one neuron, but let’s use a little network because it lets us see the general principles. Let’s begin by looking at the network and giving a label to everything we care about. This will make later discussions simpler and easier to follow. <a href="#figure14-5" id="figureanchor14-5">Figure 14-5</a> shows our little network, along with a name for each of its eight weights. For simplicity, we’ll leave out the usual softmax step after neurons C and D.</p>
<figure>
<img src="Images/F14005.png" alt="F14005" width="450" height="189"/>
<figcaption><p><a id="figure14-5">Figure 14-5</a>: A simple neural network with four neurons</p></figcaption>
</figure>
<p>Finally, we want to refer to the output and delta for every neuron. For this, let’s make little two-letter names by combining the neuron’s name with the value we want to refer to. So <em>Ao </em>and <em>Bo </em>are the names of the outputs of neurons A and B, and <em>Aδ </em>and <em>Bδ </em>are the delta values for those two neurons.</p>
<p><a href="#figure14-6" id="figureanchor14-6">Figure 14-6</a> shows these values stored with their neurons.</p>
<figure>
<img src="Images/F14006.png" alt="F14006" width="600" height="183"/>
<figcaption><p><a id="figure14-6">Figure 14-6</a>: Our simple network with the output and delta values for each neuron</p></figcaption>
</figure>
<p>We can watch what happens when neuron outputs change, causing changes to the error. Let’s label the change in the output of neuron A due to a change by an amount <em>m </em>as <em>Am</em>, the network’s final error as <em>E</em>, and the resulting change to the error as <em>Em</em>.</p>
<p>Now we can be more precise about what happens to the error when a neuron’s output changes. If we have a change <em>Am </em>in the output of neuron A, then multiplying that change by <em>Aδ </em>gives us the change in the error. That is, the change <em>Em </em>is given by <em>Am </em>× <em>Aδ</em>. We think of the action of <em>Aδ </em>as multiplying, or scaling, the change in the output of neuron A, giving us the corresponding change in the error. <a href="#figure14-7" id="figureanchor14-7">Figure 14-7</a> shows the schematic setup we use in this chapter for visualizing the way changes in a neuron’s output are scaled by its delta to produce changes to the error.</p>
<span epub:type="pagebreak" title="360" id="Page_360"/><figure>
<img src="Images/F14007.png" alt="F14007" width="844" height="423"/>
<figcaption><p><a id="figure14-7">Figure 14-7</a>: Our schematic for visualizing how changes in a neuron’s output can change the network’s error</p></figcaption>
</figure>
<p>At the left of <a href="#figure14-7">Figure 14-7</a> we start with<em> </em>neuron A. We see the starting output of A, or <em>Ao</em>, a change in output <em>Am</em>, and its new output <em>Ao</em> +<em> Am</em>. The arrow inside the box for <em>Am</em> shows that this change is positive. This change is multiplied by <em>Aδ </em>to give us <em>Em</em>, the change in the error. We show this operation as a wedge, illustrating the amplification of <em>Am</em>. Adding <em>Em</em> to the previous value of the error, <em>E</em>, gives us the new error <em>E </em>+ <em>Em</em>. In this case, both <em>Am </em>and <em>Aδ </em>are positive, so the change in the error <em>Am </em>× <em>Aδ </em>is also positive, increasing the error. When either (but not both) of <em>Am</em> or <em>Aδ</em> is negative, the error decreases.</p>
<p>Now that we’ve labeled everything, we’re finally ready to look at the backpropagation algorithm.</p>
<h3 id="h2-500723c14-0004">Finding Deltas for the Output Neurons</h3>
<p class="BodyFirst">Backpropagation is all about finding the delta value for each neuron. To do that, we find gradients of the error at the end of the network and then propagate, or move, those gradients backward to the start. So, we begin at the end: the output layer. </p>
<h4 id="h3-500723c14-0001">Calculating the Network Error</h4>
<p class="BodyFirst">The outputs of neuron C and D in our tiny network give us the probabilities that the input is in class 1 or class 2, respectively. In a perfect world, a sample that belongs to class 1 would produce a value of 1.0 for <em>P1</em> and 0.0 for <em>P2</em>, meaning that the system is certain that it belongs to class 1 and simultaneously certain that it does not<em> </em>belong to class 2. If the system’s a little less certain, we might get <em>P1</em> = 0.8 and <em>P2</em> = 0.2, telling us that it’s much more likely that the sample is in class 1.</p>
<p><span epub:type="pagebreak" title="361" id="Page_361"/>We want to come up with a single number to represent the network’s error. To do that, we compare the values of <em>P1</em> and <em>P2</em> with the label for this sample. The easiest way to make that comparison is if the label is one-hot encoded, as we saw in Chapter 10. Recall that one-hot encoding makes a list of zeros as long as the number of classes, except for a 1 in the entry corresponding to the correct class. In our case, we have only two classes, so our labels are (1, 0) for a sample in class 1, and (0, 1) for a sample in class 2. Sometimes this form of label is also called a <em>target</em>.</p>
<p>Let’s put the predictions <em>P1</em> and <em>P2</em> into a list as well: (<em>P1</em>, <em>P2</em>). Now we can just compare the lists. We almost always use cross entropy for this, as discussed in Chapter 6. <a href="#figure14-8" id="figureanchor14-8">Figure 14-8</a> shows the idea.</p>
<figure>
<img src="Images/F14008.png" alt="F14008" width="600" height="272"/>
<figcaption><p><a id="figure14-8">Figure 14-8</a>: Finding the error from a sample</p></figcaption>
</figure>
<p>Every deep learning library provides a built-in cross entropy function to help us find the error in a classifier such as this one. In addition to computing the network’s error, the function also provides a gradient to tell us how the error will change if we increase any one of its four inputs. </p>
<p>Using the error gradient, we can look at the value coming out of every neuron in the output layer, and determine if we’d like that value to become more positive or more negative. We will later nudge each neuron in the direction that causes the error to decrease.</p>
<h4 id="h3-500723c14-0002">Drawing Our Error</h4>
<p class="BodyFirst">Let’s look at an error curve. We’ll also draw the gradient with respect to one particular output or weight in the network. Remember that this is just the slope of the error at that point.</p>
<p>Let’s look at how the error varies with changes in the prediction <em>P1</em>, shown in <a href="#figure14-9" id="figureanchor14-9">Figure 14-9</a>. Suppose <em>P1</em> has the value –1.</p>
<p>In <a href="#figure14-9">Figure 14-9</a> we’ve marked the value <em>P1</em> = −1 with an orange dot, and we’ve drawn the derivative at the location on the curve directly above this value of <em>P1</em> with a green line. That derivative (or gradient) tells us that if we make <em>P1</em> more positive (that is, we move right from −1), the error in the network will decrease. </p>
<span epub:type="pagebreak" title="362" id="Page_362"/><figure>
<img src="Images/F14009.png" alt="F14009" width="675" height="475"/>
<figcaption><p><a id="figure14-9">Figure 14-9</a>: How the error depends on different values of <em>P1</em></p></figcaption>
</figure>
<p>If we knew the black curve representing the error, we wouldn’t need the gradient, since we’d just find the curve’s minimum. Unfortunately, the math doesn’t give us the black curve (we’re drawing it here just for reference). But the day is saved because the math gives us enough information to find the curve’s derivative at any location. </p>
<p>The derivative in <a href="#figure14-9">Figure 14-9</a> tells us what happens to the error if we increase or decrease <em>P1</em> by a little bit. After we’ve changed <em>P1</em>, we can find the derivative at its new location and repeat. The derivative, or gradient, accurately predicts the new error after each change to <em>P1</em> as long as we keep that change small. The bigger the change, the less accurate the prediction is.</p>
<p>We can see this characteristic in <a href="#figure14-9">Figure 14-9</a>. Suppose we move <em>P1</em> by one unit to the right from −1. According to the derivative, we now expect an error of 0. But at <em>P1</em> = 0, the error (the value of the black curve) is really about 1. We moved <em>P1</em> too far. In the interests of clear figures that are easy to read, we’ll sometimes make large moves, but in practice, we change our weights by small amounts.</p>
<p>Let’s use the derivative to predict the numerical change in the error due to a change in <em>P1</em>. What’s the slope of the green line in <a href="#figure14-9">Figure 14-9</a>? The left end is at about (−2, 8), and the right end is at about (0, 0). Thus, the line descends about four units for every one unit we move to the right, for a slope of −4/1 or −4. If <em>P1</em> changed by 0.5 (that is, it changed from −1 to −0.5), we’d predict that the error would go down by 0.5 × −4 = −2.</p>
<p>Remember that our goal is to find <em>Cδ</em>. We’ve just done it! <em>P1</em> in this discussion is just another name for <em>Co, </em>the output of neuron C. We’ve found that when P1 = –1, a change of 1 in <em>Co </em>(or <em>P1</em>) would result in a change of <span epub:type="pagebreak" title="363" id="Page_363"/>−4 in the error. As we discussed, we shouldn’t have too much confidence in this prediction after such a big change in <em>P1</em>. But for small moves, the proportion is right. For instance, if we increase <em>P1</em> by 0.01, then we expect the error to change by −4 × 0.01 = −0.04, and for such a small change in <em>P1</em>, the predicted change in the error should be pretty accurate. If we increase <em>P1</em> by 0.02, then we expect the error to change by −4 × 0.02 = −0.08. </p>
<p>The same thinking holds if we decrease the value of <em>P1</em>, or move it to the left. If <em>P1</em> changes from −1 to, say, −1.1, we expect the error to change by −0.1 × −4 = 0.4, so the error would increase by 0.4.</p>
<p>We’ve found that for any amount of change in <em>Co</em>, we can predict the change in the error by multiplying <em>Co </em>by −4. That’s exactly what we’ve been looking for! The value of <em>Cδ </em>is −4. Note that as soon as the value of <em>P1</em> changes, for any reason, the error curve changes and the value of <em>Cδ</em> has to be computed all over again.</p>
<p>We’ve just found our first delta value, which tells us how much the error will change if there’s a change to the output of C. It’s just the derivative of the error function measured at <em>P1</em> (or <em>Co</em>). <a href="#figure14-10" id="figureanchor14-10">Figure 14-10</a> shows all of this visually using our error diagram.</p>
<figure>
<img src="Images/F14010.png" alt="F14010" width="600" height="343"/>
<figcaption><p><a id="figure14-10">Figure 14-10</a>: Our error diagram illustrating the change in the error from a change in the output of neuron C due to a small increase, Cm</p></figcaption>
</figure>
<p>The original output is the green bar at the far left of <a href="#figure14-10">Figure 14-10</a>. We imagine that due to a change in one of the input weights, the output of C increases by an amount <em>Cm</em>. This is amplified by multiplying it by <em>Cδ</em>, which gives us the change in the error, <em>Em</em>. That is, <em>Em</em> = <em>Cm</em> × <em>Cδ</em>. Here the value of <em>Cm</em> is about 1/4 (the upward arrow in the box for <em>Cm</em> tells us that the change is positive), and the value of <em>Cδ</em> is −4 (the arrow in that box tells us the value is negative). So <em>Em </em>= −4 × 1/4 = −1. The new error, at the far right, is the previous error plus <em>Em</em>, or 4 + (−1) = 3.</p>
<p>Remember that at this point, we’re not yet doing anything with this delta value. Our goal right now is just to find the deltas for our neurons. We’ll use them later to change the weights.</p>
<h4 id="h3-500723c14-0003"><span epub:type="pagebreak" title="364" id="Page_364"/>Finding Dδ</h4>
<p class="BodyFirst">Let’s repeat this whole process for <em>P2</em>, to get the value of <em>Dδ</em>, or the delta for neuron D.</p>
<p>Let’s start with a recap of <em>Cδ</em>. On the left of <a href="#figure14-11" id="figureanchor14-11">Figure 14-11</a> we show the error curve for <em>P1</em>. As a result of also moving all of the other weights to better values, the error curve for <em>P1</em> now has a minimum of around 2.</p>
<figure>
<img src="Images/F14011.png" alt="F14011" width="675" height="286"/>
<figcaption><p><a id="figure14-11">Figure 14-11</a>: Left: The error for different values of <em>P1</em>. Right: The error for different values of <em>P2</em>.  </p></figcaption>
</figure>
<p>If we use the new value and error curve for <em>P1</em>, it looks like a change of about 0.5 in <em>P1</em> will result in a change of about −1.5 in the error, so <em>Cδ </em>is about −1.5 / 0.5 = −3. Instead of changing <em>P1</em>, what if we change <em>P2</em>?</p>
<p>Take a look at the graph on the right of <a href="#figure14-11">Figure 14-11</a>. A change of about −0.5 (moving left this time, toward the minimum of the bowl) results in a change of about −1.25 in the error, so <em>Dδ </em>is about 1.25 / −0.5 = 2.5. The positive result here tells us that moving <em>P2</em> to the right causes the error to go up, so we want to move <em>P2</em> to the left.</p>
<p>There are some interesting things to observe here. First, although both curves are bowl shaped, the bottoms of the bowls are at different weight values. Second, because the current values of <em>P1</em> and <em>P2</em> are on opposite sides of the bottom of their respective bowls, their derivatives have opposite signs (one is positive, the other is negative).</p>
<p>The most important observation is that we cannot currently get the error down to 0. In this example, the curves never get lower than about 2. That’s because each curve looks at changing just one value, while the other is fixed. So even if <em>P1</em> got to a value of 1, where its curve is a minimum, there would still be error in the result because <em>P2</em> is not at its ideal value of 0, and vice versa. This means that if we change just one of these two values, we can’t get down to the minimum error of 0. Getting an error of 0 is ideal, but, more generally, our goal is to move each weight, a little bit at a time, until we’ve pushed the error to as small a value as possible. For some networks, we may never be able to get to 0.</p>
<p>Note that we may not want to get the error to 0, even if we could. As we saw in Chapter 9, when a network is overfitting, its training error continues to decrease, but its ability to handle new data gets worse. We really want to minimize the error as much as possible without overfitting. In casual <span epub:type="pagebreak" title="365" id="Page_365"/>discussions, we usually say that we want to get down to zero error, with the understanding that it’s better to stop with some error than to keep training and overfit.</p>
<p>We’ll see later that we can<em> </em>improve all the weights in the network at the same time, as long as we take very small steps. Then we have to evaluate the errors again to find new curves and then new derivatives and deltas before we can make another adjustment. Rather than take many steps after each sample, we usually adjust the weights only once, and then evaluate another sample, adjust the weights once again, and so on.</p>
<h4 id="h3-500723c14-0004">Measuring Error</h4>
<p class="BodyFirst">We mentioned earlier that we often compute the error in a classifier using cross entropy. For this discussion, let’s use a simpler formula that makes it easy to find the delta for each output neuron. This error measure is called the <em>quadratic cost function</em>, or the <em>mean squared error (MSE)</em> (Nielsen 2015). As usual, we won’t get into the mathematics of this equation. We chose it because it lets us find the delta for an output neuron as the difference between the neuron’s value and the corresponding label entry (Seung 2005). <a href="#figure14-12" id="figureanchor14-12">Figure 14-12</a> shows the idea graphically.</p>
<figure>
<img src="Images/F14012.png" alt="F14012" width="328" height="357"/>
<figcaption><p><a id="figure14-12">Figure 14-12</a>: When we use the quadratic cost function, the delta for any output neuron is just the value in the label minus the output of that neuron. As shown in red, we save that delta value with its neuron.</p></figcaption>
</figure>
<p>Remember that <em>Co </em>and <em>P1</em> are two names for the same value, as are <em>Do </em>and <em>P2</em>. </p>
<p>Let’s consider <em>Co </em>(or <em>P1</em>) when the first label is 1. If <em>Co </em>= 1, then the value of <em>Cδ</em> = 1 – <em>Co </em>= 0, so any change in <em>Co</em> gets multiplied by 0, resulting in no change to the output error. </p>
<p>Now suppose that <em>Co </em>= 2. Then the difference is <em>Cδ</em> = 1 – <em>Co </em>= −1, telling us that a change to <em>Co</em> changes the error by the same amount, but with the opposite sign. If <em>Co </em>is much larger, say <em>Co </em>= 5, then 1 – <em>Co </em>= −4, which tells us that any change to <em>Co </em>is amplified by a factor of −4 in the change to <span epub:type="pagebreak" title="366" id="Page_366"/>the error. We’ve been using large numbers for convenience, but remember that the derivative only accurately predicts what happens if we take a very small step.</p>
<p>The same thought process holds for neuron D, and its output <em>Do </em>(or <em>P2</em>).</p>
<p>We’ve now completed the first step in backpropagation: we found the delta values for all the neurons in the output layer. We know from <a href="#figure14-12">Figure 14-12</a> that the delta for an output neuron depends on the value in the label and the neuron’s output. When we change the values of the weights going into that neuron, its delta changes as well. The delta is a temporary value that changes with every change to the network or its inputs. This is another reason we adjust the weights only once per sample. Since we have to recompute all the deltas after each update, we might as well evaluate a new sample first, and make use of the extra information it provides us.</p>
<p>Remember that our big goal is to find changes for the weights. When we know the deltas for all the neurons in a layer, we can update all the weights feeding into that layer. Let’s see how that’s done.</p>
<h3 id="h2-500723c14-0005">Using Deltas to Change Weights</h3>
<p class="BodyFirst">We’ve seen how to find a delta value for every neuron in the output layer. We know that a change to the neuron’s output must come from a change in an input, which in turn can come from either a change in a previous neuron’s output or the weight connecting that output to this neuron. Let’s look at these cases.</p>
<p>For convenience, let’s say that a neuron’s output or a weight is changed by a value of 1. <a href="#figure14-13" id="figureanchor14-13">Figure 14-13</a> shows that every change of 1 in the weight AC, which multiplies the output of neuron A before it’s received by neuron C, leads to a corresponding change of <em>Ao</em> × <em>Cδ</em> in the network error in our network. Subtracting that value leads to a change of <em>–Ao</em> × <em>Cδ</em> in the error. So, if we want to reduce the network’s error by subtracting <em>Ao</em> × <em>Cδ</em> from it, we can change the value of weight AC by –1 to accomplish this. </p>
<figure>
<img src="Images/F14013.png" alt="F14013" width="522" height="368"/>
<figcaption><p><a id="figure14-13">Figure 14-13</a>: When <em>AC</em> changes by 1, the network error changes by <em>Ao</em><em> </em>× <em>C</em><em>δ</em>.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="367" id="Page_367"/>We can summarize this process visually with an additional convention for our diagrams. We’ve been drawing the outputs of neurons as arrows coming out of a circle to the right. Let’s draw deltas using arrows coming out of the circles to the left, as in <a href="#figure14-14" id="figureanchor14-14">Figure 14-14</a>.</p>
<figure>
<img src="Images/F14014.png" alt="F14014" width="294" height="66"/>
<figcaption><p><a id="figure14-14">Figure 14-14</a>: Neuron C has an output <em>Co</em>, drawn with an arrow pointing right, and a delta <em>Cδ</em>, drawn with an arrow pointing left.</p></figcaption>
</figure>
<p>With this convention, the whole process for finding the updated value for weight <em>AC</em>, or <em>AC – (Ao </em>× <em>Cδ),</em> is summarized in <a href="#figure14-15" id="figureanchor14-15">Figure 14-15</a>. Showing subtraction in a diagram like this is hard, because if we have a “minus” node with two incoming arrows, it’s not clear which value is being subtracted from the other (that is, if the inputs are <em>x </em>and <em>y</em>, are we computing <em>x </em>− <em>y </em>or <em>y </em>− <em>x</em>?). To sidestep that problem, we compute <em>AC </em>− (<em>Ao </em>× <em>Cδ</em>) by finding <em>Ao </em>× <em>Cδ</em>, multiplying that by −1, and then adding that result to <em>AC</em>.</p>
<figure>
<img src="Images/F14015.png" alt="F14015" width="590" height="358"/>
<figcaption><p><a id="figure14-15">Figure 14-15</a>: Updating the value of weight AC to the new value <em>AC – </em>(<em>Ao</em> <em>×</em><em> Cδ</em><em>)</em></p></figcaption>
</figure>
<p>Let’s walk through this figure. We start with the output <em>Ao</em> from neuron A and the delta <em>Cδ</em> from output neuron C, and multiply them together (at the top of the figure). We want to subtract this from the current value of <em>AC</em>. To show this clearly in the diagram, we multiply the product by −1 and then add it to the weight <em>AC</em>. The green arrow is the update step, where this result becomes the new value of <em>AC</em>.</p>
<p><a href="#figure14-15">Figure 14-15</a> is big news! We’ve found out how to change the weights coming into the output neurons in order to reduce the network’s error. We can apply this to all four weights going into the output neurons (that is, <em>AC</em>, <em>BC</em>, <em>AD</em>, and <em>BD</em>). We’ve just trained our neural network a little bit by improving four of its weights.</p>
<p><span epub:type="pagebreak" title="368" id="Page_368"/>Sticking with the output layer, if we change the weights for both output neurons C and D to reduce the error by 1 from each neuron, we’d expect the error to go down by −2. We can predict this because the neurons sharing the same layer don’t rely on each other’s outputs. Since C and D are both in the output layer, C doesn’t depend on <em>Do </em>and D doesn’t depend on <em>Co</em>. They do depend on the outputs of neurons on previous layers, but right now we’re just focusing on the effect of changing weights for C and D.</p>
<p>It’s wonderful that we know how to adjust the weights on edges going into the output layer, but how about all the other weights? Our next goal is to figure out the deltas for all the neurons in all the preceding layers. Once we have a delta for every neuron in the network, we can use <a href="#figure14-15">Figure 14-15</a> to adjust every weight in the network to reduce the error.</p>
<p>And this brings us to the remarkable trick of backpropagation: we can use the neuron deltas at one layer to find the neuron deltas for its preceding layer. Let’s see how.</p>
<h3 id="h2-500723c14-0006">Other Neuron Deltas</h3>
<p class="BodyFirst">Now that we have the delta values for the output neurons, we can use them to compute the deltas for neurons on the layer just before the output layer. In our simple model, that layer is the hidden layer containing neurons A and B. Let’s focus for the moment just on neuron A and its connection to neuron C.</p>
<p>What happens if <em>Ao</em>, the output of A, changes for some reason? Let’s say it goes up by <em>Am</em>. <a href="#figure14-16" id="figureanchor14-16">Figure 14-16</a> follows the chain of actions using arbitrary values for <em>AC</em> and <em>Cδ</em>.</p>
<figure>
<img src="Images/F14016.png" alt="F14016" width="694" height="353"/>
<figcaption><p><a id="figure14-16">Figure 14-16</a>: Following the results if we change the output of neuron A</p></figcaption>
</figure>
<p>If we read the diagram in <a href="#figure14-16">Figure 14-16</a> from left to right, the change to A, shown as <em>Am</em>, is multiplied by the weight <em>AC</em> and then added to the values accumulated by neuron C. This raises the output of C by <em>Cm</em>. As we know, this change in C can be multiplied by <em>Cδ</em> to find the change in the network error. </p>
<p><span epub:type="pagebreak" title="369" id="Page_369"/>So now we have a chain of operations from neuron A to neuron C and then to the error. The first step of the chain says that if we multiply the change in <em>Ao</em> (that is, <em>Am</em>) by the weight <em>AC</em>, giving us <em>Am</em> × <em>AC</em>, we get <em>Cm</em>, the change in the output of C. And we know from earlier that if we multiply this value of <em>Cm </em>by <em>Cδ</em>, forming <em>Cm</em> ×<em> Cδ</em>, we get the change in the error.</p>
<p>So, mushing this all together, we find that the change in the error due to a change <em>Am </em>in the output of A is <em>Am </em>× <em>AC </em>× <em>Cδ</em>. We just found the delta for A! It’s just <em>Aδ </em>= <em>AC </em>× <em>Cδ.</em></p>
<p><a href="#figure14-17" id="figureanchor14-17">Figure 14-17</a> shows this visually.</p>
<figure>
<img src="Images/F14017.png" alt="F14017" width="600" height="379"/>
<figcaption><p><a id="figure14-17">Figure 14-17</a>: We can mush together the operations in <a href="#figure14-16">Figure 14-16</a> into a more succinct diagram. </p></figcaption>
</figure>
<p>This is kind of amazing. Neuron C has disappeared. It’s literally out of the picture in <a href="#figure14-17">Figure 14-17</a>. All we needed was its delta, <em>Cδ,</em> and from that we could find <em>Aδ</em>, the delta for A. And now that we know <em>Aδ</em>, we can update all of the weights that feed into neuron A, and then . . . no, wait a second.</p>
<p>We don’t really have <em>Aδ </em>yet. We just have one piece of it.</p>
<p>At the start of this discussion we said we’d focus on neurons A and C, and that was fine. But if we now remember the rest of the network in <a href="#figure14-8">Figure 14-8</a>, we can see that neuron D also uses the output of A. If <em>Ao </em>changes due to <em>Am</em>, then the output of D changes as well, and that also affects the error.</p>
<p>To find the change in the error due to neuron D caused by a change in the output of neuron A, we can repeat the analysis we just went through by just replacing neuron C with neuron D. If <em>Ao </em>changes by <em>Am</em>, and nothing else changes, the change in the error due to the change in D is given by <em>AD </em>× <em>Dδ</em>.</p>
<p><a href="#figure14-18" id="figureanchor14-18">Figure 14-18</a> shows these two outputs from A at the same time. This figure is set up slightly differently from previous figures of this type that we’ve seen earlier. Here, the effect of a change in A on the error due to a change in C is shown by the path from the center of the diagram moving to the right. The effect of a change in A on the error due to a change in D is shown by the path from the center of the diagram and moving left.</p>
<span epub:type="pagebreak" title="370" id="Page_370"/><figure>
<img src="Images/F14018.png" alt="F14018" width="681" height="431"/>
<figcaption><p><a id="figure14-18">Figure 14-18</a>: The output of neuron A is used by both neuron C and neuron D. </p></figcaption>
</figure>
<p><a href="#figure14-18">Figure 14-18</a> shows two separate changes to the error. Since neurons C and D don’t influence each other, their effects on the error are independent. To find the total change to the error, we just add up the two changes. <a href="#figure14-19" id="figureanchor14-19">Figure 14-19</a> shows the result of adding the change in error via neuron C and the change via neuron D.</p>
<figure>
<img src="Images/F14019.png" alt="F14019" width="300" height="341"/>
<figcaption><p><a id="figure14-19">Figure 14-19</a>: When the output of neuron A is used by both neuron C and neuron D, the resulting changes to the error add together.</p></figcaption>
</figure>
<p>Now that we’ve handled all the paths from A to the outputs, we can finally write the value for <em>Aδ</em>. Since the errors add together, as in <a href="#figure14-19">Figure 14-19</a>, we can just add up the factors that scale <em>Am</em>. If we write it out, this is <em>Aδ </em>= (<em>AC </em>× <em>Cδ</em>) + (<em>AD </em>× <em>Dδ</em>).</p>
<p><span epub:type="pagebreak" title="371" id="Page_371"/>Now that we’ve found the value of delta for neuron A, we can repeat the process for neuron B to find its delta.</p>
<p>What we’ve just done is actually far better than finding the delta for just neurons A and B. We’ve found out how to get the value of delta for every<em> </em>neuron in <em>any</em> network, no matter how many layers it has or how many neurons there are! That’s because everything we’ve done involves nothing more than a neuron, the deltas of all the neurons in the next layer that use its value as an input, and the weights that join them. With nothing more than these values, we can find the effect of a neuron’s change on the network’s error, even if the output layer is dozens of layers away.</p>
<p>To summarize this visually, let’s expand on our convention for drawing outputs and deltas as right-pointing and left-pointing arrows to include the weights, as in <a href="#figure14-20" id="figureanchor14-20">Figure 14-20</a>. Let’s say that the weight on a connection multiplies either the output moving to the right, or the delta moving to the left, depending on which step we’re thinking about.</p>
<figure>
<img src="Images/F14020.png" alt="F14020" width="693" height="190"/>
<figcaption><p><a id="figure14-20">Figure 14-20</a>: Drawing the values associated with neuron A. (a) The output <em>Ao</em> is an arrow coming out of the right of the neuron, and the delta <em>Aδ</em> as an arrow coming out of the left. (b) <em>Ao</em> is multiplied by <em>AC</em> on its way to being used by C. (c) <em>Cδ</em> is multiplied by <em>AC</em> on its way to being used by A.</p></figcaption>
</figure>
<p>Here’s a way to think about <a href="#figure14-20">Figure 14-20</a>. There is one connection with one weight joining neurons A and C. If the arrow points to the right, then the weight multiplies <em>Ao</em>, the output of A, as it heads into neuron C. If the arrow points to the left, the weight multiplies <em>Cδ</em>, the delta of C, as it heads into neuron A.</p>
<p>When we evaluate a sample, we use the feed-forward, left-to-right style of flow, where the output value from neuron A to neuron C travels over a connection with weight <em>AC</em>. The result is that the value <em>Ao</em> × <em>AC </em>arrives at neuron C where it’s added to other incoming values, as in <a href="#figure14-20">Figure 14-20</a>(b).</p>
<p>When we later want to compute <em>Aδ</em>, we follow the flow from right to left. Then the delta leaving neuron C travels over a connection with weight <em>AC</em>. The result is that the value <em>Cδ</em> × <em>AC </em>arrives at neuron A where it’s added to other incoming values, as in <a href="#figure14-20">Figure 14-20</a>(c).</p>
<p>Now we can summarize both the processing of a sample input and the computation of the deltas for some arbitrary neuron named H (remember, we’re ignoring the activation function), as in <a href="#figure14-21" id="figureanchor14-21">Figure 14-21</a>. </p>
<span epub:type="pagebreak" title="372" id="Page_372"/><figure>
<img src="Images/F14021.png" alt="F14021" width="689" height="239"/>
<figcaption><p><a id="figure14-21">Figure 14-21</a>: Left: To calculate <em>Ho</em>, we scale the output of each preceding neuron by the weight of its connection and add the results together. Right: To calculate <em>Hδ</em>, we scale the delta of each following neuron by the connection’s weight and add the results together. As usual, we’re ignoring activation functions.</p></figcaption>
</figure>
<p>This is pleasingly symmetrical. It also reveals an important practical result: calculating deltas is often as efficient as calculating output values. Even when the number of incoming connections is different from the number of outgoing connections, the amount of work involved is still close in both directions.</p>
<p>Note that <a href="#figure14-21">Figure 14-21</a> doesn’t require anything of neuron H except that it has inputs from a neighbor layer that travel on connections with weights and deltas. We can apply the left half of <a href="#figure14-21">Figure 14-21</a> and calculate the output of neuron H as soon as the outputs from the previous layer are available. We can apply the right half of <a href="#figure14-21">Figure 14-21</a> and calculate the delta of neuron H as soon as the deltas from the following layer are available.</p>
<p>The dependence of <em>Hδ</em> on the deltas of the following neurons shows why we had to treat the output layer neurons as special cases: there are no “next layer” deltas to be used.</p>
<p>Throughout this discussion we’ve left out activation functions. It turns out that we can fit them into <a href="#figure14-21">Figure 14-21</a> without changing the basic approach. Though the process is conceptually straightforward, the mechanics involve a lot of details, so we won’t go into them here. </p>
<p>This process of finding the delta for every neuron in the network is<em> </em>the heart of the backpropagation algorithm. Let’s get a feeling for how backprop works in a larger network. </p>
<h2 id="h1-500723c14-0004">Backprop on a Larger Network</h2>
<p class="BodyFirst">In the last section we saw the backpropagation algorithm, which lets us compute the delta for every neuron in a network. Because that calculation depended on the deltas in the following neurons and the output neurons don’t have any of those, and because the changes to the output neurons are driven directly by the loss function, we treat the output neurons as a special case. Once all the neuron deltas for any layer (including the output layer) have been found, we can then step backward one layer (toward the inputs), and find the deltas for all the neurons on that layer. Then we step backward <span epub:type="pagebreak" title="373" id="Page_373"/>again, compute all the deltas, step back again, and so on, until we reach the input layer. Once we have the delta for every neuron, we can adjust the values of the weights going into that neuron, thereby training our network.</p>
<p>Let’s walk through the process of using backprop to find the deltas for all the neurons in a slightly larger network.</p>
<p>In <a href="#figure14-22" id="figureanchor14-22">Figure 14-22</a> we show a network with four layers. There are still two inputs and outputs, but now we have three hidden layers of two, four, and three neurons.</p>
<figure>
<img src="Images/F14022.png" alt="F14022" width="694" height="376"/>
<figcaption><p><a id="figure14-22">Figure 14-22</a>: A new classifier network with two inputs, two outputs, and three hidden layers</p></figcaption>
</figure>
<p>We start things off by evaluating a sample. We provide the values of its X and Y features to the inputs, and eventually the network produces the output predictions <em>P1</em> and <em>P2</em>.</p>
<p>Now we can start backpropagation by finding the error in the first of the output neurons, as shown in the upper part of <a href="#figure14-23" id="figureanchor14-23">Figure 14-23</a>.</p>
<p>We’ve begun arbitrarily with the upper neuron, which gives us the prediction we’ve labeled <em>P1</em> (the probability that the sample is in class 1). From the values of <em>P1</em> and <em>P2</em> and the label, we can compute the error in the network’s output. Let’s suppose the network didn’t predict this sample perfectly, so the error is greater than zero.</p>
<p>Using the error, the label, and the values of <em>P1</em> and <em>P2</em>, we can compute the value of delta for this neuron. If we’re using the quadratic cost function, this delta is just the value of the label minus the value of the neuron, as we saw in <a href="#figure14-12">Figure 14-12</a>. But if we’re using some other function, it might be more complicated, so we’ll discuss the general case.</p>
<p>We save this delta with its neuron, and then repeat this process for all the other neurons in the output layer (here we have only one more), as shown in the lower part of <a href="#figure14-23">Figure 14-23</a>. That finishes up the output layer, since we now have a delta for every neuron in the output layer. </p>
<p>At this point, we could start adjusting the weights coming into the output layer, but we usually first find all the neuron deltas first, and then adjust all the weights. Let’s follow that typical sequence here.</p>
<span epub:type="pagebreak" title="374" id="Page_374"/><figure>
<img src="Images/F14023.png" alt="F14023" width="844" height="825"/>
<figcaption><p><a id="figure14-23">Figure 14-23</a>: Summarizing the steps for finding the delta for both output neurons</p></figcaption>
</figure>
<p>We move backward one step to the third hidden layer (the one with three neurons). Let’s consider finding the value of delta for the topmost of these three, as in the left image of <a href="#figure14-24" id="figureanchor14-24">Figure 14-24</a>.</p>
<figure>
<img src="Images/F14024.png" alt="F14024" width="844" height="168"/>
<figcaption><p><a id="figure14-24">Figure 14-24</a>: Using backpropagation to find the deltas for the next-to-last layer of neurons </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="375" id="Page_375"/>To find the delta for this neuron, we follow the recipe of <a href="#figure14-18">Figure 14-18</a> to get the individual contributions, and then the recipe of <a href="#figure14-19">Figure 14-19</a> to add them together to get the delta for this neuron.</p>
<p>Now we just work our way through the layer, applying the same process to each neuron. When we’ve completed all the neurons in this three-neuron layer, we take a step backward and start on the preceding hidden layer with four neurons. This is where things really become beautiful. To find the deltas for each neuron in this layer, we need only the weights to each neuron that uses that neuron’s output and the deltas for those neurons, which we just computed.</p>
<p>The other layers are irrelevant. We don’t care about the output layer anymore now. </p>
<p><a href="#figure14-25" id="figureanchor14-25">Figure 14-25</a> shows how we compute the deltas for the four neurons in the second hidden layer.</p>
<figure>
<img src="Images/F14025.png" alt="F14025" width="844" height="612"/>
<figcaption><p><a id="figure14-25">Figure 14-25</a>: Using backprop to find the delta values for the second hidden layer</p></figcaption>
</figure>
<p>When all four neurons have had deltas assigned to them, that layer is finished, and we take another step backward. Now we’re at the first hidden layer with two neurons. Each of these connects to the four neurons on the next layer. Once again, all we care about are the deltas in that next layer and the weights that connect the two layers. For each neuron, we find the deltas for all the neurons that consume that neuron’s output, multiply those by the weights, and add up the results, as shown in <a href="#figure14-26" id="figureanchor14-26">Figure 14-26</a>.</p>
<span epub:type="pagebreak" title="376" id="Page_376"/><figure>
<img src="Images/F14026.png" alt="F14026" width="844" height="259"/>
<figcaption><p><a id="figure14-26">Figure 14-26</a>: Using backprop to find the deltas for the first hidden layer</p></figcaption>
</figure>
<p>When <a href="#figure14-26">Figure 14-26</a> is complete, we’ve found the delta for every neuron in the network.</p>
<p>Now let’s adjust the weights. We can run through the connections between neurons and use the technique we saw in <a href="#figure14-15">Figure 14-15</a> to update every weight to a new and improved value.</p>
<p><a href="#figure14-23">Figure 14-23</a> through <a href="#figure14-26">Figure 14-26</a> show why the algorithm is called <em>backward propagation</em>. We’re taking the deltas from any layer and <em>propagating</em>, or moving, their delta (or gradient) information <em>backward </em>one layer at a time, modifying it as we go. As we’ve seen, computing each of these delta values is fast. Even when we put the activation function steps in, that doesn’t add much to the computational cost.</p>
<p>Backprop becomes highly efficient when we use parallel hardware like a GPU, because we can use a GPU to multiply all the deltas and weights for an entire layer<em> </em>simultaneously. The tremendous efficiency boost that comes from this parallelism is a key reason why backprop has made learning practical for huge neural networks.</p>
<p>Now we have all of the deltas, and we can update the weights. That’s the core process of training a neural network.</p>
<p>Before we leave the discussion, though, let’s return to the issue of how much we should move each weight.</p>
<h2 id="h1-500723c14-0005">The Learning Rate</h2>
<p class="BodyFirst">As we’ve mentioned, changing a weight by a lot in a single step is often a recipe for trouble. The derivative is only an accurate predictor of the shape of a curve for very tiny changes in the input value. If we change a weight by too much, we can jump right over the smallest value of the error and even find ourselves increasing the error.</p>
<p>On the other hand, if we change a weight by too little, we might see only the tiniest bit of learning, requiring us to spend more time learning than we should actually require. Still, that inefficiency is usually better than a system that’s constantly overreacting to errors.</p>
<p><span epub:type="pagebreak" title="377" id="Page_377"/>In practice, we control the amount of change to the weights during every update with a hyperparameter called the <em>learning rate</em>, usually symbolized by the lowercase Greek letter <em>η </em>(eta). This is a number between 0 and 1, and it tells the weights how much of each neuron’s newly computed change to use when it updates.</p>
<p>When we set the learning rate to 0, the weights don’t change at all. Our system never changes and never learns. If we set the learning rate to 1, the system applies big changes to the weights and may cause them to increase the error, not decrease it. If this happens a lot, the network can spend its time constantly overshooting and then compensating, with the weights bouncing around and never settling into their best values. Therefore, we usually set the learning rate somewhere between these extremes. In practice, we usually set it to be only slightly larger than 0. </p>
<p><a href="#figure14-27" id="figureanchor14-27">Figure 14-27</a> shows how the learning rate is applied. Starting with <a href="#figure14-15">Figure 14-15</a>, we insert an extra step to scale the value of −(<em>Ao </em>× <em>Cδ</em>) by <em>η </em>before adding it back in to <em>AC</em>.</p>
<figure>
<img src="Images/F14027.png" alt="F14027" width="590" height="457"/>
<figcaption><p><a id="figure14-27">Figure 14-27</a>: The learning rate helps us control how fast the network learns by controlling the amount by which weights change on each update. </p></figcaption>
</figure>
<p>The best value to use for the learning rate is dependent on the specific network we’ve built and the data we’re training on. Finding a good choice of learning rate can be essential to getting the network to learn at all. Once the system is learning, changing this value can affect whether that process goes quickly or slowly. Usually we have to hunt for the best value of <em>η</em> using trial and error. Happily, some algorithms automate the search for a good starting value for the learning rate and others fine-tune the learning rate as learning progresses. As a general rule of thumb, and if none of our other choices direct us to a particular learning rate, <span epub:type="pagebreak" title="378" id="Page_378"/>we often start with a value around 0.001 and then train the network for a while, watching how well it learns. Then we raise or lower it from that value and train again, over and over, hunting for the value that learns most efficiently. We’ll look at techniques for controlling the learning rate more closely in Chapter 15.</p>
<p>Let’s see how the choice of learning rate affects the performance of backprop, and thus learning. </p>
<h3 id="h2-500723c14-0007">Building a Binary Classifier</h3>
<p class="BodyFirst">Let’s build a classifier to find the boundary between two crescent moons. We will use about 1,500 points of training data, shown in <a href="#figure14-28" id="figureanchor14-28">Figure 14-28</a>. </p>
<figure>
<img src="Images/F14028.png" alt="F14028" width="675" height="492"/>
<figcaption><p><a id="figure14-28">Figure 14-28</a>: About 1,500 points assigned to two classes</p></figcaption>
</figure>
<p>Because we have only two classes, we only need a binary classifier. This lets us skip the whole one-hot encoding of labels and dealing with multiple outputs and instead lets us use just one output neuron. If the value is near 0, the input is in one class. If the output is near 1, the input is in the other class.</p>
<p>Our classifier will have just two hidden layers, each with four neurons. These are essentially arbitrary choices we’ve made that give us a network that’s just complex enough for our discussion. As shown in <a href="#figure14-29" id="figureanchor14-29">Figure 14-29</a>, both layers are fully connected.</p>
<p>This network uses ReLU activation functions for the neurons in the hidden layers and a sigmoid activation function on the output neuron.</p>
<span epub:type="pagebreak" title="379" id="Page_379"/><figure>
<img src="Images/F14029.png" alt="F14029" width="468" height="238"/>
<figcaption><p><a id="figure14-29">Figure 14-29</a>: Our binary classifier with two inputs, four neurons in each of the two hidden layers, and a single output neuron </p></figcaption>
</figure>
<p>How many weights are in our network? There are four coming out of each of the two inputs, then four times four between the layers, and then four going into the output neuron. That gives us (2 × 4) + (4 × 4) + 4 = 28. Each of the nine neurons also has a bias term, so our network has a total of 28 + 9 = 37 weights. They are all initialized as small random numbers. Our goal is to use backprop to adjust those 37 weights so that the number that comes out of the final neuron always matches the label for that sample.</p>
<p>As we discussed earlier, we evaluate one sample, calculate the error, and if the error is not zero, we compute the deltas with backprop and then update the weights using the learning rate. Then we move on to the next sample. Note that if the error is 0, then we don’t change anything, since the network gave us the answer we wanted. Each time we process all the samples in the training set, we say we’ve completed one <em>epoch</em> of training.</p>
<p>Running backprop successfully relies on making small changes to the weights. There are two reasons for this. The first, which we’ve discussed, is because the gradient is only accurate very near the point we’re evaluating. If we move too far, we may find ourselves increasing the error rather than decreasing it.</p>
<p>The second reason for taking small steps is that changes in weights near the start of the network cause changes in the outputs of neurons in later layers, which change their deltas. To prevent everything from turning into a terrible snarl of conflicting changes, we adjust the weights only by small amounts.</p>
<p>But what is “small”? For every network and dataset, we have to experiment to find out. As we saw earlier, the size of our step is controlled by the learning rate, or eta (<em>η</em>). The bigger this value, the more each weight moves toward its new value.</p>
<h3 id="h2-500723c14-0008">Picking a Learning Rate</h3>
<p class="BodyFirst">Let’s start with an unusually large learning rate of 0.5. <a href="#figure14-30" id="figureanchor14-30">Figure 14-30</a> shows the boundaries computed by our network for our test data, using a different background color for each class.</p>
<span epub:type="pagebreak" title="380" id="Page_380"/><figure>
<img src="Images/F14030.png" alt="F14030" width="675" height="495"/>
<figcaption><p><a id="figure14-30">Figure 14-30</a>: The boundaries computed by our network using a learning rate of 0.5</p></figcaption>
</figure>
<p>This is terrible: there don’t seem to be any boundaries at all! Everything is being assigned to a single class, shown by the light orange background. If we look at the accuracy and error (or loss) after each epoch, we get the graphs of <a href="#figure14-31" id="figureanchor14-31">Figure 14-31</a>.</p>
<figure>
<img src="Images/F14031.png" alt="F14031" width="844" height="324"/>
<figcaption><p><a id="figure14-31">Figure 14-31</a>: Accuracy and loss for our half-moons data with a learning rate of 0.5</p></figcaption>
</figure>
<p>Things are looking bad. As we’d expect, the accuracy is just about 0.5, meaning that half the points are being misclassified. This makes sense, since the red and blue points are roughly evenly divided. If we <span epub:type="pagebreak" title="381" id="Page_381"/>assign them all to one class, as we’re doing here, half of those assignments will be wrong. The loss, or error, starts high and doesn’t fall. If we let the network run for hundreds of epochs, it continues on in this way, never improving.</p>
<p>What are the weights doing? <a href="#figure14-32" id="figureanchor14-32">Figure 14-32</a> shows the values of all 37 weights during training.</p>
<figure>
<img src="Images/F14032.png" alt="F14032" width="675" height="487"/>
<figcaption><p><a id="figure14-32">Figure 14-32</a>: The weights of our network when using a learning rate of 0.5. One weight is constantly changing and overshooting its goal, while the others are making changes too small to show on this graph.</p></figcaption>
</figure>
<p>The graph is dominated by one weight that’s jumping all over. That weight is one of those going into the output neuron, trying to move its output around to match the label. That weight goes up, then down, then up, jumping too far almost every time, then overcorrecting by too much, then overcorrecting for that, and so on. The other neurons are changing too, but at too small a scale to see in this graph.</p>
<p>These results are disappointing, but they’re not shocking, because a learning rate of 0.5 is <em>big</em>. That’s what’s causing all the erratic bouncing around in <a href="#figure14-32">Figure 14-32</a>.</p>
<p>Let’s reduce the training rate by a factor of 10 to a more reasonable (though still big) value of 0.05. We’ll change absolutely nothing else about the network or the data, and we’ll even reuse the same sequence of pseudorandom numbers to initialize the weights. The new boundaries are shown in <a href="#figure14-33" id="figureanchor14-33">Figure 14-33</a>.</p>
<span epub:type="pagebreak" title="382" id="Page_382"/><figure>
<img src="Images/F14033.png" alt="F14033" width="675" height="495"/>
<figcaption><p><a id="figure14-33">Figure 14-33</a>: The decision boundaries when we use a learning rate of 0.05</p></figcaption>
</figure>
<p>This is much<em> </em>better! Looking at the graphs in <a href="#figure14-34" id="figureanchor14-34">Figure 14-34</a> reveals that we’ve reached 100 percent accuracy on both the training and test sets after about 16 epochs. Using a smaller learning rate gave us a huge improvement.</p>
<figure>
<img src="Images/F14034.png" alt="F14034" width="844" height="330"/>
<figcaption><p><a id="figure14-34">Figure 14-34</a>: Accuracy and loss for our network when using a learning rate of 0.05</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="383" id="Page_383"/>This shows us the importance of tuning the learning rate for every new combination of network and data. If a network refuses to learn, we can sometimes make things better by simply reducing the learning rate. </p>
<p>What are the weights doing now? <a href="#figure14-35" id="figureanchor14-35">Figure 14-35</a> shows us their history. </p>
<figure>
<img src="Images/F14035.png" alt="F14035" width="675" height="495"/>
<figcaption><p><a id="figure14-35">Figure 14-35</a>: The weights in our network over time, using a learning rate of 0.05</p></figcaption>
</figure>
<p>Overall, this is way better, because lots of weights are changing. They’re getting pretty large, which can itself inhibit or slow down learning. We usually want our weights to be in a small range, typically [–1, 1]. We’ll see some ways to control weight values when we discuss regularization in Chapter 15.</p>
<p><a href="#figure14-33">Figure 14-33</a> and <a href="#figure14-34">Figure 14-34</a> are pictures of success. Our network has learned to perfectly classify the data, and it did it in only 16 epochs, which is nice and fast (in fact, the graphs show us that it really took only 10 epochs). On a late 2014 iMac without GPU support, the whole training process for 16 epochs took less than 10 seconds.</p>
<h3 id="h2-500723c14-0009">An Even Smaller Learning Rate</h3>
<p class="BodyFirst">What if we lower the learning rate down to 0.01? Now the weights change even more slowly. Does this produce better results?</p>
<p><a href="#figure14-36" id="figureanchor14-36">Figure 14-36</a> shows the decision boundary resulting from these tiny steps. The boundary seems simpler than the boundary in <a href="#figure14-33">Figure 14-33</a>, but both boundaries separate the sets perfectly. </p>
<span epub:type="pagebreak" title="384" id="Page_384"/><figure>
<img src="Images/F14036.png" alt="F14036" width="675" height="495"/>
<figcaption><p><a id="figure14-36">Figure 14-36</a>: The decision boundaries for a learning rate of 0.01</p></figcaption>
</figure>
<p><a href="#figure14-37" id="figureanchor14-37">Figure 14-37</a> shows our accuracy and loss graphs. Because our learning rate is so much slower, our network takes around 170 epochs to get to 100 percent accuracy, rather than the 16 in <a href="#figure14-35">Figure 14-35</a>.</p>
<figure>
<img src="Images/F14037.png" alt="F14037" width="844" height="330"/>
<figcaption><p><a id="figure14-37">Figure 14-37</a>: The accuracy and learning rate for our network using a learning rate of 0.01</p></figcaption>
</figure>
<p>These graphs show an interesting learning behavior. After an initial jump, both the training and test accuracies reach about 90 percent and plateau there. At the same time, the losses plateau as well. Then around epoch 170, things improve rapidly again, with the accuracy climbing to 100 percent and the errors dropping to zero.</p>
<p><span epub:type="pagebreak" title="385" id="Page_385"/>This pattern of alternating improvement and plateaus is not unusual, and we can even see a hint of a plateau in <a href="#figure14-34">Figure 14-34</a> between epochs 3 and 8. These plateaus come from the weights finding themselves on nearly flat regions of the error surface, resulting in near-zero gradients, and thus their updates are very small.</p>
<p>Though our weights might be getting stuck in a local minimum, it’s more common for them to get caught in a flat region of a saddle, like those we saw in Chapter 5 (Dauphin et al. 2014). Sometimes it takes a long time for one of the weights to move into a region where the gradient is large enough to give it a good push. When one weight gets moving, it’s common to see the others kick in as well, thanks to the cascading effect of that weight’s changes on the rest of the network.</p>
<p>The values of the weights follow almost the same pattern over time, as shown in <a href="#figure14-38" id="figureanchor14-38">Figure 14-38</a>. The interesting thing is that at least some of the weights are not flat or on a plateau near the middle of our training process. They’re changing, but slowly. The system is getting better, but in tiny steps that don’t show up in the performance graphs until the changes become bigger around epoch 170.</p>
<figure>
<img src="Images/F14038.png" alt="F14038" width="675" height="492"/>
<figcaption><p><a id="figure14-38">Figure 14-38</a>: The history of our weights using a learning rate of 0.01</p></figcaption>
</figure>
<p>So, was there any benefit to lowering the learning rate down to 0.01? In this case, not really. Even at 0.05, the classification was already perfect on both the training and test data. For this network and this data, the smaller learning rate just meant the network took longer to learn. This investigation has shown us how sensitive the network is to our choice of learning rate. We want to find a value that’s not too big, or too small, but just right (Pyle 1918).</p>
<p><span epub:type="pagebreak" title="386" id="Page_386"/>We usually do this kind of experimenting with the learning rate as part of developing nearly every deep learning network. We need to find a value that does the best job on each specific network and data. Happily, in Chapter 15 we’ll see algorithms that can automatically adjust the learning rate for us in sophisticated ways. </p>
<h2 id="h1-500723c14-0006">Summary </h2>
<p class="BodyFirst">This chapter was all about backpropagation. We saw that we can predict how the error of a network will change in response to a change in each weight. If we can determine whether each weight should increase or decrease in value, we can reduce the error.</p>
<p>To find how we should change each weight, we started by assigning a delta value to each neuron. This value tells us the relationship between a change in a weight’s value and a change in the final error. This enabled us to determine how to change each weight in order to reduce the error.</p>
<p>The computation of these deltas proceeds from the final layer backward to the first. Because the gradient information needed to compute the delta for each neuron is propagated backward one layer at a time, we get the name <em>backpropagation</em>. Backprop can be implemented on a GPU, where we can carry out the calculations for many neurons simultaneously. </p>
<p>It’s important to keep in mind that backprop is propagating the gradient of the error, which is the information that tells us how the error changes when the weight changes. Some authors casually speak of backprop as propagating the error, but that’s a misleading simplification. We’re propagating the gradient, which tells us how to manipulate the weights to improve the network’s output.</p>
<p>Now that we know whether each weight should be adjusted to be larger or smaller, we need to decide how big a change to actually make. That’s exactly what we’ll figure out in the next chapter.</p>
</section>
</div></body></html>