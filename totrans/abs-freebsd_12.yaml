- en: '**12'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**12'
- en: THE Z FILE SYSTEM**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Z 文件系统**
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Most filesystems are, in computing terms, ancient. We discard 5-year-old hardware
    because it’s painfully slow, but we format the replacement’s hard drive with a
    40-year-old filesystem. While we’ve improved those filesystems and made them more
    robust, they still use the same basic architecture. And every time a filesystem
    breaks, we curse and scramble to fix it while desperately wishing for something
    better.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数文件系统在计算机术语中都是古老的。我们因为硬件速度慢而丢弃 5 年前的设备，但我们却用一个 40 年历史的文件系统来格式化新硬盘。虽然我们已改进了这些文件系统并使其更加稳健，但它们仍然使用相同的基本架构。每当文件系统出问题时，我们都会咒骂并急于修复，同时迫切希望有更好的选择。
- en: ZFS is something better.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS 是更好的选择。
- en: It’s not that ZFS uses revolutionary technology. All the individual pieces of
    ZFS are well understood. There’s no mystery to hashes or data trees or indexing.
    But ZFS combines all of these well-understood principles into a single cohesive,
    well-engineered whole. It’s designed with the future in mind. Today’s hashing
    algorithm won’t suffice 15 years from now, but ZFS is designed so that new algorithms
    and techniques can be added to newer versions without losing backward compatibility.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是说 ZFS 使用了革命性的技术。ZFS 的所有个别组件都已经被很好地理解。哈希、数据树和索引都没有什么神秘之处。但 ZFS 将所有这些经过验证的原理结合成一个统一、良好工程化的整体。它的设计考虑了未来。今天的哈希算法可能无法满足
    15 年后的需求，但 ZFS 的设计使得可以在不失去向后兼容性的前提下，为新版本添加新的算法和技术。
- en: This chapter won’t cover all there is to know about ZFS. ZFS is almost an operating
    system on its own, or perhaps a special-purpose database. Entire books have been
    written about using and managing ZFS. You’ll learn enough about how ZFS works
    to use it on a server, though, and understand its most important features.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不会涵盖关于 ZFS 的所有知识。ZFS 几乎是一个独立的操作系统，或者说是一个专用数据库。已经有整本书专门讲解如何使用和管理 ZFS。不过，你会学到足够的
    ZFS 工作原理，以便在服务器上使用它，并理解它最重要的特性。
- en: While ZFS expects to be installed directly on a disk partition, you can use
    other GEOM providers as ZFS storage. The most common example is when you do an
    install with encrypted disks. FreeBSD puts a geli(8) geom on the disk and installs
    ZFS atop that geom. This chapter calls any storage provider a “disk,” even though
    it could be a file or an encrypted provider or anything else.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 ZFS 预计是直接安装在磁盘分区上的，但你也可以使用其他 GEOM 提供者作为 ZFS 存储。最常见的例子是，当你进行加密磁盘的安装时，FreeBSD
    会在磁盘上放置一个 geli(8) geom 并在该 geom 上安装 ZFS。本章中，将任何存储提供者都称作“磁盘”，即使它可能是一个文件、加密提供者或其他任何东西。
- en: If you’ve never worked with ZFS before, install a ZFS-based FreeBSD system on
    a virtual machine and follow along. The installer automatically handles prerequisites,
    like setting `zfs_load=YES` in *loader.conf* and `zfs_enable=YES` in *rc.local*;
    all you need concern yourself with is the filesystem.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以前从未使用过 ZFS，可以在虚拟机上安装一个基于 ZFS 的 FreeBSD 系统并跟着操作。安装程序会自动处理一些前提条件，比如在 *loader.conf*
    中设置 `zfs_load=YES`，并在 *rc.local* 中设置 `zfs_enable=YES`；你只需要关注文件系统即可。
- en: '**WHAT DOES ZFS STAND FOR?**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**ZFS 代表什么？**'
- en: The *Z File System*. Yes, seriously. Once upon a time, it meant *Zettabyte File
    System*, but that acronym has been retconned away.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*Z 文件系统*。没错，真的是这样。曾几何时，它代表着*泽塔字节文件系统*，但这个缩写已经被重新定义了。'
- en: ZFS blends a whole bunch of well-understood technologies into a combination
    volume manager and filesystem. It expects to handle everything from the permissions
    on a file down to tracking which blocks on which storage provider get which information.
    As the sysadmin, you tell ZFS which hardware you have and how you want it configured,
    and ZFS takes it from there.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS 将一大堆已被理解的技术融合成一个组合式卷管理器和文件系统。它期望处理从文件权限到跟踪哪些存储提供者上的哪些块存储哪些信息的一切。作为系统管理员，你只需告诉
    ZFS 你拥有的硬件以及如何配置它，然后 ZFS 会继续处理其余的部分。
- en: 'ZFS has three main components: datasets, pools, and virtual devices.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS 有三个主要组件：数据集、池和虚拟设备。
- en: '**Datasets**'
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**数据集**'
- en: A *dataset* is defined as a named chunk of ZFS data. The most common dataset
    resembles a partitioned filesystem, but ZFS supports other types of datasets for
    other uses. A snapshot (see “[Snapshots](ch12.xhtml#lev453)” on [page 271](ch12.xhtml#page_271))
    is a dataset. ZFS also includes block devices for virtualization and iSCSI targets,
    clones, and more; all of those are datasets. This book focuses on filesystem datasets.
    Traditional filesystems like UFS have a variety of small programs to manage filesystems,
    but you manage all ZFS datasets with zfs(8).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据集*被定义为一块有名称的ZFS数据。最常见的数据集类似于分区文件系统，但ZFS也支持其他类型的数据集，用于其他用途。快照（参见“[快照](ch12.xhtml#lev453)”章节，见[第271页](ch12.xhtml#page_271)）就是一种数据集。ZFS还包括用于虚拟化和iSCSI目标、克隆等的块设备；所有这些都是数据集。本书主要集中在文件系统数据集上。传统文件系统，如UFS，拥有多种小程序来管理文件系统，但你可以使用zfs(8)管理所有ZFS数据集。'
- en: View your existing datasets with `zfs list`. The output looks a lot like mount(8).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`zfs list`查看现有的数据集。输出看起来很像mount(8)的结果。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Each line starts with the dataset name, starting with the storage pool—or *zpool*—that
    the dataset is on. The first entry is called *zroot* ➊. This entry represents
    the pool’s *root dataset*. The rest of the dataset tree dangles off this dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行以数据集名称开始，后面跟着该数据集所在的存储池或*zpool*。第一个条目是*zroot* ➊。这个条目代表池的*根数据集*。数据集树的其余部分悬挂在此数据集下。
- en: The next two columns show the amount of space used and available. The pool *zroot*
    has used 4.71GB and has 894GB available. While the available space is certainly
    correct, the 4.71GB is more complicated than it looks. The amount of space a dataset
    shows under USED includes everything on that dataset *and* on all of its children.
    A root dataset’s children include all the other datasets in that zpool.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两列显示了已使用空间和可用空间的数量。*zroot*池已使用4.71GB，并且有894GB的可用空间。虽然可用空间的数值显然是正确的，但4.71GB的情况比看起来更复杂。一个数据集在“USED”列下显示的空间包含了该数据集*以及*所有子数据集的空间。一个根数据集的子数据集包括该zpool中的所有其他数据集。
- en: The `REFER` column is special to ZFS. This column shows the amount of data accessible
    on this specific dataset, which isn’t necessarily the same as the amount of space
    used. Some ZFS features, such as snapshots, share data between themselves. This
    dataset has used 4.71GB of data but refers to only 88KB. Without its children,
    this dataset has only 88KB of data on it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`REFER`列是ZFS特有的。该列显示此特定数据集上可以访问的数据量，这不一定等同于已使用的空间量。一些ZFS功能，如快照，会在它们之间共享数据。这个数据集已经使用了4.71GB的数据，但只引用了88KB。没有它的子数据集，这个数据集只有88KB的数据。'
- en: At the end, we have the dataset’s mount point. This root dataset doesn’t have
    a mount point; it’s not mounted.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看到了数据集的挂载点。这个根数据集没有挂载点；它没有被挂载。
- en: Look at the next dataset, *zroot/ROOT* ➋. This is a dataset created for the
    root directory and associated files. That seems sensible, but if you look at the
    `REFER` column, you’ll see it also has only 88KB of data inside it, and there’s
    no mount point. Shouldn’t the root directory exist?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 看下一个数据集，*zroot/ROOT* ➋。这是为根目录及其相关文件创建的数据集。听起来很合理，但如果查看`REFER`列，你会发现它只包含88KB的数据，并且没有挂载点。根目录难道不应该存在吗？
- en: The next two lines explain why . . . sort of. The dataset *zroot/ROOT/2018-11-17*
    ➌ has a mountpoint of */*, so it’s a real root directory. The next dataset, *zroot/ROOT/default*
    ➍, also has a mountpoint of */*. No, ZFS doesn’t let you mount multiple datasets
    at the same mount point. A ZFS dataset records a whole bunch of its settings within
    the dataset. The mount point is one of those settings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两行解释了为什么...有点。数据集*zroot/ROOT/2018-11-17* ➌的挂载点是*/*，所以它是一个真实的根目录。下一个数据集，*zroot/ROOT/default*
    ➍，也有一个挂载点*/*。不，ZFS不允许你将多个数据集挂载到同一个挂载点。一个ZFS数据集记录了其设置的许多内容，而挂载点只是其中之一。
- en: Consider these four datasets for a moment. The *zroot/ROOT* dataset is a child
    of the zroot dataset. The *zroot/ROOT/2018-11-17* and *zroot/ROOT/default* datasets
    are children of *zroot/ROOT*. Each dataset has its children’s space usage billed
    against it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微考虑一下这四个数据集。*zroot/ROOT*数据集是*zroot*数据集的子集。*zroot/ROOT/2018-11-17*和*zroot/ROOT/default*数据集是*zroot/ROOT*的子集。每个数据集的空间使用都算在它的子数据集上。
- en: Why do this? When you boot a FreeBSD ZFS host, you can easily choose between
    multiple root directories. Each bootable root directory is called a *boot environment*.
    Suppose you apply a patch and reboot the system, but the new system won’t boot.
    By booting into an alternate boot environment, you can easily access the defective
    root directory and try to figure out the problem.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要这么做？当你启动一个 FreeBSD ZFS 主机时，你可以轻松选择多个根目录。每个可启动的根目录称为 *启动环境*。假设你应用了一个补丁并重启系统，但新系统无法启动。通过启动到一个备用的启动环境，你可以轻松访问故障的根目录并尝试找出问题所在。
- en: The next dataset, *zroot/usr* ➎, is a completely different child of *zroot*.
    It has its own child, *zroot/usr/home* ➏. The space used in *zroot/usr/home* gets
    charged against *zroot/usr*, and both get charged against its parent, but their
    allocation doesn’t affect *zroot/ROOT*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个数据集，*zroot/usr* ➎，是 *zroot* 的一个完全不同的子集。它有自己的子集，*zroot/usr/home* ➏。*zroot/usr/home*
    中使用的空间会计入 *zroot/usr*，而两者都会计入它的父集，但它们的分配不会影响 *zroot/ROOT*。
- en: '***Dataset Properties***'
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***数据集属性***'
- en: Beyond some accounting tricks, datasets so far look a lot like partitions. But
    a partition is a logical subdivision of a disk, filling very specific LBAs on
    a storage device. Partitions have no awareness of the data on the partition. Changing
    a partition means destroying the filesystem on it.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除了一些会计技巧外，数据集目前看起来与分区非常相似。但分区是磁盘的逻辑划分，填充存储设备上的非常特定的 LBA（逻辑块地址）。分区对分区上的数据没有感知。改变分区意味着摧毁其上的文件系统。
- en: ZFS tightly integrates the filesystem and the lower storage layers. It can dynamically
    divide storage space between the various filesystems as needed. Where partitions
    control the number of available blocks to constrain disk usage, datasets can use
    quotas for the same effect. Without those quotas, though, if a pool has space,
    you can use it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS 紧密集成了文件系统和底层存储层。它可以根据需要动态地在各个文件系统之间划分存储空间。当分区通过控制可用块数来约束磁盘使用时，数据集可以使用配额来实现相同的效果。不过，如果没有这些配额，只要池中有空间，就可以使用它。
- en: The amount of space a dataset can use is a ZFS *property*. ZFS supports dozens
    of properties, from the `quotas` property that controls how large a dataset can
    grow to the `mounted` property that shows whether a dataset is mounted.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以使用的空间是一个 ZFS *属性*。ZFS 支持数十种属性，从控制数据集增长大小的 `quotas` 属性，到显示数据集是否挂载的 `mounted`
    属性。
- en: '**Viewing and Changing Dataset Properties**'
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**查看和更改数据集属性**'
- en: Use `zfs set` to change properties.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `zfs set` 来更改属性。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: View a property with `zfs get`. You can either specify a particular property
    or use `all` to view all properties. You can list multiple properties by separating
    them with commas. If you specify a dataset name, you affect only that dataset.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `zfs get` 查看属性。你可以指定某个特定的属性，或者使用 `all` 查看所有属性。你可以通过逗号分隔列出多个属性。如果指定了数据集名称，则只影响该数据集。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we have the dataset’s name, the property, the property value, and something
    called source. (We’ll talk about that last one in “[Property Inheritance](ch12.xhtml#lev427)”
    on [page 261](ch12.xhtml#page_261).)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有数据集的名称、属性、属性值以及一个叫做 source 的东西。（我们将在 “[属性继承](ch12.xhtml#lev427)” 中讨论那个东西，位于
    [第 261 页](ch12.xhtml#page_261)。）
- en: My real question is, which dataset is mounted as the root directory? I could
    check the two datasets with a mount point of */*, but when I get dozens of boot
    environments, that will drive me nuts. Check a property for a dataset and all
    of its children by adding the `-r` flag.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我的真正问题是，哪个数据集被挂载为根目录？我可以检查两个挂载点为 */* 的数据集，但当我有几十个启动环境时，这会让我抓狂。通过添加 `-r` 标志，可以检查数据集及其所有子数据集的属性。
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Of the three datasets, only *zroot/ROOT/default* ➊ is mounted. That’s our active
    boot environment.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三个数据集中，只有 *zroot/ROOT/default* ➊ 被挂载。那是我们的活动启动环境。
- en: '**Property Inheritance**'
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**属性继承**'
- en: Many properties are inheritable. You set them on the parent dataset and they
    percolate down through the children. Inheritance doesn’t make sense for properties
    like mount points, but it’s right for certain more advanced features. While we’ll
    look at what the `compression` property does in “[Compression](ch12.xhtml#lev457)”
    on [page 273](ch12.xhtml#page_273), we’ll use it as an example of inheritance
    here.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 许多属性是可继承的。你可以在父数据集上设置它们，它们会传递到子数据集。在像挂载点这样的属性上，继承是没有意义的，但对于某些更高级的功能来说，继承是合适的。虽然我们将在
    “[压缩](ch12.xhtml#lev457)” 中讨论 `compression` 属性的作用，位于 [第 273 页](ch12.xhtml#page_273)，但我们将在此处以它作为继承的示例。
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The root dataset, zroot, has the `compression` property set to lz4\. The source
    is local, meaning that this property is set on this dataset. Now look at *zroot/ROOT*.
    The `compression` property is also lz4, but the source is inherited from zroot.
    This dataset inherited this property setting from its parent.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 根数据集 zroot 的 `compression` 属性设置为 lz4。源是本地的，这意味着该属性是设置在此数据集上的。现在看一下 *zroot/ROOT*。`compression`
    属性也是 lz4，但源是从 zroot 继承的。这个数据集继承了父数据集的这个属性设置。
- en: '***Managing Datasets***'
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***管理数据集***'
- en: ZFS uses datasets much as traditional filesystems use partitions. Manage datasets
    with zfs(8). You’ll want to create, remove, and rename datasets.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS 使用数据集的方式与传统文件系统使用分区的方式相似。使用 zfs(8) 管理数据集。你将需要创建、删除和重命名数据集。
- en: '**Create Datasets**'
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**创建数据集**'
- en: Create datasets with `zfs create`. Create a filesystem dataset by specifying
    the pool and the dataset name. Here, I create a new dataset for my packages. (Note
    that this breaks boot environments, as we’ll see later this chapter.)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `zfs create` 创建数据集。通过指定池和数据集名称来创建一个文件系统数据集。在这里，我为我的包创建了一个新数据集。（请注意，这会破坏启动环境，正如我们将在本章稍后看到的那样。）
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Each dataset must have a parent dataset. A default FreeBSD install has a *zroot/usr*
    dataset, so I can create a *zroot/usr/local*. I’d like to have a dataset for */var/db/pkg*,
    but while FreeBSD comes with a *zroot/var* dataset, there’s no *zroot/var/db*.
    I’d need to create *zroot/var/db* and then *zroot/var/db/pkg*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集必须有一个父数据集。默认的 FreeBSD 安装包含一个 *zroot/usr* 数据集，因此我可以创建一个 *zroot/usr/local*。我想为
    */var/db/pkg* 创建一个数据集，但虽然 FreeBSD 包含一个 *zroot/var* 数据集，却没有 *zroot/var/db*。我需要创建
    *zroot/var/db*，然后创建 *zroot/var/db/pkg*。
- en: Note that datasets are stackable, just like UFS. If I have files in my */usr/local*
    directory and I create a dataset over that directory, ZFS will mount the dataset
    over the directory. I will lose access to those files. You must shuffle files
    around to duplicate existing directories.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据集是可以叠加的，就像 UFS 一样。如果我在 */usr/local* 目录中有文件，并且我在该目录上创建了一个数据集，ZFS 将会将该数据集挂载到目录上。我将无法访问这些文件。你必须移动文件以复制现有的目录。
- en: '**Destroying and Renaming Datasets**'
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**销毁和重命名数据集**'
- en: That new *zroot/usr/local* dataset I created? It hid the contents of my */usr/local*
    directory. Get rid of it with `zfs destroy` and try again.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建的那个新的 *zroot/usr/local* 数据集？它隐藏了我 */usr/local* 目录中的内容。使用 `zfs destroy` 删除它，然后再试一次。
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The contents of */usr/local* reappear. Or, I could rename that dataset instead,
    using `zfs rename`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*/usr/local* 的内容重新出现了。或者，我可以选择重命名这个数据集，使用 `zfs rename`。'
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: I like boot environments, though, so I’m going to leave */usr/local* untouched.
    Sometimes you really need a */usr/local* dataset, though . . .
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢启动环境，因此我打算保持 */usr/local* 不变。不过，有时候你真的需要一个 */usr/local* 数据集……
- en: '**Unmounted Parent Datasets**'
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**未挂载的父数据集**'
- en: As a Postgres user, I want a separate dataset for my Postgres data. FreeBSD’s
    Postgres 9.6 package uses */var/db/pgsql/data96*. I can’t create that dataset
    without having a dataset for */var/db*, and I can’t have *that* without breaking
    boot environment support for packages. What to do?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个 Postgres 用户，我希望为我的 Postgres 数据创建一个单独的数据集。FreeBSD 的 Postgres 9.6 包使用 */var/db/pgsql/data96*。没有
    *zroot/var* 数据集，我无法创建这个数据集，而没有 *zroot/var* 又会破坏包的启动环境支持。该怎么办呢？
- en: The solution is to create a dataset for */var/db*, but not to use it, by setting
    the `canmount` dataset property. This property controls whether or not a dataset
    can be mounted. FreeBSD uses an unmounted dataset for */var* for exactly this
    reason. New datasets automatically set `canmount` to `on`, so you normally don’t
    have to worry about it. Use the `-o` flag to set a property at dataset creation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是为 */var/db* 创建一个数据集，但不使用它，通过设置 `canmount` 数据集属性。这个属性控制是否可以挂载数据集。FreeBSD
    正是因为这个原因使用了一个未挂载的数据集作为 */var*。新数据集默认将 `canmount` 设置为 `on`，因此通常无需担心它。使用 `-o` 标志在创建数据集时设置属性。
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The dataset for */var/db* exists, but it can’t be mounted. Check the contents
    of your */var/db* directory to verify everything’s still there. You can now create
    a dataset for */var/db/postgres* and even */var/db/pgsql/data96*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*/var/db* 数据集存在，但无法挂载。检查你 */var/db* 目录的内容，确认一切仍然存在。现在你可以为 */var/db/postgres*
    甚至 */var/db/pgsql/data96* 创建数据集。'
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You have a dataset for your database, and you still have the files in */var/db*
    itself as part of the root dataset. Now initialize your new Postgres database
    and go!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个用于数据库的数据集，且仍然有 */var/db* 目录中的文件，作为根数据集的一部分。现在初始化你新的 Postgres 数据库并开始使用吧！
- en: As you explore ZFS, you’ll find many situations where you might want to set
    properties at dataset creation or use unmounted parent datasets.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在你探索ZFS的过程中，你会发现许多情况可能需要在数据集创建时设置属性，或者使用未挂载的父数据集。
- en: '**Moving Files to a New Dataset**'
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**将文件移动到新数据集**'
- en: If you need to create a new dataset for an existing directory, you’ll need to
    copy the files over. I recommend you create a new dataset with a slightly different
    name, copy the files to that dataset, rename the directory, and then rename the
    dataset. Here, I want a dataset for */usr/local*, so I create it with a different
    name.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要为现有目录创建新的数据集，你需要将文件复制过去。我建议你创建一个稍有不同名称的数据集，将文件复制到该数据集，然后重命名目录，最后重命名数据集。在这里，我需要为*/usr/local*创建一个数据集，因此我使用了不同的名称创建它。
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Copy the files with tar(1), exactly as you would for a new UFS partition (see
    [Chapter 11](ch11.xhtml#ch11)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用tar(1)命令复制文件，方式与为新的UFS分区复制文件时完全相同（参见[第11章](ch11.xhtml#ch11)）。
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once it finishes, move the old directory out of the way and rename the dataset.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，将旧目录移开并重命名数据集。
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: My Postgres data now lives on its own dataset.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我的Postgres数据现在存放在自己的数据集中。
- en: '**ZFS Pools**'
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**ZFS池**'
- en: ZFS organizes its underlying storage in pools, rather than by disk. A ZFS storage
    pool, or *zpool*, is an abstraction of the underlying storage devices, letting
    you separate the physical medium and the user-visible filesystem on top of it.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS按池而不是按磁盘组织其底层存储。ZFS存储池，或称*zpool*，是对底层存储设备的抽象，使你能够将物理介质与其上方的用户可见文件系统分离。
- en: View and manage a host’s ZFS pools with zpool(8). Here, I use `zpool list` to
    see the pools from one of my hosts.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用zpool(8)查看和管理主机的ZFS池。在这里，我使用`zpool list`查看我一台主机的池。
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This host has three pools: *zroot*, *jail*, and *scratch*. Each has its own
    line.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主机有三个池：*zroot*、*jail*和*scratch*。每个池都有自己的一行。
- en: The `SIZE` column shows us the total capacity of the pool. All of these pools
    can hold 928GB. The `ALLOC` column displays how much of each pool is in use, while
    `FREE` shows how much space remains. These disks are pretty much empty, which
    makes sense as I installed this host only about three hours ago.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`SIZE`列显示了池的总容量。所有这些池的容量为928GB。`ALLOC`列显示每个池已使用的空间，而`FREE`列则显示剩余空间。这些磁盘几乎是空的，这很合理，因为我大约三小时前才安装了这个主机。'
- en: The `EXPANDSZ` column shows whether the underlying storage providers have any
    free space. When a pool has virtual device redundancy (which we’ll discuss in
    the next section), you can replace individual storage devices in the pool and
    make the pool larger. It’s like swapping out the 5TB drives in your RAID array
    with 10TB drives to make it bigger.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`EXPANDSZ`列显示底层存储提供商是否有空闲空间。当一个池具有虚拟设备冗余（我们将在下一节讨论）时，你可以替换池中的单个存储设备，并使池变大。就像把RAID阵列中的5TB硬盘换成10TB硬盘，以扩展其容量。'
- en: The `FRAG` column shows how much fragmentation this pool has. You’ve heard over
    and over that fragmentation slows performance. ZFS minimizes the impact of fragmentation,
    though.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`FRAG`列显示了此池的碎片化程度。你可能听说过，碎片化会降低性能。不过，ZFS最小化了碎片化的影响。'
- en: The `CAP` column shows what percentage of the available space is used.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`CAP`列显示可用空间的使用百分比。'
- en: The `DEDUP` column shows whether this pool uses deduplication. While many people
    trumpet deduplication as a ZFS feature, it’s not as useful as you might hope.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`DEDUP`列显示此池是否使用去重。尽管许多人将去重作为ZFS的一大特色，但它的实际效用可能并不像你期望的那么大。'
- en: The `HEALTH` column displays whether the pool is working well or the underlying
    disks have a problem.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`HEALTH`列显示池是否运行正常，或者底层磁盘是否有问题。'
- en: '***Pool Details***'
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***池的详细信息***'
- en: You can get more detail on pools, or on a single pool, by running `zpool status`.
    If you omit the pool name, you’ll see this information for all of your pools.
    Here, I check the status of my *jail* pool.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行`zpool status`获取更多关于池的信息，或者查看单个池的信息。如果省略池名称，你将看到所有池的信息。在这里，我检查了我的*jail*池的状态。
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We start with the pool name. The state is much like the `HEALTH` column; it
    displays any problems with the pool. The scan field shows information on scrubs
    (see “[Pool Integrity and Repair](ch12.xhtml#lev458)” on [page 273](ch12.xhtml#page_273)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从池名称开始。状态与`HEALTH`列类似；它显示池的任何问题。扫描字段展示了清理过程的信息（参见[“池的完整性与修复”](ch12.xhtml#lev458)，见[第273页](ch12.xhtml#page_273)）。
- en: We then have the pool configuration. The configuration shows the layout of the
    virtual devices in the pool. We’ll dive into that when we create our pools.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是池的配置。配置展示了池中虚拟设备的布局。当我们创建池时，我们会深入讨论这一点。
- en: '***Pool Properties***'
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***池属性***'
- en: Much like datasets, zpools have properties that control and display the pool’s
    settings. Some properties are inherently informational, such as the `free` property
    that expresses how much free space the pool has. You can change others.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于数据集，zpool也有属性来控制和显示池的设置。有些属性是纯粹的信息性属性，例如`free`属性，表示池中剩余的空间。你可以更改其他属性。
- en: '***Viewing Pool Properties***'
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***查看池属性***'
- en: To view all of a pool’s properties, use `zpool get`. Add the property `all`
    to view every property. You can add a pool name to include only that pool.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看池的所有属性，请使用`zpool get`。添加属性`all`可以查看每个属性。你也可以添加池名称，仅查看该池的属性。
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Some of this information gets pulled into commands like `zpool status` and `zpool
    list`. You can also query for individual properties across all pools by using
    the property name.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一些信息会被提取到诸如`zpool status`和`zpool list`这样的命令中。你也可以通过使用属性名称查询所有池的单个属性。
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Unlike dataset properties, most pool properties are set when you create or import
    the pool.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据集属性不同，大多数池属性在创建或导入池时设置。
- en: '**Virtual Devices**'
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**虚拟设备**'
- en: 'A *virtual device (VDEV)* is a group of storage devices. You might think of
    a VDEV as a RAID container: a big RAID-5 presents itself to the operating system
    as a huge device, even though the sysadmin knows it’s really a bunch of smaller
    disks. The virtual device is where ZFS’s magic happens. You can arrange pools
    for different levels of redundancy or abandon redundancy and maximize space.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*虚拟设备（VDEV）*是存储设备的集合。你可以将VDEV视为RAID容器：一个大的RAID-5向操作系统呈现为一个巨大的设备，尽管系统管理员知道它实际上是由多个较小的磁盘组成的。虚拟设备是ZFS魔法发生的地方。你可以为不同的冗余级别安排池，或者放弃冗余以最大化空间。'
- en: ZFS’s automated error correction takes place at the VDEV level. Everything in
    ZFS, from znodes (index nodes) to data blocks, is checksummed to verify integrity.
    If your pool has sufficient redundancy, ZFS will notice that data is damaged and
    restore it from a good copy. If your pool lacks redundancy, ZFS will notify you
    that the data is damaged and you can restore from backup.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS的自动错误修正发生在VDEV级别。ZFS中的一切，从znodes（索引节点）到数据块，都会进行校验和以验证完整性。如果你的池具有足够的冗余，ZFS会注意到数据已损坏，并从一个良好的副本中恢复。如果池缺乏冗余，ZFS会通知你数据已损坏，你可以从备份中恢复。
- en: A zpool consists of one or more identical VDEVs. The pool stripes data across
    all the VDEVs, with no redundancy. The loss of a VDEV means the loss of the pool.
    If you have a pool with a whole bunch of disks, make sure to use redundant VDEVs.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一个zpool由一个或多个相同的VDEV组成。池将数据条带化到所有VDEV中，没有冗余。丢失一个VDEV意味着池丢失。如果你有一个包含大量磁盘的池，确保使用冗余VDEV。
- en: '***VDEV Types and Redundancy***'
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***VDEV类型和冗余***'
- en: ZFS supports several different types of VDEV, each differentiated by the degree
    and style of redundancy they offer. The common mirrored disk, where each disk
    copies what’s on another disk, is one type of VDEV. Piles of disks with no redundancy
    is another type of VDEV. And ZFS includes three different varieties of sophisticated
    parity-based redundancy, called *RAID-Z*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS支持几种不同类型的VDEV，每种类型的冗余程度和样式有所不同。常见的镜像磁盘类型，其中每个磁盘复制另一个磁盘上的内容，是一种VDEV类型。没有冗余的磁盘堆是另一种VDEV类型。ZFS还包括三种不同种类的基于奇偶校验的高级冗余，称为*RAID-Z*。
- en: Using multiple VDEVs in a pool creates systems similar to advanced RAID arrays.
    A RAID-Z2 array looks an awful lot like RAID-6, but a ZFS pool with two RAID-Z2
    VDEVs resembles RAID-60\. Mirrored VDEVs work like RAID-1, but multiple mirrors
    in a pool behave like RAID-10\. In both of these cases, ZFS stripes the data across
    the VDEV with no redundancy. The individual VDEVs provide the redundancy.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在池中使用多个VDEV会创建类似于高级RAID阵列的系统。RAID-Z2阵列看起来非常像RAID-6，但一个具有两个RAID-Z2 VDEV的ZFS池类似于RAID-60。镜像VDEV的工作方式类似于RAID-1，但池中多个镜像的行为像RAID-10。在这两种情况下，ZFS将数据条带化到VDEV中，没有冗余。各个VDEV提供冗余。
- en: Choose your VDEV type carefully.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 小心选择你的VDEV类型。
- en: '**Striped VDEVs**'
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**条带化VDEV**'
- en: A VDEV composed of a single disk is called a *stripe* and has no redundancy.
    Losing the disk means losing your data. While a pool can contain multiple striped
    VDEVs, each disk is its own VDEV. Much like RAID-0, losing one disk means losing
    the whole pool.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 由单个磁盘组成的VDEV称为*条带*，没有冗余。丢失该磁盘意味着数据丢失。虽然一个池可以包含多个条带化VDEV，但每个磁盘都是独立的VDEV。类似于RAID-0，丢失一个磁盘意味着整个池丢失。
- en: '**Mirror VDEVs**'
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**镜像VDEV**'
- en: A mirror VDEV stores a complete copy of all the VDEV’s data on every disk. You
    can lose all but one of the drives in the VDEV and still access your data. A mirror
    can contain any number of disks.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像 VDEV 在每个磁盘上存储 VDEV 数据的完整副本。你可以丢失 VDEV 中的所有磁盘，除了一个，仍然可以访问数据。一个镜像可以包含任意数量的磁盘。
- en: ZFS can read data from all of the mirrored disks simultaneously, so reading
    data is fast. When you write data, though, ZFS must write that data to all of
    the disks simultaneously. The write isn’t complete until the slowest disk finishes.
    Write performance suffers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS 可以同时从所有镜像磁盘中读取数据，因此读取数据非常快速。然而，当你写入数据时，ZFS 必须同时将数据写入所有磁盘。直到最慢的磁盘完成写入，写入才算完成。写入性能会受到影响。
- en: '**RAID-Z**'
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**RAID-Z**'
- en: RAID-Z spreads data and parity information across all of the disks, much like
    conventional RAID. If a disk in a RAID-Z dies or starts giving corrupt data, RAID-Z
    uses the parity information to recalculate the missing data. A RAID-Z VDEV must
    contain at least three disks and can withstand the loss of any single disk. RAID-Z
    is sometimes called *RAID-Z1*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: RAID-Z 将数据和奇偶校验信息分布到所有磁盘中，类似于传统的 RAID。如果 RAID-Z 中的某个磁盘出现故障或开始提供损坏的数据，RAID-Z
    会使用奇偶校验信息重新计算丢失的数据。RAID-Z VDEV 必须包含至少三个磁盘，并且能够承受任何单个磁盘的损坏。RAID-Z 有时也被称为*RAID-Z1*。
- en: You can’t add or remove disks in a RAID-Z. If you create a five-disk RAID-Z,
    it will remain a five-disk RAID-Z forever. Don’t go thinking you can add an additional
    disk to a RAID-Z for more storage. You can’t.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能在 RAID-Z 中添加或移除磁盘。如果你创建了一个五盘的 RAID-Z，它将永远是一个五盘的 RAID-Z。不要以为你可以向 RAID-Z 中添加额外的磁盘来增加存储空间，你不能这样做。
- en: If you’re using disks over 2TB, there’s a nontrivial chance of a second drive
    failing as you repair the first drive. For large disks, you should probably consider
    RAID-Z2.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是超过 2TB 的磁盘，在修复第一个磁盘时，第二个磁盘出现故障的可能性是非同小可的。对于大容量磁盘，你应该考虑使用 RAID-Z2。
- en: '**RAID-Z2**'
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**RAID-Z2**'
- en: RAID-Z2 stripes parity and data across every disk in the VDEV, much like RAID-Z1,
    but doubles the amount of parity information. This means a RAID-Z2 can withstand
    the loss of up to two disks. You can’t add or remove disks from a RAID-Z2\. It
    is slightly slower than RAID-Z.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: RAID-Z2 将奇偶校验和数据分条存储在 VDEV 中的每个磁盘上，类似于 RAID-Z1，但将奇偶校验信息的数量加倍。这意味着 RAID-Z2 可以承受最多两个磁盘的损失。你不能在
    RAID-Z2 中添加或移除磁盘。它的速度稍慢于 RAID-Z。
- en: A RAID-Z2 must have four or more disks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: RAID-Z2 必须包含四个或更多的磁盘。
- en: '**RAID-Z3**'
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**RAID-Z3**'
- en: Triple parity is for the most important data or those sysadmins with a whole
    bunch of disks and no time to fanny about. You can lose up to three disks in your
    RAID-Z3 without losing data. As with any other RAID-Z, you can’t add or remove
    disks from a RAID-Z3.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 三重奇偶校验适用于最重要的数据，或者是那些有大量磁盘且没有时间浪费的系统管理员。你可以在 RAID-Z3 中丢失最多三个磁盘而不丢失数据。和其他 RAID-Z
    一样，你不能在 RAID-Z3 中添加或移除磁盘。
- en: A RAID-Z3 must have five or more disks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: RAID-Z3 必须包含五个或更多的磁盘。
- en: '**Log and Cache VDEVs**'
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**日志和缓存 VDEVs**'
- en: Pools can improve performance with special-purpose VDEVs. Only adjust or implement
    these if performance problems demand them; don’t add them proactively.^([1](footnote.xhtml#ch12fn1))
    Most people don’t need them, so I won’t go into details, but you should know they
    exist in case you get unlucky.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 池可以通过专用的 VDEV 来提升性能。只有在性能问题需要时才调整或实现这些 VDEV；不要主动添加它们。^([1](footnote.xhtml#ch12fn1))
    大多数人不需要它们，因此我不会详细讲解，但你应该知道它们的存在，以防万一你不走运。
- en: The *Separate Intent Log (SLOG* or *ZIL)* is ZFS’s filesystem journal. Pending
    writes get dumped to the SLOG and then arranged more properly in the primary pool.
    Every pool dedicates a chunk of disk space for a SLOG, but you can use a separate
    device for the SLOG instead. You need faster writes? Install a really fast drive
    and dedicate it to the SLOG. The pool will dump all its initial writes to the
    fast disk device and then migrate those writes to the slower media as time permits.
    A dedicated fast SLOG will also smooth out bursty I/O.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*分离意图日志（SLOG* 或 *ZIL）* 是 ZFS 的文件系统日志。待写入的数据会被转存到 SLOG，然后以更合适的方式安排到主池中。每个池都会为
    SLOG 分配一块磁盘空间，但你也可以使用一个独立的设备来作为 SLOG。如果你需要更快的写入速度，可以安装一个非常快速的磁盘并将其专门用作 SLOG。池会将所有初始写入操作转存到快速磁盘设备上，然后随着时间推移将这些写入操作迁移到较慢的介质中。一个专用的快速
    SLOG 还会平滑突发的 I/O。'
- en: The *Level 2 Adaptive Replacement Cache (L2ARC)* is like the SLOG but for reads.
    ZFS keeps the most recently accessed and the most frequently accessed data in
    memory. By adding a really fast device as an L2ARC, you expand the amount of data
    ZFS can provide from cache instead of calling from slow disk. An L2ARC is slower
    than memory but faster than the slow disk.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*二级自适应替换缓存（L2ARC）*类似于SLOG，但用于读取。ZFS将最近访问和最常访问的数据保存在内存中。通过添加一个非常快的设备作为L2ARC，你可以扩大ZFS从缓存提供的数据量，而不是从慢磁盘读取。L2ARC比内存慢，但比慢磁盘快。'
- en: '**RAID-Z and Pools**'
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**RAID-Z和池**'
- en: You can add VDEVs to a pool. You can’t add disks to a RAID-Z VDEV. Think about
    your storage needs and your hardware before creating your pools.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以向池中添加VDEV，但不能向RAID-Z VDEV添加磁盘。创建池之前，请考虑好你的存储需求和硬件配置。
- en: Suppose you have a server that can hold 20 hard drives, but you have only 12
    drives. You create a single RAID-Z2 VDEV out of those 12 drives, thinking that
    you’ll add more drives to the pool later if you need them. You haven’t even finished
    installing the server, and already you’ve failed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一台可以容纳20个硬盘的服务器，但你只有12个硬盘。你用这12个硬盘创建了一个RAID-Z2 VDEV，想着如果以后需要，可以再往池中添加更多硬盘。但你甚至还没安装完服务器，就已经失败了。
- en: You can add multiple identical VDEVs to a pool. If you create a pool with a
    12-disk VDEV, and the host can hold only another 8 disks, there’s no way to create
    a second identical VDEV. A 12-disk RAID-Z2 isn’t identical to an 8-disk RAID-Z2\.
    You can force ZFS to accept the different VDEVs, but performance will suffer.
    Adding a VDEV to a pool is irreversible.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以向池中添加多个相同的VDEV。如果你创建了一个12磁盘的VDEV，而主机只能再容纳8个磁盘，那就无法创建第二个相同的VDEV。一个12磁盘的RAID-Z2和一个8磁盘的RAID-Z2并不相同。你可以强制ZFS接受不同的VDEV，但性能会受到影响。向池中添加VDEV是不可逆的操作。
- en: Plan ahead. Look at your physical gear. Decide how you will expand your storage.
    This 20-drive server would be fine with two 10-disk RAID-Z2 VDEVs, or one 12-disk
    pool and a separate 8-disk pool. Don’t sabotage yourself.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 提前规划。看看你的物理设备。决定如何扩展存储。这个20驱动的服务器可以使用两个10磁盘的RAID-Z2 VDEV，或者一个12磁盘的池和一个独立的8磁盘池。不要让自己受阻。
- en: Once you know what sort of VDEV you want to use, you can create a pool.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你知道了想要使用哪种VDEV，就可以创建池了。
- en: '**Managing Pools**'
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**管理池**'
- en: Now that you understand the different VDEV types and have indulged in planning
    your storage, let’s create some different types of zpools. Start by setting your
    disk block size.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经理解了不同的VDEV类型并开始规划你的存储，我们来创建一些不同类型的zpool。首先设置你的磁盘块大小。
- en: '***ZFS and Disk Block Size***'
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***ZFS和磁盘块大小***'
- en: '[Chapter 10](ch10.xhtml#ch10) covered how modern disks have two different sector
    sizes, 512 bytes and 4KB. While a filesystem can safely assume a disk has 4KB
    sectors, if your filesystem assumes the disk has 512-byte sectors and the disk
    really has 4KB sectors, your performance will plunge. ZFS, of course, assumes
    that disks have 512-byte sectors. If your disk really has 512-byte sectors, you’re
    good. If you’re not sure what size the physical sectors are, though, err on the
    side of caution and tell ZFS to use 4KB sectors. Control ZFS’s disk sector assumptions
    with the *ashift* property. An ashift of 9 tells ZFS to use 512-byte sectors,
    while an ashift of 12 indicates 4KB sectors. Control ashift with the sysctl `vfs.zfs.min_auto_ashift`.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[第10章](ch10.xhtml#ch10)讲解了现代磁盘有两种不同的扇区大小，512字节和4KB。如果文件系统假设磁盘具有4KB的扇区，而实际上磁盘有512字节的扇区，你的性能将会急剧下降。ZFS当然假设磁盘具有512字节的扇区。如果你的磁盘确实有512字节的扇区，那就没问题了。但如果你不确定物理扇区的大小，最好还是谨慎些，告诉ZFS使用4KB的扇区。通过*ashift*属性控制ZFS的磁盘扇区假设。ashift为9时，ZFS使用512字节的扇区，而ashift为12时表示4KB扇区。通过sysctl
    `vfs.zfs.min_auto_ashift`来控制ashift。'
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Make this permanent by setting it in */etc/sysctl.conf*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在*/etc/sysctl.conf*中设置它，使其永久生效。
- en: You *must* set ashift before creating a pool. Setting it after pool creation
    has no effect.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建池之前，你*必须*设置ashift。池创建后再设置ashift没有效果。
- en: If you’re not sure what size sectors your disks have, use an ashift of 12\.
    That’s what the FreeBSD installer does. You’ll lose a small amount of performance,
    but using an ashift of 9 on 4KB disks will drain system performance.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不确定磁盘的扇区大小，可以使用ashift为12。这也是FreeBSD安装程序所做的。你会损失一点性能，但如果在4KB磁盘上使用ashift为9，会大幅度降低系统性能。
- en: Now create your pools.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建你的池。
- en: '***Creating and Viewing Pools***'
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***创建和查看池***'
- en: Create a pool with the zpool `create` command.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用zpool `create`命令创建一个池。
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If the command succeeds, you get no output back.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令成功执行，你不会收到任何输出。
- en: 'Here, I create a pool named *db*, using a mirror VDEV and two GPT-labeled partitions:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我创建了一个名为 *db* 的池，使用镜像 VDEV 和两个带有 GPT 标签的分区：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The structure we assign gets reflected in the pool status.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分配的结构会在池状态中反映出来。
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The pool db contains a single VDEV, named *mirror-0* ➊. It includes two partitions
    with GPT labels, */dev/gpt/zfs3* ➋ and */dev/gpt/zfs* ➌. All of those partitions
    are online.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 池 db 包含一个名为 *mirror-0* ➊ 的 VDEV。它包括两个带有 GPT 标签的分区，*/dev/gpt/zfs3* ➋ 和 */dev/gpt/zfs*
    ➌。所有这些分区都在线。
- en: 'If you don’t include a VDEV name, zpool(8) creates a striped pool with no redundancy.
    Here, I create a striped pool called *scratch*:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有包括 VDEV 名称，`zpool(8)` 会创建一个没有冗余的条带池。在这里，我创建了一个名为 *scratch* 的条带池：
- en: '[PRE21]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The pool status shows each VDEV, named after the underlying disk.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 池状态显示每个 VDEV，名字来自底层磁盘。
- en: '[PRE22]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Creating any type of RAID-Z looks much like creating a mirror. Just use the
    correct VDEV type.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 创建任何类型的 RAID-Z 看起来都很像创建镜像。只需要使用正确的 VDEV 类型。
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The pool status closely resembles that of a mirror, but with more disks in the
    VDEV.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 池状态与镜像的状态非常相似，但 VDEV 中有更多的磁盘。
- en: '***Multi-VDEV Pools***'
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***多 VDEV 池***'
- en: 'When you’re creating a pool, the keywords `mirror`, `raidz`, `raidz2`, and
    `raidz3` all tell zpool(8) to create a new VDEV. Any disks listed after one of
    those keywords goes into creating a new VDEV. To create a pool with multiple VDEVs,
    you’d do something like this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建池时，关键字 `mirror`、`raidz`、`raidz2` 和 `raidz3` 都会告诉 `zpool(8)` 创建一个新的 VDEV。列在这些关键字后的任何磁盘都会用于创建新的
    VDEV。要创建一个包含多个 VDEV 的池，你可以像这样操作：
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here, I create a pool containing two RAID-Z VDEVs, each with three disks:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我创建了一个包含两个 RAID-Z VDEV 的池，每个 VDEV 包含三个磁盘：
- en: '[PRE25]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: A zpool status on this new pool will look a little different.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个新池执行 `zpool status` 命令时，显示的信息会稍有不同。
- en: '[PRE26]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This pool contains a VDEV called `raidz1-0` ➊ with three disks in it. There’s
    a second VDEV, named `raidz1-1` ➋, with three disks in it. It’s very clear that
    these are identical pools. Data gets striped across both VDEVs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个池包含一个名为 `raidz1-0` ➊ 的 VDEV，其中有三个磁盘。还有一个名为 `raidz1-1` ➋ 的第二个 VDEV，里面也有三个磁盘。很明显，这些池是相同的。数据在两个
    VDEV 之间进行条带化存储。
- en: '***Destroying Pools***'
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***销毁池***'
- en: To destroy a pool, use `zpool destroy` and the pool name.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 要销毁一个池，使用 `zpool destroy` 和池的名称。
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note that zpool doesn’t ask whether you’re really sure before destroying the
    pool. Being sure you want to destroy the pool is your problem, not zpool(8)’s.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`zpool` 不会在销毁池之前询问你是否真的确定。是否确定要销毁池是你的问题，而不是 `zpool(8)` 的问题。
- en: '***Errors and -f***'
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***错误和 -f***'
- en: If you enter a command that doesn’t make sense, zpool(8) will complain.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你输入一个没有意义的命令，`zpool(8)` 会报错。
- en: '[PRE28]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The first thing you see when reading the error message is “use `-f` to override
    this error.” Many sysadmins read this as “`-f` makes this problem go away.” What
    ZFS is really saying, though, is “Your command line is a horrible mistake. Add
    `-f` to do something unfixable, harmful to system stability, and that you’ll regret
    as long as this system lives.”
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读错误信息时你首先看到的是“使用 `-f` 来覆盖此错误。”许多系统管理员会将其理解为“`-f` 使这个问题消失。”然而，ZFS 实际上是在说：“你的命令行是个大错误。添加
    `-f` 会做一些无法修复、对系统稳定性有害的事，而且你会后悔直到这个系统不再运行。”
- en: Most zfs(8) and zpool(8) error messages are meaningful, but you have to read
    them carefully. If you don’t understand the message, fall back on the troubleshooting
    instructions in [Chapter 1](ch01.xhtml#ch01). Often, reexamining what you typed
    will expose the problem.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 `zfs(8)` 和 `zpool(8)` 错误信息都有意义，但你需要仔细阅读。如果你不理解信息，可以查看 [第 1 章](ch01.xhtml#ch01)
    的故障排除说明。通常，重新检查你输入的内容会暴露问题所在。
- en: In this example, I asked zpool(8) to create a pool with a RAID-Z VDEV containing
    three disks and a second RAID-Z VDEV containing only two disks. I screwed up this
    command line. Adding `-f` and proceeding to install my database to the new malformed
    *db* pool would only ensure that I have to recreate this pool and reinstall the
    database at a later date.^([2](footnote.xhtml#ch12fn2)) If you find yourself in
    this situation, investigate `zfs send` and `zfs recv`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我让 `zpool(8)` 创建一个包含三个磁盘的 RAID-Z VDEV 和一个只包含两个磁盘的第二个 RAID-Z VDEV。我搞错了这个命令行。添加
    `-f` 并继续将我的数据库安装到新的错误格式的 *db* 池中，只会确保我以后不得不重新创建这个池并重新安装数据库。^([2](footnote.xhtml#ch12fn2))
    如果你发现自己处于这种情况，调查 `zfs send` 和 `zfs recv`。
- en: '**Copy-On-Write**'
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**写时复制**'
- en: 'In both ordinary filesystems and ZFS, files exist as blocks on the disk. When
    you edit a file in a traditional filesystem, the filesystem picks up the block,
    modifies it, and sets it back down in the same place on the disk. A system problem
    halfway through that write can cause a *shorn write*: a file that’s 50 percent
    the old version, 50 percent the new version, and probably 100 percent unusable.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通文件系统和 ZFS 中，文件以块的形式存在于磁盘上。当你在传统文件系统中编辑文件时，文件系统会取出块，修改它，并将其放回磁盘上的同一位置。写入过程中的系统故障可能会导致*损坏的写入*：文件的50%是旧版本，50%是新版本，可能完全无法使用。
- en: ZFS never overwrites the existing blocks in a file. When a file changes, ZFS
    identifies the blocks that must change and writes them to a new chunk of disk
    space. The old version is left intact. This is called *copy-on-write (COW)*. With
    copy-on-write, a short write might lose the newest changes to the file, but the
    previous version of the file will remain intact.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS 永远不会覆盖文件中现有的块。当文件发生更改时，ZFS 会识别必须更改的块，并将其写入新的磁盘空间。旧版本会保持不变。这被称为*写时复制（COW）*。通过写时复制，短时间的写入可能会丢失文件的最新更改，但文件的先前版本会保持完好。
- en: Never corrupting files is a great benefit to copy-on-write, but COW opens up
    other possibilities. The metadata blocks are also copy-on-write, all the way up
    to the *uberblocks* that form the root of the ZFS pool’s data tree. ZFS creates
    snapshots by tracking the blocks that contain old versions of a file. While that
    sounds simple, the details are what will lead you astray.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 不会破坏文件是写时复制（COW）的一个很大优点，但 COW 也带来了其他可能性。元数据块也是写时复制，一直到形成 ZFS 池数据树根的*uberblocks*。ZFS
    通过跟踪包含文件旧版本的块来创建快照。虽然这听起来很简单，但细节会让你迷失方向。
- en: '**Snapshots**'
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**快照**'
- en: A snapshot is a copy of a dataset as it existed at a specific instant. Snapshots
    are read-only and never change. You can access the contents of a snapshot to access
    older versions of files or even deleted files. While snapshots are read-only,
    you can roll the dataset back to the snapshot. Take a snapshot before upgrading
    a system, and if the upgrade goes horribly wrong, you can fall back to the snapshot.
    ZFS uses snapshots to provide many features, such as boot environments (see “[Boot
    Environments](ch12.xhtml#lev464)” on [page 276](ch12.xhtml#page_276)). Best of
    all, depending on your data, snapshots can take up only tiny amounts of space.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 快照是数据集在某一特定时刻的副本。快照是只读的，永远不会改变。你可以访问快照的内容来访问文件的旧版本或甚至已删除的文件。虽然快照是只读的，你可以将数据集恢复到快照状态。在升级系统之前先创建一个快照，如果升级出了严重问题，你可以回滚到快照。ZFS
    使用快照提供许多功能，例如启动环境（见 [启动环境](ch12.xhtml#lev464) 在 [第276页](ch12.xhtml#page_276)）。最棒的是，根据你的数据，快照可能只占用极少的空间。
- en: Every dataset has a bunch of metadata, all built as a tree from a top-level
    block. When you create a snapshot, ZFS duplicates that top-level block. One of
    those metadata blocks goes with the dataset, while the other goes with the snapshot.
    The dataset and the snapshot share the data blocks within the dataset.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集都有一堆元数据，所有这些元数据都是从一个顶级块构建成树形结构的。当你创建快照时，ZFS 会复制这个顶级块。一个元数据块与数据集一起，而另一个与快照一起。数据集和快照共享数据集中的数据块。
- en: Deleting, modifying, or overwriting a file on the live dataset means allocating
    new blocks for the new data and disconnecting blocks containing the old data.
    Snapshots need some of those old data blocks, however. Before discarding an old
    block, ZFS checks to see whether a snapshot still needs it. If a snapshot needs
    a block, but the dataset no longer does, ZFS keeps the block.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 删除、修改或覆盖活动数据集中的文件意味着为新数据分配新的块，并断开包含旧数据的块。然而，快照需要一些旧数据块。在丢弃旧块之前，ZFS 会检查是否有快照仍然需要该块。如果快照需要一个块，但数据集不再需要，ZFS
    会保留该块。
- en: So, a snapshot is merely a list of which blocks the dataset used at the time
    the snapshot was taken. Creating a snapshot tells ZFS to preserve those blocks,
    even if the dataset no longer needs those blocks.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，快照仅仅是记录在创建快照时数据集使用的那些块的列表。创建快照会告诉 ZFS 保留这些块，即使数据集不再需要这些块。
- en: '***Creating Snapshots***'
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***创建快照***'
- en: Use the `zfs snapshot` command to create snapshots. Specify the dataset by its
    full path, then add `@` and a snapshot name. I habitually name my snapshots after
    the date and time I create the snapshot, for reasons that will become clear by
    the end of this chapter.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `zfs snapshot` 命令来创建快照。通过其完整路径指定数据集，然后添加 `@` 和快照名称。我习惯性地根据创建快照的日期和时间命名快照，原因将在本章末尾变得清晰。
- en: I’m about to do maintenance on user home directories, removing old stuff to
    free up space. I’m pretty sure that someone will whinge about me removing their
    files,^([3](footnote.xhtml#ch12fn3)) so I want to create a snapshot before cleaning
    up.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我即将对用户的主目录进行维护，删除一些旧文件以释放空间。我很确定有人会抱怨我删除了他们的文件^([3](footnote.xhtml#ch12fn3))，所以在清理之前，我想先创建一个快照。
- en: '[PRE29]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: I don’t get any feedback. Did anything happen? View all your snapshots with
    the `-t snapshot` argument to `zfs list`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有收到任何反馈。发生了什么事吗？通过给 `zfs list` 添加 `-t snapshot` 参数来查看所有快照。
- en: '[PRE30]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The snapshot exists. The `USED` column shows that it uses zero disk space ➊:
    it’s identical to the dataset it came from. As snapshots are read-only, available
    space ➋ shown by `AVAIL` is just not relevant. The `REFER` column shows that this
    snapshot pulls in 4.68GB of disk space ➌. If you check, you’ll see that’s the
    size of *zroot/usr/home*. Finally, the `MOUNTPOINT` column shows that this snapshot
    isn’t mounted ➍.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 快照已经存在。`USED` 列显示它使用了零磁盘空间 ➊：它与它所在的数据集完全相同。由于快照是只读的，`AVAIL` 列显示的可用空间 ➋ 对它来说并不相关。`REFER`
    列显示该快照占用了 4.68GB 的磁盘空间 ➌。如果你检查一下，你会发现这是 *zroot/usr/home* 的大小。最后，`MOUNTPOINT` 列显示该快照没有被挂载
    ➍。
- en: This is an active system, and other people are logged into it. I wait a moment
    and check my snapshots again.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个活跃的系统，其他人也已经登录。我等了一会儿，然后再次检查我的快照。
- en: '[PRE31]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The snapshot now uses 96KB ➊. A user changed something on the dataset, and the
    snapshot gets charged with the space needed to maintain the difference.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 该快照现在使用了 96KB ➊。某个用户修改了数据集，快照因此占用了维持差异所需的空间。
- en: Now I go on my rampage, and get rid of the files I think are garbage.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我开始清理，删除我认为是垃圾的文件。
- en: '[PRE32]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This snapshot now uses 1.62GB of space. Those are files that I’ve deleted but
    that are still available in the snapshot. I’ll keep this snapshot for a little
    while to give the users a chance to complain.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个快照使用了 1.62GB 的空间。这些是我已删除的文件，但它们仍然可以从快照中获取。我会保留这个快照一段时间，给用户一个投诉的机会。
- en: '***Accessing Snapshots***'
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***访问快照***'
- en: Every ZFS dataset has a hidden *.zfs* directory in its root. It won’t show up
    in ls(1); you have to know it exists. That directory has a snapshot directory,
    which contains a directory named after each snapshot. The contents of the snapshot
    are in that directory.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 ZFS 数据集的根目录中都有一个隐藏的 *.zfs* 目录。它不会出现在 ls(1) 中；你需要知道它的存在。该目录包含一个快照目录，其中包含一个以每个快照命名的子目录。快照的内容就在该目录中。
- en: For our snapshot *zroot/usr/home@2018-07-21-13:09:00*, we’d go to */usr/home/.zfs/snapshot/2018-07-21-13:09:00*.
    While the *.zfs* directory doesn’t show up in ls(1), once you’re in it, ls(1)
    works normally. That directory contains every file as it existed when I created
    the snapshot, even if I’ve deleted or changed that file since creating that snapshot.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的快照 *zroot/usr/home@2018-07-21-13:09:00*，我们需要进入 */usr/home/.zfs/snapshot/2018-07-21-13:09:00*。虽然
    *.zfs* 目录在 ls(1) 中不会显示，但一旦进入该目录，ls(1) 就能正常工作。该目录包含了创建快照时文件的所有内容，即使我在创建该快照后删除或更改了这些文件。
- en: Recovering a file from the snapshot requires only copying the file from the
    snapshot to a read-write location.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 从快照中恢复文件只需要将文件从快照复制到一个可读写的位置。
- en: '***Destroying Snapshots***'
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***销毁快照***'
- en: A snapshot is a dataset, just like a filesystem-style dataset. Remove it with
    `zfs` `destroy`.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 快照是一个数据集，就像一个文件系统风格的数据集一样。使用 `zfs destroy` 命令来删除它。
- en: '[PRE33]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The space used by the snapshot is now available for more junk files.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，快照占用的空间可以用来存储更多的垃圾文件。
- en: '**Compression**'
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**压缩**'
- en: 'Snapshots aren’t the only way ZFS can save space. ZFS uses on-the-fly compression,
    transparently inspecting the contents of each file and squeezing its size if possible.
    With ZFS, your programs don’t need to compress their log files: the filesystem
    will do it for you in real time. While FreeBSD enables compression by default
    at install time, you’ll use it more effectively if you understand how it works.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 快照并不是 ZFS 节省空间的唯一方式。ZFS 使用实时压缩，透明地检查每个文件的内容，并在可能的情况下压缩其大小。使用 ZFS 时，你的程序不需要自己压缩日志文件：文件系统会实时为你完成压缩。虽然
    FreeBSD 在安装时默认启用压缩，但如果你了解它是如何工作的，你会更有效地使用它。
- en: Compression changes system performance, but probably not in the way you think
    it would. You’ll need CPU time to compress and decompress data as it goes to and
    from the disk. Most disk requests are smaller than usual, however. You essentially
    exchange processor time for disk I/O. Every server I manage, whether bare metal
    or virtual, has far, far more processor capacity than disk I/O, so that’s a trade
    I’ll gleefully make. The end result is that using ZFS compression most often *increases*
    performance.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩会改变系统性能，但可能不是你想象的方式。你需要CPU时间来压缩和解压缩数据，因为数据在磁盘之间进出。尽管如此，大多数磁盘请求通常比平常要小。你基本上是用处理器时间换取磁盘I/O。我管理的每一台服务器，无论是裸机还是虚拟机，都拥有远远超过磁盘I/O的处理器能力，所以这是我愿意做出的交换。最终结果是，使用ZFS压缩通常会*提高*性能。
- en: Compression works differently on different datasets. Binary files are already
    pretty tightly compressed; compressing */usr/bin* doesn’t save much space. Compressing
    */var/log*, though, often results in reducing file size by a factor of six or
    seven. Check the property `compressratio` to see how effectively compression shrinks
    your data. My hosts write to logs far more often than they write binaries. I’ll
    gleefully accept a sixfold performance increase for the most common task.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩在不同的数据集上表现不同。二进制文件本身已经压缩得非常紧凑；压缩*/usr/bin*几乎不会节省空间。然而，压缩*/var/log*通常会将文件大小减少六到七倍。检查属性`compressratio`以查看压缩效果如何缩小数据大小。我的主机写日志的频率远远高于写二进制文件。我会欣然接受这种常见任务的六倍性能提升。
- en: ZFS supports many compression algorithms, but the default is *lz4*. The lz4
    algorithm is special in that in quickly recognizes incompressible files. When
    you write a binary to disk, lz4 looks at it and says, “Nope, I can’t help you,”
    and immediately quits trying. This eliminates pointless CPU load. It effectively
    compresses files that can be compressed, however.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS支持多种压缩算法，但默认使用*lz4*。lz4算法的特别之处在于它能迅速识别无法压缩的文件。当你将二进制文件写入磁盘时，lz4会检查它并说：“不，我帮不上忙，”然后立即停止尝试。这消除了无谓的CPU负载。然而，它会有效地压缩那些可以压缩的文件。
- en: '**Pool Integrity and Repair**'
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**池完整性与修复**'
- en: Every piece of data in a ZFS pool has an associated cryptographic hash stored
    in its metadata to verify integrity. Every time you access a piece of data, ZFS
    recomputes the hash of every block in that data. When ZFS discovers corrupt data
    in a pool with redundancy, it transparently corrects that data and proceeds. If
    ZFS discovers corrupt data in a pool without redundancy, it gives a warning and
    refuses to serve the data. If your pool has identified any data errors, they’ll
    show up in `zpool status`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS池中的每一块数据都有一个与之关联的加密哈希，存储在其元数据中以验证数据的完整性。每次访问数据时，ZFS会重新计算该数据中每个块的哈希值。当ZFS在具有冗余的池中发现数据损坏时，它会透明地修正这些数据并继续。如果ZFS在没有冗余的池中发现数据损坏，它会发出警告并拒绝提供该数据。如果你的池中已识别任何数据错误，它们会显示在`zpool
    status`中。
- en: '***Integrity Verification***'
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***完整性验证***'
- en: In addition to the on-the-fly verification, ZFS can explicitly walk the entire
    filesystem tree and verify every chunk of data in the pool. This is called a *scrub*.
    Unlike UFS’s fsck(8), scrubs happen while the pool is online and in use. If you’ve
    previously run a scrub, that will also show up in the pool status.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 除了即时验证，ZFS还可以显式地遍历整个文件系统树并验证池中的每个数据块。这叫做*扫描*。与UFS的fsck(8)不同，扫描在池在线并且正在使用时进行。如果你之前运行过扫描，结果也会显示在池状态中。
- en: '[PRE34]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: To scrub a pool, run `zpool scrub` and give the pool name.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 要扫描池，运行`zpool scrub`并指定池名称。
- en: '[PRE35]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: You can watch the progress of the scrub with `zpool status`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`zpool status`查看扫描进度。
- en: Scrubbing a pool reduces its performance. If your system is already pushing
    its limits, scrub pools only during off hours. You can cancel a scrub^([4](footnote.xhtml#ch12fn4))
    with the `-s` option.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描池会降低其性能。如果你的系统已经接近极限，请仅在非高峰时段扫描池。你可以使用`-s`选项取消扫描^([4](footnote.xhtml#ch12fn4))。
- en: '[PRE36]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Run another scrub once the load drops.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 当负载下降时，再次运行扫描。
- en: '***Repairing Pools***'
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***修复池***'
- en: Disks fail. That’s what they’re for. The point of redundancy is that you can
    replace failing or flat-out busted disks with working disks and restore redundancy.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘会发生故障。这就是它们的用途。冗余的意义在于你可以用正常工作的磁盘替换故障或完全坏掉的磁盘，并恢复冗余。
- en: Mirror and RAID-Z virtual devices are specifically designed to reconstruct the
    data lost when a disk fails. They’re much like RAID in that regard. If one disk
    in a ZFS mirror dies, you replace the dead disk, and ZFS copies the surviving
    mirror onto the new disk. If a disk in a RAID-Z VDEV fails, you replace the busted
    drive, and ZFS rebuilds the data on that disk from parity data.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像和RAID-Z虚拟设备专门设计用来重建磁盘故障时丢失的数据。从这个角度看，它们与RAID非常相似。如果ZFS镜像中的一块磁盘故障，你只需更换故障磁盘，ZFS会将存活的镜像数据复制到新磁盘上。如果RAID-Z
    VDEV中的一块磁盘故障，你只需更换损坏的磁盘，ZFS会从奇偶校验数据中重建该磁盘上的数据。
- en: In ZFS, this reconstruction is called *resilvering*. Like other ZFS integrity
    operations, resilvering takes place only on live filesystems. Resilvering isn’t
    quite like rebuilding a RAID disk from parity, as ZFS leverages its knowledge
    of the filesystem to optimize repopulating the replacement device. Resilvering
    begins automatically when you replace a failed device. ZFS resilvers at a low
    priority so that it doesn’t interfere with normal operations.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在ZFS中，这种重建操作被称为*resilvering*。与其他ZFS完整性操作类似，resilvering只会在活动文件系统上进行。Resilvering不像从奇偶校验重建RAID磁盘那样，ZFS利用其对文件系统的了解来优化重新填充替换设备的过程。当你更换故障设备时，resilvering会自动开始。ZFS会以低优先级进行resilvering，以免干扰正常操作。
- en: '***Pool Status***'
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***池状态***'
- en: The `zpool status` command shows the health of the underlying storage hardware
    in the `STATE` field. We’ve seen a couple examples of healthy pools, so let’s
    take a look at an unhealthy pool.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`zpool status`命令在`STATE`字段中显示底层存储硬件的健康状态。我们已经看到一些健康的池，接下来让我们看看一个不健康的池。'
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The pool state is `DEGRADED` ➊. If you look further down the output, you’ll
    see more `DEGRADED` entries and an `UNAVAIL` ➍. What exactly does that mean?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 池的状态是`DEGRADED` ➊。如果你进一步查看输出内容，会看到更多`DEGRADED`条目和一个`UNAVAIL` ➍。这究竟是什么意思？
- en: Errors in a pool percolate upward. The pool state is a summary of the health
    of the pool as a whole. The whole pool shows up as `DEGRADED` because the pool’s
    virtual device *mirror-0* ➋ is `DEGRADED`. This error comes from an underlying
    disk being in the `UNAVAIL` state. We get the ZFS GUID ➌ for this disk, and the
    label used to create the pool ➎.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 池中的错误会向上蔓延。池状态是对池整体健康状况的总结。当池的虚拟设备*mirror-0* ➋处于`DEGRADED`状态时，整个池会显示为`DEGRADED`。这个错误源于底层磁盘处于`UNAVAIL`状态。我们可以获取该磁盘的ZFS
    GUID ➌，以及用于创建池的标签 ➎。
- en: ZFS pools show an error when an underlying device has an error. When a pool
    has a state other than `ONLINE`, dig through the VDEV and disk listings until
    you find the real problem.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当底层设备出现故障时，ZFS池会显示错误。当池的状态不是`ONLINE`时，需深入查看VDEV和磁盘列表，直到找到真正的问题。
- en: 'Pools, VDEVs, and disks can have six states:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 池、VDEV和磁盘可以有六种状态：
- en: '**ONLINE** The device is functioning normally.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**ONLINE** 该设备正常运行。'
- en: '**DEGRADED** The pool or VDEV has at least one provider missing, offline, or
    generating errors more quickly than ZFS tolerates. Redundancy is handling the
    error, but you need to address this right now.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**DEGRADED** 该池或VDEV至少有一个提供者丢失、离线或产生的错误超过了ZFS的容忍范围。冗余机制正在处理该错误，但你需要立即解决这个问题。'
- en: '**FAULTED** A faulted disk is corrupt or generating errors more quickly than
    ZFS can tolerate. A faulted VDEV takes the last known good copy of the data. A
    two-disk mirror with two bad disks faults.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**FAULTED** 故障磁盘已损坏或产生的错误超过了ZFS的容忍范围。故障VDEV会采用最后一个已知的良好数据副本。一个由两块故障磁盘组成的两盘镜像会处于故障状态。'
- en: '**UNAVAIL** ZFS can’t open the disk. Maybe it’s been removed, shut off, or
    that iffy cable finally failed. It’s not there, so ZFS can’t use it.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**UNAVAIL** ZFS无法打开该磁盘。也许它已经被移除、关闭，或者那个不稳定的电缆终于坏了。磁盘不在，所以ZFS无法使用它。'
- en: '**OFFLINE** This device has been deliberately turned off.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**OFFLINE** 该设备已被故意关闭。'
- en: '**REMOVED** Some hardware detects when a drive is physically removed while
    the system is running, letting ZFS set the REMOVED flag. When you plug the drive
    back in, ZFS tries to reactivate the disk.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**REMOVED** 一些硬件可以检测到在系统运行时物理移除的驱动器，从而让ZFS设置REMOVED标志。当你将驱动器重新插入时，ZFS会尝试重新激活该磁盘。'
- en: Our missing disk is in the UNAVAIL state. For whatever reason, ZFS can’t access
    */dev/gpt/zfs3*, but the disk mirror is still serving data because it has a working
    disk. Here’s where you get to run around to figure out where that disk went. How
    you manage ZFS depends on what you discover.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们丢失的磁盘处于UNAVAIL状态。由于某种原因，ZFS无法访问*/dev/gpt/zfs3*，但磁盘镜像仍然可以提供数据，因为它有一块工作中的磁盘。现在你需要跑来跑去，搞清楚那个磁盘去哪儿了。你如何管理ZFS取决于你发现了什么问题。
- en: '**Reattaching and Detaching Drives**'
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**重新连接和断开驱动器**'
- en: 'Unavailable drives might not be dead. They might be disconnected. If you wiggle
    a drive tray and suddenly get a green light, the disk is fine but the connection
    is faulty. You should address that hardware problem, yes, but in the meantime,
    you can reactivate the drive. You can also reactivate deliberately removed drives.
    Use the `zpool online` command with the pool name and the GUID of the missing
    disk as arguments. If the disk in my example pool were merely disconnected, I
    could reactivate it like so:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 不可用的磁盘可能并没有坏掉，它们可能只是断开了连接。如果你晃动磁盘托盘并突然看到绿色指示灯，那说明磁盘本身没问题，但连接出现故障。你应该解决这个硬件问题，是的，但在此期间，你可以重新激活该磁盘。你也可以重新激活故意移除的磁盘。使用`zpool
    online`命令，输入池名称和缺失磁盘的GUID作为参数。如果我示例中的池中的磁盘只是断开了连接，我可以这样重新激活它：
- en: '[PRE38]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ZFS resilvers the drive and resumes normal function.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS会重新同步磁盘并恢复正常功能。
- en: If you want to remove a drive, you can tell ZFS to take it offline with zpool
    offline. Give the pool and disk names as arguments.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想移除一个磁盘，可以告诉ZFS通过`zpool offline`将其下线。作为参数提供池名和磁盘名。
- en: '[PRE39]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Bringing disks offline, physically moving them, bringing them back online, and
    allowing the pools to resilver will let you migrate large storage arrays from
    one SAS cage to another without downtime.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 将磁盘下线、物理移动它们、重新上线并允许池进行重新同步，能够让你在不产生停机时间的情况下将大型存储阵列从一个SAS机架迁移到另一个。
- en: '**Replacing Drives**'
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**更换磁盘**'
- en: 'If the drive isn’t merely loose but flat-out busted, you’ll need to replace
    it with a new drive. ZFS lets you replace drives in several ways, but the most
    common is using `zpool replace`. Use the pool name, the failed provider, and the
    new provider as arguments. Here, I replace the db pool’s */dev/gpt/zfs3* disk
    with */dev/gpt/zfs6*:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果磁盘不仅仅是松动，而是完全坏掉了，你需要用新磁盘替换它。ZFS允许以多种方式更换磁盘，但最常见的方式是使用`zpool replace`。作为参数，提供池名、故障磁盘和新磁盘。在这里，我将db池中的*/dev/gpt/zfs3*磁盘替换为*/dev/gpt/zfs6*：
- en: '[PRE40]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The pool will resilver itself and resume normal operation.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 该池将重新同步并恢复正常操作。
- en: In a large storage array, you can also use successive `zpool replace` operations
    to empty a disk shelf. Only do this if your organization’s operation requirements
    don’t allow you to offline and online disks.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型存储阵列中，你还可以使用连续的`zpool replace`操作来清空一个磁盘架。只有当你所在组织的操作需求不允许将磁盘下线和重新上线时，才做这件事。
- en: '**Boot Environments**'
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**启动环境**'
- en: ZFS helps us cope with one of the most dangerous things sysadmins do. No, not
    our eating habits. No, not a lack of exercise. I’m talking about system upgrades.
    When an upgrade goes well, everybody’s happy. When the upgrade goes poorly, it
    can ruin your day, your weekend, or your job. Nobody likes restoring from backup
    when the mission-critical software chokes on the new version of a shared library.
    Nobody likes to restore from backup.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS帮助我们应对系统管理员所做的最危险的事情之一。不是我们的饮食习惯，也不是缺乏运动。我指的是系统升级。当升级顺利时，大家都很高兴。当升级失败时，可能会毁掉你的工作日、周末甚至工作。没有人喜欢在关键任务软件因共享库的新版本而崩溃时进行备份恢复。没有人喜欢恢复备份。
- en: Through the magic of *boot environments*, ZFS takes advantage of snapshots to
    let you fall back from a system upgrade with only a reboot. A boot environment
    is a clone of the root dataset. It includes the kernel, the base system userland,
    the add-on packages, and the core system databases. Before running an upgrade,
    create a boot environment. If the upgrade goes well, you’re good. If the upgrade
    goes badly, though, you can reboot into the boot environment. This restores service
    while you investigate how the upgrade failed and what you can do to fix those
    problems.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*启动环境*的魔力，ZFS利用快照功能，让你在系统升级后只需要重启就能回退。启动环境是根数据集的克隆。它包括内核、基础系统的用户空间、附加包以及核心系统数据库。在执行升级之前，创建一个启动环境。如果升级顺利进行，那么一切正常。如果升级出了问题，你可以重启进入启动环境。这将恢复服务，让你调查升级失败的原因并修复这些问题。
- en: Boot environments do not work when a host requires a separate boot pool. The
    installer handles boot pools for you. They appear when combining UEFI and GELI,
    or when using ZFS on an MBR-partitioned disk.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 启动环境在主机需要独立启动池时无法使用。安装程序会为你处理启动池。它们在结合UEFI和GELI时出现，或者在使用ZFS时出现在MBR分区的磁盘上。
- en: Using boot environments requires a boot environment manager. I recommend beadm(8),
    available as a package.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用启动环境需要一个启动环境管理器。我推荐使用beadm(8)，它可以作为一个包安装。
- en: '[PRE41]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: You’re now ready to use boot environments.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以使用启动环境了。
- en: '***Viewing Boot Environments***'
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***查看启动环境***'
- en: Each boot environment is a dataset under *zroot/ROOT*. A system where you’ve
    just installed beadm should have only one boot environment. Use `beadm list` to
    view them all.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 每个引导环境都是 *zroot/ROOT* 下的一个数据集。刚安装 beadm 的系统应该只有一个引导环境。使用 `beadm list` 可以查看所有引导环境。
- en: '[PRE42]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This host has one boot environment, named *default* ➊, after the dataset *zroot/ROOT/default*.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这台主机有一个引导环境，名为 *default* ➊，对应的数据集是 *zroot/ROOT/default*。
- en: The Active column ➋ shows whether this boot environment is in use. An `N` means
    that the environment is now in use. An `R` means that this environment will be
    active after a reboot. They appear together when the default environment is running.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Active 列 ➋ 显示这个引导环境是否正在使用中。`N` 表示该环境现在正在使用中。`R` 表示该环境将在重启后生效。当默认环境正在运行时，它们会一起出现。
- en: The Mountpoint column ➌ shows the location of this boot environment’s mount
    point. Most boot environments aren’t mounted unless they’re in use, but you can
    use beadm(8) to mount an unused boot environment.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Mountpoint 列 ➌ 显示了此引导环境的挂载点位置。大多数引导环境在未使用时不会挂载，但你可以使用 beadm(8) 挂载未使用的引导环境。
- en: The Space column ➍ shows the amount of disk space this boot environment uses.
    It’s built on a snapshot, so the dataset probably has more data than this amount
    in it.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Space 列 ➍ 显示此引导环境使用的磁盘空间。它是基于一个快照构建的，因此该数据集中可能有比这个数量更多的数据。
- en: The Created column ➎ shows the date this boot environment was created. In this
    case, it’s the date the machine was installed.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: Created 列 ➎ 显示了此引导环境的创建日期。在本例中，它是机器安装的日期。
- en: Before changing the system, create a new boot environment.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在更改系统之前，创建一个新的引导环境。
- en: '***Creating and Accessing Boot Environments***'
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***创建和访问引导环境***'
- en: Each boot environment needs a name. I recommend names based on the current operating
    system version and patch level or the date. Names like “beforeupgrade” and “dangitall,”
    while meaningful in the moment, will only confuse you later.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 每个引导环境都需要一个名称。我建议使用基于当前操作系统版本和补丁级别或日期的名称。像“beforeupgrade”和“dangitall”这样的名称，虽然当时有意义，但以后会让你感到困惑。
- en: 'Use `beadm create` to make your new boot environment. Here, I check the current
    FreeBSD version, and use that to create the boot environment name:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `beadm create` 创建新的引导环境。在这里，我检查当前的 FreeBSD 版本，并用它来创建引导环境的名称：
- en: '[PRE43]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: I now have two identical boot environments.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我有两个相同的引导环境。
- en: '[PRE44]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: You might notice that the new boot environment already takes up 236KB. This
    is a live system. Between when I created the boot environment and when I listed
    those environments, the filesystem or its metadata changed.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，新创建的引导环境已经占用了 236KB。这是一个实时系统。在我创建引导环境和列出这些环境之间，文件系统或其元数据发生了变化。
- en: The Active column shows that we’re currently using the default boot environment
    and that we’ll be using that on the next boot. If I change my installed packages
    or upgrade the base system, those changes will affect the default environment.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Active 列显示我们当前正在使用默认的引导环境，并且下次启动时将使用该环境。如果我更改了已安装的软件包或升级了基础系统，这些更改将影响默认环境。
- en: Each boot environment is available as a snapshot under *zroot/ROOT*. If you
    want to access a boot environment read-write, use `beadm mount` to temporarily
    mount the boot environment under */tmp*. Unmount those environments with `beadm
    umount`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 每个引导环境都可以作为 *zroot/ROOT* 下的一个快照使用。如果你想访问一个引导环境并进行读写操作，可以使用 `beadm mount` 将其临时挂载到
    */tmp* 下。使用 `beadm umount` 卸载这些环境。
- en: '***Activating Boot Environments***'
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***激活引导环境***'
- en: Suppose you upgrade your packages and the system goes belly-up. Fall back to
    an earlier operating system install by activating a boot environment and rebooting.
    Activate a boot environment with `beadm activate`.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你升级了软件包，系统崩溃了。通过激活一个引导环境并重启，你可以恢复到早期的操作系统安装状态。使用 `beadm activate` 激活引导环境。
- en: '[PRE45]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The default boot environment has its Active flag set to `N`, meaning it’s now
    running. The 11.0-p11 environment has the `R` flag, so after a reboot it will
    be live.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的引导环境的 Active 标志被设置为 `N`，表示当前正在运行。11.0-p11 环境有 `R` 标志，因此重启后它将生效。
- en: Reboot the system and suddenly you’ve fallen back to the previous operating
    system install, without the changes that destabilized your system. That’s much
    simpler than restoring from backup.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 重启系统后，你会突然回到先前的操作系统安装状态，系统中的不稳定更改也不会被保留。与其从备份中恢复，这样更简单。
- en: '***Removing Boot Environments***'
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***删除引导环境***'
- en: After a few upgrades, you’ll find that you’ll never fall back to some of the
    existing boot environments. Once I upgrade this host to, say, 12.2-RELEASE-p29,
    chances are I’ll never ever reboot into 11.0-p11 again. Remove obsolete boot environments
    and free up their disk space with `beadm destroy`.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在几次升级之后，你会发现你再也无法回到某些现有的启动环境。一旦我将这台主机升级到例如 12.2-RELEASE-p29，可能再也无法重启进入 11.0-p11。使用`beadm
    destroy`删除过时的启动环境并释放它们的磁盘空间。
- en: '[PRE46]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Answer `y` when prompted, and beadm will remove the boot environment.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统提示时，回答`y`，beadm 将删除该启动环境。
- en: '***Boot Environments at Boot***'
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***启动环境在启动时***'
- en: So you’ve truly hosed your operating system. Forget getting to multiuser mode,
    you can’t even hit *single*-user mode without generating a spew of bizarre error
    messages. You can select a boot environment right at the loader prompt. This requires
    console access, but so would any other method of rescuing yourself.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你真的搞砸了操作系统。忘了进入多用户模式吧，你甚至无法进入*单用户*模式，除非生成一堆奇怪的错误信息。你可以在加载器提示符下选择一个启动环境。这需要控制台访问，但任何其他自救方法也都需要控制台访问。
- en: The boot loader menu includes an option to select a boot environment. Choose
    that option. You’ll get a new menu listing every boot environment on the host
    by name. Choose your new boot environment and hit ENTER. The system will boot
    into that environment, giving you a chance to figure out why everything went sideways.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 启动加载器菜单包括一个选项，可以选择启动环境。选择该选项后，你会看到一个新的菜单，列出了主机上所有启动环境的名称。选择你想要的启动环境并按下 ENTER
    键。系统将进入该环境，给你一个机会来查明为什么一切都出错了。
- en: '***Boot Environments and Applications***'
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***启动环境与应用程序***'
- en: It’s not enough that your upgrade failed. It might take your application data
    with it.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 升级失败并不足够糟糕，它可能还会带走你的应用程序数据。
- en: Most applications store their data somewhere in the root dataset. MySQL uses
    */var/db/mysql*, while Apache uses */usr/local/www*. This means that falling back
    to an earlier boot environment can revert your application data with the environment.
    Depending on your application, you might want that reversion—or not.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数应用程序将它们的数据存储在根数据集的某个位置。MySQL 使用*/var/db/mysql*，而 Apache 使用*/usr/local/www*。这意味着回退到早期的启动环境时，可能会将你的应用程序数据一并回滚。根据你的应用程序，你可能希望或不希望发生这种回滚。
- en: If an application uses data that shouldn’t be included in the boot environment,
    you need to create a new dataset for that data. I provided an example in “[Unmounted
    Parent Datasets](ch12.xhtml#lev431)” on [page 262](ch12.xhtml#page_262) earlier
    this chapter. Consider your application’s need and separate out your data as appropriate.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个应用程序使用的数据不应该包含在启动环境中，你需要为该数据创建一个新的数据集。我在本章的 “[未挂载的父数据集](ch12.xhtml#lev431)”
    以及 [第 262 页](ch12.xhtml#page_262) 中提供了一个示例。考虑你的应用程序的需求，并根据需要将数据分离。
- en: 'While ZFS has many more features, this covers the topics every sysadmin *must*
    know. Many of you would find clones, delegations, or replication useful. You might
    find the books *FreeBSD Mastery: ZFS* (Tilted Windmill Press, 2015) and *FreeBSD
    Mastery: Advanced ZFS* (Tilted Windmill Press, 2016) by Allan Jude and yours truly
    helpful. You’ll also find many resources on the internet documenting all of these
    topics.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然 ZFS 具有更多功能，但这篇文章涵盖了每个系统管理员*必须*知道的主题。你们中的许多人可能会发现克隆、委派或复制功能很有用。你也许会觉得 Allan
    Jude 和我自己所著的《FreeBSD Mastery: ZFS》（Tilted Windmill Press, 2015）和《FreeBSD Mastery:
    Advanced ZFS》（Tilted Windmill Press, 2016）这两本书很有帮助。你还会在互联网上找到很多关于这些主题的资源。'
- en: Now let’s consider some other filesystems FreeBSD administrators find useful.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑一些 FreeBSD 管理员认为有用的其他文件系统。
