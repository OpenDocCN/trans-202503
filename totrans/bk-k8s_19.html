<html><head></head><body>
<h2 class="h2" id="ch17"><span epub:type="pagebreak" id="page_279"/><span class="big">17</span><br/>CUSTOM RESOURCES AND OPERATORS</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">We’ve seen many different resource types used in a Kubernetes cluster to run container workloads, scale them, configure them, route network traffic to them, and provide storage for them. One of the most powerful features of a Kubernetes cluster, however, is the ability to define custom resource types and integrate these into the cluster alongside all of the built-in resource types we’ve already seen.</p>&#13;
<p class="indent">Custom resource definitions enable us to define any new resource type and have the cluster track corresponding resources. We can use this capability to add complex new behavior to our cluster, such as automating the deployment of a highly available database engine, while taking advantage of all of the existing capabilities of the built-in resource types and the resource and status management of the cluster’s control plane.</p>&#13;
<p class="indent">In this chapter, we’ll see how custom resource definitions work and how we can use them to deploy Kubernetes operators, extending our cluster to take on any additional behavior we desire.</p>&#13;
<h3 class="h3" id="ch00lev1sec70"><span epub:type="pagebreak" id="page_280"/>Custom Resources</h3>&#13;
<p class="noindent">In <a href="ch06.xhtml#ch06">Chapter 6</a>, we discussed how the Kubernetes API server provides a declarative API, where the primary actions are to create, read, update, and delete resources in the cluster. A declarative API has advantages for resiliency, as the cluster can track the desired state of resources and work to ensure that the cluster stays in that desired state. However, a declarative API also has a significant advantage in extensibility. The actions provided by the API server are generic enough that extending them to any kind of resource is easy.</p>&#13;
<p class="indent">We’ve already seen how Kubernetes takes advantage of this extensibility to update its API over time. Not only can Kubernetes support new versions of a resource over time, but brand-new resources with new capabilities can be added to the cluster while backward compatibility is maintained through the old resources. We saw this in <a href="ch07.xhtml#ch07">Chapter 7</a> in our discussion on the new capabilities of version 2 of the HorizontalPodAutoscaler as well as the way that the Deployment replaced the ReplicationController.</p>&#13;
<p class="indent">We really see the power of this extensibility in the use of <em>CustomResourceDefinitions</em>. A CustomResourceDefinition, or CRD, allows us to add any new resource type to a cluster dynamically. We simply provide the API server with the name of the new resource type and a specification that’s used for validation, and immediately the API server will allow us to create, read, update, and delete resources of that new type.</p>&#13;
<p class="indent">CRDs are extremely useful and in widespread use. For example, the infrastructure components that are already deployed to our cluster include CRDs.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on/linebreak getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">Let’s see the CRDs that are already registered with our cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get crds</span>&#13;
NAME                                                  CREATED AT&#13;
...&#13;
clusterinformations.crd.projectcalico.org             ...&#13;
...&#13;
installations.operator.tigera.io                      ...&#13;
...&#13;
volumes.longhorn.io                                   ...</pre>&#13;
<p class="indent">To avoid naming conflicts, the CRD name must include a group, which is commonly based on a domain name to ensure uniqueness. This group is also used to establish the path to that resource for the REST API provided by the API server. In this example, we see CRDs in the <span class="literal">crd.projectcalico.org</span> group and the <span class="literal">operator.tigera.io</span> group, both of which are used by Calico. We also see a CRD in the <span class="literal">longhorn.io</span> group, used by Longhorn.</p>&#13;
<p class="indent">These CRDs allow Calico and Longhorn to use the Kubernetes API to record configuration and status information in <span class="literal">etcd</span>. CRDs also simplify <span epub:type="pagebreak" id="page_281"/>custom configuration. For example, as part of deploying Calico to the cluster, the automation created an Installation resource that corresponds to the <span class="literal">installations.operator.tigera.io</span> CRD:</p>&#13;
<p class="noindent6"><em>custom-resources.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: operator.tigera.io/v1&#13;
kind: Installation&#13;
metadata:&#13;
  name: default&#13;
spec:&#13;
  calicoNetwork:&#13;
    ipPools:&#13;
    - blockSize: 26&#13;
      cidr: 172.31.0.0/16&#13;
...</pre>&#13;
<p class="indent">This configuration is the reason why we see Pods getting IP addresses in the <span class="literal">172.31.0.0/16</span> network block. This YAML file was automatically placed in <em>/etc/kubernetes/components</em> and automatically applied to the cluster as part of Calico installation. On deployment, Calico queries the API server for instances of this Installation resource and configures networking accordingly.</p>&#13;
<h4 class="h4" id="ch00lev2sec103">Creating CRDs</h4>&#13;
<p class="noindent">Let’s explore CRDs further by creating our own. We’ll use the definition provided in <a href="ch17.xhtml#ch17list1">Listing 17-1</a>.</p>&#13;
<p class="noindent6"><em>crd.yaml</em></p>&#13;
<pre> ---&#13;
 apiVersion: apiextensions.k8s.io/v1&#13;
 kind: CustomResourceDefinition&#13;
 metadata:&#13;
<span class="ent">➊</span> name: samples.bookofkubernetes.com&#13;
 spec:&#13;
<span class="ent">➋</span> group: bookofkubernetes.com&#13;
   versions:&#13;
  <span class="ent">➌</span> - name: v1&#13;
       served: true&#13;
       storage: true&#13;
       schema:&#13;
         openAPIV3Schema:&#13;
           type: object&#13;
           properties:&#13;
             spec:&#13;
               type: object&#13;
               properties:&#13;
                 value:&#13;
                   type: integer&#13;
<span class="ent">➍</span> scope: Namespaced&#13;
<span epub:type="pagebreak" id="page_282"/>   names:&#13;
<span class="ent">➎</span> plural: samples&#13;
<span class="ent">➏</span> singular: sample&#13;
<span class="ent">➐</span> kind: Sample&#13;
     shortNames:&#13;
    <span class="ent">➑</span> - sam</pre>&#13;
<p class="caption" id="ch17list1"><em>Listing 17-1: Sample CRD</em></p>&#13;
<p class="indent">There are multiple important parts to this definition. First, several types of names are defined. The metadata <span class="literal">name</span> field <span class="ent">➊</span> must combine the plural name of the resource <span class="ent">➎</span> and the group <span class="ent">➋</span>. These naming components will also be critical for access via the API.</p>&#13;
<p class="indent">Naming also includes the <span class="literal">kind</span> <span class="ent">➐</span>, which is used in YAML files. This means that when we create specific resources based on this CRD, we will identify them with <span class="literal">kind: Sample</span>. Finally, we need to define how to refer to instances of this CRD on the command line. This includes the full name of the resource, specified in the <span class="literal">singular</span> <span class="ent">➏</span> field, as well as any <span class="literal">shortNames</span> <span class="ent">➑</span> that we want the command line to recognize.</p>&#13;
<p class="indent">Now that we’ve provided Kubernetes with all of the necessary names for instances based on this CRD, we can move on to how the CRD is tracked and what data it contains. The <span class="literal">scope</span> <span class="ent">➍</span> field tells Kubernetes whether this resource should be tracked at the Namespace level or whether resources are cluster wide. Namespaced resources receive an API path that includes the Namespace they’re in, and authorization to access and modify Namespaced resources can be controlled on a Namespace-by-Namespace basis using Roles and RoleBindings, as we saw in <a href="ch11.xhtml#ch11">Chapter 11</a>.</p>&#13;
<p class="indent">Third, the <span class="literal">versions</span> section allows us to define the actual content that is valid when we create resources based on this CRD. To enable updates over time, there can be multiple versions. Each version has a <span class="literal">schema</span> that declares what fields are valid. In this case, we define a <span class="literal">spec</span> field that contains one field called <span class="literal">value</span>, and we declare this one field to be an integer.</p>&#13;
<p class="indent">There was a lot of required configuration here, so let’s review the result. This CRD enables us to tell the Kubernetes cluster to track a brand new kind of resource for us, a <em>Sample</em>. Each instance of this resource (each Sample) will belong to a Namespace and will contain an integer in a <span class="literal">value</span> field.</p>&#13;
<p class="indent">Let’s create this CRD in our cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/crd.yaml</span>&#13;
customresourcedefinition...k8s.io/samples.bookofkubernetes.com created</pre>&#13;
<p class="indent">We can now create objects of this type and retrieve them from our cluster. Here’s an example YAML definition to create a new Sample using the CRD we defined:</p>&#13;
<p class="noindent6"><em>sample.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: bookofkubernetes.com/v1&#13;
kind: Sample&#13;
<span epub:type="pagebreak" id="page_283"/>metadata:&#13;
  namespace: default&#13;
  name: somedata&#13;
spec:&#13;
  value: 123</pre>&#13;
<p class="indent">We match the <span class="literal">apiVersion</span> and <span class="literal">kind</span> to our CRD and ensure that the <span class="literal">spec</span> is in alignment with the schema. This means that we’re required to supply a field called <span class="literal">value</span> with an integer value.</p>&#13;
<p class="indent">We can now create this resource in the cluster just like any other resource:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/somedata.yaml</span> &#13;
sample.bookofkubernetes.com/somedata created</pre>&#13;
<p class="indent">There is now a Sample called <span class="literal">somedata</span> that is part of the <span class="literal">default</span> Namespace.</p>&#13;
<p class="indent">When we defined the CRD in <a href="ch17.xhtml#ch17list1">Listing 17-1</a>, we specified a plural, singular, and short name for Sample resources. We can use any of these names to retrieve the new resource:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get samples</span>&#13;
NAME       AGE&#13;
somedata   56s&#13;
root@host01:~# <span class="codestrong1">kubectl get sample</span>&#13;
NAME       AGE&#13;
somedata   59s&#13;
root@host01:~# <span class="codestrong1">kubectl get sam</span>&#13;
NAME       AGE&#13;
somedata   62s</pre>&#13;
<p class="indent">Just by declaring our CRD, we’ve extended the behavior of our Kubernetes cluster so that it understands what <span class="literal">samples</span> are, and we can use that not only in the API but also in the command line tools.</p>&#13;
<p class="indent">This means that <span class="literal">kubectl describe</span> also works for Samples. We can see that Kubernetes tracks other data related to our new resource, beyond just the data we specified:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe sample somedata</span>&#13;
Name:         somedata&#13;
Namespace:    default&#13;
...&#13;
API Version:  bookofkubernetes.com/v1&#13;
Kind:         Sample&#13;
Metadata:&#13;
  Creation Timestamp:  ...&#13;
...&#13;
  Resource Version:  9386&#13;
  UID:               37cc58db-179f-40e6-a9bf-fbf6540aa689&#13;
<span epub:type="pagebreak" id="page_284"/>Spec:&#13;
  Value:  123&#13;
Events:   &lt;none&gt;</pre>&#13;
<p class="indent">This additional data, including timestamps and resource versioning, is essential if we want to use the data from our CRD. To use our new resource effectively, we’re going to need a software component that continually monitors for new or updated instances of our resource and takes action accordingly. We’ll run this component using a regular Kubernetes Deployment that interacts with the Kubernetes API server.</p>&#13;
<h4 class="h4" id="ch00lev2sec104">Watching CRDs</h4>&#13;
<p class="noindent">With core Kubernetes resources, the control plane components communicate with the API server to take the correct action when a resource is created, updated, or deleted. For example, the controller manager includes a component that watches for changes to Services and Pods, enabling it to update the list of endpoints for each Service. The <span class="literal">kube-proxy</span> instance on each node then makes the necessary network routing changes to send traffic to Pods based on those endpoints.</p>&#13;
<p class="indent">With CRDs, the API server merely tracks the resources as they are created, updated, and deleted. It is the responsibility of some other software to monitor instances of the resource and take the correct action. To make it easy to monitor resources, the API server offers a <span class="literal">watch</span> action, using <em>long polling</em> to keep a connection open and continually feed events as they occur. Because a long-polling connection could be cut off at any time, the timestamp and resource version data that Kubernetes tracks for us will enable us to detect what cluster changes we’ve already processed when we reconnect.</p>&#13;
<p class="indent">We could use the API server’s <span class="literal">watch</span> capability directly from a <span class="literal">curl</span> command or directly in an HTTP client, but it’s much easier to use a Kubernetes client library. For this example, we’ll use the Python client library to illustrate how to watch our custom resource. Here’s the Python script we’ll use:</p>&#13;
<p class="noindent6"><em>watch.py</em></p>&#13;
<pre>   #!/usr/bin/env python3&#13;
   from kubernetes import client, config, watch&#13;
   import json, os, sys&#13;
&#13;
   try:&#13;
  <span class="ent">➊</span> config.load_incluster_config()&#13;
   except:&#13;
     print("In cluster config failed, falling back to file", file=sys.stderr)&#13;
  <span class="ent">➋</span> config.load_kube_config()&#13;
&#13;
<span class="ent">➌</span> group = os.environ.get('WATCH_GROUP', 'bookofkubernetes.com')&#13;
   version = os.environ.get('WATCH_VERSION', 'v1')&#13;
   namespace = os.environ.get('WATCH_NAMESPACE', 'default')&#13;
   resource = os.environ.get('WATCH_RESOURCE', 'samples')&#13;
   <span epub:type="pagebreak" id="page_285"/>api = client.CustomObjectsApi()&#13;
&#13;
   w = watch.Watch()&#13;
<span class="ent">➍</span> for event in w.stream(api.list_namespaced_custom_object,&#13;
          group=group, version=version, namespace=namespace, plural=resource):&#13;
<span class="ent">➎</span> json.dump(event, sys.stdout, indent=2)&#13;
    sys.stdout.flush()</pre>&#13;
<p class="indent">To connect to the API server, we need to load cluster configuration. This includes the location of the API server as well as the authentication information we saw in <a href="ch11.xhtml#ch11">Chapter 11</a>. If we’re running in a container within a Kubernetes Pod, we’ll automatically have that information available to us, so we first try to load an in-cluster config <span class="ent">➊</span>. However, if we’re outside a Kubernetes cluster, the convention is to use a Kubernetes config file, so we try that as a secondary option <span class="ent">➋</span>.</p>&#13;
<p class="indent">After we’ve established how to talk to the API server, we use the custom objects API and a watch object to stream events related to our custom resource <span class="ent">➍</span>. The <span class="literal">stream()</span> method takes the name of a function and the associated parameters, which we’ve loaded from the environment or from default values <span class="ent">➌</span>. We use the <span class="literal">list_namespaced_custom_object</span> function because we’re interested in our custom resource. All of the various <span class="literal">list_*</span> methods in the Python library are designed to work with <span class="literal">watch</span> to return a stream of add, update, and remove events rather than simply retrieving the current list of objects. As events occur, we then print them to the console in an easy-to-read format <span class="ent">➎</span>.</p>&#13;
<p class="indent">We’ll use this Python script within a Kubernetes Deployment. I’ve built and published a container image to run it, so this is an easy task. Here’s the Deployment definition:</p>&#13;
<p class="noindent6"><em>watch.yaml</em></p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: watch&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: watch&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: watch&#13;
    spec:&#13;
      containers:&#13;
      - name: watch&#13;
        image: bookofkubernetes/crdwatcher:stable&#13;
      serviceAccountName: watcher</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_286"/>This Deployment will run the Python script that watches for events on instances of the Sample CRD. However, before we can create this Deployment, we need to ensure that our watcher script will have permissions to read our custom resource. The default ServiceAccount has minimal permissions, so we need to create a ServiceAccount for this Deployment and ensure that it has the rights to see our Sample custom resources.</p>&#13;
<p class="indent">We could bind a custom Role to our ServiceAccount to do this, but it’s more convenient to take advantage of role aggregation to add our Sample custom resource to the <span class="literal">view</span> ClusterRole that already exists. This way, any user in the cluster with the <span class="literal">view</span> ClusterRole will acquire rights to our Sample custom resource.</p>&#13;
<p class="indent">We start by defining a new ClusterRole for our custom resource:</p>&#13;
<p class="noindent6"><em>sample-reader.yaml</em></p>&#13;
<pre> ---&#13;
 apiVersion: rbac.authorization.k8s.io/v1&#13;
 kind: ClusterRole&#13;
 metadata:&#13;
   name: sample-reader&#13;
   labels:&#13;
  <span class="ent">➊</span> rbac.authorization.k8s.io/aggregate-to-view: "true"&#13;
 rules:&#13;
<span class="ent">➋</span> - apiGroups: ["bookofkubernetes.com"]&#13;
    resources: ["samples"]&#13;
    verbs: ["get", "watch", "list"]</pre>&#13;
<p class="indent">This ClusterRole gives permission to <span class="literal">get</span>, <span class="literal">watch</span>, and <span class="literal">list</span> our Sample custom resources <span class="ent">➋</span>. We also add a label to the metadata <span class="ent">➊</span> to signal the cluster that we want these permissions to be aggregated into the <span class="literal">view</span> ClusterRole. Thus, rather than bind our ServiceAccount into the <span class="literal">sample-reader</span> ClusterRole we’re defining here, we can bind our ServiceAccount into the generic <span class="literal">view</span> ClusterRole, giving it read-only access to all kinds of resources.</p>&#13;
<p class="indent">We also need to declare the ServiceAccount and bind it to the <span class="literal">view</span> ClusterRole:</p>&#13;
<p class="noindent6"><em>sa.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: ServiceAccount&#13;
metadata:&#13;
  name: watcher&#13;
  namespace: default&#13;
---&#13;
kind: RoleBinding&#13;
apiVersion: rbac.authorization.k8s.io/v1&#13;
metadata:&#13;
  name: viewer&#13;
  namespace: default&#13;
subjects:&#13;
- kind: ServiceAccount&#13;
<span epub:type="pagebreak" id="page_287"/>  name: watcher&#13;
  namespace: default&#13;
roleRef:&#13;
  kind: ClusterRole&#13;
  name: view&#13;
  apiGroup: rbac.authorization.k8s.io</pre>&#13;
<p class="indent">We use a RoleBinding to limit this ServiceAccount to read-only access solely within the <span class="literal">default</span> Namespace. The RoleBinding binds the <span class="literal">watcher</span> ServiceAccount to the generic <span class="literal">view</span> ClusterRole. This ClusterRole will have access to our Sample custom resources thanks to the role aggregation we specified.</p>&#13;
<p class="indent">We’re now ready to apply all of these resources, including our Deployment:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/sample-reader.yaml</span> &#13;
clusterrole.rbac.authorization.k8s.io/sample-reader created&#13;
root@host01:~# <span class="codestrong1">kubectl apply -f /opt/sa.yaml</span>&#13;
serviceaccount/watcher created&#13;
rolebinding.rbac.authorization.k8s.io/viewer created&#13;
root@host01:~# <span class="codestrong1">kubectl apply -f /opt/watch.yaml</span> &#13;
deployment.apps/watch created</pre>&#13;
<p class="indent">After a little while, our watcher Pod will be running:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS    RESTARTS   AGE&#13;
watch-69876b586b-jp25m   1/1     Running   0          47s</pre>&#13;
<p class="indent">We can print the watcher’s logs to see the events it has received from the API server:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl logs</span> <span class="codestrong1"><span class="codeitalic1">watch-69876b586b-jp25m</span></span>&#13;
{&#13;
  "type": "ADDED",&#13;
  "object": {&#13;
    "apiVersion": "bookofkubernetes.com/v1",&#13;
    "kind": "Sample",&#13;
    "metadata": {&#13;
...&#13;
      "creationTimestamp": "...",&#13;
...&#13;
      "name": "somedata",&#13;
      "namespace": "default",&#13;
      "resourceVersion": "9386",&#13;
      "uid": "37cc58db-179f-40e6-a9bf-fbf6540aa689"&#13;
<span epub:type="pagebreak" id="page_288"/>    },&#13;
    "spec": {&#13;
      "value": 123&#13;
    }&#13;
  },&#13;
...</pre>&#13;
<p class="indent">Note that the watcher Pod receives an <span class="literal">ADDED</span> event for the <span class="literal">somedata</span> Sample we created, even though we created that Sample before we deployed our watcher. The API server is able to determine that our watcher has not yet retrieved this object, so it sends us an event immediately on connection as if the object were newly created, which avoids a race condition that we would otherwise be forced to handle. However, note that if the client is restarted, it will appear as a new client to the API server and will see the same <span class="literal">ADDED</span> event again for the same Sample. For this reason, when we implement the logic to handle our custom resources, it’s essential to make the logic idempotent so that we can handle processing the same event multiple times.</p>&#13;
<h3 class="h3" id="ch00lev1sec71">Operators</h3>&#13;
<p class="noindent">What kinds of actions would we take in response to the creation, update, or deletion of custom resources, other than just logging the events to the console? As we saw when we examined the way that custom resources are used to configure Calico networking in our cluster, one use for custom resources is to configure for cluster infrastructure components such as networking and storage. But another pattern that really makes the best use of custom resources is the Kubernetes <em>Operator</em>.</p>&#13;
<p class="indent">The Kubernetes Operator pattern extends the behavior of the cluster to make it easier to deploy and manage specific application components. Rather than using the standard set of Kubernetes resources such as Deployments and Services directly, we simply create custom resources that are specific to the application component, and the operator manages the underlying Kubernetes resources for us.</p>&#13;
<p class="indent">Let’s look at an example to illustrate the power of the Kubernetes Operator pattern. We’ll add a Postgres Operator to our cluster that will enable us to deploy a highly available PostgreSQL database to our cluster by just adding a single custom resource.</p>&#13;
<p class="indent">Our automation has staged the files that we need into <em>/etc/kubernetes/components</em> and has performed some initial setup, so the only step remaining is to add the operator. The operator is a normal Deployment that will run in whatever Namespace we choose. It then will watch for custom <span class="literal">postgresql</span> resources and will create PostgreSQL instances accordingly.</p>&#13;
<p class="indent">Let’s deploy the operator:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /etc/kubernetes/components/postgres-operator.yaml</span> &#13;
deployment.apps/postgres-operator created</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_289"/>This creates a Deployment for the operator itself, which creates a single Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                                 READY   STATUS    RESTARTS   AGE&#13;
postgres-operator-5cdbff85d6-cclxf   1/1     Running   0          27s&#13;
...</pre>&#13;
<p class="indent">The Pod communicates with the API server to create the CRD needed to define a PostgreSQL database:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get crd postgresqls.acid.zalan.do</span>&#13;
NAME                        CREATED AT&#13;
postgresqls.acid.zalan.do   ...</pre>&#13;
<p class="indent">No instances of PostgreSQL are running in the cluster yet, but we can easily deploy PostgreSQL by creating a custom resource based on that CRD:</p>&#13;
<p class="noindent6"><em>pgsql.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: "acid.zalan.do/v1"&#13;
kind: postgresql&#13;
metadata:&#13;
  name: pgsql-cluster&#13;
  namespace: default&#13;
spec:&#13;
  teamId: "pgsql"&#13;
  volume:&#13;
    size: 1Gi&#13;
    storageClass: longhorn&#13;
  numberOfInstances: 3&#13;
  users:&#13;
    dbuser:&#13;
    - superuser&#13;
    - createdb&#13;
  databases:&#13;
    defaultdb: dbuser&#13;
  postgresql:&#13;
    version: "14"</pre>&#13;
<p class="indent">This custom resource tells the Postgres Operator to spawn a PostgreSQL database using server version 14, with three instances (a primary and two backups). Each instance will have persistent storage. The primary instance will be configured with the specified user and database.</p>&#13;
<p class="indent">The real value of the Kubernetes Operator pattern is that the YAML resource file we declare is short, simple, and clearly relates to the PostgreSQL configuration we want to see. The operator’s job is to convert this information into a StatefulSet, Services, and other cluster resources as needed to operate this database.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_290"/>We apply this custom resource to the cluster like any other resource:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pgsql.yaml</span> &#13;
postgresql.acid.zalan.do/pgsql-cluster created</pre>&#13;
<p class="indent">After we apply it, the Postgres Operator will receive the add event and will create the necessary cluster resources for PostgreSQL:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl logs</span> <span class="codestrong1"><span class="codeitalic1">postgres-operator-5cdbff85d6-cclxf</span></span>&#13;
... level=info msg="Spilo operator..."&#13;
...&#13;
... level=info msg="ADD event has been queued" &#13;
  cluster-name=default/pgsql-cluster pkg=controller worker=0&#13;
... level=info msg="creating a new Postgres cluster" &#13;
  cluster-name=default/pgsql-cluster pkg=controller worker=0&#13;
...&#13;
... level=info msg="statefulset &#13;
  \"default/pgsql-cluster\" has been successfully created" &#13;
  cluster-name=default/pgsql-cluster pkg=cluster worker=0&#13;
...</pre>&#13;
<p class="indent">Ultimately, there will be a StatefulSet and three Pods running (in addition to the Pod for the operator itself, which is still running):</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get sts</span>&#13;
NAME            READY   AGE&#13;
pgsql-cluster   3/3     2m39s&#13;
root@host01:~# <span class="codestrong1">kubectl get po</span>&#13;
NAME                                 READY   STATUS    RESTARTS   AGE&#13;
pgsql-cluster-0                      1/1     Running   0          2m40s&#13;
pgsql-cluster-1                      1/1     Running   0          2m18s&#13;
pgsql-cluster-2                      1/1     Running   0          111s&#13;
postgres-operator-5cdbff85d6-cclxf   1/1     Running   0          4m6s&#13;
...</pre>&#13;
<p class="indent">It can take several minutes for all of these resources to be fully running on the cluster.</p>&#13;
<p class="indent">Unlike the PostgreSQL StatefulSet we created in <a href="ch15.xhtml#ch15">Chapter 15</a>, all instances in this StatefulSet are configured for high availability, as we can demonstrate by inspecting the logs for each Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl logs pgsql-cluster-0</span>&#13;
...&#13;
... INFO: Lock owner: None; I am pgsql-cluster-0&#13;
... INFO: trying to bootstrap a new cluster&#13;
...&#13;
... INFO: initialized a new cluster&#13;
...&#13;
... INFO: no action. I am (pgsql-cluster-0) the leader with the lock&#13;
root@host01:~# <span class="codestrong1">kubectl logs pgsql-cluster-1</span>&#13;
<span epub:type="pagebreak" id="page_291"/>...&#13;
... INFO: Lock owner: None; I am pgsql-cluster-1&#13;
... INFO: waiting for leader to bootstrap&#13;
... INFO: Lock owner: pgsql-cluster-0; I am pgsql-cluster-1&#13;
...&#13;
... INFO: no action. I am a secondary (pgsql-cluster-1) and following &#13;
    a leader (pgsql-cluster-0)</pre>&#13;
<p class="indent">As we can see, the first instance, <span class="literal">pgsql-cluster-0</span>, has identified itself as the leader, whereas <span class="literal">pgsql-cluster-1</span> has configured itself as a follower that will replicate any updates to the leader’s databases.</p>&#13;
<p class="indent">To manage the PostgreSQL leaders and followers and enable database clients to reach the leader, the operator has created multiple Services:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get svc</span>&#13;
NAME                   TYPE        CLUSTER-IP      ... PORT(S)    AGE&#13;
...&#13;
pgsql-cluster          ClusterIP   10.101.80.163   ... 5432/TCP   6m52s&#13;
pgsql-cluster-config   ClusterIP   None            ... &lt;none&gt;     6m21s&#13;
pgsql-cluster-repl     ClusterIP   10.96.13.186    ... 5432/TCP   6m52s</pre>&#13;
<p class="indent">The <span class="literal">pgsql-cluster</span> Service routes traffic to the primary only; the other Services are used to manage replication to the backup instances. The operator handles the task of updating the Service if the primary instance changes due to failover.</p>&#13;
<p class="indent">To remove the PostgreSQL database, we need to remove only the custom resource, and the Postgres Operator handles the rest:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete -f /opt/pgsql.yaml</span> &#13;
postgresql.acid.zalan.do "pgsql-cluster" deleted</pre>&#13;
<p class="indent">The operator detects the removal and cleans up the associated Kubernetes cluster resources:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl logs</span> <span class="codestrong1"><span class="codeitalic1">postgres-operator-5cdbff85d6-cclxf</span></span>&#13;
...&#13;
... level=info msg="deletion of the cluster started" &#13;
  cluster-name=default/pgsql-cluster pkg=controller worker=0&#13;
... level=info msg="DELETE event has been queued" &#13;
  cluster-name=default/pgsql-cluster pkg=controller worker=0&#13;
...&#13;
... level=info msg="cluster has been deleted" &#13;
  cluster-name=default/pgsql-cluster pkg=controller worker=0</pre>&#13;
<p class="indent">The Postgres Operator has now removed the StatefulSet, persistent storage, and other resources associated with this database cluster.</p>&#13;
<p class="indent">The ease with which we were able to deploy and remove a PostgreSQL database server, including multiple instances automatically configured in a highly available configuration, demonstrates the power of the Kubernetes <span epub:type="pagebreak" id="page_292"/>Operator pattern. By defining a CRD, a regular Deployment can act to extend the behavior of our Kubernetes cluster. The result is a seamless addition of new cluster capability that is fully integrated with the built-in features of the Kubernetes cluster.</p>&#13;
<h3 class="h3" id="ch00lev1sec72">Final Thoughts</h3>&#13;
<p class="noindent">CustomResourceDefinitions and Kubernetes Operators bring advanced features to a cluster, but they do so by building on the basic Kubernetes cluster functionality we’ve seen throughout this book. The Kubernetes API server has the extensibility to handle storage and retrieval of any type of cluster resource. As a result, we’re able to define new resource types dynamically and have the cluster manage them for us.</p>&#13;
<p class="indent">We’ve seen this pattern across many of the features we’ve examined in <a href="part02.xhtml#part02">Part II</a> of this book. Kubernetes itself is built on the fundamental features of containers that we saw in <a href="part01.xhtml#part01">Part I</a>, and it is built so that its more advanced features are implemented by bringing together its more basic features. By understanding how those basic features work, we’re better able to understand the more advanced features, even if the behavior looks a bit magical at first.</p>&#13;
<p class="indent">We’ve now worked our way through the key capabilities of Kubernetes that we need to understand to build high-quality, performant applications. Next, we’ll turn our attention to ways to improve the performance and resiliency of our applications when running them in a Kubernetes cluster.</p>&#13;
</body></html>