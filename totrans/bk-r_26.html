<html><head></head><body>
        <h2 class="h2" id="ch22"><span epub:type="pagebreak" id="page_527"/><span class="big"><strong>22</strong></span><br/><strong>LINEAR MODEL SELECTION AND&#13;
                DIAGNOSTICS</strong></h2>&#13;
        <div class="image"><img src="../images/common-01.jpg" alt="image"/></div>&#13;
        <p class="noindent">You’ve now spent a fair amount of time on many aspects of linear&#13;
            regression models. In this chapter, I’ll cover how formal R tools and techniques&#13;
            can be used to investigate two other, and no less important, aspects of regression:&#13;
            choosing an appropriate model for your analysis and assessing the validity of the&#13;
            assumptions you’ve made.</p>&#13;
        <h3 class="h3" id="ch22lev1sec72"><strong>22.1 Goodness-of-Fit vs. Complexity</strong></h3>&#13;
        <p class="noindent">The overarching goal of fitting any statistical model is to faithfully&#13;
            represent the data and the relationships held within them. In general, fitting&#13;
            statistical models boils down to a balancing act between two things: goodness-of-fit and&#13;
            complexity. <em>Goodness-of-fit</em> refers to the goal of obtaining a model that best&#13;
            represents the relationships between the response and the predictor (or predictors).&#13;
                <em>Complexity</em> describes how complicated a model is; this is always tied to the&#13;
            number of terms in the model that require estimation—the inclusion of more&#13;
            predictors and additional functions (such as polynomial transformations and&#13;
            interactions) leads to a more complex model.</p>&#13;
        <h4 class="h4" id="ch22lev2sec207"><span epub:type="pagebreak" id="page_528"/><strong><em>22.1.1 Principle of Parsimony</em></strong></h4>&#13;
        <p class="noindent">Statisticians refer to the balancing act between goodness-of-fit and&#13;
            complexity as the <em>principle of parsimony</em>, where the goal of the associated&#13;
                <em>model selection</em> is to find a model that’s as simple as possible (in&#13;
            other words, with relatively low complexity), without sacrificing too much&#13;
            goodness-of-fit. We’d say that a model that satisfies this notion is a&#13;
                <em>parsimonious</em> fit. You’ll often hear of researchers talking about&#13;
            choosing the “best” model—they’re actually referring to the idea&#13;
            of parsimony.</p>&#13;
        <p class="indent">So, how do you decide where to draw the line on such a balance? Naturally,&#13;
            statistical significance plays a role here—and model selection often simply comes&#13;
            down to assessing the significance of the effect of predictors or functions of&#13;
            predictors on the response. In an effort to impart some amount of objectivity to such a&#13;
            process, you can use systematic selection algorithms, such as those you’ll learn&#13;
            about in <a href="ch22.xhtml#ch22lev1sec73">Section 22.2</a>, to decide between multiple&#13;
            explanatory variables and any associated functions.</p>&#13;
        <h4 class="h4" id="ch22lev2sec208"><strong><em>22.1.2 General Guidelines</em></strong></h4>&#13;
        <p class="noindentb">Performing any kind of model selection or comparing several models&#13;
            against one another involves decision making regarding the inclusion of available&#13;
            predictor variables. On this topic, there are several guidelines you should always&#13;
            follow.</p>&#13;
        <p class="bull">• First, it’s important to remember that you can’t remove&#13;
            individual levels of a categorical predictor in a given model; this makes no sense. In&#13;
            other words, if one of the nonreference levels is statistically significant but all&#13;
            others are nonsignificant, you should treat the categorical variable, as a whole, as&#13;
            making a statistically significant contribution to the determination of the mean&#13;
            response. You should only really consider <em>entire</em> removal of that categorical&#13;
            predictor if <em>all</em> nonreference coefficients are associated with a lack of&#13;
            evidence (against being zero). This also holds for interactive terms involving&#13;
            categorical predictors.</p>&#13;
        <p class="bull">• If an interaction is present in the fitted model, all lower-order&#13;
            interactions and main effects of the relevant predictors must remain in the model. This&#13;
            was touched upon in <a href="ch21.xhtml#ch21lev2sec202">Section 21.5.1</a>, when I&#13;
            discussed interpretation of interactive effects as augmentations of lower-order effects.&#13;
            As an example, you should only really consider removing the main effect of a predictor&#13;
            if there are no interaction terms present in the fitted model involving that predictor&#13;
            (even if that main effect has a high <em>p</em>-value).</p>&#13;
        <p class="bull">• In models where you’ve used a polynomial transformation of a&#13;
            certain explanatory variable (refer to <a href="ch21.xhtml#ch21lev2sec199">Section&#13;
                21.4.1</a>), keep all lower-order polynomial terms in the model if the highest is&#13;
            deemed significant. A model containing an order 3 polynomial transformation in a&#13;
            predictor, for example, must also include the order 1 and order 2 transformations of&#13;
            that variable. This is because of the mathematical behavior <span epub:type="pagebreak" id="page_529"/>of polynomial functions—only by explicitly separating out&#13;
            the linear, quadratic, and cubic (and so on) effects as distinct terms can you avoid&#13;
            confounding said effects with one another.</p>&#13;
        <h3 class="h3" id="ch22lev1sec73"><strong>22.2 Model Selection Algorithms</strong></h3>&#13;
        <p class="noindent">The job of a model selection algorithm is to sift through your available&#13;
            explanatory variables in some systematic fashion in order to establish which are best&#13;
            able to jointly describe the response, as opposed to fitting models by examining&#13;
            specific combinations of predictors in isolation, as you’ve done so far.</p>&#13;
        <p class="indent">Model selection algorithms can be controversial. There are several&#13;
            different methods, and no single approach is universally appropriate for every&#13;
            regression model. Different selection algorithms can result in different final models,&#13;
            as you’ll see. In many cases, researchers will have additional information or&#13;
            knowledge about the problem that influences the decision—for example, that certain&#13;
            predictors must always be included or that it makes no sense to ever include them. This&#13;
            must be considered at the same time as other complications, such as the possibility of&#13;
            interactions or unobserved lurking variables influencing significant relationships and&#13;
            the need to ensure any fitted model is statistically valid (which you’ll look at&#13;
            in <a href="ch22.xhtml#ch22lev1sec74">Section 22.3</a>).</p>&#13;
        <p class="indent">It’s helpful to keep in mind this famous quote from celebrated&#13;
            statistician George Box (1919–2013): “All models are wrong, but some are&#13;
            useful.”</p>&#13;
        <p class="indent">Any fitted model you produce can never be assumed to be the truth, but a&#13;
            model that’s fitted and checked carefully and thoroughly can reveal interesting&#13;
            features of the data and so have the potential to reveal associations and relationships&#13;
            by providing quantitative estimates thereof.</p>&#13;
        <h4 class="h4" id="ch22lev2sec209"><strong><em>22.2.1 Nested Comparisons: The Partial&#13;
                    F-Test</em></strong></h4>&#13;
        <p class="noindent">The <em>partial F-test</em> is probably the most direct way to compare&#13;
            several different models. It looks at two or more <em>nested</em> models, where the&#13;
            smaller, less complex model is a reduced version of the bigger, more complex model.&#13;
            Formally, let’s say you’ve fitted two linear regressions models as&#13;
            follows:</p>&#13;
        <div class="imagec"><img src="../images/f0529-01.jpg" alt="image"/></div>&#13;
        <p class="indent">Here, the reduced model, predicting <em><span class="ent">ŷ</span></em><sub>redu</sub>, has <em>p</em> predictors, plus one intercept.&#13;
            The full model, predicting <em><span class="ent">ŷ</span></em><sub>full</sub>, has&#13;
                <em>q</em> predictor terms. The notation implies that <em>q</em> &gt; <em>p</em> and&#13;
            that, along with the standard inclusion of an intercept <img class="middle" src="../images/b0.jpg" alt="image"/>, the full model involves all <em>p</em>&#13;
            predictors of the reduced model defined by <em><span class="ent">ŷ</span></em><sub>redu</sub>, as well as <em>q</em> − <em>p</em>&#13;
            additional terms. This emphasizes the fact that the model for <em><span class="ent">ŷ</span><sub>redu</sub></em> is nested within <em><span class="ent">ŷ</span></em><sub>full</sub>.</p>&#13;
        <p class="indent">It’s important to note that increasing the number of predictors in a&#13;
            regression model will always improve <em>R</em><sup>2</sup> and any other measures of&#13;
                <span epub:type="pagebreak" id="page_530"/>goodness-of-fit. The real question,&#13;
            however, is whether that improvement in goodness-of-fit is large enough to make the&#13;
            additional complexity involved with including any additional predictor terms&#13;
            “worth it.” This is precisely the question that the partial <em>F</em>-test&#13;
            tries to answer in the context of nested regression models. Its goal is to test whether&#13;
            including those extra <em>q</em> − <em>p</em> terms, which produce the full model&#13;
            rather than the reduced model, provide a statistically significant improvement in&#13;
            goodness-of-fit. The partial <em>F</em>-test addresses these hypotheses:</p>&#13;
        <div class="imagec"><a id="ch22eq1"/><img src="../images/e22-1.jpg" alt="image"/></div>&#13;
        <p class="indent">The calculation of the test statistic to address these hypotheses follows&#13;
            the same ideas behind the omnibus <em>F</em>-test automatically produced by R when&#13;
            summarizing a fitted linear model object (detailed in <a href="ch21.xhtml#ch21lev2sec197">Section 21.3.5</a>). Denote the coefficient of&#13;
            determination for the reduced and full models with <img class="middle" src="../images/r2redu.jpg" alt="image"/> and <img class="middle" src="../images/r2redu.jpg" alt="image"/>, respectively. If <em>n</em> refers to the&#13;
            sample size of the data used to fit both models, the test statistic given by</p>&#13;
        <div class="imagec"><a id="ch22eq2"/><img src="../images/e22-2.jpg" alt="image"/></div>&#13;
        <p class="noindent">follows an <em>F</em> distribution with df<sub>1</sub> = <em>q</em>&#13;
            − <em>p</em>, df<sub>2</sub> = <em>n</em> − <em>q</em> degrees of freedom&#13;
            under the assumption of H<sub>0</sub> in (22.1). The <em>p</em>-value is found as the&#13;
            upper-tail area from <img class="middle" src="../images/f.jpg" alt="image"/> as usual;&#13;
            the smaller it is, the greater the evidence against the null hypothesis, which states&#13;
            that one or more of the additional parameters has no impact on the response&#13;
            variable.</p>&#13;
        <p class="indent">Take the model objects <code>survmult</code> and <code>survmult2</code> from <a href="ch21.xhtml#ch21lev2sec193">Section&#13;
                21.3.1</a> as an example. The <code>survmult</code> model aims to&#13;
            predict mean student height from writing handspan and sex based on the <code>survey</code> data frame from the <code>MASS</code>&#13;
            package; <code>survmult2</code> adds smoking status to these predictors.&#13;
            If you need to, return to <a href="ch21.xhtml#ch21lev2sec193">Section 21.3.1</a> to&#13;
            refit these two models. Printing the objects to the console screen previews the two fits&#13;
            and makes it easy to confirm that the smaller model is indeed nested within the larger&#13;
            model in terms of its explanatory variables:</p>&#13;
        <pre>R&gt; survmult<br/><br/>Call:<br/>lm(formula = Height ~ Wr.Hnd + Sex,&#13;
            data =&#13;
            survey)<br/><br/>Coefficients:<br/>(Intercept)       Wr.Hnd      SexMale<br/>    137.687        1.594        9.490<br/><br/>R&gt;&#13;
                survmult2<br/><br/>Call:<br/><span epub:type="pagebreak" id="page_531"/>lm(formula = Height ~ Wr.Hnd + Sex + Smoke, data =&#13;
            survey)<br/><br/>Coefficients:<br/>(Intercept)       Wr.Hnd      SexMale   SmokeNever   SmokeOccas   SmokeRegul<br/>   137.4056       1.6042       9.3979      -0.0442       1.5267       0.9211</pre>&#13;
        <p class="indent">Once you’ve fitted your nested models, R can carry out partial&#13;
                <em>F</em>-tests using the <code>anova</code> function (partial&#13;
                <em>F</em>-tests fall within the suite of analysis of variance methodologies). To&#13;
            determine whether adding <code>Smoke</code> as a predictor provides any&#13;
            statistically significant improvement in fit, simply start with the reduced model and&#13;
            supply the model objects as arguments.</p>&#13;
        <pre>R&gt; anova(survmult,survmult2)<br/>Analysis of Variance&#13;
            Table<br/><br/>Model 1: Height ~ Wr.Hnd + Sex<br/>Model 2: Height ~ Wr.Hnd + Sex +&#13;
            Smoke<br/>  Res.Df    RSS Df Sum of&#13;
            Sq      F Pr(&gt;F)<br/>1    204&#13;
            9959.2<br/>2    201&#13;
            9914.3  3    44.876 0.3033  0.823</pre>&#13;
        <p class="indent">The output provides the quantities associated with calculation of <img class="middle" src="../images/r2redu.jpg" alt="image"/> and <img class="middle" src="../images/r2redu.jpg" alt="image"/> and the test statistic <img class="middle" src="../images/f.jpg" alt="image"/> from (22.2), given in the resulting table as&#13;
                <code>F</code>, which is of the most interest. Using the values of&#13;
                <em>p</em> and <em>q</em> from printing <code>survmult</code> and&#13;
                <code>survmult2</code>, you should be able to confirm, for example,&#13;
            the values of df<sub>1</sub> and df<sub>2</sub> appearing in the second row of the table&#13;
            in the columns <code>Df</code> and <code>Res.Df</code>,&#13;
            respectively.</p>&#13;
        <p class="indent">The result of this particular test, obtained from a test statistic of <img class="middle" src="../images/f0531-01.jpg" alt="image"/> associated with&#13;
                df<sub>1</sub> = 3, df<sub>2</sub> = 201, is a high <em>p</em>-value of 0.823,&#13;
            suggesting no evidence against H<sub>0</sub>. This means that adding <code>Smoke</code> to the reduced model, which includes only the&#13;
            explanatory variables <code>Wr.Hnd</code> and <code>Sex</code>, offers no tangible improvement in fit when it comes to modeling student&#13;
            height. That conclusion isn’t surprising, given the nonsignificant&#13;
            <em>p</em>-values of all non-reference levels of <code>Smoke</code>,&#13;
            seen previously in <a href="ch21.xhtml#ch21lev2sec193">Section 21.3.1</a>.</p>&#13;
        <p class="indent">This is how partial <em>F</em>-tests are used for model selection—in&#13;
            the current example, the reduced model would be the more parsimonious fit and preferred&#13;
            over the full model.</p>&#13;
        <p class="indent">You can conduct comparisons among several nested models in a given call to&#13;
                <code>anova</code>, which can be useful for investigating things&#13;
            such as the inclusion of interactive terms or including polynomial transformations of&#13;
            predictors since there’s a natural hierarchy that requires you to retain any&#13;
            lower-order terms.</p>&#13;
        <p class="indent">For an example, let’s use the <code>diabetes</code>&#13;
            data frame in the <code>faraway</code> package from <a href="ch21.xhtml#ch21lev2sec203">Section 21.5.2</a>, with the model fitted to&#13;
            predict cholesterol level (<code>chol</code>) against age (<code>age</code>) and body frame (<code>frame</code>) and&#13;
            the interaction between those two predictors. Before using the partial <em>F</em>-tests&#13;
            to compare nested variants, you need to ensure you’re using the same records for&#13;
            each model and that <span epub:type="pagebreak" id="page_532"/>there aren’t&#13;
            missing values for any of the predictors that are then going to be unavailable to the&#13;
            “fuller” models (so the sample size is the same for each comparison). To do&#13;
            this, you just need to first define a version of <code>diabetes</code>&#13;
            that removes records with missing values in the predictors you’re using.</p>&#13;
        <p class="indent">Load the <code>faraway</code> package and use logical&#13;
            subsetting to identify and delete any individuals with a missing value for <code>age</code>&#13;
            <em>or</em> for <code>frame</code>. Define this new version of the <code>diabetes</code> object:</p>&#13;
        <pre>R&gt; diab &lt;- diabetes[-which(is.na(diabetes$age) |&#13;
            is.na(diabetes$frame)),]</pre>&#13;
        <p class="indent">Now, fit the following four models using your new <code>diab</code> object:</p>&#13;
        <pre>R&gt; dia.model1 &lt;- lm(chol~1,data=diab)<br/>R&gt; dia.model2 &lt;-&#13;
            lm(chol~age,data=diab)<br/>R&gt; dia.model3 &lt;-&#13;
            lm(chol~age+frame,data=diab)<br/>R&gt; dia.model4 &lt;-&#13;
            lm(chol~age*frame,data=diab)</pre>&#13;
        <p class="indent">The first model is just an intercept, the second adds <code>age</code> as a predictor, the third has <code>age</code> and <code>frame</code>, and the fourth includes the&#13;
            interaction. Nesting is evident, and you can now compare the significance of the&#13;
            improvements in goodness-of-fit as you increase the complexity of the model at each&#13;
            step.</p>&#13;
        <pre>R&gt; anova(dia.model1,dia.model2,dia.model3,dia.model4)<br/>Analysis&#13;
            of Variance Table<br/><br/>Model 1: chol ~ 1<br/>Model 2: chol ~ age<br/>Model 3:&#13;
            chol ~ age + frame<br/>Model 4: chol ~ age *&#13;
            frame<br/>  Res.Df    RSS Df Sum of&#13;
            Sq       F    Pr(&gt;F)<br/>1    389&#13;
            747265<br/>2    388&#13;
            712078  1     35187 19.6306 1.227e-05&#13;
            ***<br/>3    386&#13;
            697527  2     14551  4.0589   0.01801&#13;
            *<br/>4    384&#13;
            688295  2      9233  2.5755   0.07743&#13;
            .<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' '&#13;
            1</pre>&#13;
        <p class="indent">If you hadn’t deleted the records containing missing values in those&#13;
            predictors, you would’ve received an error telling you that the data sets for the&#13;
            four models were not equal sizes.</p>&#13;
        <p class="indent">The results themselves suggest that including <code>age</code> provides a significant improvement to modeling <code>chol</code>; including a main effect for <code>frame</code>&#13;
            provides a further mild improvement; and there’s very weak evidence, if any, that&#13;
            including an interactive effect is beneficial to goodness-of-fit. From this, you might&#13;
            prefer to use <code>dia.mod3</code>, the main-effects-only model, as the&#13;
            most parsimonious representation of mean cholesterol out of these four models.</p>&#13;
        <h4 class="h4" id="ch22lev2sec210"><span epub:type="pagebreak" id="page_533"/><strong><em>22.2.2 Forward Selection</em></strong></h4>&#13;
        <p class="noindent">Partial <em>F</em>-tests are a natural way to investigate nested models&#13;
            but can be difficult to manage if you have many different models to fit when, for&#13;
            example, you have many predictor variables.</p>&#13;
        <p class="indent">This is where <em>forward selection</em> (also referred to as <em>forward&#13;
                elimination</em>) comes in. The idea is to start with an intercept-only model and&#13;
            then perform a series of independent tests to determine which of your predictor&#13;
            variables significantly improves the goodness-of-fit. Then you update your model object&#13;
            by adding that term and execute the series of tests again for all remaining terms to&#13;
            determine which of those would further improve the fit. The process repeats until there&#13;
            aren’t any more terms that improve the fit in a statistically significant way. The&#13;
            ready-to-use R functions <code>add1</code> and <code>update</code> perform the series of tests and update your fitted regression&#13;
            model.</p>&#13;
        <p class="indent">You’ll use the <code>nuclear</code> data frame in&#13;
            the <code>boot</code> library from <a href="ch21.xhtml#ch21exc1">Exercise 21.1</a> on <a href="ch21.xhtml#page_499">page 499</a> and <a href="ch21.xhtml#ch21lev2sec206">Section 21.5.5</a> as an example. The goal is to&#13;
            choose the most informative model for prediction of construction cost. Load <code>boot</code> and access the help file <code>?nuclear</code> to remind yourself of the variable definitions. First fit the model&#13;
            for construction cost with an overall intercept term only.</p>&#13;
        <pre>R&gt; nuc.0 &lt;- lm(cost~1,data=nuclear)<br/>R&gt;&#13;
            summary(nuc.0)<br/><br/>Call:<br/>lm(formula = cost ~ 1, data =&#13;
            nuclear)<br/><br/>Residuals:<br/>    Min      1Q  Median      3Q    Max<br/>-254.05&#13;
            -151.24  -13.46  150.40&#13;
            419.68<br/><br/>Coefficients:<br/>            Estimate&#13;
            Std. Error t value&#13;
            Pr(&gt;|t|)<br/>(Intercept)   461.56      30.07   15.35&#13;
            4.95e-16 ***<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05&#13;
            '.' 0.1 ' ' 1<br/><br/>Residual standard error: 170.1 on 31 degrees of freedom</pre>&#13;
        <p class="indent">You know from earlier exploits that this particular model is rather&#13;
            inadequate for the reliable prediction of <code>cost</code>. So,&#13;
            consider the following line of code to start the forward selection (I’ve&#13;
            suppressed the output, which I’ll show separately and discuss in a moment):</p>&#13;
        <pre>R&gt;&#13;
            add1(nuc.0,scope=.~.+date+t1+t2+cap+pr+ne+ct+bw+cum.n+pt,test="F")</pre>&#13;
        <p class="indent">The first argument to <code>add1</code> is always the&#13;
            model you’re aiming to update. The second argument, <code>scope</code>, is critical—you must supply a formula object defining the&#13;
            “fullest,” most complex model you’d consider fitting. For this <span epub:type="pagebreak" id="page_534"/>you would typically use the <code>.~.</code> notation, in which the dots refer to the definition of&#13;
            the model in the first argument. Specifically, the dots stand for “what is already&#13;
            there.” In other words, through <code>scope</code> you’re&#13;
            telling <code>add1</code> that the fullest model you’d consider&#13;
            has <code>cost</code> as the response, an intercept, and main effects of&#13;
            all other predictors in the <code>nuclear</code> data frame (I’ll&#13;
            restrict the full model to main effects only for ease of demonstration). You don’t&#13;
            need to supply the data frame as an argument since those data are contained within the&#13;
            model object in the first argument. Lastly, you tell <code>add1</code>&#13;
            the test to perform. There are a handful of variants available (see <code>?add1</code>), but here you’ll stick with <code>test="F"</code> for partial <em>F</em>-tests.</p>&#13;
        <p class="indent">Now, focus on the output that’s provided directly after the&#13;
            execution of <code>add1</code>.</p>&#13;
        <pre>Single term additions<br/><br/>Model:<br/>cost ~&#13;
            1<br/>       Df Sum of&#13;
            Sq   RSS    AIC F&#13;
            value    Pr(&gt;F)<br/>&lt;none&gt;             897172&#13;
            329.72<br/>date    1    334335 562837 316.80&#13;
            17.8205 0.0002071&#13;
            ***<br/>t1      1    186984 710189&#13;
            324.24  7.8986 0.0086296&#13;
            **<br/>t2      1        27&#13;
            897145 331.72  0.0009&#13;
            0.9760597<br/>cap     1    199673 697499&#13;
            323.66  8.5881 0.0064137&#13;
            **<br/>pr      1      9037&#13;
            888136 331.40  0.3052&#13;
            0.5847053<br/>ne      1    128641&#13;
            768531 326.77  5.0216 0.0325885&#13;
            *<br/>ct      1     43042 854130&#13;
            330.15  1.5118&#13;
            0.2284221<br/>bw      1     16205&#13;
            880967 331.14  0.5519&#13;
            0.4633402<br/>cum.n   1     67938 829234&#13;
            329.20  2.4579&#13;
            0.1274266<br/>pt      1    305334&#13;
            591839 318.41 15.4772 0.0004575 ***<br/>---<br/>Signif. codes:  0 '***'&#13;
            0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</pre>&#13;
        <p class="indent">The output comprises a series of rows, starting with <code>&lt;none&gt;</code> (doing nothing to the current model). You receive the <code>Sum of Sq</code> and <code>RSS</code> values,&#13;
            directly related to calculating the test statistic. The differences in degrees of&#13;
            freedom are also reported. Another measure of parsimony, <code>AIC</code>, is also provided (you’ll look at that in more detail in <a href="ch22.xhtml#ch22lev2sec212">Section 22.2.4</a>).</p>&#13;
        <p class="indent">Most relevant are the test outcomes; with <code>test="F"</code>, each row corresponds to an independent partial <em>F</em>-test&#13;
            comparing the model in the first argument, as <em><span class="ent">ŷ</span></em><sub>redu</sub>, with the model that results from having added&#13;
            that row term only as <em><span class="ent">ŷ</span></em><sub>full</sub>. Usually,&#13;
            therefore, you would update your model by adding only the term with the largest (and&#13;
            “most significant”) improvement.</p>&#13;
        <p class="indent">Here, you should be able to see that adding <code>date</code> as a predictor offers the largest significant improvement to modeling&#13;
                <code>cost</code>. So, let’s update <code>nuc.0</code> to include that term with the code.</p>&#13;
        <pre>R&gt; nuc.1 &lt;- update(nuc.0,formula=.~.+date)<br/>R&gt;&#13;
                summary(nuc.1)<br/><br/><span epub:type="pagebreak" id="page_535"/>Call:<br/>lm(formula = cost ~ date, data =&#13;
            nuclear)<br/><br/>Residuals:<br/>    Min      1Q  Median      3Q     Max<br/>-176.00&#13;
            -105.27  -25.24   58.63  359.46<br/><br/>Coefficients:<br/>            Estimate&#13;
            Std. Error t value Pr(&gt;|t|)<br/>(Intercept)&#13;
            -6553.57    1661.96  -3.943 0.000446&#13;
            ***<br/>date          102.29      24.23   4.221&#13;
            0.000207 ***<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05&#13;
            '.' 0.1 ' ' 1<br/><br/>Residual standard error: 137 on 30 degrees of&#13;
            freedom<br/>Multiple&#13;
            R-squared:  0.3727,        Adjusted&#13;
            R-squared:  0.3517<br/>F-statistic: 17.82 on 1 and 30 DF,  p-value:&#13;
            0.0002071</pre>&#13;
        <p class="indent">In <code>update</code> you provide the model you want to&#13;
            update as the first argument, and the second argument, <code>formula</code>, tells <code>update</code> how to update the model.&#13;
            Again using the <code>.~.</code> notation, the instruction is to update&#13;
                <code>nuc.0</code> by adding <code>date</code> as a&#13;
            predictor, resulting in a fitted model object of the same class of the first argument.&#13;
            Call a <code>summary</code> of the new model, <code>nuc.1</code>, to see this.</p>&#13;
        <p class="indent">So, let’s keep going! Call <code>add1</code> again,&#13;
            but now pass <code>nuc.1</code> as your first argument.</p>&#13;
        <pre>R&gt;&#13;
            add1(nuc.1,scope=.~.+date+t1+t2+cap+pr+ne+ct+bw+cum.n+pt,test="F")<br/>Single term&#13;
            additions<br/><br/>Model:<br/>cost ~&#13;
            date<br/>       Df Sum of&#13;
            Sq    RSS    AIC F&#13;
            value    Pr(&gt;F)<br/>&lt;none&gt;              562837&#13;
            316.80<br/>t1      1     15322&#13;
            547515 317.92  0.8115&#13;
            0.3750843<br/>t2      1     68161&#13;
            494676 314.67  3.9959 0.0550606&#13;
            .<br/>cap     1    189732 373105 305.64&#13;
            14.7471 0.0006163&#13;
            ***<br/>pr      1      4027&#13;
            558810 318.57  0.2090&#13;
            0.6509638<br/>ne      1     92256&#13;
            470581 313.07  5.6854 0.0238671&#13;
            *<br/>ct      1     54794 508043&#13;
            315.52  3.1277 0.0874906&#13;
            .<br/>bw      1      1240&#13;
            561597 318.73  0.0640&#13;
            0.8020147<br/>cum.n   1      4658 558179&#13;
            318.53  0.2420&#13;
            0.6264574<br/>pt      1     90587&#13;
            472250 313.18  5.5628 0.0252997 *<br/>---<br/>Signif. codes:  0&#13;
            '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</pre>&#13;
        <p class="indent"><span epub:type="pagebreak" id="page_536"/>Note that there’s&#13;
            now no row for adding in <code>date</code>; it’s already there in&#13;
                <code>nuc.1</code>. It seems the next most informative addition&#13;
            would be <code>cap</code>. Update <code>nuc.1</code> to&#13;
            that effect.</p>&#13;
        <pre>R&gt; nuc.2 &lt;- update(nuc.1,formula=.~.+cap)</pre>&#13;
        <p class="indent">Now keep going, testing, and updating. By calling <code>add1</code> on <code>nuc.2</code> (output not shown here),&#13;
            you’ll find that the next most significant addition is <code>pt</code> (by a small margin). Update to a new object named <code>nuc.3</code>, which includes the following term:</p>&#13;
        <pre>R&gt; nuc.3 &lt;- update(nuc.2,formula=.~.+pt)</pre>&#13;
        <p class="indent">Then test again, using <code>add1</code> on <code>nuc.3</code>. You’ll find weak evidence to additionally&#13;
            include a main effect for <code>ne</code>, so update with that inclusion&#13;
            to create <code>nuc.4</code>.</p>&#13;
        <pre>R&gt; nuc.4 &lt;- update(nuc.3,formula=.~.+ne)</pre>&#13;
        <p class="indent">At this point, you may be reasonably certain there won’t be any more&#13;
            useful additions, but check with one final call to <code>add1</code> on&#13;
            the latest fit to be thorough.</p>&#13;
        <pre>R&gt;&#13;
            add1(nuc.4,scope=.~.+date+t1+t2+cap+pr+ne+ct+bw+cum.n+pt,test="F")<br/>Single term&#13;
            additions<br/><br/>Model:<br/>cost ~ date + cap + pt +&#13;
            ne<br/>       Df Sum of&#13;
            Sq    RSS AIC F value&#13;
            Pr(&gt;F)<br/>&lt;none&gt;              222617&#13;
            293.12<br/>t1      1     107.0&#13;
            222510 295.10  0.0125&#13;
            0.9118<br/>t2      1   19229.9 203387&#13;
            292.23  2.4583&#13;
            0.1290<br/>pr      1    5230.8 217386&#13;
            294.36  0.6256&#13;
            0.4361<br/>ct      1   15764.7 206852&#13;
            292.77  1.9815&#13;
            0.1711<br/>bw      1     448.0&#13;
            222169 295.06  0.0524&#13;
            0.8207<br/>cum.n   1   13819.9 208797&#13;
            293.07  1.7209 0.2010</pre>&#13;
        <p class="indent">Indeed it appears that none of the remaining covariates, if included in&#13;
            the model, would yield a statistically significant improvement in goodness-of-fit, so&#13;
            your final model will stay at <code>nuc.4</code>.</p>&#13;
        <pre>R&gt; summary(nuc.4)<br/><br/>Call:<br/>lm(formula = cost ~ date +&#13;
            cap + pt + ne, data =&#13;
                nuclear)<br/><br/>Residuals:<br/>     Min       1Q   Median       3Q      Max<br/><span epub:type="pagebreak" id="page_537"/>-157.894  -38.424   -2.493   35.363   267.445<br/><br/>Coefficients:<br/>              Estimate&#13;
            Std. Error t value Pr(&gt;|t|)<br/>(Intercept)&#13;
            -4.756e+03  1.286e+03  -3.699 0.000975&#13;
            ***<br/>date         7.102e+01  1.867e+01   3.804&#13;
            0.000741&#13;
            ***<br/>cap          4.198e-01  8.616e-02   4.873&#13;
            4.29e-05&#13;
            ***<br/>pt          -1.289e+02  4.950e+01  -2.605&#13;
            0.014761&#13;
            *<br/>ne           9.940e+01  3.864e+01   2.573&#13;
            0.015908 *<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.'&#13;
            0.1 ' ' 1<br/><br/>Residual standard error: 90.8 on 27 degrees of&#13;
            freedom<br/>Multiple R-squared:&#13;
            0.7519,         Adjusted R-squared:&#13;
            0.7151<br/>F-statistic: 20.45 on 4 and 27 DF,  p-value: 7.507e-08</pre>&#13;
        <p class="indent">This method may seem a little cumbersome, and it’s sometimes&#13;
            difficult to decide on the fullest model to be used as the <code>scope</code>, but it’s a remarkably good way to stay involved at every stage&#13;
            of the selection process so you can consider each addition carefully. Note, however,&#13;
            that there’s an element of subjectivity; it’s possible to arrive at&#13;
            different final models by choosing one addition over another, such as if you’d&#13;
            added <code>pt</code> instead of <code>date</code> (they&#13;
            had similar levels of significance in the very first call to <code>add1</code>).</p>&#13;
        <h4 class="h4" id="ch22lev2sec211"><strong><em>22.2.3 Backward Selection</em></strong></h4>&#13;
        <p class="noindent">After learning forward selection, understanding <em>backward&#13;
                selection</em> (or <em>elimination</em>) isn’t much of a stretch. As you might&#13;
            have guessed, where forward selection starts from a reduced model and works its way up&#13;
            to a final model by adding terms, backward selection starts with your fullest model and&#13;
            systematically drops terms. The R functions for this process are <code>drop1</code> to inspect the partial <em>F</em>-tests and <code>update</code>.</p>&#13;
        <p class="indent">The choice of forward versus backward model selection is usually made on a&#13;
            case-by-case basis. If your fullest model isn’t known or is difficult to define&#13;
            and fit, then forward selection is typically preferred. On the other hand, if you do&#13;
            have a natural and easily fitted fullest model, then backward selection can be more&#13;
            convenient to implement. Sometimes, researchers will perform both to see whether the&#13;
            final model they arrive at is different (a perfectly possible occurrence).</p>&#13;
        <p class="indent">Revisit the <code>nuclear</code> example. First, define&#13;
            the fullest model as that which predicts <code>cost</code> by main&#13;
            effects of all available covariates (as you did in your use of <code>scope</code> in the forward selections).</p>&#13;
        <pre>R&gt; nuc.0 &lt;-&#13;
            lm(cost~date+t1+t2+cap+pr+ne+ct+bw+cum.n+pt,data=nuclear)<br/>R&gt;&#13;
            summary(nuc.0)<br/><br/>Call:<br/>lm(formula = cost ~ date + t1 + t2 + cap + pr + ne&#13;
            + ct + bw +<br/><span epub:type="pagebreak" id="page_538"/>    cum.n + pt, data =&#13;
            nuclear)<br/><br/>Residuals:<br/>     Min       1Q   Median       3Q       Max<br/>-128.608  -46.736   -2.668   39.782   180.365<br/><br/>Coefficients:<br/>              Estimate&#13;
            Std. Error t value Pr(&gt;|t|)<br/>(Intercept)&#13;
            -8.135e+03  2.788e+03  -2.918 0.008222&#13;
            **<br/>date         1.155e+02  4.226e+01   2.733&#13;
            0.012470&#13;
            *<br/>t1           5.928e+00  1.089e+01   0.545&#13;
            0.591803<br/>t2           4.571e+00  2.243e+00   2.038&#13;
            0.054390&#13;
            .<br/>cap          4.217e-01  8.844e-02   4.768&#13;
            0.000104&#13;
            ***<br/>pr          -8.112e+01  4.077e+01  -1.990&#13;
            0.059794&#13;
            .<br/>ne           1.375e+02  3.869e+01   3.553&#13;
            0.001883&#13;
            **<br/>ct           4.327e+01  3.431e+01   1.261&#13;
            0.221008<br/>bw          -8.238e+00  5.188e+01  -0.159&#13;
            0.875354<br/>cum.n       -6.989e+00  3.822e+00  -1.829&#13;
            0.081698&#13;
            .<br/>pt          -1.925e+01  6.367e+01  -0.302&#13;
            0.765401<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.'&#13;
            0.1 ' ' 1<br/><br/>Residual standard error: 82.83 on 21 degrees of&#13;
            freedom<br/>Multiple R-squared:&#13;
            0.8394,         Adjusted R-squared:&#13;
            0.763<br/>F-statistic: 10.98 on 10 and 21 DF,  p-value: 2.844e-06</pre>&#13;
        <p class="indent">There are clearly several predictors that appear not to contribute&#13;
            significantly to the response, and these same results are evident the first time you use&#13;
                <code>drop1</code> to examine the impact on goodness-of-fit that&#13;
            would occur from dropping each variable.</p>&#13;
        <pre>R&gt; drop1(nuc.0,test="F")<br/>Single term&#13;
            deletions<br/><br/>Model:<br/>cost ~ date + t1 + t2 + cap + pr + ne + ct + bw + cum.n&#13;
            + pt<br/>       Df Sum of&#13;
            Sq    RSS    AIC F&#13;
            value    Pr(&gt;F)<br/>&lt;none&gt;              144065&#13;
            291.19<br/>date    1     51230 195295&#13;
            298.93  7.4677 0.0124702&#13;
            *<br/>t1      1      2034&#13;
            146099 289.64  0.2965&#13;
            0.5918028<br/>t2      1     28481&#13;
            172546 294.97  4.1517 0.0543902&#13;
            .<br/>cap     1    155943 300008 312.67&#13;
            22.7314 0.0001039&#13;
            ***<br/>pr      1     27161&#13;
            171226 294.72  3.9592 0.0597943&#13;
            .<br/>ne      1     86581 230646&#13;
            304.25 12.6207 0.0018835&#13;
            **<br/>ct      1     10915&#13;
            154980 291.53  1.5911&#13;
            0.2210075<br/>bw      1       173&#13;
            144238 289.23  0.0252&#13;
            0.8753538<br/>cum.n   1     22939 167004&#13;
            293.92  3.3438 0.0816977 .<br/><span epub:type="pagebreak" id="page_539"/>pt      1       627&#13;
            144692 289.33  0.0914 0.7654015<br/>---<br/>Signif. codes:  0&#13;
            '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</pre>&#13;
        <p class="indent">One handy feature of <code>drop1</code> is that its <code>scope</code> argument is optional. If you don’t include <code>scope</code>, it defaults to the intercept-only model as the&#13;
            “most-reduced” model, which is usually a reasonable choice.</p>&#13;
        <p class="indent">Before diving right into the deletion process, remind yourself of the&#13;
            interpretation of what you’re doing. Just as adding any term will always improve&#13;
            the goodness-of-fit in forward selection, deleting any term in backward selection will&#13;
            always worsen the goodness-of-fit. The real question is the perceived significance of&#13;
            these changes in fit quality. In the same way as earlier, where you wanted to add only&#13;
            those terms that offer a <em>statistically significant improvement</em> in&#13;
            goodness-of-fit, when dropping terms, you only want to remove those that <em>do not</em>&#13;
            result in a statistically significant <em>detriment</em> to goodness-of-fit. As such,&#13;
            backward selection is the complete reverse of forward selection in the way it’s&#13;
            carried out.</p>&#13;
        <p class="indent">So, from the output of <code>drop1</code>, you want to&#13;
            choose the term to remove from the model that has the least significant effect of&#13;
            reducing the goodness of the fit. In other words, you’re looking for the term with&#13;
            the largest, nonsignificant <em>p</em>-value for its partial&#13;
            <em>F</em>-test—because dropping a term with a significantly small&#13;
            <em>p</em>-value would significantly worsen the predictive capability of the regression&#13;
            model.</p>&#13;
        <p class="indent">In the current example, it seems the predictor <code>bw</code> has the single least significant effect on reducing the goodness-of-fit,&#13;
            so let’s start the update by removing that term from <code>nuc.0</code>.</p>&#13;
        <pre>R&gt; nuc.1 &lt;- update(nuc.0,.~.-bw)</pre>&#13;
        <p class="indent">Use of <code>update</code> in this selection algorithm is&#13;
            the same as before; now, though, you use a <code>-</code> to signify the&#13;
            deletion of a term following the standard “what’s already there” <code>.~.</code> notation.</p>&#13;
        <p class="indent">The process is then repeated using the latest model <code>nuc.1</code>:</p>&#13;
        <pre>R&gt; drop1(nuc.1,test="F")<br/>Single term&#13;
            deletions<br/><br/>Model:<br/>cost ~ date + t1 + t2 + cap + pr + ne + ct + cum.n +&#13;
            pt<br/>       Df Sum of&#13;
            Sq    RSS    AIC F&#13;
            value    Pr(&gt;F)<br/>&lt;none&gt;              144238&#13;
            289.23<br/>date    1     55942 200180&#13;
            297.72  8.5326 0.007913&#13;
            **<br/>t1      1      3124&#13;
            147362 287.92  0.4765&#13;
            0.497245<br/>t2      1     30717&#13;
            174955 293.41  4.6852 0.041546&#13;
            *<br/>cap     1    159976 304214 311.11&#13;
            24.4005 6.098e-05&#13;
            ***<br/>pr      1     27140&#13;
            171377 292.75  4.1395 0.054122&#13;
            .<br/>ne      1     86408 230646&#13;
            302.25 13.1795 0.001479&#13;
            **<br/>ct      1     11815&#13;
            156053 289.75  1.8021 0.193153<br/><span epub:type="pagebreak" id="page_540"/>cum.n   1     24048 168286&#13;
            292.17  3.6680 0.068557&#13;
            .<br/>pt      1       930&#13;
            145168 287.44  0.1419 0.710039<br/>---<br/>Signif. codes:  0 '***'&#13;
            0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</pre>&#13;
        <p class="indent">It would seem that <code>pt</code> is the next most&#13;
            sensible main effect to drop. Do so and name the resulting object <code>nuc.2</code>.</p>&#13;
        <pre>R&gt; nuc.2 &lt;- update(nuc.1,.~.-pt)</pre>&#13;
        <p class="indent">Now keep going, rechecking with a call to <code>drop1</code> (not shown), and you’ll find that the predictor <code>t1</code> reveals itself as another viable deletion. Update your&#13;
            model with that predictor deleted; name the model object <code>nuc.3</code>.</p>&#13;
        <pre>R&gt; nuc.3 &lt;- update(nuc.2,.~.-t1)</pre>&#13;
        <p class="indent">Recheck the new <code>nuc.3</code> with <code>drop1</code>. You should now find the effect of <code>ct</code> remains nonsignificant, so delete that and update again,&#13;
            giving you a new <code>nuc.4</code>.</p>&#13;
        <pre>R&gt; nuc.4 &lt;- update(nuc.3,.~.-ct)</pre>&#13;
        <p class="indent">Perform yet another check with <code>drop1</code>, this&#13;
            time on <code>nuc.4</code>. At this point, you might hesitate in&#13;
            removing any more predictors, with significance at varying strengths being associated&#13;
            with the effect of their deletion. Note, however, that for at least three of the&#13;
            remaining predictors, <code>t2</code>, <code>pr</code>,&#13;
            and <code>cum.n</code>, the statistical significance should probably be&#13;
            considered borderline at best—all of their <em>p</em>-values lie between the&#13;
            conventional cutoff levels of <em>β</em> = 0.01 and <em>β</em> = 0.05. This&#13;
            again emphasizes the active role a researcher must play in model selection algorithms&#13;
            such as forward or backward elimination; whether you should delete any more variables&#13;
            from here is a difficult question to answer and is left up to your judgment.</p>&#13;
        <p class="indent">Let’s remain with <code>nuc.4</code> as the final&#13;
            model. Summarizing, you’re able to see the estimated regression parameters and the&#13;
            usual post-fit statistics.</p>&#13;
        <pre>R&gt; summary(nuc.4)<br/><br/>Call:<br/>lm(formula = cost ~ date + t2&#13;
            + cap + pr + ne + cum.n, data =&#13;
            nuclear)<br/><br/>Residuals:<br/>     Min       1Q   Median        3Q       Max<br/>-152.851  -53.929   -8.827    53.382   155.581<br/><br/>Coefficients:<br/>              Estimate&#13;
            Std. Error t value Pr(&gt;|t|)<br/>(Intercept)&#13;
            -9.702e+03  1.294e+03  -7.495 7.55e-08 ***<br/><span epub:type="pagebreak" id="page_541"/>date         1.396e+02  1.843e+01   7.574&#13;
            6.27e-08&#13;
            ***<br/>t2           4.905e+00  1.827e+00   2.685&#13;
            0.012685&#13;
            *<br/>cap          4.137e-01  8.425e-02   4.911&#13;
            4.70e-05&#13;
            ***<br/>pr          -8.851e+01  3.479e+01  -2.544&#13;
            0.017499&#13;
            *<br/>ne           1.502e+02  3.400e+01   4.419&#13;
            0.000168&#13;
            ***<br/>cum.n       -7.919e+00  2.871e+00  -2.758&#13;
            0.010703 *<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.'&#13;
            0.1 ' ' 1<br/><br/>Residual standard error: 80.8 on 25 degrees of&#13;
            freedom<br/>Multiple R-squared:&#13;
            0.8181,         Adjusted R-squared:&#13;
            0.7744<br/>F-statistic: 18.74 on 6 and 25 DF,  p-value: 3.796e-08</pre>&#13;
        <p class="indent">Immediately, you can see that your final model from forward selection in&#13;
                <a href="ch22.xhtml#ch22lev2sec210">Section 22.2.2</a> is different from the final&#13;
            model selected here, despite the fullest model being the same for both. How has that&#13;
            occurred?</p>&#13;
        <p class="indent">The answer, simply put, is that the predictors present in a model affect&#13;
            each other. Remember that the estimated coefficients of present predictors easily change&#13;
            in value as you control for different variables. As the number of predictor terms&#13;
            increases, these relationships become more and more complex, so both the order and&#13;
            direction of the selection algorithm have the potential to lead you on different paths&#13;
            through the selection process and arrive at different final destinations, which is&#13;
            exactly what’s happened here.</p>&#13;
        <p class="indent">As a perfect example of this, consider the main effect of <code>pt</code> in the <code>nuclear</code> data. In&#13;
            forward selection, <code>pt</code> was added because it offered the&#13;
            “most significant” improvement to the model <code>cost~date+cap</code>. In backward selection, <code>pt</code> was&#13;
            removed early, since it offered the least reduction in goodness-of-fit if taken from the&#13;
            model <code>cost~date+t1+t2+cap+pr+ne+ct+cum.n+pt</code>. What this&#13;
            means is that for the latter model, the contribution that <code>pt</code> might make in terms of predicting the outcome is already explained by the&#13;
            other present predictor terms. In the smaller model, that effect had not yet been&#13;
            explained, and so <code>pt</code> was an attractive addition.</p>&#13;
        <p class="indent">All this serves to highlight the fickle nature of most selection&#13;
            algorithms, in spite of the systematic way they’re implemented. It’s&#13;
            important to acknowledge that a final model fit will probably vary between approaches&#13;
            and that you should view these selection methods more as helpful guidelines for finding&#13;
            the most parsimonious model and not as providing a universal, definitive solution.</p>&#13;
        <h4 class="h4" id="ch22lev2sec212"><strong><em>22.2.4 Stepwise AIC&#13;
            Selection</em></strong></h4>&#13;
        <p class="noindent">The application of a series of partial <em>F</em>-tests is the most&#13;
            common <em>test-based</em> model selection method, but it’s not the only tool a&#13;
            researcher has at their disposal. You can also locate parsimony by adopting a&#13;
                <em>criterion-based</em> approach. One of the most famous criterion measures is&#13;
            known as <em>Akaike’s</em>&#13;
            <span epub:type="pagebreak" id="page_542"/><em>Information Criterion (AIC)</em>.&#13;
            You’ll have noticed this value as one of the columns in the output of <code>add1</code> and <code>drop1</code>.</p>&#13;
        <p class="indent">For a given linear model, AIC is calculated as follows:</p>&#13;
        <div class="imagec"><a id="ch22eq3"/><img src="../images/e22-3.jpg" alt="image"/></div>&#13;
        <p class="indent">Here, <img class="middle" src="../images/l.jpg" alt="image"/> is a&#13;
            measure of goodness-of-fit named the <em>log-likelihood</em>, and <em>p</em> is the&#13;
            number of regression parameters in the model, excluding the overall intercept. The value&#13;
            of <img class="middle" src="../images/l.jpg" alt="image"/> is a direct outcome of the&#13;
            estimation procedure used to fit the model, though its exact calculation is beyond the&#13;
            scope of this text. The thing to know is that it takes on larger values for&#13;
            better-fitting models.</p>&#13;
        <p class="indent"><a href="ch22.xhtml#ch22eq3">Equation (22.3)</a> produces a value that&#13;
            rewards goodness-of-fit with the <img class="middle" src="../images/f0542-01.jpg" alt="image"/> but simultaneously penalizes complexity with the 2 × (<em>p</em>&#13;
            + 2). The negative sign associated with <img class="middle" src="../images/l.jpg" alt="image"/> coupled with the positive sign of the <em>p</em> + 2 means that&#13;
            smaller values of AIC refer to more parsimonious models.</p>&#13;
        <p class="indent">To find the AIC for a fitted linear model, you use the <code>AIC</code> or <code>extractAIC</code> functions on&#13;
            the object resulting from <code>lm</code>; take a look at the help files&#13;
            of these functions to see the technical differences between the two. The value of <img class="middle" src="../images/l.jpg" alt="image"/> (and therefore also the AIC) has&#13;
            no direct interpretation and is useful only when you compare it against the AIC of&#13;
            another model. You can base model selection on the AIC by identifying the fit with the&#13;
            lowest AIC value. This is the reason it’s directly reported in the output of <code>add1</code> and <code>drop1</code>—you could&#13;
            decide on which term to add or drop based on the change that results in a shift to the&#13;
            smallest AIC, instead of focusing exclusively on the <em>significance</em> of the change&#13;
            via the <em>F</em>-test.</p>&#13;
        <p class="indent">Let’s go even further and combine the ideas of forward and backward&#13;
            selection. <em>Stepwise</em> model selection allows the option to either delete a&#13;
            present term <em>or</em> add a missing term and is typically implemented with respect to&#13;
            AIC. That is, a term is chosen to be added or deleted based on the one move out of all&#13;
            possible moves that yields the single biggest reduction in AIC. This affords you more&#13;
            flexibility in exploring candidate models on your way to the final model&#13;
            fit—determined as the model from which no addition or deletion would reduce the&#13;
            AIC value further.</p>&#13;
        <p class="indent">It’s possible to implement stepwise AIC selection yourself using&#13;
            either <code>add1</code> or <code>drop1</code> at each&#13;
            stage, but fortunately R provides the built-in <code>step</code>&#13;
            function to do it for you. Take the <code>mtcars</code> data from the&#13;
                <code>MASS</code> package from the past couple of chapters.&#13;
            Let’s finally try to obtain a model for mean mileage that offers the opportunity&#13;
            to include every predictor that’s available.</p>&#13;
        <p class="indent">First, take a look at the documentation in <code>?mtcars</code> and a scatterplot matrix of the data again to remind yourself of the&#13;
            variables and their format in the R data frame object. Then define the starting model&#13;
            (often called the <em>null</em> model) as the intercept-only model.</p>&#13;
        <pre>R&gt; car.null &lt;- lm(mpg~1,data=mtcars)</pre>&#13;
        <p class="indent">Your starting model can be anything you like, provided it falls within the&#13;
            domain of the models described by your <code>scope</code> argument to be&#13;
            supplied to <span epub:type="pagebreak" id="page_543"/><code>step</code>. In this example, define <code>scope</code> as the&#13;
            fullest model to be considered—set this to be the overly complex model with a&#13;
            four-way interaction among <code>wt</code>, <code>hp</code>, <code>cyl</code>, and <code>disp</code>&#13;
            (and all relevant lower-order interactions and main effects, via the cross-factor&#13;
            operator), as well as main effects for <code>am</code>, <code>gear</code>, <code>drat</code>, <code>vs</code>, <code>qsec</code>, and <code>carb</code>. The two multilevel categorical variables, <code>cyl</code> and <code>gear</code>, are explicitly&#13;
            converted to factors to avoid them being treated as numeric (refer to <a href="ch20.xhtml#ch20lev2sec188">Section 20.5.4</a>).</p>&#13;
        <p class="indent">The potential for interactions in the final model will serve to highlight&#13;
            an especially important (and convenient) feature of <code>add1</code>,&#13;
                <code>drop1</code>, and <code>step</code>. These&#13;
            functions all respect the hierarchy imposed by interactions and main effects. That is,&#13;
            for <code>add1</code> (and <code>step</code>), an&#13;
            interactive term will not be provided as an option for addition unless all relevant&#13;
            lower-order effects are already present in the current fitted model; similarly, for&#13;
                <code>drop1</code> (and <code>step</code>), an&#13;
            interactive term or main effect will not be provided as an option for deletion unless&#13;
            all relevant higher-order effects are already gone from the current fitted model.</p>&#13;
        <p class="indent">The <code>step</code> function itself returns a fitted&#13;
            model object and by default provides a comprehensive report of each stage of selection.&#13;
            Let’s call it now; for print reasons, some of the output has been snipped out, so&#13;
            you’re encouraged to bring this up on your own machine.</p>&#13;
        <pre>R&gt; car.step &lt;-&#13;
            step(car.null,scope=.~.+wt*hp*factor(cyl)*disp+am<br/>                                      +factor(gear)+drat+vs+qsec+carb)<br/>Start:  AIC=115.94<br/>mpg&#13;
            ~&#13;
            1<br/><br/>              Df&#13;
            Sum of Sq     RSS     AIC<br/>+&#13;
            wt           1    847.73  278.32  73.217<br/>+&#13;
            disp         1    808.89  317.16  77.397<br/>+&#13;
            factor(cyl)  2    824.78  301.26  77.752<br/>+&#13;
            hp           1    678.37  447.67  88.427<br/>+&#13;
            drat         1    522.48  603.57  97.988<br/>+&#13;
            vs           1    496.53  629.52  99.335<br/>+&#13;
            factor(gear) 2    483.24  642.80 102.003<br/>+&#13;
            am           1    405.15  720.90&#13;
            103.672<br/>+&#13;
            carb         1    341.78  784.27&#13;
            106.369<br/>+&#13;
            qsec         1    197.39  928.66&#13;
            111.776<br/>&lt;none&gt;                     1126.05&#13;
            115.943<br/><br/>Step:  AIC=73.22<br/>mpg ~&#13;
            wt<br/><br/>              Df&#13;
            Sum of Sq     RSS    AIC<br/>+&#13;
            factor(cyl)  2     95.26  183.06  63.810<br/>+&#13;
            hp           1     83.27  195.05  63.840<br/>+&#13;
            qsec         1     82.86  195.46  63.908<br/>+&#13;
            vs           1     54.23  224.09  68.283<br/>+&#13;
                carb         1     44.60  233.72  69.628<br/><span epub:type="pagebreak" id="page_544"/>+&#13;
            disp         1     31.64  246.68  71.356<br/>+&#13;
            factor(gear)&#13;
            2     40.37  237.95  72.202<br/>&lt;none&gt;                      278.32  73.217<br/>+&#13;
            drat         1      9.08  269.24  74.156<br/>+&#13;
            am           1      0.00  278.32  75.217<br/>-&#13;
            wt           1    847.73&#13;
            1126.05 115.943<br/><br/>Step:  AIC=63.81<br/>mpg ~ wt +&#13;
            factor(cyl)<br/><br/>                 Df&#13;
            Sum of Sq    RSS    AIC<br/>+&#13;
            hp              1    22.281&#13;
            160.78 61.657<br/>+ wt:factor(cyl)  2    27.170 155.89&#13;
            62.669<br/>&lt;none&gt;                      183.06&#13;
            63.810<br/>+&#13;
            qsec            1    10.949&#13;
            172.11 63.837<br/>+&#13;
            carb            1     9.244&#13;
            173.81 64.152<br/>+&#13;
            vs              1     1.842&#13;
            181.22 65.487<br/>+&#13;
            disp            1     0.110&#13;
            182.95 65.791<br/>+&#13;
            am              1     0.090&#13;
            182.97 65.794<br/>+&#13;
            drat            1     0.073&#13;
            182.99 65.798<br/>+&#13;
            factor(gear)    2     6.682 176.38&#13;
            66.620<br/>- factor(cyl)     2    95.263&#13;
            278.32 73.217<br/>-&#13;
            wt              1   118.204&#13;
            301.26 77.752<br/><br/>Step:  AIC=61.66<br/>mpg ~ wt + factor(cyl) +&#13;
                hp<br/><br/>--<span class="codeitalic">snip</span>--<br/><br/>Step:  AIC=55.9<br/>mpg ~ wt + factor(cyl) + hp +&#13;
                wt:hp<br/><br/>--<span class="codeitalic">snip</span>--<br/><br/>Step:  AIC=52.8<br/>mpg ~ wt + hp +&#13;
                wt:hp<br/><br/>--<span class="codeitalic">snip</span>--<br/><br/>Step:  AIC=52.57<br/>mpg ~ wt + hp + qsec +&#13;
            wt:hp<br/><br/>               Df&#13;
            Sum of&#13;
            Sq    RSS    AIC<br/>&lt;none&gt;                      121.04&#13;
            52.573<br/>-&#13;
            qsec          1     8.720&#13;
            129.76 52.799<br/>+ factor(gear)  2     9.482 111.56&#13;
            53.962<br/>+&#13;
            am            1     1.939&#13;
            119.10 54.056<br/><span epub:type="pagebreak" id="page_545"/>+&#13;
            carb          1     0.080&#13;
            120.96 54.551<br/>+&#13;
            drat          1     0.012&#13;
            121.03 54.570<br/>+&#13;
            vs            1     0.010&#13;
            121.03 54.570<br/>+&#13;
            disp          1     0.008&#13;
            121.03 54.571<br/>+ factor(cyl)   2     0.164&#13;
            120.88 56.529<br/>-&#13;
            wt:hp         1    65.018&#13;
            186.06 64.331</pre>&#13;
        <p class="indent">Each block of output displays the current model fit, its AIC value, and a&#13;
            table showing the possible moves (either adding <code>+</code>, deleting&#13;
                <code>-</code>, or doing nothing <code>&lt;none&gt;</code>). The AIC value that would result from each move alone is&#13;
            listed, and these potential single moves are ranked from smallest to largest AIC&#13;
            value.</p>&#13;
        <p class="indent">As the algorithm proceeds, you see the <code>&lt;none&gt;</code> row creeping its way up the table. For example, in the first&#13;
            table, the value of the AIC for the intercept-only model is 115.94. The biggest&#13;
            reduction in AIC would result from adding a main effect for <code>wt</code>; that move is made, and the effect of subsequent moves on the AIC is&#13;
            reassessed. Also note that the addition of the two-way interaction term between <code>wt</code> and <code>factor(cyl)</code> is considered&#13;
            only at the third step, after the main effects of those predictors have been added. That&#13;
            particular two-way interaction never ends up being included, though, because the main&#13;
            effect of <code>hp</code> is preferable at that third step, and&#13;
            subsequent interactions involving <code>hp</code> then offer a much&#13;
            better reduction in the AIC value in the fourth step. In fact, at the fifth step,&#13;
            actually <em>deleting</em> the main effect for <code>factor(cyl)</code>&#13;
            is deemed to reduce the AIC most, and so the tables for the sixth and seventh steps no&#13;
            longer include that <code>wt:factor(cyl)</code> term as an option. The&#13;
            sixth step suggests that adding the main effect for <code>qsec</code>&#13;
            offers a minor reduction in the AIC, so this is done. The seventh table signals the end&#13;
            of the algorithm because doing nothing offers the lowest AIC value and doing anything&#13;
            else would increase the AIC (shown through <code>&lt;none&gt;</code>&#13;
            taking pole position in that last table).</p>&#13;
        <p class="indent">The final model is stored as the object <code>car.step</code>; by summarizing it, you’ll note that almost 90 percent of the&#13;
            variation in the response is explained by weight, horsepower, and their interaction, as&#13;
            well as the slightly curious main effect of <code>qsec</code> (which&#13;
            itself is not deemed statistically significant).</p>&#13;
        <pre>R&gt; summary(car.step)<br/><br/>Call:<br/>lm(formula = mpg ~ wt + hp&#13;
            + qsec + wt:hp, data =&#13;
            mtcars)<br/><br/>Residuals:<br/>    Min      1Q  Median      3Q    Max<br/>-3.8243&#13;
            -1.3980  0.0303  1.1582&#13;
            4.3650<br/><br/>Coefficients:<br/>             Estimate&#13;
            Std. Error t value Pr(&gt;|t|)<br/>(Intercept)&#13;
            40.310410   7.677887   5.250 1.56e-05 ***<br/><span epub:type="pagebreak" id="page_546"/>wt          -8.681516   1.292525  -6.717&#13;
            3.28e-07&#13;
            ***<br/>hp          -0.106181   0.026263  -4.043&#13;
            0.000395&#13;
            ***<br/>qsec         0.503163   0.360768   1.395&#13;
            0.174476<br/>wt:hp        0.027791   0.007298   3.808&#13;
            0.000733 ***<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05&#13;
            '.' 0.1 ' ' 1<br/><br/>Residual standard error: 2.117 on 27 degrees of&#13;
            freedom<br/>Multiple R-squared:&#13;
            0.8925,         Adjusted&#13;
            R-squared:  0.8766<br/>F-statistic: 56.05 on 4 and 27 DF,  p-value:&#13;
            1.094e-12</pre>&#13;
        <p class="indent">From this, it seems it would be worthwhile to investigate this predictor&#13;
            further, to establish validity of the fitted model (see <a href="ch22.xhtml#ch22lev1sec74">Section 22.3</a>), and perhaps to try&#13;
            transformations of the data (such as modeling GPM instead of MPG; see <a href="ch21.xhtml#ch21lev2sec201">Section 21.4.3</a>) to see whether this effect&#13;
            persists in subsequent runs of the stepwise AIC algorithm. The presence of <code>qsec</code> in the final model illustrates the fact that the&#13;
            selection of the model wasn’t based solely on the significance of predictor&#13;
            contribution but on a criterion-based measure aiming for its own definition of&#13;
            parsimony.</p>&#13;
        <p class="indent">The AIC is sometimes criticized for a tendency to err on the side of more&#13;
            complexity and higher <em>p</em>-values. To balance this, you can increase the&#13;
            penalizing effect of extra predictors by increasing the multiplicative contribution of&#13;
            the (<em>p</em> + 2) on the right of <a href="ch22.xhtml#ch22eq3">Equation (22.3)</a>;&#13;
            though the standard multiplicative factor of 2 is used in the majority of cases (in&#13;
                <code>step</code> you can use the optional argument <code>k</code> to change this). That being said, criterion-based measures&#13;
            are incredibly useful when you have models that aren’t nested (ruling out the&#13;
            partial <em>F</em>-test) and you want to compare them for quick identification of the&#13;
            one that, potentially, provides the most parsimonious representation of the data.</p>&#13;
        <div class="ex">&#13;
            <p class="ext"><a id="ch22exc1"/><strong>Exercise 22.1</strong></p>&#13;
            <p class="noindentz">In <a href="ch22.xhtml#ch22lev2sec210">Sections 22.2.2</a> and <a href="ch22.xhtml#ch22lev2sec211">22.2.3</a>, you used forward and backward&#13;
                selection approaches to find a model for predicting the cost of the construction of&#13;
                nuclear power plants (based on the <code>nuclear</code> data frame&#13;
                in the <code>boot</code> package).</p>&#13;
            <ol type="a">&#13;
                <li><p class="noindents">Using the same fullest model (in other words, main effects&#13;
                        of all present predictors only), use stepwise AIC selection to find a&#13;
                        suitable model for the data.</p></li>&#13;
                <li><p class="noindents">Does the final model found in (a) match either of the&#13;
                        models resulting from the earlier use of forward and backward selection? How&#13;
                        does it differ?</p></li>&#13;
            </ol>&#13;
            <p class="noindentz"><span epub:type="pagebreak" id="page_547"/><a href="ch21.xhtml#ch21exc2">Exercise 21.2</a> on <a href="ch21.xhtml#page_512">page 512</a> detailed Galileo’s ball data. Enter these as a data frame in&#13;
                your current R workspace if you haven’t already.</p>&#13;
            <ol type="a" start="3">&#13;
                <li><p class="noindents">Fit five linear models to these data with distance as the&#13;
                        response—an intercept-only model and four separate polynomial models&#13;
                        of increasing order 1 to 4 in height.</p></li>&#13;
                <li><p class="noindents">Construct a table of partial <em>F</em>-tests to identify&#13;
                        your favored model for distance traveled. Is your selection consistent with&#13;
                            <a href="ch21.xhtml#ch21exc2">Exercise 21.2</a> (b) and (c)?</p></li>&#13;
            </ol>&#13;
            <p class="noindentz">You first encountered the <code>diabetes</code>&#13;
                data frame in the contributed <code>faraway</code> package in <a href="ch21.xhtml#ch21lev2sec203">Section 21.5.2</a>, where you modeled the mean&#13;
                total cholesterol. Load the package and inspect the documentation in <code>?diabetes</code> to refresh your memory of the data set.</p>&#13;
            <ol type="a" start="5">&#13;
                <li><p class="noindents">There are some missing values in <code>diabetes</code> that might interfere with model selection algorithms.&#13;
                        Define a new version of the <code>diabetes</code> data frame&#13;
                        that deletes all rows with a missing value in any of the following&#13;
                        variables: <code>chol</code>, <code>age</code>, <code>gender</code>, <code>height</code>, <code>weight</code>, <code>frame</code>, <code>waist</code>, <code>hip</code>, <code>location</code>. Hint:&#13;
                        Use <code>na.omit</code> or your knowledge of record&#13;
                        extraction or deletion for a data frame. You can create the required vector&#13;
                        of row numbers to be extracted or deleted using <code>which</code> and <code>is.na</code>, or you can try&#13;
                        using the <code>complete.cases</code> function to obtain a&#13;
                        logical flag vector—inspect its help file for details.</p></li>&#13;
                <li><p class="noindents">Use your data frame from (e) to fit two linear models with&#13;
                            <code>chol</code> as the response. The null model&#13;
                        object, named <code>dia.null</code>, should be an&#13;
                        intercept-only model. The full model object, named <code>dia.full</code>, should be the overly complex model with a four-way&#13;
                        interaction (and all lower-order terms) among <code>age</code>, <code>gender</code>, <code>weight</code>, and <code>frame</code>; a three-way&#13;
                        interaction (and all lower-order terms) among <code>waist</code>, <code>height</code>, and <code>hip</code>; and a main effect for <code>location</code>.</p></li>&#13;
                <li><p class="noindents">Starting from <code>dia.null</code> and&#13;
                        using the same terms as in <code>dia.full</code> for <code>scope</code>, implement stepwise selection by AIC to&#13;
                        choose a model for mean total cholesterol and then summarize.</p></li>&#13;
                <li><p class="noindents">Use forward selection based on partial <em>F</em>-tests&#13;
                        with a conventional significance level of <em>α</em> = 0.05 to choose a&#13;
                        model, again starting from <code>dia.null</code>. Is the&#13;
                        result here the same as the model arrived at in (g)?</p></li>&#13;
                <li><p class="noindents">Stepwise selection doesn’t have to start from the&#13;
                        simplest model. Repeat (g), but this time, set <code>dia.full</code> to be the starting model (you don’t need to&#13;
                        supply anything to <code>scope</code> if you’re&#13;
                        starting from the most complex model). What is the final model selected via&#13;
                        AIC if you start from <code>dia.full</code>? Is it different&#13;
                        than the final model from (g)? Why is this or is this not the case, do you&#13;
                        think?</p></li>&#13;
            </ol>&#13;
            <p class="noindentz"><span epub:type="pagebreak" id="page_548"/>Revisit the&#13;
                ubiquitous <code>mtcars</code> data frame from the <code>MASS</code> package.</p>&#13;
            <ol type="a" start="10">&#13;
                <li><p class="noindents">In <a href="ch22.xhtml#ch22lev2sec212">Section 22.2.4</a>,&#13;
                        you used stepwise AIC selection to model mean MPG. The selected model&#13;
                        included a main effect for <code>qsec</code>. Rerun the same&#13;
                        AIC selection process, but this time, do it in terms of GPM=1/MPG. Does this&#13;
                        change the complexity of the final model?</p></li>&#13;
            </ol>&#13;
        </div>&#13;
        <h4 class="h4" id="ch22lev2sec213"><strong><em>22.2.5 Other Selection&#13;
                Algorithms</em></strong></h4>&#13;
        <p class="noindent">Any model selection algorithm will always aim to quantitatively define&#13;
                <em>parsimony</em> and suggest a model that optimizes that definition in light of&#13;
            the available data. There are alternatives to AIC, such as the <em>corrected AIC&#13;
                    (AIC<sub>c</sub>)</em> or the <em>Bayesian Information Criterion (BIC)</em>,&#13;
            both of which impose heavier penalties on complexity than the default AIC in (22.3).</p>&#13;
        <p class="indent">Sometimes it’s tempting to simply monitor <em>R</em><sup>2</sup>,&#13;
            the coefficient of determination, for a series of models. However, as mentioned in <a href="ch22.xhtml#ch22lev2sec209">Section 22.2.1</a>, this on its own is inadequate&#13;
            for choosing between models since it doesn’t penalize complexity and will&#13;
            generally always increase as you continue to add predictors, whether they have a&#13;
            statistically significant impact or not. The <em>adjusted R</em><sup>2</sup>&#13;
            <em>statistic</em>, denoted <img class="middle" src="../images/r2.jpg" alt="image"/>&#13;
            and reported as <code>Adjusted R-squared</code> in <code>summary</code>, is a simple transformation of the original <em>R</em><sup>2</sup>&#13;
            that does incorporate a penalty for complexity relative to the sample size <em>n</em>;&#13;
            calculated as</p>&#13;
        <div class="imagec"><img src="../images/f0548-01.jpg" alt="image"/></div>&#13;
        <p class="noindent">where <em>p</em> is the number of predictor terms (excluding the&#13;
            intercept). The algorithms based on tests and criteria are always preferable (since&#13;
            interpretation of <img class="middle" src="../images/r2.jpg" alt="image"/> can be&#13;
            difficult), but monitoring <img class="middle" src="../images/r2.jpg" alt="image"/> can&#13;
            be useful as a quick check between nested models—a higher value points to a&#13;
            preferred model.</p>&#13;
        <p class="indent">For further reading, <a href="ch08.xhtml#ch08">Chapter 8</a> of Faraway&#13;
                (<a href="ref.xhtml#ref21">2005</a>) provides some excellent commentary on the&#13;
            guideline-only nature of both test- and criterion-based model selection procedures.&#13;
            Regardless of which approach you employ, always remember that any final model reached by&#13;
            using these algorithms should still be subject to scrutiny.</p>&#13;
        <h3 class="h3" id="ch22lev1sec74"><strong>22.3 Residual Diagnostics</strong></h3>&#13;
        <p class="noindent">In previous chapters, you examined the practical aspects of multiple&#13;
            linear regression models, such as fitting and interpreting, dummy coding, transforming,&#13;
            and so on, but you haven’t yet looked at methods that are essential for&#13;
            determining the <em>validity</em> of your model. The final part of this chapter will&#13;
            introduce you to <em>model diagnostics</em>, the primary goal of which is to ensure that&#13;
            your regression model is valid and accurately represents the relationships in your data.&#13;
            For this, I’ll return focus to the theoretical assumptions underpinning the&#13;
            multiple linear regression model that were noted early in <a href="ch21.xhtml#ch21lev2sec190">Section 21.2.1</a>.</p>&#13;
        <p class="indent"><span epub:type="pagebreak" id="page_549"/>As a refresher, in&#13;
            general when fitting these models, remember to keep these four things in mind:</p>&#13;
        <p class="noindenth"><strong>Errors</strong> The error term <em><span class="ent">ɛ</span></em>, which defines the departure of any observation from the&#13;
            fitted mean-outcome model, is assumed to be normally distributed with a mean of zero and&#13;
            a constant variance denoted with <em>σ</em><sup>2</sup>. The error associated with&#13;
            a given observation is also assumed to be independent of the error of any other&#13;
            observation. If a fitted model suggests violation of any of these assumptions,&#13;
            you’ll need to investigate further (usually involving refitting a variation of the&#13;
            model).</p>&#13;
        <p class="noindenth"><strong>Linearity</strong> It is critical to be able to assume that the&#13;
            mean response as a function is linear in terms of the regression parameters&#13;
                <em>β</em><sub>0</sub>, <em>β</em><sub>1</sub>, . . .,&#13;
                    <em>β</em><sub><em>p</em></sub>. Though transformations of individual&#13;
            variables and the presence of interactions can relax the specific nature of the&#13;
            estimated trends somewhat, any diagnostic suggestion that a relationship is nonlinear&#13;
            (and hence not being captured by the fitted model at hand) must be investigated.</p>&#13;
        <p class="noindenth"><strong>Extreme or unusual observations</strong> Always inspect extreme&#13;
            data points or data points that strongly influence the fitted model—for example,&#13;
            points that have been recorded incorrectly should be removed from the analysis.</p>&#13;
        <p class="noindenth"><strong>Collinearity</strong> Predictors highly correlated with one&#13;
            another can adversely affect an entire model, meaning it can be easy to misinterpret the&#13;
            effects of any included predictors. This should be avoided in any regression.</p>&#13;
        <p class="indent">You investigate the first three after fitting the model using diagnostic&#13;
            tools. Any violation of these assumptions diminishes the reliability of your model,&#13;
            sometimes severely. Collinearity and/or extreme observations can be discovered by basic&#13;
            statistical explorations (for example, viewing scatterplot matrices) of the raw data&#13;
            pre-fit, but any consequential effects are appraised post-fit.</p>&#13;
        <p class="indent">There are some statistical tests you can perform to diagnose a statistical&#13;
            model, but commonly a diagnostic inspection boils down to interpretation of the results&#13;
            of graphical tools designed to target specific assumptions. Interpreting these plots can&#13;
            be quite difficult and only really becomes easier with experience. Here, I’ll&#13;
            provide an overview of these tools in R and describe some common things to look for. For&#13;
            a more detailed discussion, look to dedicated texts on regression methods such as&#13;
            Chatterjee et al. (<a href="ref.xhtml#ref12">2000</a>), Faraway (<a href="ref.xhtml#ref21">2005</a>), or Montgomery et al. (<a href="ref.xhtml#ref48">2012</a>).</p>&#13;
        <h4 class="h4" id="ch22lev2sec214"><strong><em>22.3.1 Inspecting and Interpreting&#13;
                    Residuals</em></strong></h4>&#13;
        <p class="noindent">If you look back at the plot in <a href="ch20.xhtml#ch20fig2">Figure&#13;
                20-2</a> on <a href="ch20.xhtml#page_456">page 456</a>, you’ll see a good&#13;
            demonstration of the importance of interpreting the results given by <em><span class="ent">ŷ</span></em> as a mean response value. Under the assumed&#13;
            model, any deviation of the raw observations from the fitted line is deemed to be the&#13;
            result of the (normally distributed) errors defined by the <em><span class="ent">ɛ</span></em> term in <a href="ch20.xhtml#ch20eq1">Equation (20.1)</a> on&#13;
                <a href="ch20.xhtml#page_453">page 453</a>.</p>&#13;
        <p class="indent"><span epub:type="pagebreak" id="page_550"/>Of course, in practice,&#13;
            you don’t have the true error values because you don’t know the true model&#13;
            of your data. For the <em>i</em>th response observation <em>y</em><sub><em>i</em></sub>&#13;
            and its fitted value from the model, <em><span class="ent">ŷ</span></em><sub><em>i</em></sub>, you would typically assess diagnostic&#13;
            plots using the estimated residuals <em>e</em><sub><em>i</em></sub> =&#13;
                    <em>y</em><sub><em>i</em></sub> − <em><span class="ent">ŷ</span></em><sub><em>i</em></sub>. A call to <code>summary</code> even encourages a post-fit analysis of the residuals by providing&#13;
            you with a five-number summary of the <em>e</em><sub><em>i</em></sub> above the table of&#13;
            estimated coefficients. This allows you to take a look at their values and do a&#13;
            preliminary numeric assessment of the symmetry of their distribution (as required by the&#13;
            assumption of normality—see <a href="ch22.xhtml#ch22lev2sec215">Section&#13;
            22.3.2</a>).</p>&#13;
        <p class="indent">As well as a diagnostic inspection of the raw residuals&#13;
                    <em>e</em><sub><em>i</em></sub>, some diagnostic checks can also be done using&#13;
            their <em>standardized</em> (or <em>Studentized</em>) values. The standardized residuals&#13;
            rescale the raw residuals <em>e</em><sub><em>i</em></sub> to ensure they all have the&#13;
            same variance, which is important if you need to directly compare them to one another.&#13;
            Formally, this is achieved with the calculation <img class="middle" src="../images/f0550-01.jpg" alt="image"/>, where <img class="middle" src="../images/o.jpg" alt="image"/> is the estimate of the residual standard error&#13;
            and <em>h</em><sub><em>ii</em></sub> is the <em>leverage</em> of the <em>i</em>th&#13;
            observation (you’ll learn about leverage in <a href="ch22.xhtml#ch22lev2sec217">Section 22.3.4</a>).</p>&#13;
        <p class="indent">Arguably the most common graphical tool used for a post-fit analysis of&#13;
            the residuals is a simple scatterplot of the “observed-minus-fitted” raw&#13;
            residuals on the vertical axis against their corresponding fitted-model values from the&#13;
            regression. If the assumptions concerning <em><span class="ent">ɛ</span></em> are&#13;
            valid, then the <em>e</em><sub><em>i</em></sub> should appear randomly scattered around&#13;
            zero (since the errors aren’t assumed to be related in any way to the value of the&#13;
            response). Any systematic pattern in the plot suggests the residuals don’t agree&#13;
            with the error assumptions—this could be because of nonlinear relationships in&#13;
            your data or the presence of dependent observations (in other words, your data points&#13;
            are correlated and therefore not independent of one another). The plot can also be used&#13;
            to detect <em>heteroscedasticity</em>—a nonconstant variance in the&#13;
            residuals—commonly seen as a “fanning out” of the residuals around 0&#13;
            as the fitted values increase.</p>&#13;
        <p class="indent">Note once more that it’s important these theoretical assumptions are&#13;
            valid because they affect the validity of the estimates of the regression coefficients&#13;
            and the reliability of their standard errors (and so statistical significance)—in&#13;
            other words, the correctness of your interpretation of their impact on the response.</p>&#13;
        <p class="indent">To give you a better idea of all this, consider the three images in <a href="ch22.xhtml#ch22fig1">Figure 22-1</a>. These provide residuals versus fitted&#13;
            values plots for three hypothetical scenarios.</p>&#13;
        <p class="indent">The plot on the left is, more or less, what you’re looking&#13;
            for—the residuals appear randomly scattered around zero, and their spread around&#13;
            zero appears constant (<em>homoscedasticity</em>). In the middle plot, however, you can&#13;
            see systematic behavior in the residuals. Though the variability still seems to remain&#13;
            constant throughout the range of fitted values, the apparent trend suggests the current&#13;
            model isn’t explaining some of the relationship between response and predictor (or&#13;
            predictors). On the right, the residuals seem to be scattered randomly about zero again.&#13;
            However, the variability they exhibit isn’t constant. Among other things, this&#13;
            kind of heteroscedasticity will affect the reliability of your confidence and prediction&#13;
            intervals.</p>&#13;
        <div class="image"><span epub:type="pagebreak" id="page_551"/><img src="../images/f22-01.jpg" alt="image"/></div>&#13;
        <p class="figt"><em><a id="ch22fig1"/>Figure 22-1: Three impressions of a hypothetical&#13;
                residuals versus fitted diagnostic plot from a linear regression: random (left),&#13;
                systematic (middle), and heteroscedastic (right)</em></p>&#13;
        <p class="indent">It’s important to know that even if your graphical diagnostics&#13;
            don’t provide the well-behaved plot like the hypothetical example on the left of&#13;
                <a href="ch22.xhtml#ch22fig1">Figure 22-1</a>, this is not a reason to immediately&#13;
            give up on the analysis. These plots can form an integral part in finding an appropriate&#13;
            model for your data. You can often reduce nonlinearity by including additional&#13;
            predictors or interactions, changing the treatment of a categorical variable, or&#13;
            performing nonlinear transformations of certain continuous predictors.&#13;
            Heteroscedasticity, especially the kind in <a href="ch22.xhtml#ch22fig1">Figure 22-1</a>&#13;
            where the variability is higher for higher fitted values, is common in some fields of&#13;
            research. A first step to remedy this problem often involves a simple log transformation&#13;
            of the response followed by a reinspection of the diagnostics.</p>&#13;
        <p class="indent">It’s time for an example. In <a href="ch22.xhtml#ch22lev2sec212">Section 22.2.4</a>, you used stepwise AIC selection to choose a model for MPG for&#13;
            the <code>mtcars</code> data, creating the object <code>car.step</code>. Let’s now diagnose that same fit to see whether there are&#13;
            any problems with the assumptions of the model.</p>&#13;
        <p class="indent">When you apply the <code>plot</code> function directly to&#13;
            an <code>lm</code> object, it can conveniently produce six types of&#13;
            diagnostic plot of the fit. By default, four of these plots are produced in succession.&#13;
            Follow the signal to the user in the console, <span class="literal">Hit &lt;Return&gt;&#13;
                to see next plot</span>, to progress through them. In the examples that follow,&#13;
            however, you’ll select each plot individually using the optional <code>which</code> argument (specified by the integers <code>1</code> through <code>6</code>; see <code>?plot.lm</code> for the documentation). The residuals versus fitted&#13;
            plot is given with <code>which=1</code>; the following line produces the&#13;
            plot on the left in <a href="ch22.xhtml#ch22fig2">Figure 22-2</a>:</p>&#13;
        <pre>R&gt; plot(car.step,which=1)</pre>&#13;
        <p class="indent">As you can see, R adds a smoothed line to help the user interpret any&#13;
            trend, though this shouldn’t be used exclusively in any judgment. By default, the&#13;
            three most extreme points from zero are annotated (according to the <code>rownames</code> attribute of the data frame used in the call that&#13;
            fitted the model). The model formula itself is specified below the horizontal axis&#13;
            label.</p>&#13;
        <p class="indent">From this, you see that the residuals versus fitted plot for the <code>car.step</code> offers little, if any, cause for concern. There&#13;
            isn’t much of a discernible trend, and you can take further comfort in the fact&#13;
            that the errors (<em>e</em><sub><em>i</em></sub>) appear homoscedastic in their&#13;
            distribution.</p>&#13;
        <div class="image"><span epub:type="pagebreak" id="page_552"/><img src="../images/f22-02.jpg" alt="image"/></div>&#13;
        <p class="figt"><em><a id="ch22fig2"/>Figure 22-2: Residuals versus fitted and&#13;
                scale-location diagnostic plots for the</em>&#13;
            <code>car.step</code>&#13;
            <em>model</em></p>&#13;
        <p class="indent">The <em>scale-location</em> plot is similar to the residuals versus fitted&#13;
            plot, though instead of the raw <em>e</em><sub><em>i</em></sub> on the vertical axis,&#13;
            the scale-location plot provides <img class="middle" src="../images/f0552-01.jpg" alt="image"/>, that is, the square root of the absolute value (denoted by | ·&#13;
            |; this renders all negative values positive) of the standardized residuals. These are&#13;
            plotted against the respective fitted values on the horizontal axis. By restricting&#13;
            attention to the magnitude of each residual in this way, the scale-location plot is used&#13;
            to reveal trends in the size of the departure of each data point from its fitted value,&#13;
            as the fitted values increase. This means such a plot can, for example, be more useful&#13;
            than the raw residuals versus fitted plot in detecting things such as&#13;
            heteroscedasticity. Just as with the original residuals versus fitted plot, you’re&#13;
            looking for a plot with no discernible pattern as an indication that no error&#13;
            assumptions have been violated.</p>&#13;
        <p class="indent">The right plot of <a href="ch22.xhtml#ch22fig2">Figure 22-2</a> shows the&#13;
            scale-location plot for <code>car.step</code>, selected with <code>which=3</code>. This plot also demonstrates the ability to remove&#13;
            the default smoothed trend line with the <code>add.smooth</code>&#13;
            argument and to control how many extreme points are labeled using the <code>id.n</code> argument.</p>&#13;
        <pre>R&gt; plot(car.step,which=3,add.smooth=FALSE,id.n=2)</pre>&#13;
        <p class="indent">As with the original residuals versus fitted plot, there doesn’t&#13;
            seem to be much to be concerned about in the scale-location plot for this <code>mtcars</code> model.</p>&#13;
        <p class="indent">Return to Galileo’s ball-rolling data first laid out in <a href="ch21.xhtml#ch21exc2">Exercise 21.2</a> on <a href="ch21.xhtml#page_512">page&#13;
                512</a>. In their use in the following example, the response variable&#13;
            “distance traveled” is given as column <code>d</code>, and&#13;
            the explanatory variable “height” is column <code>h</code>,&#13;
            in the data frame <code>gal</code>. I’ll re-create some of the&#13;
            exercise to give you a couple of straightforward examples of cause for concern in&#13;
            residual diagnostic plots. Execute the following code to define the data frame of the&#13;
            seven observations and fit two regression models—the first linear in height and&#13;
            the second quadratic (refer to <a href="ch21.xhtml#ch21lev2sec199">Section 21.4.1</a>&#13;
            for details on polynomial transformations).</p>&#13;
        <pre><span epub:type="pagebreak" id="page_553"/>R&gt; gal &lt;-&#13;
            data.frame(d=c(573,534,495,451,395,337,253),<br/>                     h=c(1,0.8,0.6,0.45,0.3,0.2,0.1))<br/>R&gt;&#13;
            gal.mod1 &lt;- lm(d~h,data=gal)<br/>R&gt; gal.mod2 &lt;- lm(d~h+I(h^2),data=gal)</pre>&#13;
        <p class="indent">Now, take a look at the three images in <a href="ch22.xhtml#ch22fig3">Figure 22-3</a>, created with the following code:</p>&#13;
        <pre>R&gt; plot(gal$d~gal$h,xlab="Height",ylab="Distance")<br/>R&gt;&#13;
            abline(gal.mod1)<br/>R&gt; plot(gal.mod1,which=1,id.n=0)<br/>R&gt;&#13;
            plot(gal.mod2,which=1,id.n=0)</pre>&#13;
        <div class="image"><img src="../images/f22-03.jpg" alt="image"/></div>&#13;
        <p class="figt"><em><a id="ch22fig3"/>Figure 22-3: Demonstrating residual diagnostics for&#13;
                Galileo’s ball-rolling data. Top left: The raw data with a simple linear trend&#13;
                corresponding to</em>&#13;
            <code>gal.mod1</code>&#13;
            <em>superimposed. Top right: Residuals versus fitted for the linear-trend-only model.&#13;
                Bottom: Residuals versus fitted for the quadratic model</em>&#13;
            <code>gal.mod2</code>.</p>&#13;
        <p class="indent"><span epub:type="pagebreak" id="page_554"/>The top-left plot shows&#13;
            the data and provides the straight line of the simple linear model. Although this&#13;
            clearly captures the increasing trend, this plot suggests some curvature is also&#13;
            present. The diagnostic residuals versus fitted plot (top-right) shows that the&#13;
            linear-trend-only model is inadequate—the systematic pattern throws up a red flag&#13;
            concerning the assumptions surrounding the linear model errors. The bottom image shows&#13;
            the residuals versus fitted plot based on the quadratic version of the model in <code>gal.mod2</code>. Including a quadratic term in “height”&#13;
            removes this prominent curve in the residuals. However, these latest&#13;
                    <em>e</em><sub><em>i</em></sub> values still seem to exhibit systematic behavior&#13;
            in a wavelike form, perhaps suggesting you try a cubic model, which is difficult with&#13;
            such a small sample size.</p>&#13;
        <h4 class="h4" id="ch22lev2sec215"><strong><em>22.3.2 Assessing Normality</em></strong></h4>&#13;
        <p class="noindent">To assess the assumption that the error is normally distributed, you can&#13;
            use a normal QQ plot, as first discussed in <a href="ch16.xhtml#ch16lev2sec142">Section&#13;
                16.2.2</a>. You select <code>which=2</code> when calling <code>plot</code> on an <code>lm</code> object to produce&#13;
            a normal quantile-quantile plot of the (standardized) residuals. Return to the <code>car.step</code> model object and enter the following line to produce&#13;
                <a href="ch22.xhtml#ch22fig4">Figure 22-4</a>.</p>&#13;
        <pre>R&gt; plot(car.step,which=2)</pre>&#13;
        <div class="image"><img src="../images/f22-04.jpg" alt="image"/></div>&#13;
        <p class="figt"><em><a id="ch22fig4"/>Figure 22-4: Normal QQ plot of the residuals from&#13;
                the</em>&#13;
            <code>car.step</code>&#13;
            <em>model</em></p>&#13;
        <p class="indent">You interpret the QQ plot of the residuals in the same way as in <a href="ch16.xhtml#ch16lev2sec142">Section 16.2.2</a>. The gray diagonal line&#13;
            represents the true normal quantiles, and the plotted points are the corresponding&#13;
            numeric quantiles of the estimated regression errors. Normally distributed data should&#13;
            lie close to the straight line.</p>&#13;
        <p class="indent"><span epub:type="pagebreak" id="page_555"/>For the <code>car.step</code> regression model, the points generally seem to&#13;
            follow the path laid out by the theoretical normal quantiles. There is some deviation,&#13;
            which is to be expected, but no apparent major departure from normality.</p>&#13;
        <p class="indent">There are also other ways to test for normality, such as the famous&#13;
            Shapiro-Wilk hypothesis test. The null hypothesis for the Shapiro-Wilk test is that the&#13;
            data are normally distributed, so a small <em>p</em>-value would suggest non-normality&#13;
            of your data (see <a href="ref.xhtml#ref55">Royston, 1982</a>, for technical details).&#13;
            To execute the procedure, use the <code>shapiro.test</code> function in&#13;
            R. By first extracting the standardized residuals of your fitted model with <code>rstandard</code>, you’ll see that this test applied to <code>car.step</code> offers up a large <em>p</em>-value.</p>&#13;
        <pre>R&gt;&#13;
            shapiro.test(rstandard(car.step))<br/><br/>        Shapiro-Wilk&#13;
            normality test<br/><br/>data:  rstandard(car.step)<br/>W = 0.97105, p-value&#13;
            = 0.5288</pre>&#13;
        <p class="indent">In other words, there’s no evidence (according to this test) that&#13;
            the residuals of <code>car.step</code> aren’t normal.</p>&#13;
        <p class="indent">Being able to assume normality of the error term supports the methodology&#13;
            used to produce reliable estimates of the regression coefficients. As long as your data&#13;
            are <em>approximately</em> normal, though, you shouldn’t be too concerned with&#13;
            mild indications of non-normality. Some transformations of your data, and an increase in&#13;
            your sample size, can reduce concerns about more severe indications of non-normal&#13;
            residuals.</p>&#13;
        <h4 class="h4" id="ch22lev2sec216"><strong><em>22.3.3 Illustrating Outliers, Leverage, and&#13;
                    Influence</em></strong></h4>&#13;
        <p class="noindent">It’s always important to investigate any individual observations&#13;
            that appear unusual or extreme compared to the bulk of your observations. In general, an&#13;
            exploratory analysis of your data, perhaps involving summary statistics or scatterplot&#13;
            matrices, is a good idea since it can help you identify any such values—they have&#13;
            the potential to adversely affect your model fits. Before going further, it’s&#13;
            important to clarify some frequently used terms.</p>&#13;
        <p class="noindenth"><strong>Outlier</strong> This is a general term used to describe an&#13;
            unusual observation in the context of the data, as you saw in <a href="ch13.xhtml#ch13lev2sec121">Section 13.2.6</a>. In linear regression, an&#13;
            outlier usually has a large residual but is identified as an outlier only if it&#13;
            doesn’t conform to the trend of the fitted model. An outlier can, but&#13;
            doesn’t always, significantly alter the trends described by the fitted model.</p>&#13;
        <p class="noindenth"><strong>Leverage</strong> This term refers to the extremity of the&#13;
            values of the present predictors. A high-leverage point is an observation with predictor&#13;
            values extreme enough to potentially significantly affect the slopes or trends in the&#13;
            fitted model. An outlier can have a high or low leverage.</p>&#13;
        <p class="noindenth"><span epub:type="pagebreak" id="page_556"/><strong>Influence</strong> An observation with high leverage that&#13;
                <em>does</em> affect the estimated trends is deemed influential. In other words,&#13;
            influence is judged only when the response value is taken into account alongside the&#13;
            corresponding predictor values.</p>&#13;
        <p class="indent">These definitions have some overlap, so a given observation can be&#13;
            described using a combination of these terms. Let’s look at some hypothetical&#13;
            examples. Create the following two vectors of ten supposed responses (<code>y</code>) and explanatory (<code>x</code>)&#13;
            values:</p>&#13;
        <pre>R&gt; x &lt;- c(1.1,1.3,2.3,1.6,1.2,0.1,1.8,1.9,0.2,0.75)<br/>R&gt; y&#13;
            &lt;- c(6.7,7.9,9.8,9.3,8.2,2.9,6.6,11.1,4.7,3)</pre>&#13;
        <p class="indent">These will form the bulk of the data. Now, consider the following six&#13;
            objects, <code>p1x</code> to <code>p3y</code>, which&#13;
            will be used to store the predictor and response values for three additional observation&#13;
            points:</p>&#13;
        <pre>p1x &lt;- 1.2<br/>p1y &lt;- 14<br/>p2x &lt;- 5<br/>p2y &lt;-&#13;
            19<br/>p3x &lt;- 5<br/>p3y &lt;- 5</pre>&#13;
        <p class="indent">That is, point 1 is (1.2,14); point 2 is (5,19); and point 3 is (5,5).</p>&#13;
        <p class="indent">Next, the following four uses of <code>lm</code> provide&#13;
            four simple linear model fits. The first is the regression of <code>y</code> on <code>x</code> only. The next three additionally&#13;
            include points 1, 2, and 3, separately, as an 11th observation.</p>&#13;
        <pre>R&gt; mod.0 &lt;- lm(y~x)<br/>R&gt; mod.1 &lt;-&#13;
            lm(c(y,p1y)~c(x,p1x))<br/>R&gt; mod.2 &lt;- lm(c(y,p2y)~c(x,p2x))<br/>R&gt; mod.3&#13;
            &lt;- lm(c(y,p3y)~c(x,p3x))</pre>&#13;
        <p class="indent">Now, you can use these objects to visually clarify the definitions of&#13;
                <em>outlier</em>, <em>leverage</em>, and <em>influence</em>, as shown in <a href="ch22.xhtml#ch22fig5">Figure 22-5</a>. Enter the following code to initialize&#13;
            the scatterplot with set axis limits of <code>x</code> and <code>y</code>:</p>&#13;
        <pre>R&gt; plot(x,y,xlim=c(0,5),ylim=c(0,20))</pre>&#13;
        <p class="indent">Then use <code>points</code>, <code>abline</code>, and <code>text</code> to build the top-left plot of&#13;
                <a href="ch22.xhtml#ch22fig5">Figure 22-5</a>, as follows:</p>&#13;
        <pre>R&gt; points(p1x,p1y,pch=15,cex=1.5)<br/>R&gt; abline(mod.0)<br/>R&gt;&#13;
            abline(mod.1,lty=2)<br/>R&gt; text(2,1,labels="Outlier, low leverage, low&#13;
            influence",cex=1.4)</pre>&#13;
        <p class="indent"><span epub:type="pagebreak" id="page_557"/>Create the middle and&#13;
            right plots by replacing <code>p1x</code>, <code>p1y</code>, and <code>mod.1</code> with those corresponding to&#13;
            points 2 and 3 and altering the <code>labels</code> argument in <code>text</code>.</p>&#13;
        <div class="image"><img src="../images/f22-05.jpg" alt="image"/></div>&#13;
        <p class="figt"><em><a id="ch22fig5"/>Figure 22-5: Three examples of the definitions of&#13;
                outlier, leverage, and influence in a linear regression context. In each plot, the&#13;
                solid line represents the model fitted to the original observations in</em>&#13;
            <code>x</code>&#13;
            <em>and</em>&#13;
            <code>y</code><em>, and the dashed line represents the model fitted&#13;
                including the extra point, plotted with</em>&#13;
            <span class="ent">▪</span>.</p>&#13;
        <p class="indent">In the top-left plot of <a href="ch22.xhtml#ch22fig5">Figure 22-5</a>, the&#13;
            additional point is an example of an outlier since it sits away from the bulk of the&#13;
            data <em>and</em> doesn’t conform to the trend suggested by the original&#13;
            observations. Despite this, it’s considered to have low leverage only because of&#13;
            its predictor value of 1.2 (<code>p1x</code>), which isn’t deemed&#13;
            unusual compared to that of the other values of <code>x</code>. In fact,&#13;
            its proximity to the overall mean of the <code>x</code> values indicates&#13;
            that the effect of including it, incorporated in <code>mod.1</code>, is&#13;
            mainly a modification to the original intercept. You could even classify this as a low&#13;
            influence point—the overall change to the fitted model seems minimal.</p>&#13;
        <p class="indent">In the top-right plot, you can see an example of an observation that would&#13;
                <em>not</em> be considered an outlier. Although point 2 does sit apart from the 10&#13;
            original observations, it conforms quite well to the model fitted to only <code>x</code> and <code>y</code>, which is important in&#13;
            the context of regression. That being said, <span epub:type="pagebreak" id="page_558"/>point 2 is considered a high leverage point since it sits at an extreme&#13;
            predictor value compared to all other values of <code>x</code> (in other&#13;
            words, it has the <em>potential</em> to dramatically alter the fit should its response&#13;
            value be different). As it stands, it’s a low influence point since the model fit&#13;
            itself is barely affected by its inclusion.</p>&#13;
        <p class="indent">Lastly, the bottom plot shows a clear example of an outlier in a&#13;
            high-leverage position that also has a high influence—it sits away from the&#13;
            original 10 observations and isn’t a clear part of the original trend; its extreme&#13;
            predictor value means high leverage; and its inclusion substantially alters the entire&#13;
            model by dragging down the slope and raising the intercept. These ideas remain the same&#13;
            in higher dimensions (that is, when you have several predictors) for multiple linear&#13;
            regression models.</p>&#13;
        <h4 class="h4" id="ch22lev2sec217"><strong><em>22.3.4 Calculating&#13;
            Leverage</em></strong></h4>&#13;
        <p class="noindent">Leverage itself is calculated using the design matrix structure&#13;
                    <strong><em>X</em></strong> defined in <a href="ch21.xhtml#ch21lev2sec191">Section 21.2.2</a>. Specifically, if you have <em>n</em> observations, then the&#13;
            leverage of the <em>i</em>th point (<em>i</em> = 1, . . . , <em>n</em>) is denoted&#13;
                <em>h</em><sub><em>ii</em></sub>. These are the diagonal elements (<em>i</em>th row,&#13;
                <em>i</em>th column) of the <em>n</em> × <em>n</em> matrix <em>H</em> such&#13;
            that</p>&#13;
        <div class="imagec"><a id="ch22eq4"/><img src="../images/e22-4.jpg" alt="image"/></div>&#13;
        <p class="indent">In R, constructing the design matrix for the 10 illustrative predictor&#13;
            observations you defined in <a href="ch22.xhtml#ch22lev2sec216">Section 22.3.3</a> as&#13;
                <code>x</code> is achieved in a straightforward fashion using&#13;
            knowledge of <code>cbind</code> (refer to <a href="ch03.xhtml#ch03lev2sec25">Section 3.1.2</a>). <em>H</em> is subsequently&#13;
            calculated using the corresponding functions for matrix multiplication (<code>%*%</code>), matrix transposition (<code>t</code>),&#13;
            matrix inversion (<code>solve</code>), and diagonal element extraction&#13;
                (<code>diag</code>). Then you can plot the values&#13;
                    <em>h</em><sub><em>ii</em></sub> against the values of <code>x</code> themselves. The following code produces <a href="ch22.xhtml#ch22fig6">Figure 22-6</a>:</p>&#13;
        <pre>R&gt; X &lt;- cbind(rep(1,10),x)<br/>R&gt; hii &lt;-&#13;
            diag(X%*%solve(t(X)%*%X)%*%t(X))<br/>R&gt; hii<br/> [1] 0.1033629 0.1012107&#13;
            0.3487221 0.1302663 0.1001345 0.3723971 0.1711595<br/> [8] 0.1980630 0.3261232&#13;
            0.1485607<br/>R&gt; plot(hii~x,ylab="Leverage",main="",pch=4)</pre>&#13;
        <p class="indent">You would typically use the built-in R function <code>hatvalues</code>, named after the style of the matrix algebra in <a href="ch22.xhtml#ch22eq4">Equation (22.4)</a>, to obtain the leverages (rather than&#13;
            manually constructing the design matrix <strong><em>X</em></strong> and doing the math&#13;
            yourself). Simply provide <code>hatvalues</code> with your fitted model&#13;
            object. You can confirm your earlier calculations by using the corresponding <code>lm</code> object fitted to the <code>x</code> and&#13;
                <code>y</code> data (<code>mod.0</code> created&#13;
            earlier).</p>&#13;
        <pre>R&gt;&#13;
            hatvalues(mod.0)<br/>        1         2         3         4         5         6         7<br/>0.1033629&#13;
            0.1012107 0.3487221 0.1302663 0.1001345 0.3723971&#13;
            0.1711595<br/>        8         9        10<br/>0.1980630&#13;
            0.3261232 0.1485607</pre>&#13;
        <div class="image"><span epub:type="pagebreak" id="page_559"/><img src="../images/f22-06.jpg" alt="image"/></div>&#13;
        <p class="figt"><em><a id="ch22fig6"/>Figure 22-6: Plotting the leverage of the 10&#13;
                illustrative predictor observations in</em>&#13;
            <code>x</code></p>&#13;
        <p class="indent">Looking at <a href="ch22.xhtml#ch22fig6">Figure 22-6</a>, the appearance&#13;
            of the leverages plotted against the corresponding predictor values themselves makes&#13;
            sense—leverage gets progressively higher as you move away from the mean of the&#13;
            predictor data in either direction. This is essentially the pattern you’ll see for&#13;
            any plot of the raw leverages.</p>&#13;
        <h4 class="h4" id="ch22lev2sec218"><strong><em>22.3.5 Cook’s&#13;
            Distance</em></strong></h4>&#13;
        <p class="noindent">Of course, leverage alone isn’t enough to determine the overall&#13;
            influence of each observation on a fitted model. For that, the response value must also&#13;
            be taken into account.</p>&#13;
        <p class="indent">Arguably the most well-known measure of influence is <em>Cook’s&#13;
                distance</em>, which estimates the magnitude of the effect of deleting the&#13;
                <em>i</em>th value from the fitted model. Cook’s distance for observation&#13;
                <em>i</em> (denoted <em>D</em><sub><em>i</em></sub>) is given with the following&#13;
            equation:</p>&#13;
        <div class="imagec"><a id="ch22eq5"/><img src="../images/e22-5.jpg" alt="image"/></div>&#13;
        <p class="indent">It turns out that this equation is a specific function of a point’s&#13;
            leverage and residual. Here, the value <em><span class="ent">ŷ</span></em><sub><em>j</em></sub> is the predicted mean response of&#13;
            observation <em>j</em> for the model fitted with all <em>n</em> observations, and <img class="middle" src="../images/f0560-01.jpg" alt="image"/> represents the predicted&#13;
            mean response of observation <em>j</em> for the model fitted <em>without</em> the&#13;
                <em>i</em>th observation. As usual, the term <em>p</em> is the number of predictor&#13;
            regression parameters (excluding the intercept), and the value <img class="middle" src="../images/o.jpg" alt="image"/> is the estimate of the residual standard&#13;
            error.</p>&#13;
        <p class="indent">Put simply, the larger the value of <em>D</em><sub><em>i</em></sub>, the&#13;
            larger the influence the <em>i</em>th observation has on the fitted model, meaning&#13;
            outlying observations in high-leverage positions will correspond to higher values of&#13;
                <em>D</em><sub><em>i</em></sub>. The important question is, how big does&#13;
                    <em>D</em><sub><em>i</em></sub> have to be in order for point <em>i</em> to be&#13;
            considered influential? In practice, this is difficult to universally answer, and&#13;
            there’s <span epub:type="pagebreak" id="page_560"/>no formal hypothesis test&#13;
            for it, but there are several rule-of-thumb cutoff values. One such rule states that if&#13;
                <em>D</em><sub><em>i</em></sub> &gt; 1, the point should be considered influential;&#13;
            another, more sensitive rule suggests <em>D</em><sub><em>i</em></sub> &gt; 4/<em>n</em>&#13;
            (see, for example, <a href="ref.xhtml#ref08">Bollen and Jackman, 1990</a>; <a href="ref.xhtml#ref12">Chatterjee et al., 2000</a>). It’s generally advised&#13;
            that you compare multiple Cook’s distances for a given fitted model rather than&#13;
            analyzing one single value, and that any point corresponding to a comparatively large&#13;
                <em>D</em><sub><em>i</em></sub> might need further inspection.</p>&#13;
        <p class="indent">Continue with the objects created in <a href="ch22.xhtml#ch22lev2sec216">Section 22.3.3</a>, with the 10 illustrative observations in <code>x</code> and <code>y</code>, as well as the additional point&#13;
            defined in <code>p1x</code> and <code>p1y</code>. The&#13;
            linear regression model fitted to those data was stored as the object <code>mod.1</code>. It’s a good exercise to write some code that&#13;
            calculates the Cook’s distance measures following (22.5).</p>&#13;
        <p class="indent">To that end, enter the following code in the R editor:</p>&#13;
        <pre>x1 &lt;- c(x,p1x)<br/>y1 &lt;- c(y,p1y)<br/>n &lt;-&#13;
            length(x1)<br/>param &lt;- length(coef(mod.1))<br/>yhat.full &lt;-&#13;
            fitted(mod.1)<br/>sigma &lt;- summary(mod.1)$sigma<br/>cooks &lt;-&#13;
            rep(NA,n)<br/>for(i in 1:n){<br/>    temp.y &lt;-&#13;
            y1[-i]<br/>    temp.x &lt;-&#13;
            x1[-i]<br/>    temp.model &lt;-&#13;
            lm(temp.y~temp.x)<br/>    temp.fitted &lt;-&#13;
            predict(temp.model,newdata=data.frame(temp.x=x1))<br/>    cooks[i]&#13;
            &lt;- sum((yhat.full-temp.fitted)^2)/(param*sigma^2)<br/>}</pre>&#13;
        <p class="indent">First, create new objects <code>x1</code> and <code>y1</code> to hold all 11 observations. The objects <code>n</code>, <code>param</code>, and <code>sigma</code> extract the data set size, the total number of&#13;
            estimated regression parameters (two in this case), and the estimated residual standard&#13;
            error for the model originally fitted to all 11 data points. The latter two items, <code>param</code> and <code>sigma</code>, represent&#13;
                (<em>p</em> + 1) and <img class="middle" src="../images/o.jpg" alt="image"/> in <a href="ch22.xhtml#ch22eq5">Equation (22.5)</a>, respectively. The object <code>yhat.full</code> uses the <code>fitted</code>&#13;
            function on the object <code>mod.1</code> to provide the fitted mean&#13;
            response values, representing the <em><span class="ent">ŷ</span></em><sub><em>j</em></sub> values in (22.5).</p>&#13;
        <p class="indent">To store the Cook’s distances, a vector <code>cooks</code> of 11 positions is created (initialized to be filled with <code>NA</code>s) with <code>rep</code>. Now, to calculate&#13;
            each <em>D</em><sub><em>i</em></sub> value, set a <code>for</code> loop&#13;
            (see <a href="ch10.xhtml#ch10">Chapter 10</a>) to scroll through each index from <code>1</code> to <code>11</code>. The first step of the&#13;
            loop is to create two temporary vectors <code>temp.x</code> and <code>temp.y</code> to be <code>x1</code> and <code>y1</code> with the observation at index <code>i</code> removed. A new temporary linear model is fitted to <code>temp.y</code> based on <code>temp.x</code>; then <code>predict</code> finds the mean responses from <code>temp.model</code> for each of the 11 predictor values (in other words, including&#13;
            the one that was deleted). As such, the resulting vector <code>temp.fitted</code> represents the <img class="middle" src="../images/f0560-01.jpg" alt="image"/> values in <a href="ch22.xhtml#ch22eq5">Equation (22.5)</a>. Finally,&#13;
                <code>sum</code> and the product of <code>param</code> with <code>sigma^2</code> compute&#13;
                    <em>D</em><sub><em>i</em></sub>, and the result is stored in <code>cooks[i]</code>.</p>&#13;
        <p class="indent"><span epub:type="pagebreak" id="page_561"/>After highlighting and&#13;
            executing the code, the resulting Cook’s distances are as follows:</p>&#13;
        <pre>R&gt; cooks<br/> [1] 2.425993e-03 4.060891e-07 1.027322e-01&#13;
            1.844150e-03 2.923667e-03<br/> [6] 7.213229e-02 1.387284e-01 3.021075e-02&#13;
            7.099904e-03 1.251882e-01<br/>[11] 3.136855e-01</pre>&#13;
        <p class="indent">Unsurprisingly, the largest value of these is the last one, at around&#13;
            0.314. This corresponds to the 11th observation in <code>x1</code> and&#13;
                <code>y1</code>, which is the additional point originally defined in&#13;
                <code>p1x</code> and <code>p1y</code>. The value&#13;
            0.314 is less than 1 and less than 4/11 = 0.364, the cutoffs defined by the earlier&#13;
            rules of thumb. This ties in with the assessment of the top-left image in <a href="ch22.xhtml#ch22fig5">Figure 22-5</a>—that the influence of the point&#13;
                <code>p1x</code> and <code>p1y</code> is minimal&#13;
            when compared to the influence of a point like <code>p3x</code> and&#13;
                <code>p3y</code> in the rightmost image.</p>&#13;
        <p class="indent">Just as the <code>hatvalues</code> function computes the&#13;
            leverages for you, the builtin <code>cooks.distance</code> function does&#13;
            the same for the <em>D</em><sub><em>i</em></sub>. You can confirm the previous values in&#13;
                <code>cooks</code>, which are based on <code>mod.1</code>.</p>&#13;
        <pre>R&gt;&#13;
            cooks.distance(mod.1)<br/>           1            2            3            4            5            6<br/>2.425993e-03&#13;
            4.060891e-07 1.027322e-01 1.844150e-03 2.923667e-03&#13;
            7.213229e-02<br/>           7            8            9           10           11<br/>1.387284e-01&#13;
            3.021075e-02 7.099904e-03 1.251882e-01 3.136855e-01</pre>&#13;
        <p class="indent">R automatically calculates and provides Cook’s distances as a&#13;
            diagnostic plot of a fitted linear model object when you select <code>which=4</code> in the relevant usage of <code>plot</code>. The&#13;
            following code uses <code>mod.1</code>, <code>mod.2</code>, and <code>mod.3</code> from earlier to produce the&#13;
            three images in <a href="ch22.xhtml#ch22fig7">Figure 22-7</a>; these correspond to the&#13;
            three data sets in <a href="ch22.xhtml#ch22fig5">Figure 22-5</a>.</p>&#13;
        <pre>R&gt; plot(mod.1,which=4)<br/>R&gt; plot(mod.2,which=4)<br/>R&gt;&#13;
            plot(mod.3,which=4)<br/>R&gt; abline(h=c(1,4/n),lty=2)</pre>&#13;
        <p class="indent">The <em>D</em><sub><em>i</em></sub> displayed on the top left of <a href="ch22.xhtml#ch22fig7">Figure 22-7</a> match the values you manually calculated&#13;
            earlier, stored in <code>cooks</code>. The influences of all the data&#13;
            points in the top-right plot remain relatively small, which reflects what you can see in&#13;
            the top-right plot of <a href="ch22.xhtml#ch22fig5">Figure 22-5</a>, where the&#13;
            additional point (<code>p2x</code>, <code>p2y</code>)&#13;
            doesn’t greatly affect the overall fit. In the bottom plot, <code>abline</code> superimposes two horizontal lines marking off the values 1 (highest&#13;
            line) and 4/11 = 0.364, both of which are clearly breached by the 11th point (<code>p3x</code>, <code>p3y</code>), just as you’d&#13;
            expect given the corresponding bottom image in <a href="ch22.xhtml#ch22fig5">Figure&#13;
                22-5</a>.</p>&#13;
        <div class="image"><span epub:type="pagebreak" id="page_562"/><img src="../images/f22-07.jpg" alt="image"/></div>&#13;
        <p class="figt"><em><a id="ch22fig7"/>Figure 22-7: Three illustrative examples of the&#13;
                Cook’s distance plots produced in R, based on</em>&#13;
            <code>mod.1</code>&#13;
            <em>(top left),</em>&#13;
            <code>mod.2</code>&#13;
            <em>(top right), and</em>&#13;
            <code>mod.3</code>&#13;
            <em>(bottom)</em></p>&#13;
        <p class="indent">Turn your attention back to the <code>car.step</code>&#13;
            model, where you modeled MPG using the <code>mtcars</code> data set,&#13;
            with the final fit achieved using stepwise AIC selection in <a href="ch22.xhtml#ch22lev2sec212">Section 22.2.4</a>. You’ve already looked at&#13;
            the residuals versus fitted values and QQ plot in <a href="ch22.xhtml#ch22fig2">Figures&#13;
                22-2</a> and <a href="ch22.xhtml#ch22fig4">22-4</a>. <a href="ch22.xhtml#ch22fig8">Figure 22-8</a> provides a plot of Cook’s distances for the model with these&#13;
            two lines:</p>&#13;
        <pre>R&gt; plot(car.step,which=4)<br/>R&gt;&#13;
            abline(h=4/nrow(mtcars),lty=2)</pre>&#13;
        <p class="indent">The plot labels the three points with the highest&#13;
                    <em>D</em><sub><em>i</em></sub> by default; two of these breach the 4/<em>n</em>&#13;
            = 4/32 = 0.125 mark. In light of the fitted model based on various effects of car weight&#13;
                (<code>wt</code>), horsepower (<code>hp</code>), and&#13;
            quarter-mile time (<code>qsec</code>), the Chrysler Imperial and Toyota&#13;
            Corolla are deemed to be in high-leverage positions with residuals large enough to be&#13;
            designated as highly influential. It should also be noted that the Fiat 128, though it&#13;
            doesn’t quite breach the 0.125 line, is still rather influential and was in fact&#13;
            also one of the extreme labeled points in the residual plots (<a href="ch22.xhtml#ch22fig2">Figure 22-2</a>) and the QQ plot (<a href="ch22.xhtml#ch22fig4">Figure 22-4</a>).</p>&#13;
        <div class="image"><span epub:type="pagebreak" id="page_563"/><img src="../images/f22-08.jpg" alt="image"/></div>&#13;
        <p class="figt"><em><a id="ch22fig8"/>Figure 22-8: Cook’s distances for the model&#13;
                in</em>&#13;
            <code>car.step</code><em>; a dashed horizontal line marks off 4/</em>n&#13;
                <em>for the</em>&#13;
            <code>mtcars</code>&#13;
            <em>data frame</em></p>&#13;
        <p class="indent">This could reasonably suggest investigating these highly influential&#13;
            observations further. Was everything recorded correctly? Has your model been selected&#13;
            carefully? Are there alternative options for the model, such as additional predictor&#13;
            terms or transformations? You could explore these options and continue to monitor a plot&#13;
            of the Cook’s distances (along with the other diagnostics).</p>&#13;
        <p class="indent">Whatever your result, the presence of influential points doesn’t&#13;
            necessarily mean there is a serious problem with your model—this is more a tool to&#13;
            help you detect observations that are extreme in terms of their specific combination of&#13;
            the predictor values <em>and</em> that have a larger residual, suggesting their response&#13;
            value sits away from the trends predicted by the model itself. This is especially useful&#13;
            in multiple regression, when high dimensionality of the response-predictor data can make&#13;
            conventional visualization of the raw data in a single plot difficult.</p>&#13;
        <h4 class="h4" id="ch22lev2sec219"><strong><em>22.3.6 Graphically Combining Residuals,&#13;
                    Leverage, and Cook’s Distance</em></strong></h4>&#13;
        <p class="noindent">The last two diagnostic plots from <code>plot</code>&#13;
            combine the standardized residual, the leverage, and the Cook’s distance for the&#13;
                <em>i</em>th observation. These combination plots are especially useful in allowing&#13;
            you to see whether it is the high leverage or large residual, or both, that contributes&#13;
            more to a high influence observation.</p>&#13;
        <p class="indent">Using the data models <code>mod.1</code>, <code>mod.2</code>, and <code>mod.3</code>, enter the&#13;
            following code with <code>which=5</code> to produce the three images in&#13;
            the left column of <a href="ch22.xhtml#ch22fig9">Figure 22-9</a>:</p>&#13;
        <pre>R&gt;&#13;
            plot(mod.1,which=5,add.smooth=FALSE,cook.levels=c(4/11,0.5,1))<br/>R&gt;&#13;
            plot(mod.2,which=5,add.smooth=FALSE,cook.levels=c(4/11,0.5,1))<br/>R&gt;&#13;
            plot(mod.3,which=5,add.smooth=FALSE,cook.levels=c(4/11,0.5,1))</pre>&#13;
        <div class="image"><span epub:type="pagebreak" id="page_564"/><img src="../images/f22-09.jpg" alt="image"/></div>&#13;
        <p class="figt"><em><a id="ch22fig9"/>Figure 22-9: Combination diagnostic plot of&#13;
                standardized residuals against leverage (left column) and Cook’s distance&#13;
                against leverage (right column) for the three illustrative models</em>&#13;
            <code>mod.1</code>&#13;
            <em>(top),</em>&#13;
            <code>mod.2</code>&#13;
            <em>(middle), and</em>&#13;
            <code>mod.3</code>&#13;
            <em>(bottom)</em></p>&#13;
        <p class="indent"><span epub:type="pagebreak" id="page_565"/>These plots show leverage&#13;
            on the horizontal axis and the standardized residuals on the vertical axis for each&#13;
            observation. As a function of residual and leverage, the Cook’s distances can be&#13;
            plotted as <em>contours</em> on each of these scatterplots. These contours delineate the&#13;
            spatial areas of the plots that correspond to high influence (in the right extreme&#13;
            corners).</p>&#13;
        <p class="indent">The closer a point falls to the horizontal line at zero, the smaller its&#13;
            residual. A point that lies more left than right has a smaller leverage. If a point lies&#13;
            far enough from the horizontal line given its leverage (<em>x</em>-axis) position, it&#13;
            will breach the contours marking off certain values of <em>D</em><sub><em>i</em></sub>&#13;
            (defaulting to just 0.5 and 1), indicating a high influence. Indeed, you can see by the&#13;
            narrowing of the contours as you move from left to right on these plots that&#13;
            classification as a high-influence point is easier if a given observation is in a&#13;
            high-leverage position, which makes perfect sense. In the previous calls to <code>plot</code> with <code>which=5</code>, the optional&#13;
                <code>cook.levels</code> argument is used to include a contour for&#13;
            the rule-of-thumb value of 4/11 for these three examples.</p>&#13;
        <p class="indent">The plot for <code>mod.1</code> shows that the added point&#13;
                (<code>p1x</code>, <code>p1y</code>) has a large&#13;
            residual, but it doesn’t breach the 4/11 contour because it’s in a&#13;
            low-leverage position. The plot for <code>mod.2</code> shows that the&#13;
            added point (<code>p2x</code>,<code>p2y</code>) is in a&#13;
            high-leverage position but isn’t influential because its residual is small.&#13;
            Lastly, the plot for <code>mod.3</code> clearly identifies the added&#13;
            point (<code>p3x</code>, <code>p3y</code>) as highly&#13;
            influential—with a large residual and high leverage, it’s in clear breach of&#13;
            the high-level contours. Looking back at all the previous plots of these three&#13;
            illustrative data sets, it’s easy to note that these three plots clearly reflect&#13;
            the nature of each of the individually added extra observations.</p>&#13;
        <p class="indent">The final diagnostic plot, using <code>which=6</code>,&#13;
            displays the same information as the <code>which=5</code> combination&#13;
            diagnostic, but this time the vertical axis displays Cook’s distance, and the&#13;
            horizontal axis displays a transformation of the leverage, namely,&#13;
                    <em>h</em><sub><em>ii</em></sub>/(1 − <em>h</em><sub><em>ii</em></sub> ).&#13;
            This transformation amplifies larger leverage points in terms of their horizontal&#13;
            position, an effect that, in part, indirectly displays itself as a&#13;
            “stretched-out” scale on the <em>x</em>-axis—useful if you’re&#13;
            particularly interested in the extremity of the observations with respect to the&#13;
            collection of predictor variables.</p>&#13;
        <p class="indent">As such, the contours now define standardized residuals as a function of&#13;
            the scaled leverage and Cook’s distance. The following three lines produce the&#13;
            three images in the rightmost column of <a href="ch22.xhtml#ch22fig9">Figure&#13;
            22-9</a>:</p>&#13;
        <pre>R&gt; plot(mod.1,which=6,add.smooth=FALSE)<br/>R&gt;&#13;
            plot(mod.2,which=6,add.smooth=FALSE)<br/>R&gt; plot(mod.3,which=6,add.smooth=FALSE)</pre>&#13;
        <p class="indent">Points positioned further to the right are high-leverage points; points&#13;
            positioned higher up are high-influence points. Looking down the right column of plots&#13;
            in <a href="ch22.xhtml#ch22fig9">Figure 22-9</a>, you can see that the three additional&#13;
            points are found where you would expect them to be, according to their characteristics&#13;
            in <code>mod.1</code>, <code>mod.2</code>, and <code>mod.3</code>.</p>&#13;
        <p class="indent"><span epub:type="pagebreak" id="page_566"/>For a real-data example,&#13;
            return to the model stored in <code>car.step</code>. Enter the following&#13;
            code to produce the two combination diagnostic plots in <a href="ch22.xhtml#ch22fig10">Figure 22-10</a>:</p>&#13;
        <pre>R&gt;&#13;
            plot(car.step,which=5,cook.levels=c(4/nrow(mtcars),0.5,1))<br/>R&gt;&#13;
            plot(car.step,which=6,cook.levels=c(4/nrow(mtcars),0.5,1))</pre>&#13;
        <div class="image"><img src="../images/f22-10.jpg" alt="image"/></div>&#13;
        <p class="figt"><em><a id="ch22fig10"/>Figure 22-10: Combination diagnostic plot of&#13;
                standardized residuals against leverage (left) and Cook’s distance against&#13;
                leverage (right) for the</em>&#13;
            <code>car.step</code>&#13;
            <em>model</em></p>&#13;
        <p class="indent">The two images in <a href="ch22.xhtml#ch22fig10">Figure 22-10</a> show the&#13;
            Corolla and the Imperial as influential observations with&#13;
                <em>D</em><sub><em>i</em></sub> values greater than the <code>4/nrow(mtcars)</code> rule-of-thumb cutoff. Interestingly, this plot reveals that&#13;
            the Imperial (which was shown to have the largest <em>D</em><sub><em>i</em></sub> by far&#13;
            in <a href="ch22.xhtml#ch22fig8">Figure 22-8</a>) actually has a smaller residual than&#13;
            both the Corolla and the Fiat 128. Its high influence is clearly because of its&#13;
            high-leverage position with respect to the predictor values of the variables present in&#13;
                <code>car.step</code>. The Fiat 128, on the other hand, has one of&#13;
            the largest residuals in the entire data set (which is why it was flagged in some of the&#13;
            earlier diagnostic plots) but just misses out on being labeled a high-influence&#13;
            observation because of its relatively low-leverage position (based purely on the&#13;
            rule-of-thumb cutoff).</p>&#13;
        <p class="indent">Any linear regression model will have observations that influence the&#13;
            model more than others, and these plots aim to help you identify them. But deciding what&#13;
            to actually do with highly influential observations can be difficult and is application&#13;
            specific. Although it’s not ideal to have a single observation exerting heavy&#13;
            influence over the final estimated model, it’s also extremely unwise to remove&#13;
            these observations without careful thought since they might be pointing to other issues,&#13;
            such as deficiencies in your current fit or previously undetected trends.</p>&#13;
        <div class="ex">&#13;
            <p class="ext"><span epub:type="pagebreak" id="page_567"/><a id="ch22exc2"/><strong>Exercise 22.2</strong></p>&#13;
            <p class="noindentz">In <a href="ch22.xhtml#ch22lev2sec210">Section 22.2.2</a>, you used&#13;
                the <code>nuclear</code> data frame in the <code>boot</code> package to illustrate forward selection, where a model was selected&#13;
                for <code>cost</code> as a function of main effects of <code>date</code>, <code>cap</code>, <code>pt</code>, and <code>ne</code>.</p>&#13;
            <ol type="a">&#13;
                <li><p class="noindents">Access the data frame; fit and summarize the model&#13;
                        described earlier.</p></li>&#13;
                <li><p class="noindents">Inspect the raw residuals versus fitted values and a normal&#13;
                        QQ plot of the residuals and comment on your interpretations—do the&#13;
                        assumptions underpinning the error component of the linear model appear&#13;
                        satisfied in this case?</p></li>&#13;
                <li><p class="noindents">Determine the rule-of-thumb cutoff for influential&#13;
                        observations based on the Cook’s distances. Produce a plot of the&#13;
                        Cook’s distances and add a horizontal line corresponding to the&#13;
                        cutoff. Comment on your findings.</p></li>&#13;
                <li><p class="noindents">Produce a combination diagnostic plot of the standardized&#13;
                        residuals against leverage. Set the Cook’s distance contours to&#13;
                        include the cutoff value from (c) as well as the default contours. Interpret&#13;
                        the plot—how are any individually influential points&#13;
                        characterized?</p></li>&#13;
                <li><p class="noindents">Based on (c) and (d), you should be able to identify the&#13;
                        record in <code>nuclear</code> exerting the largest&#13;
                        influence on the fitted model. For the sake of argument, let’s assume&#13;
                        the observation was recorded incorrectly. Refit the model from (a), this&#13;
                        time omitting the offending row from the data frame. Summarize the&#13;
                        model—which coefficients have changed the most? Produce the diagnostic&#13;
                        plots from (b) for the new model and compare them to the ones from&#13;
                        earlier.</p></li>&#13;
            </ol>&#13;
            <p class="noindentz">Load the <code>faraway</code> package and access&#13;
                the <code>diabetes</code> data frame. In <a href="ch22.xhtml#ch22exc1">Exercise 22.1</a> (g), you used stepwise AIC&#13;
                selection to choose a model for <code>chol</code>.</p>&#13;
            <ol type="a" start="6">&#13;
                <li><p class="noindents">Using <code>diabetes</code>, fit the&#13;
                        multiple linear model identified in the earlier exercise, that is, with main&#13;
                        effects and a two-way interaction between <code>age</code>&#13;
                        and <code>frame</code> and a main effect for <code>waist</code>. By summarizing the fit, determine the&#13;
                        number of records that contained missing values in <code>diabetes</code> that were deleted from the estimation.</p></li>&#13;
                <li><p class="noindents">Produce the raw residuals versus fitted and QQ diagnostic&#13;
                        plots for the model in (f). Comment on the validity of the error&#13;
                        assumptions.</p></li>&#13;
                <li><p class="noindents"><span epub:type="pagebreak" id="page_568"/>Investigate influential points. Make use of the familiar&#13;
                        rule-of-thumb cutoff (note you’ll need to subtract the number of&#13;
                        missing values from the total size of the data frame to get the effective&#13;
                        sample size for your model). In the combination plot of the standardized&#13;
                        residuals against leverage, use one, three, and five times the cutoff as the&#13;
                        Cook’s distance contours.</p></li>&#13;
            </ol>&#13;
            <p class="noindentz">Recall the discussion of reading in web-based files in <a href="ch08.xhtml#ch08lev2sec75">Section 8.2.3</a>. There, you called in a data&#13;
                frame containing data on the prices of 308 diamonds (in Singapore dollars), as well&#13;
                as weight (in carats—continuous), color (categorical—six levels from&#13;
                    <code>D</code>, the least yellow and the reference level, to&#13;
                    <code>I</code>, the most yellow), clarity&#13;
                (categorical—five levels with <code>IF</code>, essentially&#13;
                flawless and the reference level, <code>VVS1</code>, <code>VVS2</code>, <code>VS1</code>, and <code>VS2</code>, with the last being the least clear), and&#13;
                certification (categorical—three levels for different diamond certification&#13;
                bodies with levels <code>GIA</code> as the reference, <code>HRD</code> and <code>IGI</code>). Seek out the&#13;
                freely available article by Chu (<a href="ref.xhtml#ref13">2001</a>) for more&#13;
                information on these data. With an Internet connection, run the following lines,&#13;
                which will read in the data as the object <code>diamonds</code> and&#13;
                name each variable column appropriately.</p>&#13;
            <pre>R&gt; dia.url &lt;-&#13;
                "http://www.amstat.org/publications/jse/v9n2/4cdata.txt"<br/>R&gt; diamonds &lt;-&#13;
                read.table(dia.url)<br/>R&gt; names(diamonds) &lt;-&#13;
                c("Carat","Color","Clarity","Cert","Price")</pre>&#13;
            <ol type="a" start="9">&#13;
                <li><p class="noindents">Using either base R graphics or <code>ggplot2</code>, to get a feel for the data, produce a scatterplot of&#13;
                        the price on the <em>y</em>-axis and carat weight on the <em>x</em>-axis.&#13;
                        Experiment with using plotting color to split the points according to the&#13;
                        following:</p>&#13;
                    <p class="dash">– Diamond clarity</p>&#13;
                    <p class="dash">– Diamond color</p>&#13;
                    <p class="dash">– Diamond certification</p></li>&#13;
                <li><p class="noindents">Fit a multiple linear model with <code>Price</code> as the response and main effects for the other variables&#13;
                        as the predictors. Summarize the model and produce the three diagnostic&#13;
                        plots that tell you about the assumptions surrounding the error term.&#13;
                        Comment on the plots—are you satisfied that this is an appropriate&#13;
                        model for the diamond prices? Why or why not?</p></li>&#13;
                <li><p class="noindents">Repeat (j) but use the log transformation of <code>Price</code>. Again, inspect and comment on the validity&#13;
                        of the error assumptions.</p></li>&#13;
                <li><p class="noindents">Repeat (k), but in modeling the log-price, this time&#13;
                        include an additional quadratic term for <code>Carat</code>&#13;
                        (refer to <a href="ch21.xhtml#ch21lev2sec199">Section 21.4.1</a> for details&#13;
                        on polynomial transformations). How do the residual diagnostics look&#13;
                        now?</p></li>&#13;
            </ol>&#13;
        </div>&#13;
        <h3 class="h3" id="ch22lev1sec75"><span epub:type="pagebreak" id="page_569"/><strong>22.4 Collinearity</strong></h3>&#13;
        <p class="noindent">This final aspect of fitting regression models isn’t technically&#13;
            classified as a diagnostic check but still has substantial potential to adversely affect&#13;
            the validity of any conclusions you draw from a fitted model and occurs frequently&#13;
            enough to warrant discussion here. <em>Collinearity</em> (also referred to as&#13;
                <em>multicollinearity</em>) is when two or more of the explanatory variables are&#13;
            highly correlated with each other.</p>&#13;
        <h4 class="h4" id="ch22lev2sec220"><strong><em>22.4.1 Potential Warning&#13;
            Signs</em></strong></h4>&#13;
        <p class="noindent">High correlation between two predictors implies there will be some level&#13;
            of redundancy in terms of the information they contain when it comes to the response&#13;
            variable. It’s a problem since it can destabilize the ability to reliably fit a&#13;
            model and, as noted earlier, therefore be detrimental to any subsequent model-based&#13;
            inference.</p>&#13;
        <p class="indent">The following items serve as potential warnings of collinearity when&#13;
            you’re inspecting a model summary:</p>&#13;
        <p class="bull">• The omnibus <em>F</em>-test (<a href="ch21.xhtml#ch21lev2sec197">Section 21.3.5</a>) result is statistically significant, but none of the individual&#13;
                <em>t</em>-test results for the regression parameters are significant.</p>&#13;
        <p class="bull">• The sign of a given coefficient estimate contradicts what you would&#13;
            reasonably expect to see, for example, drinking more wine resulting in a lower blood&#13;
            alcohol level.</p>&#13;
        <p class="bull">• Parameter estimates are associated with unusually high standard&#13;
            errors or vary wildly when the model is fitted to different random record subsets of the&#13;
            data.</p>&#13;
        <p class="indent">As the last point notes, collinearity tends to have more of a detrimental&#13;
            effect on the standard errors of the coefficients (and associated outcomes such as&#13;
            confidence intervals, significance tests, and prediction intervals) than it does on&#13;
            point predictions per se. In most cases, you can avoid collinearity simply by being&#13;
            careful. Be aware of the variables present and how the data have been collected. For&#13;
            example, ensure any given predictors you intend to include in the model don’t just&#13;
            represent a rescaled value of another included predictor. It’s also advisable to&#13;
            perform an exploratory analysis of your data, producing summary statistics and basic&#13;
            statistical plots. You can tabulate counts between categorical variables or look at&#13;
            estimated correlation coefficients between continuous variables, for example. In the&#13;
            latter case, as a rough guide, some statisticians suggest that a correlation of 0.8 or&#13;
            more could lead to potential problems.</p>&#13;
        <h4 class="h4" id="ch22lev2sec221"><strong><em>22.4.2 Correlated Predictors: A Quick&#13;
                    Example</em></strong></h4>&#13;
        <p class="noindent">Consider the <code>survey</code> data of the statistics&#13;
            students again, located in the <code>MASS</code> package. In most models&#13;
            you’ve looked at for these data, you’ve attempted to predict student height&#13;
            from certain explanatory variables, often including <span epub:type="pagebreak" id="page_570"/>the handspan of the writing hand (<code>Wr.Hnd</code>). The help page <code>?survey</code> shows that data&#13;
            have also been collected on the nonwriting handspan (<code>NW.Hnd</code>). It’s reasonable to expect that these two variables will be highly&#13;
            correlated, which is precisely why I’ve avoided any use of <code>NW.Hnd</code> previously. Indeed, executing</p>&#13;
        <pre>R&gt; cor(survey$Wr.Hnd,survey$NW.Hnd,use="complete.obs")<br/>[1]&#13;
            0.9483103</pre>&#13;
        <p class="noindent">reveals a high correlation coefficient, suggesting a strong positive&#13;
            linear association between the writing and nonwriting handspans of the students. In&#13;
            other words, these two variables should represent much the same information in any given&#13;
            model.</p>&#13;
        <p class="indent">Now, you know from previously fitted models that writing handspan has a&#13;
            significant and positive impact on predicting mean student height. The following code&#13;
            quickly confirms this via a simple linear regression.</p>&#13;
        <pre>R&gt;&#13;
            summary(lm(Height~Wr.Hnd,data=survey))<br/><br/>Call:<br/>lm(formula = Height ~&#13;
            Wr.Hnd, data =&#13;
            survey)<br/><br/>Residuals:<br/>     Min      1Q    Median      3Q       Max<br/>-19.7276&#13;
            -5.0706   -0.8269  4.9473   25.8704<br/><br/>Coefficients:<br/>              Estimate&#13;
            Std. Error t value Pr(&gt;|t|)<br/>(Intercept)&#13;
            113.9536       5.4416   20.94   &lt;2e-16&#13;
            ***<br/>Wr.Hnd        3.1166       0.2888   10.79   &lt;2e-16&#13;
            ***<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' '&#13;
            1<br/><br/>Residual standard error: 7.909 on 206 degrees of&#13;
            freedom<br/>  (29 observations deleted due to missingness)<br/>Multiple&#13;
            R-squared: 0.3612,         Adjusted&#13;
            R-squared: 0.3581<br/>F-statistic: 116.5 on 1 and 206 DF,  p-value: &lt;&#13;
            2.2e-16</pre>&#13;
        <p class="indent">The high positive correlation between <code>Wr.Hnd</code>&#13;
            and <code>NW.Hnd</code> suggests that using <code>NW.Hnd</code> instead should have a similar effect.</p>&#13;
        <pre>R&gt;&#13;
            summary(lm(Height~NW.Hnd,data=survey))<br/><br/>Call:<br/>lm(formula = Height ~&#13;
            NW.Hnd, data =&#13;
                survey)<br/><br/>Residuals:<br/>     Min       1Q   Median      3Q       Max<br/>-21.8285  -5.1397  -0.2867  4.5611   25.5750<br/><span epub:type="pagebreak" id="page_571"/>Coefficients:<br/>             Estimate&#13;
            Std. Error t value Pr(&gt;|t|)<br/>(Intercept)&#13;
            118.0324      5.2912   22.31   &lt;2e-16&#13;
            ***<br/>NW.Hnd        2.9107      0.2818   10.33   &lt;2e-16&#13;
            ***<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' '&#13;
            1<br/><br/>Residual standard error: 8.032 on 206 degrees of&#13;
            freedom<br/>   (29 observations deleted due to&#13;
            missingness)<br/>Multiple R-squared:&#13;
            0.3412,          Adjusted R-squared:&#13;
            0.338<br/>F-statistic: 106.7 on 1 and 206 DF,   p-value: &lt;&#13;
            2.2e-16</pre>&#13;
        <p class="indent">You can see from these results that this is certainly the case.</p>&#13;
        <p class="indent">Look, however, at what happens if you try to model height based on both of&#13;
            these predictors at the same time:</p>&#13;
        <pre>R&gt;&#13;
            summary(lm(Height~Wr.Hnd+NW.Hnd,data=survey))<br/><br/>Call:<br/>lm(formula = Height&#13;
            ~ Wr.Hnd + NW.Hnd, data =&#13;
            survey)<br/><br/>Residuals:<br/>     Min       1Q  Median        3Q      Max<br/>-20.0144  -5.0533&#13;
            -0.8558    4.7486  25.8380<br/><br/>Coefficients:<br/>            Estimate&#13;
            Std. Error t value Pr(&gt;|t|)<br/>(Intercept)&#13;
            113.9962     5.4545 20.900 &lt;2e-16&#13;
            ***<br/>Wr.Hnd        2.7451     1.0728&#13;
            2.559 0.0112&#13;
            *<br/>NW.Hnd        0.3707     1.0309&#13;
            0.360 0.7195<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05&#13;
            '.' 0.1 ' ' 1<br/><br/>Residual standard error: 7.926 on 205 degrees of&#13;
            freedom<br/>  (29 observations deleted due to missingness)<br/>Multiple&#13;
            R-squared: 0.3616,         Adjusted&#13;
            R-squared: 0.3554<br/>F-statistic: 58.06 on 2 and 205 DF,  p-value: &lt;&#13;
            2.2e-16</pre>&#13;
        <p class="indent">Since the effects of <code>Wr.Hnd</code> and <code>NW.Hnd</code> on <code>Height</code> are&#13;
            intermingled with one another, including both at the same time heavily masks any&#13;
            individual contribution to modeling the response. Statistical significance of the&#13;
            predictors is almost nonexistent; at the least, the effects are both associated with&#13;
            much, much higher <em>p</em>-values than in the individual single-predictor fits. That&#13;
            said, the omnibus <em>F</em>-test remains highly significant, giving an example of the&#13;
            first warning sign noted previously.</p>&#13;
        <h5 class="h5" id="ch22lev3sec104"><span epub:type="pagebreak" id="page_572"/><strong>Important Code in This Chapter</strong></h5>&#13;
        <table class="topbot">&#13;
            <thead>&#13;
                <tr>&#13;
                    <td style="vertical-align: top;" class="table2r"><p class="table"><strong>Function/operator</strong></p></td>&#13;
                    <td style="vertical-align: top;" class="table2r"><p class="table"><strong>Brief&#13;
                                description</strong></p></td>&#13;
                    <td style="vertical-align: top;" class="table2r"><p class="table"><strong>First&#13;
                                occurrence</strong></p></td>&#13;
                </tr>&#13;
            </thead>&#13;
            <tbody>&#13;
                <tr>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><code>anova</code></p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table">Partial&#13;
                                <em>F</em> -tests</p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><a href="ch22.xhtml#ch22lev2sec209">Section 22.2.1</a>, <a href="ch22.xhtml#page_531">p. 531</a></p></td>&#13;
                </tr>&#13;
                <tr>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><code>add1</code></p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table">Review&#13;
                            single-term additions</p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><a href="ch22.xhtml#ch22lev2sec210">Section 22.2.2</a>, <a href="ch22.xhtml#page_533">p. 533</a></p></td>&#13;
                </tr>&#13;
                <tr>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><code>update</code></p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table">Make changes to&#13;
                            fitted model</p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><a href="ch22.xhtml#ch22lev2sec210">Section 22.2.2</a>, <a href="ch22.xhtml#page_534">p. 534</a></p></td>&#13;
                </tr>&#13;
                <tr>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><code>drop1</code></p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table">Review&#13;
                            single-term deletions</p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><a href="ch22.xhtml#ch22lev2sec211">Section 22.2.3</a>, <a href="ch22.xhtml#page_538">p. 538</a></p></td>&#13;
                </tr>&#13;
                <tr>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><code>step</code></p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table">Stepwise AIC&#13;
                            model selection</p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><a href="ch22.xhtml#ch22lev2sec212">Section 22.2.4</a>, <a href="ch22.xhtml#page_543">p. 543</a></p></td>&#13;
                </tr>&#13;
                <tr>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><code>plot</code> (used on <code>lm</code>&#13;
                            object)</p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table">Model&#13;
                            diagnostics</p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><a href="ch22.xhtml#ch22lev2sec214">Section 22.3.1</a>, <a href="ch22.xhtml#page_551">p. 551</a></p></td>&#13;
                </tr>&#13;
                <tr>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><code>rstandard</code></p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table">Extract&#13;
                            standardized residuals</p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><a href="ch22.xhtml#ch22lev2sec215">Section 22.3.2</a>, <a href="ch22.xhtml#page_555">p. 555</a></p></td>&#13;
                </tr>&#13;
                <tr>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><code>shapiro.test</code></p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table">Shapiro-Wilk&#13;
                            test of normality</p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><a href="ch22.xhtml#ch22lev2sec215">Section 22.3.2</a>, <a href="ch22.xhtml#page_555">p. 555</a></p></td>&#13;
                </tr>&#13;
                <tr>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><code>hatvalues</code></p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table">Calculate&#13;
                            leverages</p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><a href="ch22.xhtml#ch22lev2sec217">Section 22.3.4</a>, <a href="ch22.xhtml#page_558">p. 558</a></p></td>&#13;
                </tr>&#13;
                <tr>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><code>cooks.distance</code></p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table">Calculate&#13;
                            Cook’s distances</p></td>&#13;
                    <td style="vertical-align: top;" class="table"><p class="table"><a href="ch22.xhtml#ch22lev2sec218">Section 22.3.5</a>, <a href="ch22.xhtml#page_561">p. 561</a></p></td>&#13;
                </tr>&#13;
            </tbody>&#13;
        </table>&#13;
    </body></html>