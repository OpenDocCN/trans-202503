<html><head></head><body>
<h2 class="h2" id="ch07"><span epub:type="pagebreak" id="page_183"/><strong><span class="big">7</span><br/>ORGANIZING DATA</strong></h2>
<div class="image1"><img src="../images/common.jpg" alt="Image"/></div>
<p class="noindent">If you’ve been paying attention, you may have noticed a bit of an obsession when it comes to dealing with memory. Back in <a href="ch03.xhtml#ch03">Chapter 3</a>, you learned that the order in which memory devices such as DRAM, flash memory, and disk drives are accessed affects their speed. And in <a href="ch05.xhtml#ch05">Chapter 5</a>, you learned that performance also depends on whether or not the data that you need is present in cache memory. Keeping these characteristics of the memory system in mind when organizing your data leads to better performance. To help you do this, in this chapter we’ll examine a number of <em>data structures</em>, or standard ways of organizing data. Many of these exist to support the efficient use of different types of memory. This often involves a space/time trade-off wherein more memory is used to make certain operations faster. (Note that higher-level data structures are provided by programming languages, not the computer hardware itself.)</p>
<p class="indent"><span epub:type="pagebreak" id="page_184"/>The phrase <em>locality of reference</em> sums up much of what this chapter covers in a fully buzzword-compliant manner. Or “keep the data you need close, the data you’ll need soon even closer.”</p>
<h3 class="h3" id="ch07lev1sec1"><strong>Primitive Data Types</strong></h3>
<p class="noindent">Programming languages offer a variety of primitive data <em>types</em>. There are two aspects to these types: their size (number of bits) and their interpretation (signed, unsigned, floating point, char, pointer, Boolean). <a href="ch07.xhtml#ch07fig01">Figure 7-1</a> shows the data types available to programmers on a typical modern machine via the C programming language. Different implementations of C on the same machine, as well as different languages such as Pascal or Java, may present these data types differently. Some language environments include facilities that allow the programmer to query the endianness (see <a href="ch04.xhtml#ch04fig04">Figure 4-4</a> on <a href="ch04.xhtml#page_96">page 96</a>), number of bits per byte, and more.</p>
<div class="image"><a id="ch07fig01"/><img src="../images/07fig01.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-1: Typical C language primitive data types</em></p>
<p class="indent">We saw all of these in <a href="ch01.xhtml#ch01">Chapter 1</a> except the pointer; the only difference here is that we’re using the C language names for them.</p>
<p class="indent">American engineer Harold Lawson invented the pointer for the PL/I (Programming Language One) in 1964. A <em>pointer</em> is just an unsigned <span epub:type="pagebreak" id="page_185"/>integer of some architecture-dependent size, but it’s interpreted as a memory address. It’s like the address of your house—it’s not the house itself, but it can be used to find your house. We’ve seen how this works before; it’s indirect addressing from “<a href="ch04.xhtml#ch04lev2sec5">Addressing Modes</a>” on <a href="ch04.xhtml#page_104">page 104</a>. A zero-valued, or <em>NULL</em>, pointer is not generally considered a valid memory address.</p>
<p class="indent">C popularized pointers. Some languages have implemented more abstract <em>references</em> in order to try to avoid problems resulting from sloppy pointer use, a subject I’ll touch on this later in the chapter. Pointers tend to be the size of the natural word on a machine so that they can be accessed in a single cycle.</p>
<p class="indent">Advances in chip technology spurred the development of a large number of new machines in the 1980s, which included the transition from 16-bit to 32-bit computers. A lot of code written in the 1970s and early 1980s was very cavalier about pointer use; for example, it assumed that pointers and integers were the same size and used them interchangeably. This code broke in often difficult-to-debug ways when ported to these new machines, spawning two independent remediation approaches. First, people started paying a lot more attention to portability issues. This solution was quite successful; portability and pointer issues are much less of a problem today. Second, languages that eliminated pointers were developed, such as Java. This approach has helped in some places but is not always worth the price.</p>
<h3 class="h3" id="ch07lev1sec2"><strong>Arrays</strong></h3>
<p class="noindent">The data types you saw in the previous section are simple; you can think of them as houses. Languages also support <em>arrays</em>, which can instead be likened to apartment buildings. Apartment buildings have an address, and the individual apartments have unit numbers. Programmers call the unit number the <em>index</em> (starting at 0, unlike most apartments), and the individual apartments are called array <em>elements</em>. Typical computer building codes mandate that all apartments in a building be identical. <a href="ch07.xhtml#ch07fig02">Figure 7-2</a> shows a building that contains ten 16-bit apartments in C.</p>
<div class="image"><a id="ch07fig02"/><img src="../images/07fig02.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-2: Ten-element array of 16-bit numbers</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_186"/>Each box in <a href="ch07.xhtml#ch07fig02">Figure 7-2</a> is a byte. In this array of 16-bit items, therefore, each element occupies two 8-bit bytes. The element subscript indicates the array’s index.</p>
<p class="indent">An alternative way to view an array is the through the lens of relative addressing (see “<a href="ch05.xhtml#ch05lev1sec5">Relative Addressing</a>” on <a href="ch05.xhtml#page_128">page 128</a>). Each element is an offset from the address of the 0th element, or <em>base address</em>. Thus, element<sub>1</sub> is 2 bytes away from element<sub>0</sub>.</p>
<p class="indent">The array in <a href="ch07.xhtml#ch07fig02">Figure 7-2</a> is a <em>one-dimensional</em> array—an ugly one-story building with all the apartments on one hall. Programming languages also support <em>multidimensional</em> arrays—for example, a building with four floors of three byte-sized apartments. This would be a two-dimensional array with two indices, one for the floor number and another for the apartment number on that floor. We can even make three-dimensional buildings with indices for wing, floor, and apartment; four-dimensional buildings with four indices; and so on.</p>
<p class="indent">It’s important to know how multidimensional arrays are laid out in memory. Let’s say we’re putting a flyer under every door in a 4×3 apartment building. We could do that in one of two ways. We could start on floor 0 and put a flier in apartment 0, then go to floor 1 and put a flier into apartment 0, and so on. Or we could start on floor 0 and put a flier under every door on that floor, then do the same on floor 1, and so on. This is a <em>locality of reference</em> issue. The second approach (doing all the doors on one floor) has better locality of reference and is much easier on the legs. You can see this in <a href="ch07.xhtml#ch07fig03">Figure 7-3</a>, where the numbers in parentheses are the addresses relative to the start of the array.</p>
<div class="image"><a id="ch07fig03"/><img src="../images/07fig03.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-3: Two-dimensional array layout</em></p>
<p class="indent">The column index moves between adjacent columns, whereas the row index moves between rows, which are farther apart in the address space.</p>
<p class="indent">This approach extends to higher dimensions. If we had a five-building complex with four floors of three apartments per floor, <a href="ch07.xhtml#ch07fig03">Figure 7-3</a> would be replicated five times, once for each building. In address space, adjacent buildings are farther apart than adjacent rows, which are farther apart than adjacent columns.</p>
<p class="indent">Going back to <a href="ch07.xhtml#ch07fig02">Figure 7-2</a>, think about what would happen if you tried to access element<sub>10</sub>. Some programming languages, such as Pascal, check to make sure that array indices are within the bounds of the array, but many others (including C) don’t. Without being checked, element<sub>10</sub> would land us at bytes 20 and 21 relative to the start of the array. That could crash a program if there’s no memory at that address, or it could be a security hole <span epub:type="pagebreak" id="page_187"/>allowing unintended access to data stored past the end of the array. It’s your job as a programmer to stay within bounds if the language doesn’t do it for you.</p>
<h3 class="h3" id="ch07lev1sec3"><strong>Bitmaps</strong></h3>
<p class="noindent">You’ve seen how you can construct arrays out of the primitive data types, but sometimes there isn’t a primitive data type that’s small enough for your purposes. For example, say Santa needs to track naughty versus nice for a large number of innocent children. Two values means that we need only 1 bit per child. We could easily use a byte for each value, but that’s less efficient—which translates into more warming at the North Pole and bad news for Frosty the Snowman because meltiness is considered a preexisting condition and not covered. What we really need is an array of bits, or a <em>bitmap</em>.</p>
<p class="indent">Bitmaps are easy to create. For example, say we want to keep track of 35 bits. We know that an array of five 8-bit bytes would be enough memory, as shown in <a href="ch07.xhtml#ch07fig04">Figure 7-4</a>.</p>
<div class="image"><a id="ch07fig04"/><img src="../images/07fig04.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-4: Array as bitmap</em></p>
<p class="indent">There are four basic operations that we can do on bitmaps: set a bit, clear a bit (set it to 0), test a bit to see if it is set, and test a bit to see if it is clear.</p>
<p class="indent">We can use integer division to find the byte containing a particular bit; all we have to do is divide by 8. We can do that quickly on machines with barrel shifters (see “<a href="ch04.xhtml#ch04lev2sec2">Shiftiness</a>” on <a href="ch04.xhtml#page_99">page 99</a>) by right-shifting the desired bit number by 3. For example, bit number 17 would be in the third byte because 17 ÷ 8 is 2 in integer division, and byte 2 is the third byte counting from 0.</p>
<p class="indent">The next step is to make a mask for the bit position. Similar to its physical counterpart, a <em>mask</em> is a bit pattern with holes that we can “see through.” We start by ANDing our desired bit number with a mask of 0x07 to get the lower three bits; for 17, that’s 00010001 AND 00000111, which yields 00000001, or bit position 1. We then left-shift a 1 by that amount, giving us a mask of 00000010, which is the position of bit 17 in byte 2.</p>
<p class="indent">Using the array index and bit mask, we can easily perform the following operations:</p>
<table class="topbot-d">
<colgroup>
<col style="width:20%"/>
<col style="width:80%"/>
</colgroup>
<tbody>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba"><strong>Set a bit</strong></p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">bits<sub>index</sub> = bits<sub>index</sub> OR mask</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba"><strong>Clear a bit</strong></p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">bits<sub>index</sub> = bits<sub>index</sub> AND (NOT mask)</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba"><strong>Test for set bit</strong></p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">(bits<sub>index</sub> AND mask) ≠ 0</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba"><strong>Test for clear bit</strong></p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">(bits<sub>index</sub> AND mask) = 0</p></td>
</tr>
</tbody>
</table>
<p class="indent"><span epub:type="pagebreak" id="page_188"/>There’s another useful application of bitmaps: to indicate whether resources are available or busy. If a set bit represents a busy resource, we can scan the array looking for a byte that’s not all 1s. This lets us test eight at a time. Of course, we would need to find the clear bit once we find a byte that contains one, but that’s much more efficient than testing each bit individually. Note that in cases like this, it’s more efficient to use an array of the largest primitive data type, such as C’s <span class="literal">unsigned long long</span>, instead of an array of bytes.</p>
<h3 class="h3" id="ch07lev1sec4"><strong>Strings</strong></h3>
<p class="noindent">You learned about encoding characters in “<a href="ch01.xhtml#ch01lev1sec10">Representing Text</a>” on <a href="ch01.xhtml#page_22">page 22</a>. A sequence of characters, such as those in this sentence, is called a <em>string</em>.</p>
<p class="indent">As with arrays, we often need to know a string’s length in order to be able to operate on it. Usually, it’s not enough to just make an array for each string, because many programs operate on variable-length string data; large arrays are often used when the length of a string isn’t known in advance. Since the array size is unrelated to the string length, we need some other method to track the string length. The most convenient way to do that is to somehow bundle the string length in with the string data.</p>
<p class="indent">One approach is to store the length in the string itself—for example, in the first byte. This works well but limits the length of the string to a maximum of 255 characters, which is insufficient for many applications. More bytes can be used to support longer strings, but at some point, the amount of overhead (bookkeeping bytes) exceeds the length of many strings. Also, because strings are bytes, they can have any alignment, but if multibyte counts are needed, strings would have to be aligned on those boundaries.</p>
<p class="indent">C uses a different approach, borrowed from the PDP-11 assembly language’s <span class="literal">.ASCIZ</span> pseudo-instruction, which doesn’t have a special data type for strings like some languages do. It just uses one-dimensional arrays of bytes; the fact that strings are arrays of characters is why the byte-sized data type in C is a <span class="literal">char</span>. But there’s a twist: C doesn’t store a string length. Instead, it adds an extra byte at the end of the array of characters for a NUL terminator. C uses the ASCII NUL character (refer back to <a href="ch01.xhtml#ch01tab11">Table 1-11</a>), which has a value of 0, as a <em>string terminator</em>. In other words, the NUL terminator is used to mark the end of a string. This works both for ASCII and UTF-8, and it looks like <a href="ch07.xhtml#ch07fig05">Figure 7-5</a>.</p>
<div class="image"><a id="ch07fig05"/><img src="../images/07fig05.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-5: C string storage and termination</em></p>
<p class="indent">As you can see, C uses 7 bytes of memory for the string, even though it’s only six characters long, because an extra byte is needed for the terminator.</p>
<p class="indent"><span epub:type="pagebreak" id="page_189"/>NUL turns out to be a good choice for the terminator because most machines include an instruction that tests whether or not a value is 0. Any other choice would involve extra instructions to load the value against which we’d be testing.</p>
<p class="indent">The use of a string terminator instead of an explicit length has its benefits and drawbacks. On one hand, storage is compact, which is important, and there’s essentially no overhead to do something like “print each character in the string until the end is reached.” But when you need the string’s length, you have to scan the string for the end, counting the characters. Also, with this approach you can’t have a NUL character in a string.</p>
<h3 class="h3" id="ch07lev1sec5"><strong>Compound Data Types</strong></h3>
<p class="noindent">Although simple rooms are good for some things, the market often demands fancier accommodations, such as suites. Most modern languages include facilities that allow you to roll your own data types—the “suites,” often called <em>structures</em>. The various rooms in each suite are its <em>members</em>.</p>
<p class="indent">Let’s say we’re writing a calendar program that includes a list (array) of events with their starting and ending dates and times. If we were doing this in C, the day, month, hours, minutes, and seconds would each be held in an <span class="literal">unsigned char</span>, but the year would need to be in an <span class="literal">unsigned short</span>. <a href="ch07.xhtml#ch07fig06">Figure 7-6</a> creates a structure for the date and time.</p>
<div class="image"><a id="ch07fig06"/><img src="../images/07fig06.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-6: Structure for date and time</em></p>
<p class="indent">Note that this isn’t strictly necessary; we could just have arrays of hours, minutes, and so on. But it’s certainly more convenient to have an array of date-time structures, and it makes programs easier to read and understand. British computer scientist Peter Landin coined the term <em>syntactic sugar</em> in 1964 for constructs such as this that make programs “sweeter.” Of course, one person’s sweetener is often another person’s essential functionality, leading to intense philosophical debates. Many would argue that syntactic sugar is limited to things like replacing <span class="literal">a = a + 1</span> with <span class="literal">a += 1</span> or <span class="literal">a++</span>, while fewer would claim that arrays of structures are syntactic sugar for sets of arrays. Time further complicates this fuzzy definition: <span class="literal">a += 1</span> and <span class="literal">a++</span> were not syntactic sugar when they were introduced, as compilers weren’t as good and these constructs generated better machine language. On the other hand, structures were more sugary when they were introduced, because prior code used arrays; they’re more essential now that programs are designed with structures in mind.</p>
<p class="indent">We can use compound data types, such as our date-time structure, as if they’re primitive data types. <a href="ch07.xhtml#ch07fig07">Figure 7-7</a> combines a pair of date-time structures with a small array to hold an event name string to make a complete calendar event structure.</p>
<span epub:type="pagebreak" id="page_190"/>
<div class="image"><a id="ch07fig07"/><img src="../images/07fig07.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-7: Structure for calendar entry</em></p>
<p class="indent">Structures often take up more memory space than you might expect. I discussed aligned and nonaligned memory in “<a href="ch04.xhtml#ch04lev1sec1">Memory</a>” on <a href="ch04.xhtml#page_94">page 94</a>. Say we built our date-time structure in an area zoned for 32-bit computers, as in <a href="ch04.xhtml#ch04fig02">Figure 4-2</a> on <a href="ch04.xhtml#page_95">page 95</a>. The language keeps the structure members in the order specified by the programmer because it might matter. But the language also has to respect the alignment (<a href="ch04.xhtml#ch04fig03">Figure 4-3</a> on <a href="ch04.xhtml#page_95">page 95</a>), which means that it can’t put the year in the fourth and fifth bytes, as shown in <a href="ch07.xhtml#ch07fig07">Figure 7-7</a>, because that crosses a boundary. The language tools solve this problem by automatically adding <em>padding</em> as needed. The actual memory layout of our structure would look like <a href="ch07.xhtml#ch07fig08">Figure 7-8</a>.</p>
<div class="image"><a id="ch07fig08"/><img src="../images/07fig08.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-8: Structure for date and time with padding</em></p>
<p class="indent">You could rearrange the structure members to make sure that you ended up with a 7-byte structure with no padding. Of course, when you combine a pair of these into the calendar structure, the language tools will likely pad them out to 8 bytes anyway.</p>
<p class="indent">It’s worth mentioning that this is a contrived example and you shouldn’t necessary handle dates and times this way. The standard in many systems, which came from UNIX, is to use a 32-bit number to represent the number of seconds since the “UNIX epoch” began on January 1, 1970. This scheme will run out of bits in 2038, but many systems have expanded this to 64 bits in preparation.</p>
<p class="indent"><a href="ch01.xhtml#ch01fig21">Figure 1-21</a> showed a way to use four 8-bit values to represent color with transparency. That’s a great use for a structure, but it’s not always the best way to view that data. For example, if we needed to copy a color, it would be much more efficient to copy all 32 bits at once rather than doing four 8-bit copies. Another compound data type to the rescue.</p>
<p class="indent">Not only can we have suites, as we saw in the previous section, but we can also have offices with movable partitions, which are called <em>unions</em> in C. A union allows multiple views of the same space or content. The difference between a structure and a union is that everything in a structure takes memory, whereas everything in a union shares memory. <a href="ch07.xhtml#ch07fig09">Figure 7-9</a> combines the RGBα structure with an unsigned long to form a union.</p>
<div class="image"><a id="ch07fig09"/><img src="../images/07fig09.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-9: Pixel union</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_191"/>Using the union and C language syntax, we could set the <span class="literal">pixel.color</span> to <span class="literal">0x12345678</span> and then <span class="literal">pixel.components.red</span> would be <span class="literal">0x12</span>, <span class="literal">pixel.components.green</span> would be <span class="literal">0x34</span>, and so on.</p>
<h3 class="h3" id="ch07lev1sec6"><strong>Singly Linked Lists</strong></h3>
<p class="noindent">Arrays are the most efficient way to keep lists of things. They only hold actual data, without requiring any additional bookkeeping information. But they don’t work as well for arbitrary amounts of data, because if we didn’t make the array large enough, then we have to create a new, larger array and copy all the data into it. And they waste space if we make them larger than necessary. Similarly, copying is required if you need to insert an element into the middle of a list or delete an element.</p>
<p class="indent"><em>Linked lists</em> can perform better than arrays when you don’t know in advance how many things you’ll be tracking. Singly linked lists, implemented using structures, look like <a href="ch07.xhtml#ch07fig10">Figure 7-10</a>.</p>
<div class="image"><a id="ch07fig10"/><img src="../images/07fig10.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-10: Singly linked list</em></p>
<p class="indent">Note that <span class="literal">next</span> is a pointer that holds the address of the next element in the list. The first thing in the list is known as the <em>head</em>; the last thing is the <em>tail</em>. We can recognize the tail because <span class="literal">next</span> is a value that can’t be another list element, usually a <span class="literal">NULL</span> pointer.</p>
<p class="indent">A big difference between the list shown in <a href="ch07.xhtml#ch07fig10">Figure 7-10</a> and an array is that all array elements are contiguous in memory. List elements can be anywhere in memory and look more like <a href="ch07.xhtml#ch07fig11">Figure 7-11</a>.</p>
<div class="image"><a id="ch07fig11"/><img src="../images/07fig11.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-11: Singly linked list in memory</em></p>
<p class="indent">Adding an element to a list is easy; just pop it on the head, as shown in <a href="ch07.xhtml#ch07fig12">Figure 7-12</a>.</p>
<span epub:type="pagebreak" id="page_192"/>
<div class="image"><a id="ch07fig12"/><img src="../images/07fig12.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-12: Singly linked list insertion</em></p>
<p class="indent">Deleting an element is a bit more complicated because we need to make the <span class="literal">next</span> of the previous element point to the following element, as shown in <a href="ch07.xhtml#ch07fig13">Figure 7-13</a>.</p>
<div class="image"><a id="ch07fig13"/><img src="../images/07fig13.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-13: Singly linked list deletion</em></p>
<p class="indent">One way to do that is by using a pair of pointers, as shown in <a href="ch07.xhtml#ch07fig14">Figure 7-14</a>.</p>
<div class="image"><a id="ch07fig14"/><img src="../images/07fig14.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-14: Singly linked list deletion using a pair of pointers</em></p>
<p class="indent">The <span class="literal">current</span> pointer walks the list looking for the node to delete. The <span class="literal">previous</span> pointer allows us to adjust the <span class="literal">next</span> of the node before the one to delete. We use a dot (<span class="literal">.</span>) to indicate a member of a structure, so <span class="literal">current.next</span> means the next member of the current node.</p>
<div class="note">
<p class="notet"><span epub:type="pagebreak" id="page_193"/><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em><a href="ch07.xhtml#ch07fig14">Figure 7-14</a> isn’t a great example; although to be fair, I looked online while writing this section and found algorithms that were much worse. The problem with the code shown here is that it’s complicated because a special test is needed for the list head.</em></p>
</div>
<p class="indent">The algorithm in <a href="ch07.xhtml#ch07fig15">Figure 7-15</a> shows how the power of <em>double indirect addressing</em> eliminates the special case, resulting in simpler code.</p>
<div class="image"><a id="ch07fig15"/><img src="../images/07fig15.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-15: Singly linked list deletion using indirect addressing</em></p>
<p class="indent">Let’s examine how this algorithm works in more detail. Have a look at <a href="ch07.xhtml#ch07fig16">Figure 7-16</a>. The subscripts show how <span class="literal">current</span> changes as the algorithm proceeds.</p>
<div class="image"><a id="ch07fig16"/><img src="../images/07fig16.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-16: Singly linked list deletion in action</em></p>
<p class="indent">The steps shown in <a href="ch07.xhtml#ch07fig16">Figure 7-16</a> are complicated, so let’s walk through them.</p>
<ol>
<li class="noindent">We start by setting <span class="literal">current</span><sub><span class="literal">0</span></sub> to the address of <span class="literal">head</span>, which results in <span class="literal">current</span><sub><span class="literal">1</span></sub>, which in turn points to <span class="literal">head</span>. This means that <span class="literal">current</span> points to <span class="literal">head</span>, which points to list element <span class="literal">A</span>.</li>
<li class="noindent">We’re not looking for element <span class="literal">A</span>, so we move along.</li>
<li class="noindent"><span epub:type="pagebreak" id="page_194"/>As shown by the dashed arrow, we set <span class="literal">current</span> to the address of the <span class="literal">next</span> pointer in the element pointed to by whatever <span class="literal">current</span> points to. Since <span class="literal">current</span><sub><span class="literal">1</span></sub> points to <span class="literal">head</span>, which points to element <span class="literal">A</span>, <span class="literal">current</span><sub><span class="literal">2</span></sub> ends up pointing to <span class="literal">A.next</span>.</li>
<li class="noindent">It’s still not the element that we want to delete, so we do it all again, causing <span class="literal">current</span><sub><span class="literal">3</span></sub> to reference <span class="literal">B.next</span>.</li>
<li class="noindent">It’s still not the element that we want to delete, so we do it all again, causing <span class="literal">current</span><sub><span class="literal">4</span></sub> to reference <span class="literal">C.next</span>.</li>
<li class="noindent"><span class="literal">C.next</span> points to element <span class="literal">D</span>, which is the one we want to delete. Following the light dashed arrow, we follow <span class="literal">current</span> to <span class="literal">C.next</span> to <span class="literal">D</span>, and replace <span class="literal">C.next</span> with the contents of <span class="literal">D.next</span>. Since <span class="literal">D.next</span> points to element <span class="literal">E</span>, <span class="literal">C.next</span> now points to <span class="literal">E</span> as shown by the heavy dashed arrow, removing <span class="literal">D</span> from the list.</li>
</ol>
<p class="indent">We could modify the preceding algorithm to insert links into the middle of the list. That might be useful if we, for example, wanted the list to be ordered by date, name, or some other criteria.</p>
<p class="indent">Earlier I mentioned that this second algorithm produced better code. Let’s compare the two as written in the C programming language. You don’t have to understand this code to see the difference between <a href="ch07.xhtml#ch07list01">Listing 7-1</a> and <a href="ch07.xhtml#ch07list02">Listing 7-2</a>.</p>
<p class="programs">struct node {<br/>
    struct node *next;<br/>
    // data<br/>
};<br/>
<br/>
struct node *head;<br/>
struct node *node_to_delete;<br/>
struct node *current;<br/>
struct node *previous;<br/>
<br/>
previous = (struct node *)0;<br/>
current = head;<br/>
<br/>
while (current != (struct node *)0) {<br/>
    if (current == node_to_delete) {<br/>
        if (previous == (struct node *)0)<br/>
            head = current-&gt;next;<br/>
        else<br/>
            previous-&gt;next = current-&gt;next;<br/>
        break;<br/>
    }<br/>
    else {<br/>
            previous = current;<br/>
            current = current-&gt;next;<br/>
    }<br/>
}</p>
<p class="listing" id="ch07list01"><em>Listing 7-1: C language code for singly linked list deletion using a pair of pointers</em></p>
<span epub:type="pagebreak" id="page_195"/>
<p class="programs">struct node {<br/>
    struct node *next;<br/>
    // data<br/>
};<br/>
<br/>
struct node *head;<br/>
struct node *node_to_delete;<br/>
struct node **current;<br/>
<br/>
for (current = &amp;head; *current != (struct node *)0; current = &amp;((*current)-&gt;next))<br/>
    if (*current == node_to_delete) {<br/>
            *current = node_to_delete-&gt;next;<br/>
            break;<br/>
    }<br/>
}</p>
<p class="listing" id="ch07list02"><em>Listing 7-2: C language code for singly linked list deletion using double indirect addressing</em></p>
<p class="indent">As you can see, the indirect addressing version of this code in <a href="ch07.xhtml#ch07list02">Listing 7-2</a> is much simpler than the code using a pair of pointers in <a href="ch07.xhtml#ch07list01">Listing 7-1</a>.</p>
<h3 class="h3" id="ch07lev1sec7"><strong>Dynamic Memory Allocation</strong></h3>
<p class="noindent">Our discussion of linked list insertion conveniently omitted something important. I showed how to insert a new node but didn’t say where the memory for that node came from.</p>
<p class="indent">We saw back in <a href="ch05.xhtml#ch05fig16">Figure 5-16</a> that program data space starts with a section for statically allocated data followed by the heap that the runtime library sets up for the program. This is all of the data memory available to a program (except for the stack and interrupt vectors) on machines that don’t have memory management units (MMUs). On systems with MMUs, the runtime library requests the amount of memory it thinks it needs, because tying up all of the main memory doesn’t make sense. The <em>break</em> is the end of the memory available to a program, and there are some system calls that grow or shrink the amount of available memory.</p>
<p class="indent">Memory for variables such as arrays is static; that is, it’s assigned an address that doesn’t change. Things like list nodes are dynamic; they come and go as needed. We get memory for them from the heap.</p>
<p class="indent">A program needs some way to manage the heap. It needs to know what memory is in use and what’s available. There are library functions for this so that you don’t have to write your own. In C, they’re the <span class="literal">malloc</span> and <span class="literal">free</span> functions. Let’s look at how they can be implemented.</p>
<p class="indent">One implementation of <span class="literal">malloc</span> works by using a singly linked list data structure. The heap is divided up into blocks, each of which has a size and a pointer to the next block, as shown in <a href="ch07.xhtml#ch07fig17">Figure 7-17</a>.</p>
<span epub:type="pagebreak" id="page_196"/>
<div class="image"><a id="ch07fig17"/><img src="../images/07fig17.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-17: <span class="codeitalic">malloc</span> structure for heap management</em></p>
<p class="indent">Initially there’s just one block for the entire heap. When a program asks for memory, <span class="literal">malloc</span> looks for a block that has enough space, returns the caller a pointer to the requested space, and adjusts the size of the block to reflect the memory that it gave away. When a program frees memory using the <span class="literal">free</span> function, it just puts the block back in the list.</p>
<p class="indent">At various times, <span class="literal">malloc</span> scans the list for adjacent free blocks and coalesces them into a single larger block. One way of doing this is when allocating memory (calling <span class="literal">malloc</span>) because allocation requires going through the list looking for a large enough block. Over time, the memory space can become <em>fragmented</em>, which means there’s no available block of memory large enough to satisfy a request, even if not all memory has been used up. On systems with MMUs, the break is adjusted to get more memory if needed.</p>
<p class="indent">You can see that there’s a certain amount of overhead to this approach: <span class="literal">next</span> and <span class="literal">size</span> add 16 bytes to each block on a 64-bit machine.</p>
<p class="indent">Freeing unallocated memory is a common error that inexperienced programmers make. Another is continuing to use memory that has already been freed. As you can see in <a href="ch07.xhtml#ch07fig17">Figure 7-17</a>, if you write data outside the bounds of allocated memory, you can corrupt the <span class="literal">size</span> and <span class="literal">next</span> fields. That’s particularly insidious because the problems this causes may not show up until a later operation needs to use the information in those fields.</p>
<p class="indent">One side effect of technological advances is that small machines often come with way more RAM than your program needs. In these cases, it’s better to just statically allocate everything because that reduces overhead and eliminates memory allocation bugs.</p>
<h3 class="h3" id="ch07lev1sec8"><strong>More Efficient Memory Allocation</strong></h3>
<p class="noindent">Linked lists that include text strings are common. Suppose we have a linked list where the node includes a pointer to a string, as shown in <a href="ch07.xhtml#ch07fig18">Figure 7-18</a>.</p>
<div class="image"><a id="ch07fig18"/><img src="../images/07fig18.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-18: List node with string</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_197"/>We have to allocate memory not only for each node but also for the string attached to the node. The <span class="literal">malloc</span> overhead can be significant, especially on a 64-bit machine where we would have 16 bytes of overhead for the 16-byte node, and then another 16 bytes of overhead for a string such as the 4-byte <span class="literal">cat</span> in <a href="ch07.xhtml#ch07fig18">Figure 7-18</a>.</p>
<p class="indent">We can reduce the overhead by allocating the node and string at the same time. Instead of allocating the node and then the string, we can allocate space for the sum of the node and string sizes plus whatever padding might be necessary for alignment. This means that nodes are of variable size, which is okay. This trick cuts the overhead in half. The result looks like <a href="ch07.xhtml#ch07fig19">Figure 7-19</a>, with a string of <span class="literal">cat</span>.</p>
<div class="image"><a id="ch07fig19"/><img src="../images/07fig19.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-19: More efficient memory allocation</em></p>
<p class="indent">This approach is also more efficient when you are deleting nodes. In the less efficient case, two calls to <span class="literal">free</span> would be required, one for the string and another for the node. In the more efficient case, both get freed with a single call.</p>
<h3 class="h3" id="ch07lev1sec9"><strong>Garbage Collection</strong></h3>
<p class="noindent">Two problems can arise from explicit dynamic memory management that are really problems of sloppy pointer use. Remember, a pointer is just a number that represents a memory address. But not all numbers are valid memory addresses. Using a pointer to try to access nonexistent memory or memory that doesn’t meet the processor alignment rules can cause an exception and crash a program.</p>
<p class="indent">You might be learning a programming language such as Java or JavaScript that doesn’t have pointers but supports dynamic memory allocation without equivalents to <span class="literal">malloc</span> and <span class="literal">free</span>. These languages instead implement <em>garbage collection</em>, a technique invented in 1959 by American computer and cognitive scientist John McCarthy (1927–2011) for the LISP programming language. Garbage collection has experienced a renaissance, partly as a proscriptive remedy for bad pointer use.</p>
<p class="indent"><span epub:type="pagebreak" id="page_198"/>Languages like Java use references instead of pointers. <em>References</em> are an abstraction for pointers that provide much of the same functionality without actually exposing memory addresses.</p>
<p class="indent">Garbage-collected languages often have a <span class="literal">new</span> operator that creates items and allocates memory for them (this operator also appears in non-garbage-collected languages such as C++). There is no corresponding operator for item deletion. Instead, the language runtime environment tracks the use of variables and automatically deletes those it deems no longer in use. There are many ways in which this is done, one of which is to keep a count of references to variables so the variables can be deleted when there are no references left.</p>
<p class="indent">Garbage collection is a trade-off; it’s not without its issues. One issue is similar to the LSI-11 refresh problem (see “<a href="ch03.xhtml#ch03lev2sec8">Random-Access Memory</a>” on <a href="ch03.xhtml#page_82">page 82</a>) in that the programmer doesn’t have much control over the garbage collection system, which may decide to run even though the program needs to do something more important. Also, programs tend to take a lot of memory because it’s easy to leave unnecessary references around, which prevents memory from being reclaimed. This makes programs run slowly as opposed to just crashing due to bad pointers. It turns out that despite good intentions of solving the pointer problem, tracking down unnecessary references is actually harder to debug.</p>
<h3 class="h3" id="ch07lev1sec10"><strong>Doubly Linked Lists</strong></h3>
<p class="noindent">Our singly linked list <span class="literal">delete</span> function can be pretty slow because we have to find the element before the one we want to delete so that we can adjust its pointer. This could involve traversing a very long list. Fortunately, there’s a different type of list that solves this problem at the expense of some extra memory.</p>
<p class="indent">A doubly linked list includes a link not only to the next element but also to the previous element, as you can see in <a href="ch07.xhtml#ch07fig20">Figure 7-20</a>. This doubles the per-node overhead, but it eliminates the need for list walking in the <span class="literal">delete</span> case, so it’s a space/time trade-off.</p>
<div class="image"><a id="ch07fig20"/><img src="../images/07fig20.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-20: Doubly linked list</em></p>
<p class="indent">The advantage of a doubly linked list is that you can insert and delete anywhere without having to spend time traversing the list. <a href="ch07.xhtml#ch07fig21">Figure 7-21</a> shows how you’d add a new node into a list after element A.</p>
<span epub:type="pagebreak" id="page_199"/>
<div class="image"><a id="ch07fig21"/><img src="../images/07fig21.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-21: Doubly linked list insertion</em></p>
<p class="indent"><a href="ch07.xhtml#ch07fig22">Figure 7-22</a> shows that deleting an element is just as simple.</p>
<div class="image"><a id="ch07fig22"/><img src="../images/07fig22.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-22: Doubly linked list deletion</em></p>
<p class="indent">As you can see, these operations on doubly linked list elements don’t require traversal.</p>
<h3 class="h3" id="ch07lev1sec11"><strong>Hierarchical Data Structures</strong></h3>
<p class="noindent">So far, we’ve looked only at <em>linear</em> data structures. They’re great for many applications, but at some point their linearity can be a problem. That’s because storing data is only half of the work; we also need to be able to retrieve it efficiently. Let’s say we have a list of things stored in a linked list. We might need to walk the entire list to find a particular one; for a list of length <em>n</em>, it could take <em>n</em> lookups. This is fine for small numbers of things but impractical for large values of <em>n</em>.</p>
<p class="indent">Earlier we saw how pointers could be used to connect nodes into linked lists. We’re not restricted to any number of pointers, so the ways in which we can organize data are limited only by our imagination and memory space. For example, we could come up with a hierarchical arrangement of nodes, as in the example back in <a href="ch05.xhtml#ch05fig04">Figure 5-4</a>.</p>
<p class="indent">The simplest hierarchical data structure is the <em>binary tree</em>—“binary” not because of binary numbers but because a node can connect to two other nodes. Let’s make a node that contains a number arranged as shown in <a href="ch07.xhtml#ch07fig23">Figure 7-23</a>.</p>
<span epub:type="pagebreak" id="page_200"/>
<div class="image"><a id="ch07fig23"/><img src="../images/07fig23.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-23: Binary tree nodes containing numbers</em></p>
<p class="indent">The <em>root</em> is the tree equivalent of a linked list’s head.</p>
<p class="indent">We’re going to hang out in a bingo parlor and record the numbers in a binary tree as they’re called out. We’ll then be able to look up numbers to see if they’ve been called. <a href="ch07.xhtml#ch07fig24">Figure 7-24</a> shows an algorithm that inserts a number into a tree. It works in a manner similar to our singly linked list deletion in that it relies on indirect addressing.</p>
<div class="image"><a id="ch07fig24"/><img src="../images/07fig24.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-24: Binary tree insertion algorithm</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_201"/>Let’s look at this in action by inserting the numbers 8, 6, 9, 4, and 5. Nothing is attached to the root when we insert the 8, so we attach it there. When we insert the 6, the root spot is taken, so we compare that node; then because 6 is less than 8, we hit the left side. It’s vacant, so we plop a new node there. The 9 goes on the right-hand side of the 8, the 4 on the left-hand side of the 6, and so on, as shown in <a href="ch07.xhtml#ch07fig25">Figure 7-25</a>.</p>
<div class="image"><a id="ch07fig25"/><img src="../images/07fig25.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-25: Binary tree</em></p>
<p class="indent">You can see that even though there are five things in this data structure, worst case we can find one by checking three nodes. This beats a linked list, where we may have to check all five. It’s easy to look something up in a binary tree, as shown in <a href="ch07.xhtml#ch07fig26">Figure 7-26</a>. Note that we don’t need a pointer to a pointer to a node here because we don’t have to modify the tree.</p>
<div class="image"><a id="ch07fig26"/><img src="../images/07fig26.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-26: Binary tree look-up algorithm</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_202"/>You may have noticed that the arrangement of the tree depends on insertion order. <a href="ch07.xhtml#ch07fig27">Figure 7-27</a> shows what happens if we insert the numbers in order: 4, 5, 6, 8, and 9.</p>
<div class="image"><a id="ch07fig27"/><img src="../images/07fig27.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-27: Poorly balanced binary tree</em></p>
<p class="indent">This degenerate case looks a lot like a singly linked list. Not only do we lose the benefits of a binary tree, but now we have the additional overhead of the unused left pointers as well. We’d really prefer that our tree ended up looking like the one on the right in <a href="ch07.xhtml#ch07fig28">Figure 7-28</a>.</p>
<div class="image"><a id="ch07fig28"/><img src="../images/07fig28.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-28: Unbalanced versus balanced binary trees</em></p>
<p class="indent">Searching for something in a binary tree is a function of the depth in the tree; if it’s <em>n</em> levels down, then it takes <em>n</em> tests to find it. It takes only log<sub>2</sub><em>n</em> in a balanced binary tree as opposed to <em>n</em> in a linked list. Putting that in perspective, in the worst case you’d have to visit 1,024 nodes in a linked list containing 1,024 nodes, but you’d need to visit only 10 nodes in a balanced binary tree.</p>
<p class="indent">There are numerous tree-balancing algorithms, which I’m not going to cover here in detail. It takes time to rebalance a tree, so there’s a trade-off between algorithm speed, insert/lookup time, and rebalancing time. Tree-balancing algorithms have more computational overhead, and some have <span epub:type="pagebreak" id="page_203"/>additional storage overhead. That overhead is quickly overcome, however, as the size of the tree increases, because log<sub>2</sub><em>n</em> becomes much smaller than <em>n</em>.</p>
<h3 class="h3" id="ch07lev1sec12"><strong>Storage for the Masses</strong></h3>
<p class="noindent">We talked about disk drives back in “<a href="ch03.xhtml#ch03lev1sec3">Block Devices</a>” on <a href="ch03.xhtml#page_85">page 85</a>. Let’s look at them in more detail so we can understand their data organization peculiarities. Warning: we’re going to go pointer-crazy here!</p>
<p class="indent">I mentioned that the basic unit on a disk is a <em>block</em> and consecutive blocks are called <em>clusters</em>. It would be nice if we could just store data in clusters, which are contiguous sectors on a track. Although that’s done in certain circumstances where very high performance is required, it’s not a good general-purpose solution, and there might be more data than would fit on a track anyway. Instead, data is stored in whatever sectors are available; the operating system’s device driver provides the illusion of contiguous storage. Now we’re sort of in familiar territory, with a twist: instead of finding a block of storage to hold an object, we now have to find enough fixed-size blocks to hold an object and divide the object up among them.</p>
<p class="indent">Linked lists are not a great solution for keeping track of which disk blocks are free and which are in use, because traversing a list would be too slow. An 8 TiB disk has almost 2 billion blocks, and with worst-case behavior, 250 blocks can be accessed per second. That adds up to more than 15 years, which makes it impractical. That sounds really bad, but keep in mind that’s 1 MiB of data per second.</p>
<p class="indent">When we’re managing data in memory, it suffices to reference it using a pointer. But those are transient, and because disks are used for long-term data storage, we need something more persistent. You’ve already seen the answer: <em>filenames</em>. We need some way to both store those filenames on the disk and associate them with the blocks used to store the file data.</p>
<p class="indent">One way to manage all of this comes from—yup, you guessed it—UNIX. A number of blocks are set aside as <em>inodes</em>, a contraction of the disk block <em>index</em> and <em>node</em>; thus, inodes are index nodes. An inode contains various pieces of information about a file, such as its owner, size, and permissions. It also contains the indices of the blocks containing the file data, as you can see in <a href="ch07.xhtml#ch07fig29">Figure 7-29</a>.</p>
<div class="image"><a id="ch07fig29"/><img src="../images/07fig29.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-29: Filesystem data structure</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_204"/>This looks really complicated, but it isn’t. An inode typically has 12 <em>direct block</em> pointers (they’re really not pointers, just block indices), which support files up to 4,096 × 12 = 49,152 bytes in length. That’s good enough for most files. If a file is larger, it uses <em>indirect blocks</em>. Assuming 32-bit indices (though these will need to be 64-bit soon), 1,024 indirect blocks which at 4 bytes each fit in one block, add another 4 MiB to the maximum file size. If that’s not enough, 4 GiB are available via the <em>double indirect</em> blocks, and finally another 4 PiB via the <em>triple indirect</em> blocks.</p>
<p class="indent">One piece of information an inode indicates is whether the blocks contain <em>directory</em> information instead of other data. A directory maps filenames to the inodes that reference the file data. One of the nice things about the way UNIX does things is that a directory is really just another type of file. That means a directory can reference other directories, which is what gives us our familiar tree-structured <em>hierarchical filesystems</em>.</p>
<p class="indent">At this point, you may be thinking that all this looks a lot like an arbitrary tree, which was true for a while. One of the features of this arrangement is that multiple inodes can reference the same blocks. Each reference is called a <em>link</em>. Links allow the same file to appear in multiple directories. It turns out that it’s very convenient to also be able to link to directories, so <em>symbolic links</em> were invented to make that possible. But symbolic links allow loops in the filesystem graph, so we need special code to detect that to prevent infinite looping. In any case, we have this complex structure that tracks the blocks used, but we’re still missing an efficient way to track the <em>free space</em>.</p>
<p class="indent">One way to accomplish this is by using a bitmap (see “<a href="ch07.xhtml#ch07lev1sec3">Bitmaps</a>” on <a href="ch07.xhtml#page_187">page 187</a>) with 1 bit for each disk block. A bitmap can be pretty large: an 8 TB disk drive would need almost 2 billion bits, which would consume about 256 MiB. It’s still a reasonable way to go—it’s way less than 0.01 percent of the total disk space, and it doesn’t all have to be in memory at the same time.</p>
<p class="indent">Working with bitmaps is pretty simple and efficient, especially if they’re stored in 64-bit words. Assuming that a 1 indicates a block in use and a 0 indicates a free block, we can easily look for words that are not all 1s to find free blocks.</p>
<p class="indent">But there is a problem with this approach: it’s possible for the filesystem graph and the free space bitmap to get out of sync. For example, the power could fail while data is being written to the disk. In the dark ages when computers had front panels with switches and blinking lights, you’d have to repair a damaged filesystem by inputting inode numbers through the front panel switches. This ordeal was remedied by programs such as <span class="literal">fsck</span>, which traverse the filesystem graph and compare it to the free block data. That’s a better approach, but it’s increasingly time-consuming as disks get larger. New journaling filesystem designs make damage control more efficient.</p>
<h3 class="h3" id="ch07lev1sec13"><strong>Databases</strong></h3>
<p class="noindent">Binary trees are a great way to store data in memory, but they don’t work as well when it comes to storing huge amounts of data that doesn’t fit in memory. That’s partly because tree nodes tend to be small and therefore don’t map well to disk sectors.</p>
<p class="indent"><span epub:type="pagebreak" id="page_205"/>A <em>database</em> is just a collection of data organized in some way. A <em>database management system (DBMS)</em> is a program that allows information to be stored in and retrieved from a database. A DBMS usually includes a number of interfaces layered on top of the underlying storage mechanism.</p>
<p class="indent">Databases are a common application of the <em>B-tree</em> data structure invented by German computer scientist Rudolf Bayer and American computer scientist Ed McCreight at Boeing in 1971. The B-tree is a balanced tree, but not a binary tree. It’s a bit less space efficient than a balanced binary tree but performs better, especially when data is stored on disk. This is yet another case where an understanding of memory architecture leads to more efficient code.</p>
<p class="indent">Say we have a balanced binary tree of names sorted alphabetically. It would look something like <a href="ch07.xhtml#ch07fig30">Figure 7-30</a>.</p>
<div class="image"><a id="ch07fig30"/><img src="../images/07fig30.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-30: Balanced binary tree</em></p>
<p class="indent">A B-tree node has many more legs (children) than a binary tree node. The number of legs is chosen such that a node fits exactly into a disk block, as shown in <a href="ch07.xhtml#ch07fig31">Figure 7-31</a>.</p>
<div class="image"><a id="ch07fig31"/><img src="../images/07fig31.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-31: B-tree</em></p>
<p class="indent">As you can see, the interior nodes are balanced, which yields a predictable search time. There are unused child links in <a href="ch07.xhtml#ch07fig31">Figure 7-31</a> that consume space. You can easily rebalance the tree when child links run out by changing the range covered by the node. For example, if the A-M node ran out of children, it could be subdivided into A-G and H-M nodes. This isn’t a great example, because power-of-2 subdivision is most often used but we don’t have an even number of things to subdivide here.</p>
<p class="indent">More keys per node means less fetching of nodes. The larger nodes aren’t a problem because they’re the size of a disk block, which is fetched as a unit. There is some wasted space because of unused child links, but it’s a reasonable trade-off.</p>
<h3 class="h3" id="ch07lev1sec14"><span epub:type="pagebreak" id="page_206"/><strong>Indices</strong></h3>
<p class="noindent">Accessing sorted data is efficient, but we often need to access data sorted in more than one way. We might have both first and last names, or names and favorite bands.</p>
<p class="indent"><a href="ch07.xhtml#ch07fig31">Figure 7-31</a> shows nodes organized by name. These nodes are often referred to as the <em>primary index</em>. But we can have more than one index, as shown in <a href="ch07.xhtml#ch07fig32">Figure 7-32</a>, which allows us to efficiently search for things in different ways.</p>
<div class="image"><a id="ch07fig32"/><img src="../images/07fig32.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-32: Multiple indices</em></p>
<p class="indent">The trade-off with indices is that they need maintenance. Every index must be updated when the data changes. That’s a worthwhile cost when searching is a more common activity than modification.</p>
<h3 class="h3" id="ch07lev1sec15"><strong>Moving Data Around</strong></h3>
<p class="noindent">I mentioned earlier that using arrays instead of linked lists requires copying data if the array needs to grow in size. You need copying in order to move page tables in and out of MMUs, free disk bitmaps on and off disk, and so on. Programs spend a lot of time moving data from one place to another, so it’s important to do it efficiently.</p>
<p class="indent">Let’s start with a half-measure: setting a block of <span class="literal">length</span> memory bytes to all 0s, as shown in <a href="ch07.xhtml#ch07fig33">Figure 7-33</a>.</p>
<span epub:type="pagebreak" id="page_207"/>
<div class="image"><a id="ch07fig33"/><img src="../images/07fig33.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-33: Zeroing a block of memory</em></p>
<p class="indent">That algorithm works fine, but it’s not very efficient. Assuming that each box in <a href="ch07.xhtml#ch07fig33">Figure 7-33</a> takes the same amount of time to execute, we spend more time bookkeeping than zeroing memory locations. The <em>loop unrolling</em> technique can make this more efficient, as shown in <a href="ch07.xhtml#ch07fig34">Figure 7-34</a>. For example, assuming that <span class="literal">length</span> is an even number, we can unroll the loop so that now more of the time is spent zeroing and less is spent on other things.</p>
<div class="image"><a id="ch07fig34"/><img src="../images/07fig34.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-34: Zeroing a block of memory with loop unrolling</em></p>
<p class="indent">It would be nice to have a more general implementation, and fortunately there is one. When he worked at Lucasfilm, Canadian programmer <span epub:type="pagebreak" id="page_208"/>Tom Duff invented <em>Duff’s Device</em> to speed up the copying of data; <a href="ch07.xhtml#ch07fig35">Figure 7-35</a> shows a variant for zeroing memory. This approach works only if the <span class="literal">length</span> is greater than zero.</p>
<div class="image"><a id="ch07fig35"/><img src="../images/07fig35.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-35: Zeroing a block of memory using a modified Duff’s Device</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_209"/>Duff’s Device unrolls the loop eight times and jumps into the middle to handle any leftover bytes. Though you might be tempted to unroll the loop further, this approach must be balanced with the code size because having it fit into the instruction cache is worth a lot of speed.</p>
<p class="indent">You can see on the loop side of the figure that the ratio of memory-zeroing time to bookkeeping time is much improved. Though the initial setup and branching to the proper place in the loop looks complicated, it really isn’t. It doesn’t take a pile of conditional branches, just some address manipulation as follows:</p>
<ol>
<li class="noindent">Mask off all but the lower 3 bits of the length by ANDing with 0x7.</li>
<li class="noindent">Subtract the result from 8.</li>
<li class="noindent">Mask off all but the lower 3 bits by ANDing with 0x7.</li>
<li class="noindent">Multiply by the number of bytes between zeroing instructions.</li>
<li class="noindent">Add the address of the first zeroing instruction.</li>
<li class="noindent">Branch to that address.</li>
</ol>
<p class="indent">Another way to increase the efficiency is to recognize that, for example, on a 64-bit machine, 8 bytes can be zeroed at a time. Of course, a bit of extra code is needed to handle leftover bytes at the beginning and the end. We need to use the algorithm from <a href="ch07.xhtml#ch07fig36">Figure 7-36</a> without the loop on the <span class="literal">eights</span> for the beginning and end. In the middle, we zero as many 8-byte chunks as possible.</p>
<p class="indent">This all becomes more complicated when we’re copying a block of data instead of just setting it to a value, because chances are, the source and destination won’t have the same byte alignment. It’s often worth testing for the case where both the source and destination are word aligned because it’s a pretty common case.</p>
<p class="indent">Copying has yet another complication, which is that it’s common to use copying to move data around in a region of memory. For example, we may have a buffer full of space-separated words in which we want to read the first word out of the buffer and then cram everything else down so that there’s room for more at the end. You have to take care when copying data in overlapping regions; sometimes you have to copy backward in order to avoid overwriting the data.</p>
<p class="indent">An interesting historical case was an early raster graphics terminal (see “<a href="ch06.xhtml#ch06lev2sec22">Raster Graphics</a>” on <a href="ch06.xhtml#page_180">page 180</a>) called the <em>blit</em>, designed by Canadian programmer Rob Pike at Bell Telephone Laboratories in the early 1980s, an era before it became practical to make custom integrated circuits to do this sort of thing. Source and destination data could overlap, such as in the case of dragging a window, and the data could be of any bit alignment. Performance was very important because processors weren’t very fast compared to today; the blit used a Motorola 68000. There was no MMU, so Pike wrote code that looked at the source and destination and generated optimal code on the fly to do the fastest copy. I did a similar implementation on a system that used the Motorola 68020. This achieved even better performance because the 68020 had an instruction cache into which the generated code <span epub:type="pagebreak" id="page_210"/>fit, so it didn’t have to keep accessing instruction memory. Note that this was a precursor to the JIT (just-in-time) techniques used in many virtual machines, including Java.</p>
<h3 class="h3" id="ch07lev1sec16"><strong>Vectored I/O</strong></h3>
<p class="noindent">Copying data efficiently is important for system performance, but avoiding copying altogether helps even more. A lot of data is moved through the operating system to and from user space programs, and this data is often not in contiguous memory.</p>
<p class="indent">For example, say we’re generating some audio data in the mp3 format that we want to write to an audio device. Like many file formats, mp3 files consist of a number of <em>frames</em>, each of which includes a <em>header</em> followed by some data. A typical audio file contains multiple frames that, in many cases, have identical headers, as shown in <a href="ch07.xhtml#ch07fig36">Figure 7-36</a>.</p>
<div class="image"><a id="ch07fig36"/><img src="../images/07fig36.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-36: mp3 frame layout</em></p>
<p class="indent">We could build each frame by copying all the data into a buffer, but then when we write that data to an audio device, we’ll have to copy it yet again. Alternatively, we could write each portion of each frame separately, but that would increase the context-switching overhead and might cause problems for an audio device if only a partial frame gets written.</p>
<p class="indent">It would be more efficient if we could just hand the system a set of pointers to each piece of the frame and let the system gather the pieces together as they’re written, as shown in <a href="ch07.xhtml#ch07fig37">Figure 7-37</a>. This is sufficiently worthwhile to justify system call (<span class="literal">readv</span>, <span class="literal">writev</span>) support.</p>
<div class="image"><a id="ch07fig37"/><img src="../images/07fig37.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-37: Data gather</em></p>
<p class="indent">The idea is to hand a vector of sizes and data pointers to the operating system, which then assembles them in order. There are versions for both <span epub:type="pagebreak" id="page_211"/>reading and writing: writing is known as <em>gathering</em> because data is collected from many places, while reading is known as <em>scattering</em> because data is dispersed to many places. The whole concept is called <em>scatter/gather</em>.</p>
<p class="indent">Scatter/gather became mainstream with the Berkeley networking code that became a foundation of the internet. I mentioned back in “<a href="ch06.xhtml#ch06lev3sec1">TCP/IP</a>” on <a href="ch06.xhtml#page_158">page 158</a> that IP data is sent in packets and TCP is responsible for making sure that the packets arrive and are in the correct order. Packets arriving from a communications endpoint (well, it might be a communications endpoint to you, but it’s a socket to me) are gathered into a contiguous stream for presentation to user programs.</p>
<h3 class="h3" id="ch07lev1sec17"><strong>Object-Oriented Pitfalls</strong></h3>
<p class="noindent">Since you’re learning to code, you may be learning an <em>object-oriented</em> language such as Java, C++, Python, or JavaScript. Object-oriented programming is a great methodology, but it can lead to performance issues if not used judiciously.</p>
<p class="indent">Object-oriented programming first gained serious traction with C++. C++ is an interesting case because it was initially built on top of C, which gives us an opportunity to see how it works.</p>
<p class="indent"><em>Objects</em> have <em>methods</em>, which are equivalent to functions, and <em>properties</em>, which are equivalent to data. Everything needed for an object can be collected into a single data structure. C’s support for type casting and pointers, especially pointers to functions, wins big here. A C structure for an object might look something like <a href="ch07.xhtml#ch07fig38">Figure 7-38</a>.</p>
<div class="image"><a id="ch07fig38"/><img src="../images/07fig38.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-38: A C structure for an object</em></p>
<p class="indent">Some properties, such as those with integer values (<span class="literal">property 1</span>), reside in the object structure itself, whereas others require additional memory allocation (<span class="literal">property 2</span>) that’s referenced by the object structure.</p>
<p class="indent">Clearly this structure could get quite large, especially if there are a lot of methods. We can address that by breaking the methods out into a separate structure—another space/time trade-off—as shown in <a href="ch07.xhtml#ch07fig39">Figure 7-39</a>.</p>
<span epub:type="pagebreak" id="page_212"/>
<div class="image"><a id="ch07fig39"/><img src="../images/07fig39.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-39: Separate method structure</em></p>
<p class="indent">Programmers used this sort of approach to object-oriented programming long before Danish programmer Bjarne Stroustrup invented C++. The original C++ was a wrapper around C that did things like this.</p>
<p class="indent">Why does this matter? Object-oriented ideologues believe that objects are the answer for everything. But as you can see in the previous figures, there’s a certain amount of overhead associated with objects. They have to carry around their own methods instead of using globally available functions. The upshot is that objects don’t pack as densely as pure data types, so stick to classic arrays when performance is paramount.</p>
<h3 class="h3" id="ch07lev1sec18"><strong>Sorting</strong></h3>
<p class="noindent">There are many reasons to sort data. Sometimes we just want sorted results, like when we alphabetize names to make them easier for people to find. Many times we want to store data in sorted form because it speeds up searching by reducing the number of memory accesses.</p>
<p class="indent">I’m not going to go into sorting algorithms in depth here, because it’s a pretty mature subject covered in many books. And plenty of good sort functions are available, so it’s not likely that you’ll need to write your own except as a homework problem. But there are a few important points to keep in mind.</p>
<p class="indent">One is that if the size of the things you’re sorting is larger than the size of a pointer, you should sort by rearranging the pointers to the data instead of by moving the data itself around.</p>
<p class="indent">Also, a convention for sorting has evolved. Our bingo parlor tree example enabled decisions based on an arithmetic comparison; we made decisions based on whether one number was less than, equal to, or greater than another. This method of decision making is rooted in the FORTRAN programming language from 1956, which included a statement that looked like <a href="ch07.xhtml#ch07list03">Listing 7-3</a>.</p>
<p class="programs">IF (expression) branch1, branch2, branch3</p>
<p class="listing" id="ch07list03"><em>Listing 7-3: A FORTRAN arithmetic <span class="codeitalic">IF</span> statement</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_213"/>This <span class="literal">IF</span> statement evaluated the <span class="literal">expression</span> and went to <span class="literal">branch1</span> if the result was less than zero, <span class="literal">branch2</span> if it was zero, and <span class="literal">branch3</span> if it was greater than zero; the branches are similar to what we saw in “<a href="ch04.xhtml#ch04lev2sec7">Branching</a>” on <a href="ch04.xhtml#page_105">page 105</a>.</p>
<p class="indent">Sorting numbers is straightforward. It would be nice to apply this same methodology to sorting other things. We saw back in <a href="ch07.xhtml#ch07fig10">Figure 7-10</a> that a list node can include arbitrary data; the same is true with tree nodes and other data structures.</p>
<p class="indent">UNIX version III introduced a library function called <span class="literal">qsort</span> that implemented a variation of the classic <em>quicksort</em> algorithm. The interesting thing about the <span class="literal">qsort</span> implementation is that although it knew how to sort things, it didn’t know how to compare them. Therefore, it took advantage of C’s pointers to functions; when calling <span class="literal">qsort</span> with a list of things to sort, you also provided a comparison function that returned <span class="literal">&lt;0</span>, <span class="literal">0</span>, or <span class="literal">&gt;0</span> for less than, equal to, or greater than, just like the FORTRAN arithmetic <span class="literal">IF</span>. This approach allowed the caller to use <span class="literal">qsort</span> to sort things however they wanted. For example, if a node contained both a name and an age, the supplied function could compare first by age and then name so that <span class="literal">qsort</span> would produce results organized by age first and name second. This approach worked well and has been copied by many other systems.</p>
<p class="indent">The standard C library string comparison function <span class="literal">strcmp</span> was designed with this in mind; it returns a value of less than, equal to, or greater than zero. This has also become the de facto way of doing things.</p>
<p class="indent">The original ASCII version of <span class="literal">strcmp</span> just walked the strings, subtracting the character of one from the other. It kept going if the value was zero and returned <span class="literal">0</span> if the end of the strings was reached. Otherwise, it returned the subtraction result.</p>
<p class="indent">This is all well and good if you’re just sorting to distribute data in a tree, but it falls apart if you’re sorting to put things into alphabetical order. It worked in the ASCII days—you can see in <a href="ch01.xhtml#ch01tab10">Table 1-10</a> that the numerical order and alphabetical order are the same. Where it falls apart is with support for other <em>locales</em>. A side effect of support for other languages coming later is that only the ASCII characters are numerically in the correct <em>collating order</em>, or language-specific sorting rules.</p>
<p class="indent">For example, what value should be assigned to the German letter <em>β</em>, the sharp <em>S</em> (<em>Eszett</em> or <em>scharfes S</em>)? Its Unicode value is 0x00DF. Because of that, the word <em>Straβe</em> would get sorted after the word <em>Strasse</em> using a vanilla string comparison. But these are actually different representations of the same word. The <em>β</em> is equivalent to <em>ss</em>. A string comparison that heeded the locale would say that the two words are equal.</p>
<h3 class="h3" id="ch07lev1sec19"><strong>Making a Hash of Things</strong></h3>
<p class="noindent">All the searching methods we’ve seen so far involve repeated testing while traversing a data structure. There’s another approach that performs better in some circumstances, called <em>hashing</em>. Hashing has many applications. We’re talking about in-memory storage and retrieval here, not mass storage. The general concept is to apply some <em>hash function</em> to the search keys that evenly splatter them onto the wall. If the hash function is easy to compute <span epub:type="pagebreak" id="page_214"/>and transforms a key into a splat in a unique location on the wall, then single-step lookup should be fast. Of course, there are some practical realities to consider.</p>
<p class="indent">Each splat represents the storage for the object associated with the key. The hash function must produce values that fit in memory. And it shouldn’t splatter things across too much memory or performance will suffer, both from using too much memory and from lack of locality of reference. Coming up with a perfect hash function isn’t really possible because we don’t have any prior knowledge of our keys.</p>
<p class="indent">One way to bound the storage is to have a hash function that maps keys into array indices. The array is called a <em>hash table</em>, shown in <a href="ch07.xhtml#ch07fig40">Figure 7-40</a>. The array elements are called <em>buckets</em>.</p>
<div class="image"><a id="ch07fig40"/><img src="../images/07fig40.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-40: Hashing</em></p>
<p class="indent">What makes a good hash function? It needs to be easy to compute, and it needs to distribute keys evenly into the buckets. A simple hash function that works pretty well for text is just to sum up the character values. That’s not quite enough, because the sum might produce an index that’s beyond the end of the hash table, but we can easily solve this by making the index the sum modulo the hash table size. Let’s look at how this works in practice. We’ll use a table size of 11; prime numbers make good table sizes because multiples of the sum end up in different buckets, improving the splatter pattern.</p>
<p class="indent">Say we have an application that keeps track of songs played at our favorite jam band concerts. Maybe it stores the last played date. We’ll just use the first word of each song name.</p>
<p class="indent">As you can see in <a href="ch07.xhtml#ch07fig41">Figure 7-41</a>, we start with <em>Hell</em> in a bucket—in this case, bucket 4. Next is <em>Touch</em> in bucket 9, followed by <em>Scarlet</em> in 3. But when we get to <em>Alligator</em>, we have a problem because the value of the hash function is the same as it was for <em>Scarlet</em>. This is called a <em>collision</em>.</p>
<div class="image"><a id="ch07fig41"/><img src="../images/07fig41.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-41: Hash collision</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_215"/>We solve this by replacing the buckets with <em>hash chains</em>, which in their simplest form are singly linked lists, as shown in <a href="ch07.xhtml#ch07fig42">Figure 7-42</a>.</p>
<div class="image"><a id="ch07fig42"/><img src="../images/07fig42.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-42: Hash chains</em></p>
<p class="indent">There are a number of trade-offs in hash chain management. We can just insert collisions at the head of the chain, as in <a href="ch07.xhtml#ch07fig42">Figure 7-42</a>, because it’s fast. But lookup can slow down as the chains get longer, so we could also do an insertion sort, which takes longer but means we don’t have to traverse a chain to the end to determine whether or not an item exists. There are also many different collision-handling methods—for example, eliminating hash chains and using some algorithm to find an empty slot in the table.</p>
<p class="indent">It’s difficult to pick a good hash table size without knowing the expected number of symbols in advance. You can keep track of chain length and grow the hash table if the chains are getting too long. This can be an expensive operation, but it can pay off because it doesn’t need to be done very often.</p>
<p class="indent">There are many variations on hash functions. The holy grail of hash functions is the <em>perfect hash</em>, which maps each key to a unique bucket. It’s pretty much impossible to create a perfect hash function unless all of the keys are known in advance, but mathematicians have come up with much better functions than the one used in this example.</p>
<h3 class="h3" id="ch07lev1sec20"><strong>Efficiency vs. Performance</strong></h3>
<p class="noindent">A lot of effort has gone into making efficient search algorithms. Much of this work was done in an era when computers were expensive. Performance and efficiency were linked.</p>
<p class="indent">The cost of electronics has plunged so dramatically that it’s almost impossible to purchase anything that doesn’t include a gratuitous blue LED. Performance and efficiency are decoupled; there are cases where better performance can be achieved by using less efficient algorithms on more processors than more efficient algorithms on fewer processors.</p>
<p class="indent"><span epub:type="pagebreak" id="page_216"/>One application of this decoupling is database <em>sharding</em>, also called <em>horizontal partitioning</em>. Sharding involves breaking up a database into multiple shards, each of which lives on its own machine, as shown in <a href="ch07.xhtml#ch07fig43">Figure 7-43</a>.</p>
<div class="image"><a id="ch07fig43"/><img src="../images/07fig43.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 7-43: Database sharding</em></p>
<p class="indent">Database operations requested over the interface are sent to all of the shards, and the results are assembled by a controller. This technique improves performance because operations are split across multiple workers.</p>
<p class="indent">A variation on sharding is called <em>MapReduce</em>, which essentially allows you to provide code to the controller for assembly of the intermediate results. This makes it possible to do operations such as “count the number of students in all math classes” without having to first request a list of students and then count them.</p>
<p class="indent">Databases aren’t the only application of this multiple processor approach. A historically interesting use is the Electronic Frontier Foundation’s DES (Data Encryption Standard) cracker built in 1998; see the book <em>Cracking DES</em> (O’Reilly, 1998) for the full story. A machine was constructed that used 1,856 custom processor chips, each of which tried a range of keys on the encrypted data. Any “interesting” results were forwarded to a controller for further analysis. This machine could test 90 billion keys per second.</p>
<h3 class="h3" id="ch07lev1sec21"><strong>Summary</strong></h3>
<p class="noindent">This chapter introduced you to a number of ways in which data can be organized to take advantage of what you’ve learned so far about computer hardware. In the next chapter, you’ll see how your programs get converted into forms that computer hardware can understand.</p>
</body></html>