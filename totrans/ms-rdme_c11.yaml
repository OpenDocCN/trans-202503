- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating Evolvable Architectures
  prefs: []
  type: TYPE_NORMAL
- en: Requirements volatility—changing customer demands—is an unavoidable challenge
    for software projects. Product requirements and context will change over time;
    your application must change as well. But changing requirements can cause instability
    and derail development.
  prefs: []
  type: TYPE_NORMAL
- en: Managers try to deal with requirements volatility using iterative development
    processes like Agile development (discussed in the next chapter). You can do your
    part to accommodate changing requirements by building *evolvable architectures*.
    Evolvable architectures eschew complexity, the enemy of evolvability.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will teach you techniques that can make your software simpler and
    thus easier to evolve. Paradoxically, achieving simplicity in software can be
    difficult; without conscious effort, code will grow tangled and complex. We’ll
    begin by describing complexity and how it leads to rigid and confusing codebases.
    We’ll then show you design principles that reduce complexity. Finally, we’ll translate
    these design principles into concrete API and data layer best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In *A Philosophy of Software Design* (Yaknyam Press, 2018), Stanford computer
    science professor John Ousterhout writes, “Complexity is anything related to the
    structure of a system that makes it hard to understand and modify the system.”
    Per Ousterhout, complex systems have two characteristics: high *dependency* and
    high *obscurity*. We add a third: high *inertia*.'
  prefs: []
  type: TYPE_NORMAL
- en: High *dependency* leads software to rely on other code’s API or behavior. Dependency
    is obviously unavoidable and even desirable, but a balance must be struck. Every
    new connection and assumption makes code harder to change. High-dependency systems
    are hard to modify because they have *tight coupling* and high *change amplification*.
    Tight coupling describes modules that depend heavily on one another. It leads
    to high change amplification, where a single change requires modifications in
    dependencies as well. Thoughtful API design and a restrained use of abstraction
    will minimize tight coupling and change amplification.
  prefs: []
  type: TYPE_NORMAL
- en: High *obscurity* makes it difficult for programmers to predict a change’s side
    effects, how code behaves, and where changes need to be made. Obscure code takes
    longer to learn, and developers are more likely to inadvertently break things.
    *God objects*that “know” too much, global state that encourages side effects,
    excessive indirection that obscures code, and *action at distance* that affects
    behavior in distant parts of the program are all symptoms of high obscurity. APIs
    with clear contracts and standard patterns reduce obscurity.
  prefs: []
  type: TYPE_NORMAL
- en: '*Inertia*, the characteristic that we’ve added to Ousterhout’s list, is software’s
    tendency to stay in use. Easily discarded code used for a quick experiment has
    low inertia. A service that powers a dozen business-critical applications has
    high inertia. Complexity’s cost accrues over time, so high-inertia, high-change
    systems should be simplified, while low-inertia or low-change systems can be left
    complex (as long as you discard them or continue to leave them alone).'
  prefs: []
  type: TYPE_NORMAL
- en: Complexity cannot always be eliminated, but you can choose where to put it.
    Backward-compatible changes (discussed later) might make code simpler to use but
    more complicated to implement. Layers of indirection to decouple subsystems reduce
    dependency but increase obscurity. Be thoughtful about when, where, and how to
    manage complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Design for Evolvability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Faced with unknown future requirements, engineers usually choose one of two
    tactics: they try to guess what they’ll need in the future, or they build abstractions
    as an escape hatch to make subsequent code changes easier. Don’t play this game;
    both approaches lead to complexity. Keep things simple (known as KISS—*keep it
    simple, stupid*—thanks to the US Navy’s penchant for acronyms and tough love).
    Use the KISS mnemonic to remember to build with simplicity in mind. Simple code
    lets you add complexity later, when the need becomes clear and the change becomes
    unavoidable.'
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to keep code simple is to avoid writing it altogether. Tell
    yourself that *you ain’t gonna need it* *(YAGNI**)*. When you do write code, use
    the principle of least astonishment and encapsulation. These design principles
    will keep your code easy to evolve.
  prefs: []
  type: TYPE_NORMAL
- en: You Ain’t Gonna Need It
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'YAGNI is a deceptively simple design practice: don’t build things you don’t
    need. YAGNI violations happen when developers get excited, fixated, or worried
    about some aspect of their code. It’s difficult to predict what you’ll need and
    what you won’t. Every wrong guess is wasted effort. After the initial waste, the
    code continues to bog things down, it needs to be maintained, developers need
    to understand it, and it must be built and tested.'
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, there are a few habits you can build to avoid unnecessary development.
    Avoid premature optimization, unnecessarily flexible abstractions, and product
    features that aren’t needed for a *minimum viable product* *(MVP**)*—the bare-minimum
    feature set that you need to get user feedback.
  prefs: []
  type: TYPE_NORMAL
- en: '*Premature optimization* occurs when a developer adds a performance optimization
    to code before it’s proven to be needed. In the classic scenario, a developer
    sees an area of code that could be made faster or more scalable by adding complex
    logic and architectural layers such as caches, sharded databases, or queues. The
    developer optimizes the code before it’s been shipped, before anyone has used
    it. After shipping the code, the developer discovers that the optimization was
    not needed. Removing optimization never happens, and complexity accrues. Most
    performance and scalability improvements come with a high complexity cost. For
    example, a cache is fast, but it can also become inconsistent with underlying
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Flexible abstractions—plugin architectures, wrapper interfaces, and generic
    data structures like key-value pairs—are another temptation. Developers think
    they can easily adjust if some new requirement pops up. But abstraction comes
    with a cost; it boxes implementations into rigid boundaries that the developer
    ends up fighting later. Flexibility also makes code harder to read and understand.
    Take this distributed queuing interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`IDistributedQueue` looks simple enough: you send and receive messages. But
    what if the underlying queue supports both keys and values for a message (as Apache
    Kafka does) or message ACKing (as Amazon’s Simple Queue Service does)? Developers
    are faced with a choice: Should the interface be the union of all features from
    all message queues or the intersection of all features? The union of all features
    produces an interface where no single implementation works for all methods. The
    intersection of all features leads to a limited interface that doesn’t have enough
    features to be useful. You’re better off directly using an implementation. You
    can refactor later if you decide you need to support another implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: The best way to keep your code flexible is to simply have less of it. For everything
    you build, ask yourself what is absolutely necessary, and throw away the rest.
    This technique—called *Muntzing*—will keep your software trim and adaptable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding cool new product features is also tempting. Developers talk themselves
    into the cool-feature pitfall for a variety of reasons: they mistake their usage
    for what most users want, they think it’ll be easy to add, or they think it’ll
    be neat! Each new feature takes time to build and maintain, and you don’t know
    if the feature will actually be useful.'
  prefs: []
  type: TYPE_NORMAL
- en: Building an MVP will keep you honest about what you really need. MVPs allow
    you to test an idea without investing in a full-fledged implementation.
  prefs: []
  type: TYPE_NORMAL
- en: There are, of course, caveats to YAGNI. As your experience grows, you’ll get
    better at predicting when flexibility and optimization are necessary. In the meantime,
    place interface shims where you suspect optimizations can be inserted, but don’t
    actually implement them. For example, if you are creating a new file format and
    suspect you’ll need compression or encryption later, provide a header that specifies
    the encoding, but only implement the uncompressed encoding. You can add compression
    in the future, and the header will make it easy for new code to read older files.
  prefs: []
  type: TYPE_NORMAL
- en: Principle of Least Astonishment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The *principle of least astonishment* is pretty clear: don’t surprise users.
    Build features that behave as users first expect. Features with a high learning
    curve or strange behavior frustrate users. Similarly, don’t surprise developers.
    Surprising code is obscure, which causes complexity. You can eliminate surprises
    by keeping code specific, avoiding implicit knowledge, and using standard libraries
    and patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: Anything nonobvious that a developer needs to know to use an API and is not
    part of the API itself is considered *implicit knowledge*. APIs that require implicit
    knowledge will surprise developers, causing bugs and a high learning curve. Two
    common implicit knowledge violations are hidden ordering requirements and hidden
    argument requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '*Ordering requirements* dictate that actions take place in a specific sequence.
    Method ordering is a frequent violation: method A must be called before method
    B, but the API allows method B to be called first, surprising the developer with
    a runtime error. Documenting an ordering requirement is good, but it’s better
    not to have one in the first place. Avoid method ordering by having methods invoke
    submethods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'There are other approaches to avoiding ordering: combining the methods into
    one, using the *builder pattern*, using the type system and having `pontoonWorples`
    work only on `FlubberizedWorples` rather than all `Worples`, and so on. All of
    these are better than requiring your user to know about hidden requirements. If
    nothing else, you can at least make the method name give developers a heads-up
    by calling it `pontoonFlubberizedWorples()`. Counterintuitively, short method
    and variable names actually increase cognitive load. Specific, longer method names
    are more descriptive and easier to understand.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hidden argument requirements* occur when a method signature implies a wider
    range of valid inputs than the method actually accepts. For example, accepting
    an `int` while only allowing numbers 1 to 10 is a hidden constraint. Requiring
    that a certain value field is set in a plain JSON object is also requiring implicit
    knowledge on the part of the user. Make argument requirements specific and visible.
    Use specific types that accurately capture your constraints; when using flexible
    types like JSON, consider using JSON Schema to describe the expected object. At
    the least, advertise argument requirements in documentation when they can’t be
    made programmatically visible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, use standard libraries and development patterns. Implementing your
    own square root method is surprising; using a language’s built-in `sqrt()` method
    is not. The same rule applies for development patterns: use idiomatic code style
    and development patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: Encapsulate Domain Knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Software changes as business requirements change. Encapsulate domain knowledge
    by grouping software based on business domain—accounting, billing, shipping, and
    so on. Mapping software components to business domains will keep code changes
    focused and clean.
  prefs: []
  type: TYPE_NORMAL
- en: Encapsulated domains naturally gravitate toward *high cohesion* and *low coupling*—desirable
    traits. Highly cohesive software with low coupling is more evolvable because changes
    tend to have a smaller “blast radius.” Code is highly *cohesive* when methods,
    variables, and classes that relate to one another are near each other in modules
    or packages. *Decoupled* code is self-contained; a change to its logic does not
    require changes to other software components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Developers often think about software in terms of layers: frontend, middle
    tier, and backend. Layered code is grouped according to technical domain, with
    all the UI code in one place and all object persistence in another. Grouping code
    by technical domain works great within a single business domain but grows messy
    as businesses grow. Separate teams form around each tier, increasing coordination
    cost since every business logic change slices through all tiers. And shared horizontal
    layers make it too easy for developers to mix business logic between domains,
    leading to complex code.'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying domain boundaries and encapsulating domain knowledge is as much
    art as science. There is an entire architectural approach called *Domain-Driven
    Design* *(DDD**)*, which defines an extensive set of concepts and practices to
    map business concepts to software. Full-blown DDD is necessary only for the most
    complex situations. Still, familiarizing yourself with DDD will help you make
    better design decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Evolvable APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As requirements change, you’ll need to change your APIs, the shared interfaces
    between code. Changing an API is easy to do, but it’s hard to do right. Many small,
    rational changes can lead to a sprawling mess. Worse, a minor API change can completely
    break compatibility. If an API changes in an incompatible way, clients will break;
    these breakages may not be immediately obvious, especially changes that break
    at runtime, not during compilation. APIs that are small, clearly defined, compatible,
    and versioned will be easier to use and evolve.
  prefs: []
  type: TYPE_NORMAL
- en: Keep APIs Small
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Small APIs are easier to understand and evolve. Larger APIs put more cognitive
    load on developers, and you’ll have more code to document, support, debug, and
    maintain. Every new method or field grows the API and locks you further into a
    specific usage pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the YAGNI philosophy: only add API methods or fields that are immediately
    needed. When creating an API data model, only add methods you need at the time.
    When bootstrapping your API using a framework or generator tool, eliminate fields
    or methods you’re not using.'
  prefs: []
  type: TYPE_NORMAL
- en: API methods with a lot of fields should have sensible defaults. Developers can
    focus on relevant fields knowing they’ll inherit defaults for the others. Defaults
    make large APIs feel small.
  prefs: []
  type: TYPE_NORMAL
- en: Expose Well-Defined Service APIs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Evolvable systems have clearly defined request and response schemas that are
    versioned and have clear compatibility contracts. Schema definitions should be
    published so they can be used to automatically test both client and server code
    (see “Package Different Resources Separately” in Chapter 8 for more).
  prefs: []
  type: TYPE_NORMAL
- en: Use standard tools to define service APIs. A well-defined service will declare
    its schemas, request and response methods, and exceptions. OpenAPI is commonly
    used for RESTful services, while non-REST services use Protocol Buffers, Thrift,
    or a similar *interface definition language* *(IDL**)*. Well-defined service APIs
    make compile-time validation easier and keep clients, servers, and documentation
    in sync.
  prefs: []
  type: TYPE_NORMAL
- en: Interface definition tools come with code generators that convert service definitions
    to client and server code. Documentation can also be generated, and test tools
    can use IDLs to generate stubs and mock data. Some tools even have discoverability
    features to find services, learn who maintains them, and show how the service
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: Use your company’s API definition framework if they’ve already chosen one; choosing
    a “better” one will require too much interoperability work. If your company still
    hand-rolls REST APIs and the JSON interfaces are evolved in code without a formal
    spec, your best bet is OpenAPI, as it can be retrofitted on preexisting REST services
    and does not require major migrations to adopt.
  prefs: []
  type: TYPE_NORMAL
- en: Keep API Changes Compatible
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Keeping API changes compatible lets client and server versions evolve independently.
    There are two forms of compatibility to consider: forward and backward.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Forward-compatible* changes allow clients to use a new version of an API when
    invoking an older service version. A web service that’s running version 1.0 of
    an API but can receive calls from a client using version 1.1 of the API is forward
    compatible.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Backward-compatible* changes are the opposite: new versions of the library
    or service do not require changes in older client code. A change is backward compatible
    if code developed against version 1.0 of an API continues to compile and run when
    used with version 1.1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at a simple gRPC Hello World service API defined with Protocol
    Buffers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `Greeter` service has one method, called `SayHello`, which receives a `HelloRequest`
    and returns a `HelloReply` with a fun message about `favorite_number`. The numbers
    next to each field are *field ordinals*; Protocol Buffers internally refer to
    fields using numbers rather than strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we decide to send our greeting over email. We’d need to add an email
    field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is a *backward-incompatible* change because old clients don’t supply an
    email. When a client invokes the new `SayHello` with the old `HelloRequest`, the
    email will be missing, so the service can’t parse the request. Removing the `required`
    keyword and skipping emails when no address is supplied will maintain backward
    compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: Required fields are such an evolvability problem that they were removed from
    Protocol Buffers v3\. Kenton Varda, the primary author of Protocol Buffers v2,
    said, “The ‘required’ keyword in Protocol Buffers turned out to be a horrible
    mistake” ([https://capnproto.org/faq.html#how-do-i-make-a-field-required-like-in-protocol-buffers](https://capnproto.org/faq.html#how-do-i-make-a-field-required-like-in-protocol-buffers)).
    Many other systems have required fields, so be careful, and remember, “Required
    is forever.”
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a *forward-incompatible* change by tweaking `HelloRequest`. Perhaps
    we want to accommodate negative favorite numbers. The Protocol Buffer documentation
    reads, “If your field is likely to have negative values, use `sint32` instead,”
    so we change the type of `favorite_number` accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Changing `int32` to `sint32` is both backward and forward incompatible. A client
    with the new `HelloRequest` will encode `favorite_number` using a different serialization
    scheme than an old version of `Greeter`, so the service will fail to parse it;
    and the new version of `Greeter` will fail to parse messages from old clients!
  prefs: []
  type: TYPE_NORMAL
- en: The `sint32` change can be made forward compatible by adding a new field. Protocol
    Buffers let us rename fields as long as the field number remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The server code needs to handle both fields for as long as the old clients are
    supported. Once rolled out, we can monitor how often clients use the deprecated
    field and clean up once clients upgrade or a deprecation schedule expires.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our example uses Protocol Buffers because they have a strongly typed system,
    and type compatibility is more straightforward to demonstrate and reason about.
    The same problems occur in other contexts, including “simple” REST services. When
    client and server content expectations diverge, errors crop up no matter what
    format you’re using. Moreover, it’s not just the message fields you need to worry
    about: a change in *semantics* of the message, or the logic of what happens when
    certain events transpire, can also be backward or forward incompatible.'
  prefs: []
  type: TYPE_NORMAL
- en: Version APIs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As APIs evolve over time, you will need to decide how to handle compatibility
    across multiple versions. Fully backward- and forward-compatible changes interoperate
    with all previous and future versions of an API; this can be hard to maintain,
    creating *cruft* like the logic for dealing with deprecated fields. Less stringent
    compatibility guarantees allow for more radical changes.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, you’ll want to change your API in a way that isn’t compatible with
    old clients—requiring a new field, for example. Versioning your APIs means you
    introduce a new version when changes are made. Old clients can continue using
    the old API version. Tracking versions also helps you communicate with your customers—they
    can tell you what version they’re using, and you can market new features with
    a new version.
  prefs: []
  type: TYPE_NORMAL
- en: 'API versions are usually managed with an API gateway or a service mesh. Versioned
    requests are routed to the appropriate service: a v2 request will be routed to
    a v2.X.X service instance, while a v3 request will be routed to a v3.X.X service
    instance. Absent a gateway, clients invoke RPC calls directly to version-specific
    service hosts, or a single service instance runs multiple versions internally.'
  prefs: []
  type: TYPE_NORMAL
- en: API versioning comes with a cost. Older major versions of the service need to
    be maintained, and bug fixes need to be backported to prior versions. Developers
    need to keep track of which versions support which features. Lack of version management
    tooling can push version management on to engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Be pragmatic about versioning methodologies. Semantic versioning, discussed
    in Chapter 5, is a common API versioning scheme, but many companies version APIs
    using dates or other numeric schemes. Version numbers can be specified in URI
    paths, query parameters, or HTTP Accept headers, or using a myriad of other techniques.
    There are trade-offs between all of these approaches, and a lot of strong opinions.
    Defer to whatever the standard is at your company; if there isn’t one, ask your
    manager and tech leads for their thoughts on what’s best.
  prefs: []
  type: TYPE_NORMAL
- en: Keep documentation versioned along with your APIs. Developers dealing with older
    versions of your code need accurate documentation. It’s really confusing for users
    to reference a different version of the documentation and discover that the API
    they’re using doesn’t match up. Committing API documentation in the main code
    repository helps to keep documentation and code in sync. Pull requests for code
    can update documentation as they change APIs and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: API versioning is most valuable when client code is hard to change. You’ll usually
    have the least control over external (customer) clients, so customer-facing APIs
    are the most important to version. If your team controls both the service and
    client, you might be able to get away without internal API versioning.
  prefs: []
  type: TYPE_NORMAL
- en: Evolvable Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: APIs are more ephemeral than persisted data; once the client and server APIs
    are upgraded, the work is done. Data must be evolved as applications change. Data
    evolution runs the gamut from simple schema changes such as adding or removing
    a column to rewriting records with new schemas, fixing corruption, rewriting to
    match new business logic, and massive migrations from one database to another.
  prefs: []
  type: TYPE_NORMAL
- en: Isolating databases and using explicit schemas will make data evolution more
    manageable. With an isolated database, you need only worry about the impact of
    a change on your own application. Schemas protect you from reading or writing
    malformed data, and automated schema migrations make schema changes predictable.
  prefs: []
  type: TYPE_NORMAL
- en: Isolate Databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Shared databases are difficult to evolve and will result in a loss of *autonomy*—a
    developer’s or team’s ability to make independent changes to the system. You will
    not be able to safely modify schemas, or even read and write, without worrying
    about how everyone is using your database. The architecture grows brittle as schemas
    become an unofficial, deeply entrenched API. Separate application databases make
    changes easier.
  prefs: []
  type: TYPE_NORMAL
- en: Isolated databases are accessed by only a single application, while shared databases
    are accessed by more than one (see [Figure 11-1](#figure11-1)).
  prefs: []
  type: TYPE_NORMAL
- en: '![f11001](image_fi/501836c11/f11001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-1: Shared databases'
  prefs: []
  type: TYPE_NORMAL
- en: Shared databases present several problems. Applications with shared databases
    can grow to depend directly on each other’s data. Applications act as a control
    point for the underlying data they serve. You can’t apply business logic on your
    raw data before serving it, and you can’t easily redirect queries to a new data
    store during a migration if queries bypass your application. If there are multiple
    applications writing, the meaning (*semantics*) of the data might diverge, making
    it harder for readers to reason about. Application data is not protected, so other
    applications might mutate it in unexpected ways. Schemas aren’t isolated; a change
    in one application’s schema can impact others. Nor is performance isolated, so
    if an application overwhelms the database, all other applications will be affected.
    In some cases, security boundaries might be violated.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, isolated databases have just a single reader and writer (see [Figure
    11-2](#figure11-2)). All other traffic goes through remote procedural calls.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11002](image_fi/501836c11/f11002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-2: Isolated databases'
  prefs: []
  type: TYPE_NORMAL
- en: Isolated databases afford you all of the flexibility and isolation that shared
    databases do not. You need only worry about your own application when making database
    schema changes, and database performance is governed by your usage.
  prefs: []
  type: TYPE_NORMAL
- en: There are occasional cases where a shared database is valuable. When breaking
    a monolith up, sharing a database serves as a useful intermediate step before
    data has been migrated to a new isolated database. Managing many databases comes
    at a high operational cost. Early on, it might make sense to co-locate many databases
    on the same machines. But make sure any shared databases eventually get isolated
    and split up or replaced.
  prefs: []
  type: TYPE_NORMAL
- en: Use Schemas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rigid predefined columns and types, and the heavyweight processes for evolving
    them, have led to the emergence of popular *schemaless* data management. Most
    modern datastores support storing JSON or JSON-like objects without predeclaring
    its structure. Schemaless doesn’t literally mean “no schema” (data would be unusable);
    rather, schemaless data has an implicit schema that is supplied or inferred at
    read time.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we’ve found that a schemaless approach has significant data integrity
    and complexity problems. A strongly typed schema-forward approach decreases the
    obscurity, and therefore complexity, of your application. The short-term simplicity
    is not usually worth the obscurity trade-off. Like code itself, data is sometimes
    described as “write once, read many”; use schemas to make reads easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’d think not having a schema would make a change easier: you simply start
    or stop writing fields as you need to evolve data. Schemaless data actually makes
    changes harder because you don’t know what you’re breaking as you evolve your
    data. Data quickly becomes an unintelligible hodgepodge of different record types.
    Developers, business analysts, and data scientists struggle to keep up. It’s going
    to be a tough day for the data scientist that wants to parse this JSON data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Defining explicit schemas for your data will keep your application stable and
    make your data usable. Explicit schemas let you sanity-check data as it is written.
    Parsing data using explicit schemas is usually faster, too. Schemas also help
    you detect when forward- and backward-incompatible changes are made. Data scientists
    know what to expect with data in the following table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The rigidity of explicit schemas also carries a cost: they can be difficult
    to change. This is by design. Schemas force you to slow down and think through
    how existing data is going to be migrated and how downstream users will be affected.'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t hide schemaless data inside schematized data. It’s tempting to be lazy
    and stuff a JSON string into a field called “data” or define a map of strings
    to contain arbitrary key-value pairs. Hiding schemaless data is self-defeating;
    you get all of the pain of explicit schemas but none of the gain.
  prefs: []
  type: TYPE_NORMAL
- en: There are some cases where a schemaless approach is warranted. If your primary
    goal is to move fast—perhaps before you know what you need, when you are iterating
    rapidly, or when old data has little to no value—a schemaless approach lets you
    cut corners. Some data is legitimately nonuniform; some records have certain fields
    that others don’t. Flipping data from explicit to implicit schema is also a helpful
    trick when migrating data; you might temporarily make data schemaless to ease
    the transition to a new explicit schema.
  prefs: []
  type: TYPE_NORMAL
- en: Automate Schema Migrations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Changing a database’s schema is dangerous. A minor tweak—adding an index or
    dropping a column—can cause the entire database or application to grind to a halt.
    Managing database changes by manually executing *database description language*
    *(DDL**)* commands directly on the database is error prone. Database schemas in
    different environments diverge, the state of the database is uncertain, no one
    knows who changed what when, and performance impacts are unclear. The mix of error-prone
    changes and the potential for major downtime is an explosive combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Database schema management tools make database changes less error prone. Automated
    tooling does two things for you: it forces you to track the entire history of
    a schema, and it gives you tools to migrate a schema from one version to another.
    Track schema changes, use automated database tools, and work with your database
    team to manage schema evolution.'
  prefs: []
  type: TYPE_NORMAL
- en: The entire history of a schema is usually kept in a series of files defining
    every change from the initial creation of a schema all the way to its current
    form. Tracking DDL changes in files helps developers see how the schema has changed
    over time. Files tracked in a version control system will show who made which
    changes, when, and why. Pull requests will afford the opportunity for schema reviews
    and linting.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can take our users table from the “Use Schemas” section and put it in a
    versioned file for a schema migration tool like Liquibase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then define an `ALTER` in a separate block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Liquibase can use these files to upgrade or downgrade schemas through the CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If Liquibase is pointed at an empty database, it will run both the `CREATE`
    and `ALTER` commands. If it’s pointed at a database where the `CREATE` has already
    been executed, it will run only the `ALTER`. Tools like Liquibase often track
    the current version of a database schema in special metadata tables in the database
    itself, so don’t be surprised if you find tables with names like `DATABASECHANGELOG`
    or `DATABASECHANGELOGLOCK`.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, the Liquibase command is run from the command line,
    usually by a *database administrator* *(DBA**)*. Some teams will automate the
    execution itself through a commit hook or a web UI.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t couple database and application lifecycles. Tying schema migrations to
    application deployment is dangerous. Schema changes are delicate and can have
    serious performance implications. Separating database migrations from application
    deployment lets you control when schema changes go out.
  prefs: []
  type: TYPE_NORMAL
- en: Liquibase is just one tool that can manage database migrations; there are others
    like Flyway and Alembic. Many *object-resource mappers* *(ORMs**)* come with schema
    migration managers as well. If your company already has one in place, use it;
    if not, work with the team to figure out what to use. Once selected, use the database
    migration system for all the changes; circumventing it will negate the tool’s
    benefit since reality has diverged from what’s tracked.
  prefs: []
  type: TYPE_NORMAL
- en: More sophisticated database operations tools also exist. Tools like GitHub’s
    gh-ost and Percona’s pt-online-schema-change help database administrators run
    large schema changes without impacting performance. Other tools like Skeema and
    Square’s Shift provide more sophisticated versioning that lets you “diff” database
    schemas and automatically derive changes. All of these tools help make database
    evolution safer.
  prefs: []
  type: TYPE_NORMAL
- en: Most migration tools support rollbacks, which undo a migration’s changes. Rollbacks
    can only do so much, so be careful. For example, rolling back a column deletion
    will recreate a column, but it will not recreate the data that used to be stored
    in that column! Backing up a table prior to deletion is prudent.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the permanent and large-scale nature of these types of changes, organizations
    will often have specific subteams responsible for ensuring the changes are done
    correctly. These might be DBAs, operations or SREs, or a set of senior engineers
    familiar with the tools, performance implications, and application-specific concerns.
    These teams are a great resource for understanding the nuances of data storage
    systems; learn from them.
  prefs: []
  type: TYPE_NORMAL
- en: Maintain Schema Compatibility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data written to disk has the same compatibility problems that APIs have. Like
    APIs, the reader and writer of the data can change independently; they might not
    be the same software and might not be on the same machine. And like APIs, data
    has a schema with field names and types. Changing schemas in a forward- or backward-incompatible
    way can break applications. Use schema compatibility checks to detect incompatible
    changes, and use data products to decouple internal and external schemas.
  prefs: []
  type: TYPE_NORMAL
- en: Developers think databases are an implementation detail that’s hidden from other
    systems. Fully encapsulated databases are ideal but not often realized in practice.
    Even if a production database is hidden behind an application, the data is often
    exported to data warehouses.
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouses are databases used for analytic and reporting purposes. Organizations
    set up an *extract, transform, load* *(ETL**)* data pipeline that extracts data
    from production databases and transforms and loads it into a data warehouse (see
    [Figure 11-3](#figure11-3)).
  prefs: []
  type: TYPE_NORMAL
- en: '![f11003](image_fi/501836c11/f11003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-3: ETL data pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: ETL pipelines depend heavily on database schemas. Simply dropping the column
    in a production database could cause the entire data pipeline to grind to a halt.
    Even if dropping a column doesn’t break the data pipeline, downstream users might
    be using the field for reporting, machine learning models, or ad hoc queries.
  prefs: []
  type: TYPE_NORMAL
- en: Other systems might also depend on your database schemas. *Change data capture*
    *(CDC**)* is an event-based architecture that converts insert, update, and delete
    operations into messages for downstream consumers. An insert into a “members”
    table might trigger a message that an email service uses to send an email to the
    new user. Such messages are an implicit API, and making backward-incompatible
    schema changes can break other services.
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouse pipelines and downstream users must be protected from breaking
    schema changes. Validate that your schema changes are safe before executing them
    in production. Compatibility checks should be done as early as possible, ideally
    at code commit time by inspecting database DDL statements. Executing DDL statements
    in a preproduction integration testing environment, if one exists, can also protect
    changes. Run your DDL statements and integration tests to verify that downstream
    systems don’t break.
  prefs: []
  type: TYPE_NORMAL
- en: You can also protect internal schemas by exporting a *data product* that explicitly
    decouples internal schemas from downstream users. Data products map internal schemas
    to separate user-facing schemas; the development team owns both the production
    database and the published data products. Separate data products, which might
    simply be database views, allow teams to maintain compatibility with data consumers
    without having to freeze their internal database schemas.
  prefs: []
  type: TYPE_NORMAL
- en: Do’s and Don’ts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| **Do’s** | **Don’ts** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** remember YAGNI: “You Ain’t Gonna Need It.” | **DON’T** build too many
    abstractions without purpose. |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** use standard libraries and development patterns. | **DON’T** write
    methods with hidden ordering or argument requirements. |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** use an IDL to define your APIs. | **DON’T** surprise other developers
    with exotic code. |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** version external APIs and documentation. | **DON’T** make incompatible
    API changes. |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** isolate application databases from each other. | **DON’T** be dogmatic
    about internal API versioning. |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** define explicit schemas for all your data. | **DON’T** embed schemaless
    data in string or byte fields. |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** use migration tools to automate database schema management. |  |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** maintain schema compatibility if downstream consumers use your data.
    |  |'
  prefs: []
  type: TYPE_TB
- en: Level Up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A book-length treatment on evolvable architectures can be found in *Building
    Evolutionary Architectures*, written by Neal Ford, Rebecca Parsons, and Patrick
    Kua (O’Reilly Media, 2017). For more depth on evolvable APIs and data, see their
    work. They discuss DDD briefly; for the full experience, refer to *Implementing
    Domain Driven Design* by Vaughn Vernon (Addison-Wesley Professional, 2013).
  prefs: []
  type: TYPE_NORMAL
- en: We cite John Ousterhout’s work on complexity in the beginning of the chapter.
    See his excellent (and short) book, *A Philosophy of Software Design* (Yaknyam
    Press, 2018), to learn more about complexity and how to manage it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zach Tellman’s *Elements of Clojure* ([lulu.com](http://lulu.com), 2019) is
    a wonderful book that has only four chapters: “Names,” “Idioms,” “Indirection,”
    and “Composition.” It is a lucid, concise discussion of these four topics, which
    will help you build evolvable architectures (even if you never touch a line of
    Clojure).'
  prefs: []
  type: TYPE_NORMAL
- en: Richard Hickey has a beautiful talk called *Simple Made Easy* ([https://www.youtube.com/watch?v=oytL881p-nQ](https://www.youtube.com/watch?v=oytL881p-nQ)).
    Hickey’s talk discusses simplicity, complexity, “easiness,” and how to build good
    software. The talk is a must-watch.
  prefs: []
  type: TYPE_NORMAL
- en: '*Data Mesh: Building a Next Generation Data Architecture* by Zhamak Dehghani
    (coming out end of 2021) contains a deeper discussion of data products.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Designing Data-Intensive Applications* by Martin Kleppman (O’Reilly Media,
    2017) is an excellent book that covers, among other things, the subjects of data
    evolution, data schemas, IDLs, and change data capture. This book is an instant
    classic, and we highly recommend it.'
  prefs: []
  type: TYPE_NORMAL
