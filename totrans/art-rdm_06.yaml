- en: '**6'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**6'
- en: MACHINE LEARNING**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: The goal of machine learning is to train models to generate correct outputs
    when given previously unseen inputs. This is usually done by repeatedly presenting
    the model with a collection of known inputs and outputs until the model succeeds
    in properly assigning outputs to inputs, or *learns*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的目标是训练模型，在给定以前未见过的输入时生成正确的输出。这通常通过反复向模型提供一组已知的输入和输出，直到模型成功地将输出正确地分配给输入，或者*学习*成功为止。
- en: In this chapter, we’ll explore randomness in machine learning by building two
    datasets for histology slides and images of handwritten digits. As we’ll learn,
    randomness is essential to building suitable machine learning datasets.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过构建两个数据集来探讨机器学习中的随机性，这些数据集分别用于组织学切片和手写数字图像。正如我们将要学习的，随机性在构建合适的机器学习数据集时至关重要。
- en: Next, we’ll explore randomness in neural networks—the driving force behind the
    AI revolution. We’ll restrict ourselves to traditional neural network architectures;
    randomness is just as important, if not more so, when working with advanced models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索神经网络中的随机性——这是人工智能革命的驱动力。我们将专注于传统的神经网络架构；在处理高级模型时，随机性同样重要，甚至更为关键。
- en: After neural networks come *extreme learning machines*, simple neural networks
    that fundamentally depend on randomness. Unlike their grown-up cousins, extreme
    learning machines don’t require extensive training, but instead rely on the power
    of randomness to do most of the learning for them.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络之后是*极限学习机*，这是一种简单的神经网络，基本上依赖于随机性。与它们的“成年”版本不同，极限学习机不需要大量训练，而是依赖随机性的力量来完成大部分学习任务。
- en: '*Random forests* close out the chapter, and are also critically dependent on
    randomness for their success.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机森林*将作为本章的结尾，它们的成功也极度依赖于随机性。'
- en: I’ll point out where randomness appears as we move through the chapter. Randomness
    is central to the success of machine learning, from your favorite smart speaker
    to the self-driving car you may (someday soon) be riding in.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在本章中指出随机性出现的地方。随机性是机器学习成功的核心，从你最喜欢的智能音响到你可能（不久之后）会乘坐的自动驾驶汽车，随机性都起到了关键作用。
- en: '**Datasets**'
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**数据集**'
- en: In machine learning, we train models from sample data. Therefore, we must build
    datasets before beginning our explorations. Randomness plays a critical role in
    this process.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们从样本数据中训练模型。因此，在开始探索之前，我们必须构建数据集。随机性在这一过程中起着至关重要的作用。
- en: We’ll build two datasets. The first consists of measurements of cells from histology
    slides that hopefully enable the model to learn whether the tissue sample is benign
    (class 0) or malignant (class 1).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建两个数据集。第一个由组织学切片中细胞的测量数据组成，旨在帮助模型学习组织样本是良性（类别0）还是恶性（类别1）。
- en: 'The second dataset consists of 28×28-pixel grayscale images of handwritten
    digits: 1, 4, 7, and 9\. The images aren’t stored in the usual format; rather,
    they’re unraveled into vectors where the first row is followed by the second,
    and so on, to map the 28×28 pixels to 784-element vectors.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个数据集由28×28像素的手写数字图像组成：1、4、7和9。图像不是以常规格式存储的；相反，它们被展开成向量，第一行图像接着第二行，以此类推，从而将28×28像素映射到784维向量。
- en: '***Histology Slide Data***'
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***组织学切片数据***'
- en: Machine learning etiquette dictates that training a model requires a minimum
    of two datasets. The first is the *training set*, a collection of pairs, (***x***,
    *y*), where ***x*** are input vectors and *y* are the corresponding output labels.
    The second is a *test set*, of the same kind as the training set, but it’s not
    used until training is complete. The model’s performance on the test set decides
    how well it’s learned.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的礼仪规定，训练一个模型至少需要两个数据集。第一个是*训练集*，它由成对的（***x***，*y*）组成，其中***x***是输入向量，*y*是相应的输出标签。第二个是*测试集*，它与训练集的性质相同，但直到训练完成后才使用。模型在测试集上的表现决定了它学习得如何。
- en: The *raw* directory holds the *bc_data.npy* and *bc_labels.npy* files. The first
    is a dataset that contains a two-dimensional NumPy array of 569 rows and 30 columns.
    Each row is a *sample*, and each column a *feature*. The 30 elements of a sample
    represent 10 measurements of three different cells on the histology slide. The
    second file contains the label, 0 for benign and 1 for malignant. There’s a one-to-one
    correspondence between the rows of the data and the labels. Therefore, row 0 of
    *bc_data.npy* represents features from a benign sample, while row 2 has features
    from a malignant sample because the first element of the vector in *bc_labels.npy*
    is a 0, and the third is a 1.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*raw*目录包含*bc_data.npy*和*bc_labels.npy*文件。第一个文件是一个包含569行30列的二维NumPy数组的数据集。每一行是一个*样本*，每一列是一个*特征*。每个样本的30个元素表示三种不同细胞在组织切片上的10个测量值。第二个文件包含标签，0代表良性，1代表恶性。数据行与标签之间是一一对应的。因此，*bc_data.npy*的第0行表示一个良性样本的特征，而第2行表示恶性样本的特征，因为*bc_labels.npy*中向量的第一个元素是0，第三个元素是1。'
- en: We’ll build two datasets from the 569 samples, using a 70/30 split, meaning
    70 percent of the samples are for training (398) and the remaining 30 percent
    for testing (171).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从569个样本中构建两个数据集，使用70/30的划分方式，这意味着70%的样本用于训练（398个），剩余30%用于测试（171个）。
- en: As machine learning models are notoriously slow to learn, we should be concerned
    that 398 samples are not enough to condition the model. We want more data, but
    don’t have any more; what are we to do?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习模型的学习过程通常较慢，我们应该担心398个样本不足以训练模型。我们需要更多的数据，但目前没有更多样本，我们该怎么办？
- en: Randomness comes to our aid. We can *augment* the data by creating fake samples
    that, plausibly, come from the same source as our training data. We’ll apply random
    alterations to the existing data—enough to make it different, but not so much
    that the labels are no longer accurate. Data augmentation is a powerful part of
    modern machine learning that helps models learn not to pay too much attention
    to the particulars of the training set, but instead seek more general characteristics
    that differentiate between the classes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 随机性帮助了我们。我们可以通过创建看似来自与训练数据相同来源的伪样本来*增强*数据。我们将对现有数据进行随机更改——足够让它变得不同，但不会过度改变以致标签不再准确。数据增强是现代机器学习中一个强大的部分，它帮助模型学会不专注于训练集的细节，而是寻求区分不同类别的更一般特征。
- en: Before we augment the training samples, we need to standardize the data. Many
    machine learning models have difficulty with features that have different ranges.
    For example, one feature might sit in the range [0, 2] while another uses [–30,000,
    30,000]. To bring both features into the same relative range, we subtract the
    mean value of each feature and then divide by the standard deviation of the features.
    After this transformation, each feature has a mean value close to zero and a standard
    deviation of one.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在增强训练样本之前，我们需要对数据进行标准化。许多机器学习模型在处理特征值范围不同的情况下表现困难。例如，一个特征的范围可能是[0, 2]，而另一个特征的范围可能是[–30,000,
    30,000]。为了将两个特征转换到相同的相对范围，我们会减去每个特征的均值，然后除以特征的标准差。经过这种转换后，每个特征的均值接近零，标准差为一。
- en: We have standardized features that we’ve split into two disjoint groups, one
    for training and one for testing. We’re now ready to augment the training data
    by employing *principal component analysis (PCA)*. If we were able to plot the
    data in 30 dimensions, we’d see that it’s spread out in some directions more than
    others. PCA finds these directions and, in effect, rotates the 30-dimensional
    coordinate system so that the first coordinate aligns with the direction where
    there’s the most variation in the data, then the second coordinate with the next,
    and so on. This means that later coordinates are less important in representing
    the data (though perhaps not in distinguishing between classes). We’ll take advantage
    of this decreasing importance in directions to randomly alter the coordinate directions,
    producing training data similar to the original but not identical. By making small
    changes, we can be (reasonably) confident that the new data represents an instance
    of the original class.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经标准化了特征，并将其分为两个互不重叠的组，一个用于训练，另一个用于测试。现在我们准备通过使用 *主成分分析（PCA）* 来增强训练数据。如果我们能够在
    30 个维度上绘制数据，我们会看到数据在某些方向上比在其他方向上分布得更广。PCA 找出这些方向，实际上是旋转 30 维坐标系，使得第一个坐标对准数据中变异性最大的方向，第二个坐标对准下一个方向，依此类推。这意味着，后面的坐标在表示数据时的重要性较小（尽管可能在区分不同类别时并非如此）。我们将利用这些方向的重要性逐渐降低的特性，随机改变坐标方向，生成与原始数据相似但不完全相同的训练数据。通过做出微小的改变，我们可以（合理地）确信新数据仍然代表原始类别的一个实例。
- en: The code we need is in *build_bc_data.py*. Let’s walk through the important
    bits, beginning with loading the raw data and separating it into train and test
    ([Listing 6-1](ch06.xhtml#ch06list01)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的代码在 *build_bc_data.py* 文件中。让我们逐步讲解关键部分，从加载原始数据并将其分成训练集和测试集开始（[列表 6-1](ch06.xhtml#ch06list01)）。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Listing 6-1: Splitting the raw histology data*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6-1：拆分原始组织学数据*'
- en: First, we fix NumPy’s pseudorandom number seed so the same dataset is built
    each time the code is run. Generally, altering NumPy’s seed this way is not a
    good idea, as it affects *all* code using NumPy, even inside other modules (like
    the scikit-learn modules we’ll use later in the chapter). However, in this case,
    we’ll take the risk.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们修正 NumPy 的伪随机数种子，这样每次运行代码时都会构建相同的数据集。通常，改变 NumPy 的种子并不是一个好主意，因为它会影响到 *所有*
    使用 NumPy 的代码，甚至是其他模块（比如我们在本章稍后会使用的 scikit-learn 模块）。然而，在这种情况下，我们愿意冒这个风险。
- en: Next, we load the raw data and labels before standardizing ➊. We want the mean
    value of each feature. The columns of `x` are the features, requiring the use
    of `axis=0`. This keyword applies the function, `mean`, across the rows of `x`,
    thereby delivering a 30-element vector where each element is the mean of the corresponding
    column of `x`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在标准化之前加载原始数据和标签 ➊。我们想要每个特征的均值。`x` 的列是特征，因此需要使用 `axis=0`。这个关键词会将 `mean`
    函数应用到 `x` 的每一行，从而返回一个包含 30 个元素的向量，每个元素都是 `x` 相应列的均值。
- en: We subtract this mean from each sample, or each row of `x`. With NumPy’s broadcast
    rules, we can do this automatically, with no looping required. NumPy is smart
    enough to see that we’re attempting to subtract a 30-element vector from a 2D
    array where the second dimension is 30, so it performs the subtraction and repeats
    it for each row.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从每个样本或每行`x`中减去这个均值。通过 NumPy 的广播规则，我们可以自动完成这个操作，无需使用循环。NumPy 足够聪明，能够察觉到我们正在尝试从一个二维数组的第二维为
    30 的数据中减去一个 30 元素的向量，因此它执行减法操作，并为每一行重复这一过程。
- en: Next, we divide the mean-subtracted data by the standard deviation of each feature.
    Again, `axis=0` lets us apply the function across the rows. Because of NumPy’s
    broadcast rules, the division is applied down the rows of `x` to produce the final,
    standardized dataset.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将均值已被减去的数据除以每个特征的标准差。同样，`axis=0` 让我们能够跨行应用该函数。由于 NumPy 的广播规则，除法操作会应用到
    `x` 的每一行，生成最终的标准化数据集。
- en: The next three lines randomize the dataset by assigning `i` to a random permutation
    of the numbers 0 through 568\. NumPy’s `argsort` function doesn’t sort a vector,
    but instead returns the sequence of indices that would sort it. The following
    two lines apply this permutation to both `x` and `y` to scramble the data and
    the labels in sync.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的三行通过将 `i` 分配为从 0 到 568 的数字的随机排列来对数据集进行随机化。NumPy 的 `argsort` 函数并不对向量进行排序，而是返回能够将其排序的索引序列。接下来的两行将这个排列应用到
    `x` 和 `y` 上，从而同步地打乱数据和标签。
- en: The final five lines split the raw dataset into training sets (`xtrn`, `ytrn`)
    and testing sets (`xtst`, `ytst`). Note that we split the dataset before augmenting;
    if we split it after, augmented versions of a sample will likely end up in the
    test set, thereby making the model seem better than it is.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的五行代码将原始数据集分割为训练集（`xtrn`，`ytrn`）和测试集（`xtst`，`ytst`）。注意，我们在增强之前就已将数据集分割；如果在增强之后再进行分割，样本的增强版本很可能会出现在测试集中，这样会使得模型看起来比实际更好。
- en: We’re now in a position to augment the data. First, we need to learn the principal
    components of the raw data, then we build a new training set where each original
    sample is kept, along with nine augmented versions of that sample.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始增强数据了。首先，我们需要学习原始数据的主成分，然后构建一个新的训练集，其中每个原始样本都被保留，并附加九个该样本的增强版本。
- en: In [Listing 6-2](ch06.xhtml#ch06list02), scikit-learn supplies `PCA`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 6-2](ch06.xhtml#ch06list02)中，scikit-learn提供了`PCA`。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Listing 6-2: Using PCA to learn the principal components*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6-2：使用PCA学习主成分*'
- en: The `PCA` class follows scikit-learn’s standard approach of defining an instance
    of a class before calling `fit` on that instance. We set the number of components
    to the number of features in the dataset (30).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`PCA`类遵循scikit-learn的标准方法，即在对实例调用`fit`之前，先定义一个类的实例。我们将组件的数量设置为数据集中的特征数量（30）。'
- en: We’ll use the trained `pca` object in a loop to build a new training set of
    augmented samples, as shown in [Listing 6-3](ch06.xhtml#ch06list03).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用训练好的`pca`对象在循环中构建一个包含增强样本的新训练集，如[列表 6-3](ch06.xhtml#ch06list03)所示。
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*Listing 6-3: Augmenting the samples*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6-3：增强样本*'
- en: The new training data is in `newx` and `newy`. Each existing training sample
    will be accompanied by nine augmented versions of it, so the new training set
    will have 3,980 samples instead of a mere 398.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 新的训练数据位于`newx`和`newy`中。每个现有的训练样本将伴随九个增强版本，因此新的训练集将包含3,980个样本，而不是仅有398个。
- en: The loop constructs the dataset in blocks of 398 samples. The first pass stores
    the original data, and subsequent passes call `generateData` to return a new,
    augmented version of the samples in the original dataset. The order of the new
    samples is identical to the original, meaning the labels are in the same order
    as well.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 循环将数据集按398个样本的块构建。第一次遍历存储原始数据，后续的遍历调用`generateData`函数，返回原始数据集中的样本的一个新的增强版本。新样本的顺序与原始顺序相同，这意味着标签的顺序也保持不变。
- en: In [Listing 6-4](ch06.xhtml#ch06list04), the function `generateData` applies
    the PCA transformation and alters the least important coordinate directions beginning
    with `start` (24).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 6-4](ch06.xhtml#ch06list04)中，`generateData`函数应用PCA变换，并从`start`（第24个）开始，改变最不重要的坐标方向。
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Listing 6-4: Applying PCA*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6-4：应用PCA*'
- en: PCA is a reversible transformation. The `generateData` function alters the PCA
    components by adding a small, normally distributed value to each one beginning
    with component 24 (out of 30). When the inverse transform uses the altered components,
    the resulting values (`b`), a block of 398 samples, are no longer identical to
    the original. These augmented versions build the next block of the new dataset.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一个可逆的变换。`generateData`函数通过在从第24个（共30个）主成分开始的每个主成分上添加一个小的、正态分布的值来改变PCA成分。当逆变换使用这些更改后的成分时，得到的值（`b`），即一个398样本的块，将不再与原始数据完全相同。这些增强版本将构建新数据集的下一个块。
- en: The new training dataset is in `newx` and `newy`, but the order of the samples
    is not random because it was built in blocks. Therefore, we perform a final randomization
    before writing the train and test datasets to disk ([Listing 6-5](ch06.xhtml#ch06list05)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 新的训练数据集位于`newx`和`newy`中，但样本的顺序并非随机的，因为它是按块构建的。因此，在将训练集和测试集写入磁盘之前，我们执行最终的随机化操作（[列表
    6-5](ch06.xhtml#ch06list05)）。
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Listing 6-5: Storing the augmented datasets*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6-5：存储增强后的数据集*'
- en: The histology training and test datasets are now ready for use. We applied randomness
    multiple times to scramble the order of the data and to alter the principal components
    to build augmented versions of the training data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 组织学训练和测试数据集现在可以使用了。我们多次应用随机化操作，打乱数据的顺序，并改变主成分，以构建增强版本的训练数据。
- en: '***Handwritten Digits***'
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***手写数字***'
- en: One of the first great successes of the deep learning revolution involved correctly
    identifying objects in images. While the dataset we’ll build isn’t very advanced
    comparably, it is part of MNIST, a larger, workhorse dataset commonly used in
    machine learning. We’ll build a dataset consisting of handwritten 1s, 4s, 7s,
    and 9s. I selected these four digits because even humans often confuse one for
    another, so we might expect a machine learning model to do the same (time will
    tell).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习革命的第一个重大成功之一是正确识别图像中的物体。虽然我们将构建的数据集在相对上并不先进，但它是MNIST数据集的一部分，MNIST是一个常用于机器学习的更大的“工作马”数据集。我们将构建一个包含手写数字1、4、7和9的数据集。我选择这四个数字，因为即使是人类也常常把它们混淆，所以我们可能会预期机器学习模型也会犯同样的错误（时间会证明）。
- en: '[Figure 6-1](ch06.xhtml#ch06fig01) shows samples of each digit type.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 6-1](ch06.xhtml#ch06fig01)展示了每种数字类型的样本。'
- en: '![Image](../images/06fig01.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/06fig01.jpg)'
- en: '*Figure 6-1: Sample digits, where 1s and 7s are often confused, as are 4s and
    9s*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*Figure 6-1: 示例数字，其中1和7常常被混淆，4和9也是如此*'
- en: The images are 28×28-pixel grayscale, with each pixel an integer in [0, 255].
    We’ll work with the digits as 784-element vectors (28 × 28 = 784). I collected
    the digits and their labels in the files *mnist_data.npy* and *mnist_labels.npy*,
    respectively.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图像是28×28像素的灰度图，每个像素的值是[0, 255]范围内的整数。我们将把数字当作784维的向量（28 × 28 = 784）来处理。我将数字和它们的标签分别收集在文件*mnist_data.npy*和*mnist_labels.npy*中。
- en: The raw digits dataset is relatively small, with 100 samples of each digit;
    we split the raw data to have 50 train and 50 test samples. We’ll augment each
    image multiple times to expand the size of the training set.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数字数据集相对较小，每个数字有100个样本；我们将原始数据拆分为50个训练样本和50个测试样本。我们将对每个图像进行多次增强，以扩展训练集的大小。
- en: We used PCA to augment the histology data, but because we’re working with images
    here, we’ll apply basic image processing transformations to randomly produce slightly
    altered versions of each training image. In particular, we’ll rotate each image
    in the range [–3, 3] degrees; about 10 percent of the time, we’ll zoom the image
    to [0.8, 1.2] times its original size while maintaining a final size of 28×28
    pixels by cropping or embedding a smaller image inside a blank 28×28 image.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用PCA来增强组织学数据，但由于这里处理的是图像，我们将应用基本的图像处理变换，随机生成每个训练图像的略微改变版本。特别地，我们将把每个图像旋转在[–3,
    3]度之间；大约10%的时间，我们将图像缩放至[0.8, 1.2]倍原始尺寸，同时保持最终尺寸为28×28像素，通过裁剪或将较小的图像嵌入到一个空白的28×28图像中。
- en: The code we need is in *build_mnist_dataset.py*. While it mirrors the code that
    built the histology dataset, differences include splitting the raw data 50/50
    between train and test, storing the unaugmented training data, and augmenting
    the training data 20 times instead of 10 times, resulting in an augmented training
    set of 4,200 samples (200 original plus 20 more for each original sample); see
    [Listing 6-6](ch06.xhtml#ch06list06).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的代码在*build_mnist_dataset.py*中。虽然它与构建组织学数据集的代码类似，但有所不同，主要包括将原始数据按50/50分配为训练集和测试集、存储未增强的训练数据，并将训练数据增强20次而不是10次，最终得到一个包含4,200个样本的增强训练集（200个原始样本加上每个原始样本增强的20个版本）；参见[Listing
    6-6](ch06.xhtml#ch06list06)。
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Listing 6-6: Augmenting the digit images*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 6-6: 增强数字图像*'
- en: The original 200 training samples (`xtrn`, `ytrn`) are examined, one by one.
    First, we add the original image to the augmented output (`newx`, `newy`). Then,
    we add 20 augmented versions of the same image by making repeated calls to `augment`
    ➊. After adding the augmented images, we scramble the entire training set again
    to mix the order of the images ➋.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的200个训练样本（`xtrn`，`ytrn`）逐一检查。首先，我们将原始图像添加到增强后的输出（`newx`，`newy`）。然后，我们通过多次调用`augment`
    ➊，添加该图像的20个增强版本。添加完增强图像后，我们会再次打乱整个训练集，以混合图像的顺序 ➋。
- en: '[Listing 6-7](ch06.xhtml#ch06list07) shows the code to augment images.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 6-7](ch06.xhtml#ch06list07)展示了增强图像的代码。'
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Listing 6-7: Augmenting an image*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 6-7: 增强图像*'
- en: The input image (`x`), a NumPy vector, is first reshaped into a 28×28-element
    two-dimensional array, `im`. Next, two `if` statements ask whether a random value
    is less than 0.5 or 0.1\. The first, executed 50 percent of the time, applies
    a random rotation of the image by some value in the range [–3, 3] degrees. Notice
    the use of `rotate` from `scipy.ndimage`. The `reshape=False` keyword forces `rotate`
    to return an output array that’s the same size as the input array.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像（`x`），一个NumPy向量，首先被重塑为一个28×28元素的二维数组，`im`。接下来，两个`if`语句会检查随机值是否小于0.5或0.1。第一个语句在50%的时间内执行，应用一个随机旋转操作，使图像旋转某个在[–3,
    3]度范围内的角度。注意这里使用了`scipy.ndimage`中的`rotate`。`reshape=False`关键字强制`rotate`返回与输入数组大小相同的输出数组。
- en: The second `if` statement, executed 10 percent of the time, uses `zoom` to magnify
    the image by a random scale factor in [0.8, 1.2], meaning the zoomed image is
    anywhere from 80 to 120 percent the size of the original. The code after the call
    to `zoom` ensures the output image is still 28×28 pixels by either embedding the
    smaller image in a blank 28×28 image or selecting the central 28×28 pixels if
    magnifying beyond 100 percent. The newly augmented image is returned after unraveling
    it and transformed back into a 784-element vector.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个`if`语句，在10%的时间内执行，使用`zoom`按一个随机的缩放因子在[0.8, 1.2]范围内放大图像，这意味着缩放后的图像大小从原始图像的80%到120%不等。调用`zoom`后的代码确保输出图像仍然是28×28像素，通过将较小的图像嵌入到一个空白的28×28图像中，或者在放大超过100%时，选择中央的28×28像素。经过展开处理并重新转换为784元素向量后，返回新增强的图像。
- en: The code in *build_mnist_dataset.py* stores the smaller, unaugmented training
    set and the augmented training set. The file *mnist_test.py* uses `sklearn`’s
    `MLPCLassifier`—which we’ll learn about soon—to train 40 models using the training
    sets, keeping the overall accuracy of each. The models use default values and
    a single hidden layer of 100 nodes. The mean accuracy for the unaugmented dataset
    was 87.3 percent, while augmented training data led to a mean overall accuracy
    of 90.3 percent, a statistically significant difference, thereby delivering evidence
    that the random augmentation process aids training models. It may seem tedious
    to spend this section detailing how the datasets are constructed, but it’s hard
    to overemphasize the importance randomness plays in the process. Dataset construction
    is so crucial to modern machine learning that competitions are held where the
    models are fixed and good dataset construction is required to produce winning
    results (search for “Data-Centric AI Competition”).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*build_mnist_dataset.py*中的代码存储了较小的、未经增强的训练集和增强后的训练集。文件*mnist_test.py*使用`sklearn`的`MLPClassifier`（我们稍后会学习）来训练40个模型，使用训练集并保持每个模型的整体准确性。模型使用默认值，并且有一个100个节点的单一隐藏层。未经增强的数据集的平均准确率为87.3%，而增强后的训练数据的平均准确率为90.3%，这是一个具有统计学意义的差异，证明了随机增强过程有助于训练模型。虽然花时间详细讲解数据集的构建可能显得有些繁琐，但很难过分强调随机性在这个过程中的重要性。数据集的构建对现代机器学习至关重要，以至于有些比赛的模型是固定的，必须良好构建数据集才能获得获胜结果（可以搜索“Data-Centric
    AI Competition”）。'
- en: Now that we have our datasets, let’s put them to the test.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了数据集，让我们来进行测试。
- en: '**Neural Networks**'
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**神经网络**'
- en: A *neural network* is a set of nodes that, layer by layer, successively transforms
    an input to an output. Network nodes accept multiple inputs and produce a single
    output, an operation sufficiently similar to the operation of a biological neuron
    that the name “neural network” has persisted. Neural networks are not artificial
    brains; rather, they are feed-forward, directed, acyclic *graphs*, a data structure
    commonly used in computer science.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*神经网络*是一个由节点组成的集合，逐层将输入转换为输出。网络节点接受多个输入并产生一个输出，这一操作与生物神经元的工作原理足够相似，因此得名“神经网络”。神经网络不是人工的大脑；它们是前馈的、定向的、无环的*图*，这是一种在计算机科学中常用的数据结构。'
- en: The operation of the network transforms an input to an output; in other words,
    neural networks are a type of function, ***y*** = *f*(***x***; ***θ***). Training
    the neural network means locating ***θ***, the set of parameters causing the network
    to perform as desired.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的操作将输入转化为输出；换句话说，神经网络是一种函数，***y*** = *f*(***x***; ***θ***)。训练神经网络意味着寻找***θ***，一组使网络按预期执行的参数。
- en: '***Anatomy Analysis***'
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***解剖分析***'
- en: If a neural network is a set of nodes working in layers, then it makes sense
    to begin with a node. See [Figure 6-2](ch06.xhtml#ch06fig02).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果神经网络是由层级中的节点构成的集合，那么从一个节点开始是有意义的。请参见[图6-2](ch06.xhtml#ch06fig02)。
- en: '![Image](../images/06fig02.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/06fig02.jpg)'
- en: '*Figure 6-2: A neural network node*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-2：神经网络节点*'
- en: 'Data flows from left to right. The inputs (*x*), either the input to the network
    or the output of a previous network layer, are multiplied by *weights* (*w*) and
    summed along with a *bias* (*b*) before passing the total to an *activation* function
    (*h*) to produce the output, *a*. In symbols:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据从左到右流动。输入（*x*），无论是网络的输入还是先前网络层的输出，都与*权重*（*w*）相乘并与*偏置*（*b*）相加，然后将总和传递给*激活*函数（*h*）以产生输出，*a*。用符号表示为：
- en: '*a* = *h*(*w*[0]*x*[0] + *w*[1]*x*[1] + *w*[2]*x*[2] + *b*)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*a* = *h*(*w*[0]*x*[0] + *w*[1]*x*[1] + *w*[2]*x*[2] + *b*)'
- en: The activation function is nonlinear, or some function beyond *x* to the first
    power. Modern networks often use *rectified linear unit (ReLU)* activation functions,
    which output the input unless the input is less than zero, in which case the output
    is zero, *h* = max(0, *x*).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是非线性的，或者是某个超越*x*一次方的函数。现代网络通常使用*修正线性单元（ReLU）*激活函数，它会输出输入，除非输入小于零，在这种情况下输出为零，*h*
    = max(0, *x*)。
- en: A network layer consists of a collection of nodes that, collectively, receive
    the output of the previous layer as their inputs and produce new output, one per
    node, passed to the next layer. Each node in a layer receives each input; in graph
    terms, the network is *fully connected*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 网络层由一组节点组成，这些节点共同接收上一层的输出作为它们的输入，并产生新的输出，每个节点一个，传递给下一层。层中的每个节点都会接收每个输入；用图论的术语来说，网络是*完全连接*的。
- en: Data flows through the network, layer by layer, to the output. In this way,
    the network maps input ***x*** to output ***y***. The output is often a vector,
    but can be a scalar, *y*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通过网络逐层流动，直到输出。通过这种方式，网络将输入***x***映射到输出***y***。输出通常是一个向量，但也可以是标量，*y*。
- en: Moving data through a neural network is most easily accomplished by representing
    the weights between layers as a matrix and the biases as a vector. This representation
    automatically applies each input to each node and activation function, thereby
    reducing the entire layer operation to a matrix multiplied by a vector plus another
    vector passed to a vector version of the activation function. Training means learning
    a set of matrices for the weights, one matrix per layer, and a set of bias vectors,
    one per layer.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中传递数据最简单的方式是将层之间的权重表示为矩阵，将偏置表示为向量。这种表示法自动地将每个输入应用于每个节点和激活函数，从而将整个层的操作简化为矩阵乘以向量，再加上另一个传递给激活函数向量版本的向量。训练意味着学习一组矩阵作为权重，每层一个矩阵，以及一组偏置向量，每层一个。
- en: The weights and biases, which live on the connections between the nodes of the
    layers, are the parameters of ***θ***; these are the things the network learns
    during training. This is analogous to fitting a curve, but in this case, the function
    is determined by the architecture of the network. Unlike curve fitting, training
    a neural network usually doesn’t involve reducing the error on the training set
    to zero. Instead, the goal is to coax the network to learn weights and biases
    that make the network generally applicable to new inputs. After all, the entire
    point of training the model is to use it with new, unknown inputs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 权重和偏置存在于层的节点之间的连接上，是***θ***的参数；这些是网络在训练过程中学习的内容。这类似于拟合曲线，但在这种情况下，函数由网络的架构决定。与曲线拟合不同，训练神经网络通常不涉及将训练集上的误差降到零。相反，目标是引导网络学习权重和偏置，使其能够广泛适用于新的输入。毕竟，训练模型的整个目的是使其能够处理新的、未知的输入。
- en: 'A detailed discussion of neural network training is beyond our present scope,
    as our goal is to understand randomness’s role in the process. If you’re interested
    in learning more about neural networks, I recommend my book *Practical Deep Learning:
    A Python-Based Introduction* (2021), also available from No Starch Press. Remember
    that the training process produces a specific set of weights and biases that tailor
    the network to the problem at hand.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络训练的详细讨论超出了我们当前的范围，因为我们的目标是理解随机性在过程中的作用。如果你有兴趣了解更多神经网络的内容，我推荐我的书籍《实践深度学习：基于
    Python 的入门》（2021年），也可以从 No Starch Press 获得。记住，训练过程会产生一组特定的权重和偏置，使网络能够针对当前问题进行调整。
- en: '***Randomness***'
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***随机性***'
- en: Randomness is critically important at the beginning of the training process,
    in the selection of the initial weights and biases.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 随机性在训练过程的开始阶段至关重要，特别是在选择初始权重和偏置时。
- en: 'Training a neural network follows this general algorithm:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络遵循以下一般算法：
- en: Randomly initialize the weights and biases of the network.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化网络的权重和偏差。
- en: Pass a randomly selected subset of the training data through the network.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将随机选择的训练数据子集传入网络。
- en: Use a measure of the error between the network’s output and the desired output
    to update the weights and biases.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用网络输出与期望输出之间的误差度量来更新权重和偏差。
- en: Repeat from step 2 until training is done (however decided).
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第二步开始重复，直到训练完成（具体如何决定）。
- en: 'In this section, we’re concerned with step 1\. Steps 2 and 3 involve a *loss
    function*, a measure of the mistakes the network has made, and a two-step process
    to update the weights and biases: *backpropagation* and *gradient descent*. The
    former uses the chain rule from differential calculus to determine how each weight
    and bias value contributes to the error, and the latter uses the gradient formed
    from those measures to alter the weights and biases to minimize the loss function.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们关注的是第一步。第二步和第三步涉及到一个*损失函数*，这是衡量网络错误的指标，并且包括一个两步过程来更新权重和偏差：*反向传播*和*梯度下降*。前者使用微积分中的链式法则来确定每个权重和偏差值如何影响误差，后者则利用这些度量形成的梯度来调整权重和偏差，从而最小化损失函数。
- en: '***Initialization***'
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***初始化***'
- en: It was only in 2010–2012 that deep neural networks (with many layers) exploded
    on the scene. One of the factors contributing to this was the realization that
    previous algorithms for initializing the weights and biases of models were relatively
    poor; better options exist.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 直到2010–2012年，深度神经网络（具有多层结构）才在场景中爆发。促成这一发展的因素之一是认识到以往初始化模型的权重和偏差的算法相对较差；现在有了更好的选择。
- en: We will explore these options by employing the `MLPCLassifier` class from `sklearn`
    to implement a traditional neural network. *MLP* stands for *multilayer perceptron*,
    with “perceptron” being an old name for a neural network (I recommend searching
    for Frank Rosenblatt and his Perceptron machine—fascinating research that was
    not sufficiently appreciated in its time).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用`sklearn`中的`MLPClassifier`类来实现一个传统的神经网络，从而探索这些选项。*MLP*代表*多层感知器*，“感知器”是神经网络的一个古老名称（我建议你搜索Frank
    Rosenblatt和他的感知器机器——这项研究在当时没有得到足够的重视，实在是太迷人了）。
- en: '**Initializing the Network**'
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**初始化网络**'
- en: The `MLPClassifier` class includes an internal method, `_init_coef`, which is
    responsible for assigning the initial weights and biases. We will subclass `MLPClassifier`
    and override this method, allowing us to alter the initialization approach while
    still taking advantage of everything else the `MLPClassifier` has to offer. Take
    a look at [Listing 6-8](ch06.xhtml#ch06list08).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`MLPClassifier`类包括一个内部方法`_init_coef`，该方法负责分配初始权重和偏差。我们将对`MLPClassifier`进行子类化，并重写这个方法，从而改变初始化方法，同时仍然能利用`MLPClassifier`所提供的其他功能。请查看[Listing
    6-8](ch06.xhtml#ch06list08)。'
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Listing 6-8: Overriding the initialization method*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 6-8: 重写初始化方法*'
- en: The subclass overrides scikit-learn’s approach ➋ with three other approaches.
    The method returns a weight matrix and a bias vector for a layer that has `fan_in`
    inputs and `fan_out` outputs. The `dtype` parameter specifies the data type, typically
    32- or 64-bit floating point.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该子类覆盖了scikit-learn的做法 ➋，提供了三种其他方法。该方法返回一个权重矩阵和一个偏差向量，适用于具有`fan_in`输入和`fan_out`输出的层。`dtype`参数指定数据类型，通常是32位或64位浮点数。
- en: By default, scikit-learn uses *Glorot initialization*. We get it by calling
    the superclass version of the method ➋. Glorot initialization depends on the number
    of inputs and outputs and is one of the initialization approaches leading to improved
    model performance. At least, that’s the claim. We’ll put it to the test.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，scikit-learn使用*Glorot初始化*。我们通过调用父类版本的方法 ➋ 来实现。Glorot初始化依赖于输入和输出的数量，它是提高模型性能的初始化方法之一。至少，这是它的说法。我们将对它进行测试。
- en: Another modern initialization approach is *He initialization* ➎, which is suitable
    for networks using ReLU activation functions. He initialization depends on a matrix
    of samples from a normal distribution with a mean of 0 and a standard deviation
    of 1\. We get that here via the embedded `normvec` function ➊, which permits us
    to use our `RE` class. We’ll see precisely how momentarily.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种现代初始化方法是*He初始化* ➎，它适用于使用ReLU激活函数的网络。He初始化依赖于一个来自正态分布的样本矩阵，该分布的均值为0，标准差为1。我们通过嵌入式的`normvec`函数
    ➊来实现这一点，这使我们能够使用`RE`类。稍后我们将具体看看如何操作。
- en: We initialize classical neural networks through the use of small uniformly distributed
    ➌ or normally distributed ➍ random values. The first draws random samples from
    a uniform distribution, [–0.005, 0.005]. The second uses normally distributed
    samples scaled by 0.005\. Both approaches are intuitively reasonable, but, as
    we’ll see, they are not ideal.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用小的均匀分布 ➌ 或正态分布 ➍ 随机值来初始化经典的神经网络。第一个从均匀分布中抽取随机样本，范围是 [–0.005, 0.005]。第二个使用按
    0.005 缩放的正态分布样本。两种方法从直觉上看都是合理的，但正如我们将看到的，它们并不理想。
- en: 'The `normal` function returns a normally distributed sample with a mean value
    of `mu` and a standard deviation of 1\. A normal distribution selects samples
    symmetrically around the mean; return to [Figure 1-1](ch01.xhtml#ch01fig01) on
    [page 4](ch01.xhtml#ch01fig01) as an example. NumPy provides a function to select
    samples from a normal distribution, but we want to use our `RE` class, so we need
    to define a function that creates normally distributed samples from the uniformly
    distributed samples `RE` returns. We can use the Box-Muller transformation introduced
    in [Chapter 1](ch01.xhtml):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`normal` 函数返回一个正态分布的样本，其均值为 `mu`，标准差为 1。正态分布的样本对称地围绕均值选择；请参见[图 1-1](ch01.xhtml#ch01fig01)中
    [第 4 页](ch01.xhtml#ch01fig01)的示例。NumPy 提供了一个函数来从正态分布中选择样本，但我们想使用我们的 `RE` 类，因此我们需要定义一个函数，该函数从
    `RE` 返回的均匀分布样本中创建正态分布样本。我们可以使用[第 1 章](ch01.xhtml)中介绍的 Box-Muller 变换：'
- en: '![Image](../images/f0184-01.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0184-01.jpg)'
- en: Here *u*[1] and *u*[2] are uniformly distributed samples in [0, 1), precisely
    what `RE` returns (how convenient!).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *u*[1] 和 *u*[2] 是 [0, 1) 范围内的均匀分布样本，这正是 `RE` 返回的内容（多么方便！）。
- en: 'The code implementing `normal` requires explanation. Note the indentation on
    the final line: it’s not part of `normal`, but refers to it immediately after
    its definition.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 `normal` 的代码需要解释。请注意最后一行的缩进：它不是 `normal` 的一部分，而是在定义 `normal` 后立即引用它。
- en: The Box-Muller transformation equations use a pair of uniform samples to produce
    a pair of normally distributed samples. While we can ask `RE` objects to return
    a pair of uniform samples, we want `normal` to return only one normally distributed
    sample when called. We can generate two samples and throw one away, or carefully
    try to preserve both. This way, each call to `normal` returns a single sample
    and generates new samples only when required. Accomplishing this requires `normal`
    to preserve state between calls. The Python way to do this is to use a class,
    but that seems a bit overkill. Instead, we’ll make use of the fact that Python
    functions are objects; we can add new attributes (member variables) to objects
    at will. We’ll define `normal` as a Python function and then immediately add a
    `state` variable to it initialized to `False`—the final line in [Listing 6-8](ch06.xhtml#ch06list08).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Box-Muller 变换方程使用一对均匀分布的样本来生成一对正态分布的样本。虽然我们可以要求 `RE` 对象返回一对均匀分布的样本，但我们希望 `normal`
    在调用时仅返回一个正态分布的样本。我们可以生成两个样本并丢弃一个，或者小心地尝试保留两个样本。这样，每次调用 `normal` 时返回一个单一的样本，并且仅在需要时生成新样本。要实现这一点，要求
    `normal` 在调用之间保留状态。Python 中实现这一点的方法是使用类，但这似乎有些过于复杂。相反，我们将利用 Python 函数是对象这一特性；我们可以随意为对象添加新属性（成员变量）。我们将
    `normal` 定义为一个 Python 函数，然后立即为其添加一个初始化为 `False` 的 `state` 变量——这就是[Listing 6-8](ch06.xhtml#ch06list08)中的最后一行。
- en: When we call `normal`, the `state` variable has a value, `False`. This value
    persists between calls because it’s an attribute of the `normal` function, which
    uses the value of `state` to decide whether to generate two new samples and return
    the first caching the second, or to return the second. If `state` is `True`, return
    the second sample, `z2`, after multiplying it by the desired standard deviation
    (`sigma`) and adding the mean value (`mu`). Then set `state` to `False`. If `state`
    is `False`, get two uniform samples from `rng` and use them to calculate two normal
    samples, `z1` and `z2`. Cache `z2` and return `z1`, properly scaled and offset.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调用 `normal` 时，`state` 变量的值是 `False`。这个值会在调用之间持续存在，因为它是 `normal` 函数的一个属性，`normal`
    使用 `state` 的值来决定是生成两个新样本并返回第一个，同时缓存第二个，还是直接返回第二个。如果 `state` 是 `True`，则返回第二个样本
    `z2`，并将其乘以所需的标准差 (`sigma`)，加上均值 (`mu`)。然后将 `state` 设置为 `False`。如果 `state` 是 `False`，则从
    `rng` 获取两个均匀样本，并使用它们计算两个正态样本 `z1` 和 `z2`。缓存 `z2` 并返回 `z1`，并适当进行缩放和偏移。
- en: 'To use `Classifier` we create a neural network using scikit-learn and add two
    new attributes: `init_scheme` to specify the desired initialization scheme, and
    `rng`, an instance of `RE`, to allow access to the different randomness sources
    we’ve developed throughout the book.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`Classifier`，我们通过scikit-learn创建一个神经网络，并添加两个新的属性：`init_scheme`用来指定所需的初始化方案，以及`rng`，它是`RE`的一个实例，用于访问我们在全书中开发的不同随机数源。
- en: '**Experimenting with Initialization**'
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**实验初始化**'
- en: Let’s experiment with `Classifier` and neural network initialization. Along
    the way, we’ll set up a scikit-learn training session.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一下`Classifier`和神经网络初始化。过程中，我们将设置一个scikit-learn训练会话。
- en: The code we need is in *init_test.py*. It loads the digits dataset created previously
    and trains 12 models using each initialization scheme. Training multiple models
    for each is essential because initialization is random; for example, we might
    get a bad dice roll for one initialization scheme when it’s actually a good approach.
    Averaging the results from multiple models lets us apply statistical tests.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的代码在*init_test.py*中。它加载了之前创建的数字数据集，并使用每个初始化方案训练了12个模型。对于每个初始化方案，训练多个模型是必要的，因为初始化是随机的；例如，在一个初始化方案下我们可能会得到一个不太好的结果，而实际上它是一个有效的方法。通过对多个模型的结果取平均值，我们可以应用统计测试。
- en: '[Listing 6-9](ch06.xhtml#ch06list09) shows the beginning of the main part of
    *init_test.py*.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单6-9](ch06.xhtml#ch06list09)显示了*init_test.py*主要部分的开头。'
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Listing 6-9: Setting up the initialization experiment*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单6-9：设置初始化实验*'
- en: 'The imports make the `Classifier` class available, along with `RE` and two
    statistical tests: the t-test and the *Mann-Whitney U* test, a nonparametric version
    of the t-test. *Nonparametric tests* make no assumptions about the data and are
    harder to satisfy. I often consider both together to account for the possibility
    that the t-test results are invalid because the data isn’t normally distributed—a
    fundamental assumption of the t-test.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这些导入语句使得`Classifier`类可用，同时导入了`RE`和两个统计测试：t检验和*曼-惠特尼U检验*，它是t检验的非参数版本。*非参数检验*对数据没有假设，且要求更为严格。我常常同时使用这两种检验，以考虑到t检验结果无效的可能性——因为数据并非正态分布，这是t检验的一个基本假设。
- en: Next, we load the digits dataset, first the training (`xtrn`) followed by the
    test (`xtst`). We divide the data by 256 to map it from [0, 255] to [0, 1).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载数字数据集，首先是训练集（`xtrn`），然后是测试集（`xtst`）。我们将数据除以256，将其从[0, 255]映射到[0, 1)。
- en: The following code paragraph accumulates the output of `Run` for initialization
    scheme 0\. The return value is the overall accuracy of a model using scikit-learn’s
    Glorot initialization. Similar code captures the accuracy for the other four initialization
    schemes.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码段累计了初始化方案0的`Run`输出。返回值是使用scikit-learn的Glorot初始化的模型的整体准确率。类似的代码会捕获其他四个初始化方案的准确率。
- en: We then transform the results for all schemes into means and standard errors
    before printing, as in [Listing 6-10](ch06.xhtml#ch06list010).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将所有方案的结果转化为均值和标准误差后再打印，如[清单6-10](ch06.xhtml#ch06list010)所示。
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*Listing 6-10: Reporting the results*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单6-10：报告结果*'
- en: Finally, we run the statistical tests to compare one initialization scheme against
    another. Theoretically, we might expect initialization scheme 3 (He) to be the
    best performing, so we compare it (`init3`) with each of the others and report
    the corresponding p-values (the “u” is the p-value for the Mann-Whitney U test);
    see [Listing 6-11](ch06.xhtml#ch06list011).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们运行统计测试，将一个初始化方案与另一个进行比较。从理论上讲，我们可能会预期初始化方案3（He）是表现最好的，因此我们将它（`init3`）与其他方案进行比较，并报告相应的p值（“u”是曼-惠特尼U检验的p值）；见[清单6-11](ch06.xhtml#ch06list011)。
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*Listing 6-11: Running the statistical tests*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单6-11：运行统计测试*'
- en: The statistical tests, `ttest_ind` and `mannwhitneyu`, first return their respective
    test statistics and then their associated p-value. The p-value tells us the probability
    that we’ll measure the difference in the means between the two sets of accuracies,
    given the two sets are from the same distribution. The smaller the p-value, the
    less likely the two sets are from the same distribution, thereby giving us confidence
    that the difference observed is real.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 统计测试`ttest_ind`和`mannwhitneyu`首先返回各自的检验统计量，然后返回相应的p值。p值告诉我们，假设两组准确率来自同一分布时，我们测得的均值差异的概率。p值越小，两组来自同一分布的可能性越小，从而增加我们对观察到的差异是真实的信心。
- en: '**Training the Network**'
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**训练网络**'
- en: The code to configure and train a neural network is in the `Run` function, as
    in [Listing 6-12](ch06.xhtml#ch06list012).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 配置和训练神经网络的代码位于 `Run` 函数中，详见 [Listing 6-12](ch06.xhtml#ch06list012)。
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Listing 6-12: Training a neural network*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 6-12: 训练神经网络*'
- en: Here’s where we begin to appreciate the power of scikit-learn. The arguments
    to `Run` include which initialization scheme to use, [0, 3], followed by the train
    and test datasets. First, we create the neural network by creating an instance
    of `Classifier`, our subclass of scikit-learn’s `MLPClassifier`. By default, the
    neural network uses a ReLU activation function, which is what we want. It also
    trains until training no longer improves things or `max_iter` passes through the
    entire training set. Each pass through is known as an *epoch*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们开始体会到 scikit-learn 的强大。`Run` 函数的参数包括要使用的初始化方案，[0, 3]，接着是训练集和测试集。首先，我们通过创建
    `Classifier` 的实例来创建神经网络，`Classifier` 是我们对 scikit-learn 中 `MLPClassifier` 的子类。默认情况下，神经网络使用
    ReLU 激活函数，这正是我们需要的。它还会训练，直到训练不再改善或 `max_iter` 完成整个训练集的迭代。每次通过训练集的过程被称为 *epoch*。
- en: 'The `hidden_layer_sizes` keyword defines the architecture of the model. We
    know the input has 784 elements and the output 4 because there are four classes,
    the digits 1, 4, 7, and 9\. The layers between the input and output are hidden;
    we’re specifying two: the first with 100 nodes and the second with 50.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`hidden_layer_sizes` 关键字定义了模型的架构。我们知道输入有 784 个元素，输出有 4 个类别，因为有四个类别，分别是数字 1、4、7
    和 9。输入和输出之间的层是隐藏层；我们指定了两个：第一个有 100 个节点，第二个有 50 个节点。'
- en: Before training, we need to set the initialization scheme (`init_scheme`) and
    define `rng`, an instance of `RE`, using the default of PCG64 and floating-point
    values in [0, 1).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，我们需要设置初始化方案（`init_scheme`）并定义 `rng`，它是 `RE` 的一个实例，使用 PCG64 默认值并且浮动值在 [0,
    1) 之间。
- en: Train the neural network with `clf.fit(xtrn,ytrn)`, in which the `fit` method
    accepts the input vectors (`xtrn`) and the associated labels (`ytrn`). When the
    method returns, the model is trained and has found values for all the weights
    and biases (***θ***).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `clf.fit(xtrn, ytrn)` 来训练神经网络，其中 `fit` 方法接受输入向量（`xtrn`）和相关标签（`ytrn`）。当该方法返回时，模型已经训练完毕，并为所有的权重和偏置（***θ***）找到了合适的值。
- en: '**Evaluating the Network**'
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**评估网络**'
- en: When given the test data (`xtst`), `predict` generates a set of predicted class
    labels, `pred`—a vector of the same length as `ytst`, the known class labels.
    We compare the two to evaluate the model by building a *confusion matrix*. The
    rows of the confusion matrix represent the known, true class labels. The columns
    are the model’s assigned labels. The matrix elements are counts of the number
    of times each possible pairing of the true label and assigned label happened.
    A perfect classifier will always assign correct labels, meaning the confusion
    matrix will be diagonal. Any counts off the main diagonal represent errors.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当给定测试数据（`xtst`）时，`predict` 会生成一组预测类别标签 `pred`，其长度与已知类别标签 `ytst` 相同。我们通过构建 *混淆矩阵*
    来比较两者，评估模型的表现。混淆矩阵的行表示已知的真实类别标签，列表示模型分配的标签。矩阵元素是每种可能的真实标签与分配标签配对出现的次数。一个完美的分类器会始终分配正确的标签，这意味着混淆矩阵将是对角线的。任何偏离主对角线的计数都表示错误。
- en: The `Confusion` function generates a confusion matrix, and overall accuracy,
    from a set of known and predicted labels; see [Listing 6-13](ch06.xhtml#ch06list013).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`Confusion` 函数生成一个混淆矩阵和整体准确率，基于已知标签和预测标签的集合；详见 [Listing 6-13](ch06.xhtml#ch06list013)。'
- en: '[PRE12]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*Listing 6-13: Creating a confusion matrix*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 6-13: 创建混淆矩阵*'
- en: The confusion matrix (`cm`) is a 4×4 matrix because there are four classes.
    The most important line updates `cm` by indexing first by the known class label
    (`y`) and then by the assigned class label (`p`). Adding one counts each time
    that particular pairing of the true class label and assigned label occurs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵（`cm`）是一个 4×4 矩阵，因为有四个类别。最重要的操作是通过先按已知类别标签（`y`）索引，再按分配的类别标签（`p`）索引来更新 `cm`。每次发生特定的真实类别标签与分配标签的配对时，添加一个计数。
- en: The overall accuracy is either the number of times the known and assigned labels
    matched or the sum along the main diagonal divided by the sum of all matrix elements.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 整体准确率要么是已知标签与分配标签匹配的次数，要么是主对角线的和除以所有矩阵元素的和。
- en: 'Running *init_test.py* takes a few minutes. My run produced:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 *init_test.py* 需要几分钟时间。我的运行结果如下：
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first block displays the mean accuracy over the 12 models for each initialization
    scheme (mean ± SE). Glorot and He initialization averaged 92.7 and 92.5 percent,
    respectively. The older uniform and normal initialization strategies achieved
    90.0 and 90.1 percent, respectively. These are large differences, but are they
    statistically significant? For that, look at the second block of results.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个块显示了每种初始化方案下12个模型的平均准确度（均值 ± 标准误）。Glorot和He初始化的平均准确度分别为92.7%和92.5%。较旧的均匀和正态初始化策略分别达到了90.0%和90.1%。这些差异很大，但它们在统计上显著吗？为此，看看第二个结果块。
- en: Comparing He initialization (`init3`) with Glorot (`init0`) returns p-values
    of about 0.5 for both the t-test and Mann-Whitney U. Such p-values strongly indicate
    that there is no difference between either approach.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 将He初始化（`init3`）与Glorot（`init0`）进行比较，t检验和Mann-Whitney U的p值都约为0.5。这些p值强烈表明两种方法之间没有差异。
- en: Now, look at the p-values comparing He initialization with the older methods.
    They are virtually zero, implying the difference in mean accuracy is very likely
    real—He and Glorot initialization both lead to significantly better performing
    models. Modern deep learning is vindicated. While there was never any doubt, it’s
    beneficial to confirm directly rather than take it purely on faith.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看看比较He初始化和旧方法的p值。它们几乎为零，意味着平均准确度的差异很可能是实质性的——He和Glorot初始化都导致了显著更好的模型性能。现代深度学习得到了验证。虽然从未怀疑过，但直接确认而不是盲目相信是有益的。
- en: '**Extreme Learning Machines**'
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**极限学习机**'
- en: An *extreme learning machine* is a simple, single, hidden-layer neural network.
    What distinguishes an extreme learning machine from a traditional neural network
    is the source of the weights and biases.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*极限学习机*是一个简单的单隐藏层神经网络。极限学习机与传统神经网络的区别在于权重和偏置的来源。'
- en: To map an input vector through the first hidden layer of a neural network involves
    a weight matrix, ***W***, and a bias vector, ***b***
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入向量映射通过神经网络的第一个隐藏层涉及一个权重矩阵，***W***，和一个偏置向量，***b***
- en: '***z*** = *h*(***Wx*** + ***b***)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '***z*** = *h*(***Wx*** + ***b***)'
- en: where ***x*** and ***z*** are vectors, and *h* is the activation function that
    accepts a vector input and produces a vector output.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其中***x***和***z***是向量，*h*是一个激活函数，接受一个向量输入并输出一个向量。
- en: In a traditional neural network, ***W*** and ***b*** are learned in the training
    process. In an extreme learning machine, ***W*** and ***b*** are generated randomly,
    with no regard for the training data. Random matrices are (sometimes) capable
    of mapping inputs, like the digit images as vectors, to a new space where it’s
    easier to separate the classes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统神经网络中，***W***和***b***是在训练过程中学习得到的。在极限学习机中，***W***和***b***是随机生成的，与训练数据无关。随机矩阵有时能够将输入（例如数字图像作为向量）映射到一个新的空间，在这个空间中，分类更容易分离。
- en: If we use the equation to map all training data, the output of the hidden layer
    (***z***) becomes a matrix (***Z***) with as many rows as there are training samples
    and as many columns as there are nodes in the hidden layer. In other words, the
    random ***W*** matrix and ***b*** bias vector have produced a new version of the
    training data, ***Z***.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用这个方程映射所有的训练数据，隐藏层的输出（***z***）变成一个矩阵（***Z***），它的行数与训练样本的数量相同，列数与隐藏层节点的数量相同。换句话说，随机的***W***矩阵和***b***偏置向量已经生成了一个新的训练数据版本，***Z***。
- en: The random weight matrix and bias vector link the input and the hidden layer.
    To finish building the neural network, we need a weight matrix between the hidden
    layer’s output and the model’s output; there is no bias vector in this case. Learning
    happens here, but we need to take a detour to see it.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 随机权重矩阵和偏置向量连接输入层和隐藏层。为了完成神经网络的构建，我们需要在隐藏层输出和模型输出之间添加一个权重矩阵；在这种情况下没有偏置向量。学习过程发生在这里，但我们需要绕个圈子来看它。
- en: The training dataset consists of input samples and associated class labels,
    [0, 3]. Many machine learning models don’t use integer class labels, but transform
    the labels into *one-hot vectors*. For example, if there are four classes, the
    one-hot vector has four elements, all 0 except for the element corresponding to
    the class label, which is 1\. If the class label is 2, then the one-hot vector
    is {0, 0, 1, 0}. Likewise, if the class label is 0, the one-hot vector is {1,
    0, 0, 0}. To finish building the extreme learning machine, we need the training
    set class labels in this form. I assume that the labels are so transformed for
    the remainder of this section.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集由输入样本和相应的类标签[0, 3]组成。许多机器学习模型不使用整数类标签，而是将标签转换为*one-hot向量*。例如，如果有四个类，则one-hot向量有四个元素，除了对应类标签的元素为1，其余为0。如果类标签是2，则one-hot向量为{0,
    0, 1, 0}。同样，如果类标签是0，one-hot向量为{1, 0, 0, 0}。为了完成极限学习机的构建，我们需要将训练集类标签转化为这种形式。我假设接下来的部分中标签已经被如此转化。
- en: 'As one-hot vectors, the class labels become a matrix, ***Y***, with as many
    rows as there are training samples and as many columns as there are classes. A
    linear mapping from the output of the hidden layer, ***Z***, to the known labels
    (as one-hot vectors), ***Y***, uses a matrix, ***B***:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 作为one-hot向量，类标签变成了一个矩阵***Y***，行数与训练样本的数量相同，列数与类别数相同。从隐层输出***Z***到已知标签（作为one-hot向量）***Y***的线性映射使用矩阵***B***：
- en: '***Y*** = ***BZ***'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '***Y*** = ***BZ***'
- en: We want to find ***B*** as the second weight matrix.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望找到作为第二个权重矩阵的***B***。
- en: Because we know ***Z*** by pushing the training data through the hidden layer
    and ***Y***, the known labels, we generate ***B*** via
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们知道通过将训练数据推送通过隐层得到的***Z***和已知标签***Y***，我们通过以下方式生成***B***：
- en: '***YZ***^(–1) = ***B***'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '***YZ***^(–1) = ***B***'
- en: where ***Z***^(–1) is the inverse of the matrix ***Z***. Multiplying by the
    inverse of a matrix is akin to multiplying by the reciprocal of a scalar value;
    they cancel each other. To find the inverse of ***Z***, we need the Moore-Penrose
    pseudoinverse, which NumPy provides in its linear algebra module, `linalg`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 其中***Z***^(–1)是矩阵***Z***的逆。矩阵的逆相当于标量值的倒数，它们互相抵消。为了找到***Z***的逆，我们需要Moore-Penrose伪逆，这是NumPy在其线性代数模块`linalg`中提供的。
- en: 'We now have all we need to build the extreme learning machine:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了构建极限学习机所需的所有内容：
- en: Select a weight matrix, ***W***, and bias vector, ***b***, at random.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择权重矩阵***W***和偏置向量***b***。
- en: Pass the training data through the first hidden layer, ***Z*** = *h*(***WX***
    + ***b***), where ***X*** and ***Z*** are now matrices, one row for each training
    sample.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据通过第一隐层，***Z*** = *h*(***WX*** + ***b***)，其中***X***和***Z***现在是矩阵，每一行代表一个训练样本。
- en: Calculate the output weight matrix, ***B*** = ***YZ***^(–1), using the pseudoinverse
    of ***Z***, the output of the hidden layer.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用***Z***（隐层输出）的伪逆来计算输出权重矩阵，***B*** = ***YZ***^(–1)。
- en: Use ***W***, ***b***, and ***B*** as the weights and biases of the extreme learning
    machine.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用***W***、***b***和***B***作为极限学习机的权重和偏置。
- en: This is rather abstract. Let’s make it concrete in code.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这有些抽象。让我们通过代码使其具体化。
- en: '***Implementation***'
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***实现***'
- en: The file *elm.py* implements an extreme learning machine and applies it to the
    digits dataset. [Listing 6-14](ch06.xhtml#ch06list014) shows the `train` function.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 文件*elm.py*实现了一个极限学习机，并将其应用于数字数据集。[列表 6-14](ch06.xhtml#ch06list014)展示了`train`函数。
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '*Listing 6-14: Defining an extreme learning machine*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6-14：定义一个极限学习机*'
- en: The first line sets `inp` to the number of features in the training data, here
    784\. The next two lines define the smallest training feature value (`m`) and
    the difference between that and the largest (`d`).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行将`inp`设置为训练数据中的特征数，这里是784。接下来的两行定义了最小训练特征值(`m`)和它与最大特征值的差(`d`)。
- en: The following two lines generate the random weight matrix and bias vector, ***W***i
    (`w0`) and ***b*** (`b0`), by randomly sampling in the range of the training data,
    [*m*, *m* + *d*]. Again, ***W*** and ***b*** are completely random, but fixed
    once selected.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两行通过在训练数据范围[*m*, *m* + *d*]内随机采样生成随机权重矩阵和偏置向量，***W***i（`w0`）和***b***（`b0`）。同样，***W***和***b***是完全随机的，但一旦选择就固定下来。
- en: The final piece of the extreme learning machine is ***B*** (`w1`), which is
    found by passing the training data through the hidden layer
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 极限学习机的最后一部分是***B***（`w1`），它是通过将训练数据传递通过隐层得到的。
- en: '[PRE15]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: where `activation` is the selected activation function (*h*()).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`activation`是选择的激活函数（*h*()）。
- en: 'Finally, to define ***B***, we multiply ***YZ***^(–1):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了定义 ***B***，我们将 ***YZ*** 乘以 (–1)：
- en: '[PRE16]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The return value for the function is the defined and trained extreme learning
    machine: `(w0,b0,w1)`. Let’s take it for a test drive.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的返回值是已定义和训练好的极限学习机：`(w0,b0,w1)`。让我们进行测试。
- en: '***Testing***'
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***测试***'
- en: 'The file *elm.py*, from which we extracted `train`, trains an extreme learning
    machine to classify the digits dataset using a user-supplied number of nodes in
    the hidden layer. For example:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 文件 *elm.py* 从中提取的 `train` 函数，训练一个极限学习机来分类数字数据集，使用用户提供的隐藏层节点数。例如：
- en: '[PRE17]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We use the Mersenne Twister pseudorandom generator to build a machine with 200
    hidden layer nodes. The output shows the confusion matrix and the overall accuracy,
    83.5 percent. The confusion matrix is almost diagonal, which is a good sign.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Mersenne Twister 伪随机数生成器构建一个包含 200 个隐藏层节点的机器。输出显示混淆矩阵和总体准确率 83.5%。混淆矩阵几乎是对角线的，这很有意义。
- en: Let’s scrutinize the confusion matrix. The rows represent the true class labels
    from top to bottom, ones, fours, sevens, and nines. The columns, from left to
    right, are the model’s assigned class label (we’ll see how soon). Looking at the
    top row, of the 50 ones in the test dataset, the model called a one a one 49 times,
    but once called it a seven.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细分析混淆矩阵。行表示真实的类别标签，从上到下分别是 1、4、7 和 9。列从左到右是模型分配的类别标签（我们很快会看到）。查看最上面的一行，在测试数据集中，50
    个 1 中，模型将 1 预测为 1 次达 49 次，但有一次将其预测为 7。
- en: The model had the most difficulty with nines, the final row of the confusion
    matrix. The model got it right 37 out of 49 times, but it called it a seven 6
    times and a four 5 times. Only once did the model confuse a nine for a one.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在混淆矩阵的最后一行，处理数字 9 时遇到最大困难。在 49 次测试中，模型正确预测了 37 次，但错将 9 预测为 7 次和 4 次。只有一次，模型将
    9 错误预测为 1。
- en: The `train` function creates the extreme learning machine. To use it, we need
    `predict`, as shown in [Listing 6-15](ch06.xhtml#ch06list015).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`train` 函数创建了极限学习机。为了使用它，我们需要 `predict`，如 [清单 6-15](ch06.xhtml#ch06list015)
    所示。'
- en: '[PRE18]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*Listing 6-15: Prediction with an extreme learning machine*'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 6-15：使用极限学习机进行预测*'
- en: We pull the weight matrices and bias vector from the supplied model, and then
    pass the test data through the hidden layer and, finally, the output layer. The
    `activation` variable is set to the specific activation function currently in
    use—by default, the ReLU. We’ll experiment with different activation functions
    in the next section.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从提供的模型中提取权重矩阵和偏置向量，然后将测试数据通过隐藏层，最后通过输出层。`activation` 变量设置为当前使用的特定激活函数—默认情况下是
    ReLU。我们将在下一节中尝试不同的激活函数。
- en: The output of `predict` is a two-dimensional matrix with as many rows as are
    in `xtst` (200) and as many columns as there are classes (4). For example, the
    output of predict for the first test sample (`xtst[0]`) is
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict` 的输出是一个二维矩阵，行数与 `xtst` 中的行数相同（200），列数与类别数相同（4）。例如，第一个测试样本（`xtst[0]`）的预测输出为：'
- en: 0.19551782, 0.90971894, 0.05398019, –0.06542743
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 0.19551782, 0.90971894, 0.05398019, –0.06542743
- en: 'and the first known test label, as a one-hot vector, is:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个已知的测试标签，作为一个 one-hot 向量为：
- en: 0, 1, 0, 0
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 0, 1, 0, 0
- en: 'Notice the largest value in the model’s output vector is at index 1, as is
    the largest for the one-hot label. In other words, the first test sample belongs
    to class 1 (four), and the model successfully predicted class 1 as the most likely
    class. The last output value is negative: the model is not outputting a probability,
    but a decision function value where the largest is the most likely class label,
    even if we don’t have a true probability associated with that value.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模型输出向量中最大值的索引为 1，与 one-hot 标签中最大的值相同。换句话说，第一个测试样本属于类别 1（4），并且模型成功地预测了类别
    1 为最可能的类别。最后的输出值是负数：模型输出的不是概率，而是决策函数值，其中最大值表示最可能的类别标签，即使我们没有与该值相关联的真实概率。
- en: Therefore, to construct a confusion matrix, we will need code like [Listing
    6-16](ch06.xhtml#ch06list016).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要构建混淆矩阵，我们需要类似 [清单 6-16](ch06.xhtml#ch06list016) 的代码。
- en: '[PRE19]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Listing 6-16: Building a confusion matrix for an extreme learning machine*'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 6-16：为极限学习机构建混淆矩阵*'
- en: The `confusion` function returns the confusion matrix and overall accuracy,
    but instead of directly using the values in `ytst` and `prob`, we apply NumPy’s
    `argmax` function to return the index of the largest value in the four-element
    vectors.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`confusion` 函数返回混淆矩阵和总体准确率，但我们不是直接使用 `ytst` 和 `prob` 中的值，而是应用 NumPy 的 `argmax`
    函数来返回四个元素向量中最大值的索引。'
- en: '[Listing 6-17](ch06.xhtml#ch06list017) shows the remainder of *elm.py*, which
    loads the digit data-sets, scaling them by 256, and then trains and tests the
    extreme learning machine with three lines of code.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6-17](ch06.xhtml#ch06list017) 显示了 *elm.py* 的其余部分，该部分加载数字数据集，将其缩放到 256，然后使用三行代码训练和测试极限学习机。'
- en: '[PRE20]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*Listing 6-17: Training and testing an extreme learning machine*'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6-17：训练和测试极限学习机*'
- en: The `nodes` parameter is the number of nodes in the hidden layer as read from
    the command line.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`nodes` 参数是从命令行读取的隐藏层节点数。'
- en: How sensitive is the extreme learning machine to the number of nodes in the
    hidden layer? That’s a good question.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 极限学习机对隐藏层节点数有多敏感？这是一个很好的问题。
- en: By default, the code in *elm.py* uses a ReLU activation function. However, the
    file defines several other activation functions. In this section, we’ll explore
    each in combination with differing numbers of hidden layer nodes to find whether
    there’s a best combination.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，*elm.py* 中的代码使用 ReLU 激活函数。然而，文件中定义了几个其他的激活函数。在本节中，我们将探索每种激活函数与不同数量的隐藏层节点的组合，看看是否有最佳组合。
- en: 'The ReLU activation function uses NumPy’s `maximize` function, which returns
    the largest of the two arguments, element by element:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 激活函数使用 NumPy 的 `maximize` 函数，该函数按元素返回两个参数中的最大值：
- en: '[PRE21]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We’re not constrained to use only the ReLU. Classically, neural networks made
    heavy use of the sigmoid and hyperbolic tangent functions:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不局限于仅使用 ReLU。传统上，神经网络大量使用了 sigmoid 和双曲正切函数：
- en: '[PRE22]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Both of these functions return S-shaped curves. I added a factor of 0.01 to
    scale *x* in the sigmoid and hyperbolic tangent. This is not usually done, but
    it was necessary here to prevent overflow errors.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数都返回 S 形曲线。我在 sigmoid 和双曲正切函数中加入了 0.01 的系数来缩放 *x*。这通常不是做法，但在这里是必要的，以防止溢出错误。
- en: 'For fun, I defined several other activation functions:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有趣，我定义了几个其他激活函数：
- en: '[PRE23]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Let’s test the activation functions as the number of nodes in the hidden layer
    is varied from 10 to 400\. The code is in *elm_test.py*. It makes use of the `train`,
    `predict`, and `confusion` functions. The main loop looks like [Listing 6-18](ch06.xhtml#ch06list018).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试激活函数，随着隐藏层节点数从 10 到 400 的变化。代码位于 *elm_test.py* 中。它使用了 `train`、`predict`
    和 `confusion` 函数。主循环如 [列表 6-18](ch06.xhtml#ch06list018) 所示。
- en: '[PRE24]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '*Listing 6-18: Testing activation functions and hidden layer sizes*'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6-18：测试激活函数和隐藏层大小*'
- en: Fifty extreme learning machines are trained for each combination of activation
    function and hidden layer size, tracking the overall accuracy (`acc`). Notice
    the assignment of `act` to `activation`. Functions may be freely assigned to variables
    in Python and then used when the variable is referenced (as in `train`).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对每种激活函数和隐藏层大小的组合，训练了 50 个极限学习机，并跟踪整体准确度（`acc`）。注意将 `act` 赋值给 `activation`。在
    Python 中，函数可以自由赋值给变量，然后在引用该变量时使用（如在 `train` 中）。
- en: Run *elm_test.py* by passing it a randomness source (I used MT19937). When it
    finishes, run *elm_test_results.py* to parse the output and generate a plot like
    that of [Figure 6-3](ch06.xhtml#ch06fig03) showing the mean accuracy by hidden
    layer size and activation function. Error bars are present but small.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 *elm_test.py*，传入一个随机源（我使用了 MT19937）。当它完成后，运行 *elm_test_results.py* 解析输出并生成类似于
    [图 6-3](ch06.xhtml#ch06fig03) 的图表，显示按隐藏层大小和激活函数的平均准确率。误差条存在但较小。
- en: '![Image](../images/06fig03.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/06fig03.jpg)'
- en: '*Figure 6-3: Extreme learning machine performance as a function of the hidden
    layer size and activation function*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-3：极限学习机性能作为隐藏层大小和激活函数的函数*'
- en: '[Figure 6-3](ch06.xhtml#ch06fig03)’s most obvious statement is that *y* = *x*³
    is a lousy activation function, as it always results in inferior models compared
    to the others. Another interesting observation is that the activation functions
    follow the same shape: a rapid increase in model accuracy as the number of nodes
    in the hidden layer increases, followed by a maximum and a slower decline. The
    difference between activation functions is slight, especially near the maximum
    of 100 hidden nodes; however, the hyperbolic tangent came out on top in this run.
    In fact, `tanh` wins for most runs, so it’s fair to say that for this particular
    dataset, an extreme learning machine using `tanh` and 100 nodes in the hidden
    layer is the way to go.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-3](ch06.xhtml#ch06fig03)最显而易见的结论是，*y* = *x*³是一个糟糕的激活函数，因为它总是导致比其他激活函数更差的模型。另一个有趣的观察是激活函数的形状相似：随着隐藏层节点数量的增加，模型准确性迅速提高，然后达到最大值后缓慢下降。激活函数之间的差异很小，尤其是在100个隐藏节点的最大值附近；然而，双曲正切函数在本次实验中表现最佳。实际上，`tanh`在大多数实验中表现最好，因此可以公平地说，对于这个特定数据集，使用`tanh`和100个隐藏层节点的极限学习机是最合适的选择。'
- en: The identity activation function, *f*(***x***) = ***x***, eschews nonlinearity;
    all the performance comes from the linear top layer mapping the hidden layer output
    to the predictions per class.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 恒等激活函数，*f*(***x***) = ***x***，避免了非线性；所有的性能都来自于线性顶层将隐藏层的输出映射到每个类别的预测。
- en: '***Reckless Swarm Optimizations***'
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***鲁莽的群体优化***'
- en: An extreme learning machine’s attraction is the incredible speed with which
    a model is trained compared to all the calculations necessary to train a traditional
    neural network of the same size. Sure, the first model might not be that good,
    but trying a few times in a row and keeping the best-performing model seems a
    reasonable thing to do. I can imagine a scenario where an autonomous system might
    want to train a model quickly in response to rapidly changing input data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 极限学习机的吸引力在于，与训练相同规模的传统神经网络相比，模型训练的速度惊人。的确，第一次训练的模型可能不是那么好，但连续尝试几次并保留表现最好的模型似乎是合理的。我可以想象一种情景，其中自主系统可能需要快速训练一个模型，以应对快速变化的输入数据。
- en: 'However, the random part of the extreme learning machine—that is, the weight
    matrix and bias vector from the input to the hidden layer—made me wonder: Might
    we be able to learn the weight matrix and bias vector via a swarm optimization
    instead? Would this work, and if so, might it be any better than the random versions?
    My thought is clearly missing the point that extreme learning machines are meant
    to use random weights and biases, but I want to know if swarm optimization might
    prove helpful, even if vastly more computationally intensive than traditional
    neural network training.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，极限学习机中的随机部分——即从输入到隐藏层的权重矩阵和偏置向量——让我产生了疑问：我们是否可以通过群体优化来学习权重矩阵和偏置向量？这行得通吗？如果可以，是否比随机版本更有效？我的想法显然忽略了极限学习机本来就是通过随机权重和偏置来工作的，但我想知道即使比传统神经网络训练更具计算开销，群体优化是否可能发挥作用。
- en: 'The true challenge is the dimensionality of the problem. Our inputs are 784-element
    vectors. We learned that 100 nodes in the hidden layer seems a good thing to have,
    so the total dimensionality of the weight matrix and bias vector is:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的挑战在于问题的维度。我们的输入是784维的向量。我们已经得出结论，100个隐藏层节点似乎是一个不错的选择，因此权重矩阵和偏置向量的总维度是：
- en: 784 × 100 + 100 = 78,500
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 784 × 100 + 100 = 78,500
- en: We’ll be asking the swarms to search a space of 78,500 dimensions to come up
    with a good position, one that leads to a model with the highest accuracy. That’s
    a tall order.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要求群体在78,500维的空间中搜索，以找到一个好的位置，进而获得具有最高准确性的模型。这是一个艰巨的任务。
- en: The code I experimented with is in *elm_swarm.py*. I won’t walk through it,
    but you’ll see it follows similar optimization code from earlier chapters. The
    objective function uses each particle position as a weight matrix and bias vector,
    and then learns the output weight matrix and evaluates the model on the test set
    to produce an overall accuracy. Therefore, each call to the `Evaluate` method
    results in a trained and tested model.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我实验过的代码在*elm_swarm.py*中。我不会逐步讲解它，但你会看到它遵循了前几章中的类似优化代码。目标函数使用每个粒子的位置作为权重矩阵和偏置向量，然后学习输出权重矩阵，并在测试集上评估模型以得出整体准确性。因此，每次调用`Evaluate`方法都会得到一个经过训练和测试的模型。
- en: 'To run the code, use a command line like this:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行代码，可以使用这样的命令行：
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, differential evolution (`de`) searches for a model with `tanh` activation
    functions and 100 hidden layer nodes. The swarm has 20 particles and runs for
    60 iterations before reporting the final confusion matrix and accuracy. The best
    model is stored in the file *de.pkl*. The current best accuracy is shown on each
    iteration, so you can watch the swarm learn. Run *elm_swarm.py* without arguments
    to see all options.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，差分进化（`de`）用于寻找一个具有`tanh`激活函数和100个隐藏层节点的模型。该种群有20个粒子，并运行60次迭代，之后报告最终的混淆矩阵和准确率。最佳模型被存储在文件*de.pkl*中。当前最佳的准确率会在每次迭代时显示，因此你可以看到种群的学习过程。运行*elm_swarm.py*而不带参数可以查看所有选项。
- en: 'For example, the previous command produced the following output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，前一个命令产生了以下输出：
- en: '[PRE26]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The per-iteration best accuracy is shown along with the mean distance between
    the particles in the swarm. If the swarm is converging, this distance shrinks
    during the search, as it does here. The distance between two swarm particles uses
    the formula to find the distance between two points in two-dimensional or three-dimensional
    space
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代的最佳准确率与种群中粒子之间的平均距离一起显示。如果种群正在收敛，那么在搜索过程中，这个距离会缩小，正如这里所示。两个粒子之间的距离使用公式计算，找到二维或三维空间中两点之间的距离。
- en: '![Image](../images/f0196-01.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0196-01.jpg)'
- en: but extended to 78,500 dimensions. The result is still a scalar.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 但扩展到78,500维。结果仍然是一个标量。
- en: After 60 iterations, the swarm located a weight and bias vector leading to a
    93 percent overall accuracy on the held-out test set. There were four swarm best
    updates. A second run, using GWO for 600 iterations, found a model with 94 percent
    overall accuracy. In that run, the swarm collapsed from an initial inter-particle
    distance of around 90 to less than 1 by the time it reached iteration 600\. There
    were eight swarm best updates.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 经过60次迭代，种群找到了一个权重和偏置向量，使得在保留的测试集上的总体准确率达到了93%。这次运行有四次最佳种群更新。第二次运行使用GWO进行600次迭代，找到了一个总体准确率为94%的模型。在这次运行中，种群从初始的粒子间距离约90缩小到第600次迭代时不到1。总共进行了8次最佳种群更新。
- en: I conducted runs of *elm_swarm.py* for all algorithms, five runs for each. [Table
    6-1](ch06.xhtml#ch06tab01) displays the resulting average accuracies.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我对所有算法运行了*elm_swarm.py*，每个算法运行了五次。[表 6-1](ch06.xhtml#ch06tab01)显示了结果的平均准确率。
- en: '**Table 6-1:** Average Model Accuracies for Each Optimization Algorithm'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 6-1：** 每个优化算法的模型平均准确率'
- en: '| Algorithm | Accuracy (mean ± SE) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 准确率（均值 ± 标准误差） |'
- en: '| --- | --- |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GWO | 0.9260 ± 0.0087 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| GWO | 0.9260 ± 0.0087 |'
- en: '| Differential evolution | 0.9200 ± 0.0016 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 差分进化 | 0.9200 ± 0.0016 |'
- en: '| Bare-bones PSO | 0.9190 ± 0.0019 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 基本粒子群优化 | 0.9190 ± 0.0019 |'
- en: '| Jaya | 0.9180 ± 0.0034 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| Jaya算法 | 0.9180 ± 0.0034 |'
- en: '| GA | 0.9170 ± 0.0058 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| GA | 0.9170 ± 0.0058 |'
- en: '| RO | 0.9170 ± 0.0025 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| RO | 0.9170 ± 0.0025 |'
- en: '| PSO | 0.9160 ± 0.0024 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 粒子群优化（PSO） | 0.9160 ± 0.0024 |'
- en: The results are not statistically significantly different, but the ranking from
    best to worst is typical (except PSO). The GWO standard error of the mean is larger
    because one search found a model with an accuracy of 95.5 percent, the highest
    I encountered.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 结果在统计学上没有显著差异，但从最好到最差的排名是典型的（除了PSO之外）。GWO的均值标准误差较大，因为一次搜索找到了一个准确率为95.5%的模型，这是我遇到的最高准确率。
- en: How many runs of *elm.py* do we need, on average, to find a model that meets
    or exceeds a given accuracy? The file *elm_brute.py* generates extreme learning
    machine after extreme learning machine, for up to a given maximum number of iterations,
    attempting to find a model that meets or exceeds the specified test set accuracy.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们平均需要运行多少次*elm.py*，才能找到一个符合或超过给定准确率的模型？文件*elm_brute.py*会生成一个又一个极限学习机，最多进行给定最大次数的迭代，尝试找到一个符合或超过指定测试集准确率的模型。
- en: Structurally, *elm_brute.py* is a tweak to *elm.py* to do the model creation
    and testing in a loop and then report the performance if successful or note that
    it wasn’t able to meet the accuracy threshold. Running *elm_brute.py* for different
    thresholds, 10 runs for each with a maximum of 2,000 iterations, produced [Table
    6-2](ch06.xhtml#ch06tab02).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 从结构上看，*elm_brute.py*是对*elm.py*的一个改动，它将在一个循环中进行模型创建和测试，并在成功时报告性能，或记录无法满足准确率阈值的情况。对不同阈值运行*elm_brute.py*，每个阈值进行10次运行，最多2000次迭代，生成了[表
    6-2](ch06.xhtml#ch06tab02)。
- en: '**Table 6-2:** Number of Models Tried to Achieve a Given Accuracy'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 6-2：** 为达到给定准确率尝试的模型数量'
- en: '| **Target accuracy** | **Mean** | **Min** | **Max** | **Successes** |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| **目标准确率** | **均值** | **最小值** | **最大值** | **成功次数** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0.70 | 1 | 1 | 1 | 10 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 0.70 | 1 | 1 | 1 | 10 |'
- en: '| 0.75 | 1 | 1 | 1 | 10 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 0.75 | 1 | 1 | 1 | 10 |'
- en: '| 0.80 | 1 | 1 | 1 | 10 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 0.80 | 1 | 1 | 1 | 10 |'
- en: '| 0.85 | 2.2 | 1 | 6 | 10 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 0.85 | 2.2 | 1 | 6 | 10 |'
- en: '| 0.90 | 147.2 | 15 | 270 | 10 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 0.90 | 147.2 | 15 | 270 | 10 |'
- en: '| 0.91 | 504.2 | 87 | 1,174 | 9 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 0.91 | 504.2 | 87 | 1,174 | 9 |'
- en: '| 0.915 | 858.4 | 69 | 1,710 | 9 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 0.915 | 858.4 | 69 | 1,710 | 9 |'
- en: '| 0.92 | 788.3 | 74 | 1,325 | 4 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 0.92 | 788.3 | 74 | 1,325 | 4 |'
- en: The first column shows the target accuracy. Any model meeting or exceeding this
    accuracy is considered a success. The mean number of models needed to find one
    at or above the threshold comes next, followed by the minimum and maximum numbers.
    Finally, the number of successful runs at that threshold, out of 10, is shown.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列显示目标准确率。任何达到或超过该准确率的模型都被视为成功。接下来是找到一个达到或超过阈值模型所需的平均模型数，之后是最小和最大值。最后，显示在该阈值下，10次实验中成功的次数。
- en: Targets at or below 85 percent are easy to find, averaging a little more than
    two searches. At the 90 percent threshold, however, there is a sudden jump requiring
    the creation of about 150 models on average. The minimum of 15 and the maximum
    of 270 implies a long tail to the distribution of the number of models tested.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 85% 或以下的目标容易找到，平均只需进行两次搜索左右。然而，在 90% 的阈值下，突然出现一个跃升，平均需要创建大约 150 个模型。最少为 15，最多为
    270，这意味着测试模型数量的分布存在长尾现象。
- en: Above 90 percent, the mean number of models increases again, rather dramatically,
    including long tails of up to 1,710 for 91.5 percent. The number of successful
    searches also goes down, implying that 2,000 iterations were an insufficient maximum
    in most cases to find a model with 92 percent or greater accuracy.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在 90% 以上时，模型的平均数量再次大幅增加，甚至出现了长尾现象，在 91.5% 时最多可达到 1,710。成功搜索的次数也减少，这意味着大多数情况下，2000
    次迭代不足以找到准确率为 92% 或更高的模型。
- en: We now have two different approaches. The first blindly tries to find a suitable
    model by randomly assigning weights and biases and then testing over and over—a
    brute force approach. The second uses a principled swarm search to locate the
    weights and biases, and is more successful. For example, the previous swarm approach
    used 1,220 candidate models to find one with an accuracy of 93 percent, and a
    run of *elm_brute.py* needed 21,680 candidates to find a model with an accuracy
    of 93.5 percent.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有两种不同的方法。第一种方法通过随机分配权重和偏置并反复测试来盲目地寻找合适的模型——一种蛮力方法。第二种方法使用有原则的群体搜索来定位权重和偏置，并且更为成功。例如，前述的群体方法使用了
    1,220 个候选模型来找到一个准确率为 93% 的模型，而运行 *elm_brute.py* 需要 21,680 个候选模型才能找到一个准确率为 93.5%
    的模型。
- en: It’s impressive that the swarm techniques can find good models while searching
    such a high-dimensional space. The approach isn’t practical without seriously
    reengineering the code to be orders of magnitude faster, but that we achieved
    any level of success is fascinating.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 群体技术在搜索如此高维空间时能够找到优良模型，令人印象深刻。这个方法如果不对代码进行大规模优化以提高速度，是不切实际的，但我们取得的任何成功都让人着迷。
- en: Extreme learning machines are a prime example of randomness in action. Their
    structure invites experimentation, so please experiment. If you discover something
    interesting, let me know. In the meantime, let’s move on to our last example of
    randomness in machine learning.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 极限学习机是随机性在实际应用中的典型例子。它们的结构鼓励实验，所以请大胆尝试。如果你发现了有趣的东西，请告诉我。与此同时，让我们继续探讨机器学习中随机性的最后一个例子。
- en: '**Random Forests**'
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**随机森林**'
- en: A *random forest* is a collection (or ensemble) of *decision trees*. We’ll define
    these terms shortly. The ideas behind random forests were developed in the 1990s
    and brought together by Breiman in his appropriately named 2001 paper, “Random
    Forests.” Hence they have some history behind them. Decision trees themselves
    are even older, dating from the early 1960s. Let’s begin there.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机森林* 是一组（或集成）*决策树*。我们稍后会定义这些术语。随机森林背后的思想是在 1990 年代发展起来的，并由 Breiman 在他那篇恰如其分地命名为《随机森林》的
    2001 年论文中提出。因此，它们有着一定的历史背景。决策树本身甚至更早，可以追溯到 1960 年代初期。让我们从那里开始。'
- en: '***Decision Trees***'
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***决策树***'
- en: A decision tree is a machine learning model consisting of a series of yes or
    no questions asked not of a person, but of a feature vector. The sequence of answers
    for that feature vector moves through the tree from the root node to a leaf, a
    terminal node. We then assign the class label associated with the leaf to the
    input feature vector.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种机器学习模型，由一系列是或否的问题组成，这些问题并不是问人，而是问特征向量。特征向量的答案序列从根节点开始，沿着树的路径走到一个叶子节点，即终端节点。然后，我们将与叶子节点关联的类标签赋予输入的特征向量。
- en: Decision trees have the benefit of interpretability—by their very operation,
    they explain themselves. Neural networks can’t easily explain themselves, an issue
    that’s given birth to *Explainable AI (XAI)*, a subfield of modern deep learning.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的一个优点是可解释性——通过它们的操作，决策树能够自我解释。神经网络难以自我解释，这个问题催生了 *可解释人工智能（XAI）*，这是现代深度学习的一个子领域。
- en: Decision trees are best understood with an example, for which we’ll use a tiny
    dataset of two measurements of three species of iris flowers. The dataset is two-dimensional;
    there are two features with 150 samples total. We’ll use the first 100 for training
    the decision tree and the remaining 50 for testing. As the dataset has only two
    dimensions, we can plot its features by class; see [Figure 6-4](ch06.xhtml#ch06fig04).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树最好通过一个示例来理解，在这个示例中，我们将使用一个包含三种鸢尾花的两项测量的小数据集。该数据集是二维的；有两个特征，总共有 150 个样本。我们将使用前
    100 个样本来训练决策树，其余的 50 个样本用于测试。由于数据集只有两个维度，我们可以按类别绘制其特征；见 [图 6-4](ch06.xhtml#ch06fig04)。
- en: '![Image](../images/06fig04.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/06fig04.jpg)'
- en: '*Figure 6-4: The iris features*'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-4：鸢尾花特征*'
- en: In [Figure 6-4](ch06.xhtml#ch06fig04), circles represent class 0, squares class
    1, and diamonds class 2\. Class 0 is well separated from the other two classes,
    which overlap considerably. Therefore, we might expect the decision tree classifier
    to do well with class 0 and be most often confused by class 1 and class 2.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 6-4](ch06.xhtml#ch06fig04) 中，圆圈代表类别 0，方块代表类别 1，菱形代表类别 2。类别 0 与其他两个类别有很好的分离，而后者的重叠较大。因此，我们可以预期决策树分类器对类别
    0 会表现得很好，但经常会把类别 1 和类别 2 混淆。
- en: 'Let’s build a decision tree for this dataset. The code I’m using, which also
    generates [Figure 6-4](ch06.xhtml#ch06fig04), is in the file *iris_tree.py*. It
    uses scikit-learn’s `DecisionTreeClassifier` class and limits the depth of the
    tree to three. As with most scikit-learn classes, the `fit` method uses the training
    data and the `predict` method uses the test data. The output of *iris_tree.py*
    is:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为这个数据集构建一棵决策树。我使用的代码也生成了 [图 6-4](ch06.xhtml#ch06fig04)，代码文件是 *iris_tree.py*。它使用了
    scikit-learn 的 `DecisionTreeClassifier` 类，并将树的深度限制为三层。像大多数 scikit-learn 类一样，`fit`
    方法使用训练数据，`predict` 方法使用测试数据。*iris_tree.py* 的输出是：
- en: '[PRE27]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This is a confusion matrix and has an overall accuracy of 74 percent. Class
    0 was perfectly classified, 18 out of 18, while the decision tree labeled most
    of class 1 as class 2.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个混淆矩阵，总体准确率为 74%。类别 0 完全被正确分类，18 个样本中有 18 个被正确分类，而决策树将大多数类别 1 错误标记为类别 2。
- en: '[Figure 6-5](ch06.xhtml#ch06fig05) shows what the tree looks like.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-5](ch06.xhtml#ch06fig05) 显示了决策树的样子。'
- en: '![Image](../images/06fig05.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/06fig05.jpg)'
- en: '*Figure 6-5: The iris decision tree*'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-5：鸢尾花决策树*'
- en: The tree’s root is at the top. Each box is a node, and the first line of each
    box contains a question—in this case, “Is *x*[0] ≤ 5.45?” or is feature 0 less
    than or equal to 5.45? If the answer is yes, move to the left; otherwise, move
    to the right. Then consider the question in that node. Continue this process until
    you reach a *leaf*, or a node with no children.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 树的根位于顶部。每个框是一个节点，每个框的第一行包含一个问题——在本例中是“*x*[0] ≤ 5.45？”或者说特征 0 是否小于或等于 5.45？如果答案是“是”，则向左移动；否则，向右移动。然后考虑该节点中的问题。继续这个过程，直到到达一个*叶节点*，或者没有子节点的节点。
- en: The value part of a node indicates the number of training samples of each class
    present at that node. For example, the leftmost leaf node has `value=[1,0,0]`,
    meaning only one member of class 0 is present at this node. Therefore, any path
    leading to this node assigns class 0 to the input feature vector. Likewise, the
    leaf immediately to the right is labeled `[0,5,0]`, so feature vectors leading
    to this node are assigned to class 1\. Finally, the leaf second from the right
    is labeled `[1,18,24]`, meaning one class 0 training sample landed in this node,
    as did 24 samples of class 2\. The majority rules when more than one class is
    represented, so this node assigns feature vectors to class 2.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的值部分表示该节点处每个类别的训练样本数量。例如，最左边的叶节点的 `value=[1,0,0]`，表示该节点中只有一个类别 0 的样本。因此，任何指向该节点的路径都会将类别
    0 分配给输入特征向量。同样，紧邻右侧的叶节点标记为 `[0,5,0]`，因此指向该节点的特征向量会被分配为类别 1。最后，第二个右侧的叶节点标记为 `[1,18,24]`，表示该节点中有
    1 个类别 0 的训练样本和 24 个类别 2 的样本。当多个类别都出现在同一节点时，按照多数规则，该节点会将特征向量分配为类别 2。
- en: 'There are two other lines in each node: samples and the Gini index. The former
    is the sum of the values vector, the number of training samples present at that
    node. The decision tree algorithm uses the Gini index to split nodes; we won’t
    go into details here, but you can check out the scikit-learn documentation to
    learn more.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点中有另外两行：样本数和基尼指数。前者是该节点上训练样本的数量，后者是该节点的值向量的和。决策树算法使用基尼指数来拆分节点；我们在这里不详细讨论，但你可以查看
    scikit-learn 文档了解更多信息。
- en: Classically, decision trees are deterministic; the same dataset generates the
    same decision tree. Scikit-learn modifies this behavior somewhat, but by fixing
    the pseudorandom seed, which I do recklessly in *iris_tree.py*, we can generate
    a repeatable tree. We’ll assume going forward that decision trees are entirely
    deterministic.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的决策树是确定性的；相同的数据集会生成相同的决策树。Scikit-learn 对此行为做了一些修改，但通过固定伪随机种子（我在*iris_tree.py*中鲁莽地这么做），我们可以生成一个可重复的决策树。我们接下来假设决策树是完全确定性的。
- en: Decision trees explain themselves. For example, an input feature vector of {5.6,
    3.3} will traverse the path in [Figure 6-6](ch06.xhtml#ch06fig06).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以自我解释。例如，输入特征向量{5.6, 3.3}将遍历[图6-6](ch06.xhtml#ch06fig06)中的路径。
- en: '![Image](../images/06fig06.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/06fig06.jpg)'
- en: '*Figure 6-6: A path through the decision tree*'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-6：决策树的路径*'
- en: This is an example of class 0 because feature 0 is between 5.45 and 5.75, and
    feature 1 is greater than 3.25.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这是类0的一个示例，因为特征0的值在5.45和5.75之间，特征1的值大于3.25。
- en: The deterministic nature of decision trees led to the development of random
    forests. The decision tree for a particular dataset either works well or doesn’t;
    they tend to *overfit* the data, being too sensitive to the particulars of the
    training set and not sensitive enough to the general characteristics of the type
    of data the model might encounter—at least not as sensitive as we’d like. In other
    words, they might do well on the training set and less well on everything else.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的确定性特征促成了随机森林的出现。对于某个特定的数据集，决策树要么表现很好，要么表现不好；它们通常会*过拟合*数据，对训练集的具体特征过于敏感，而对模型可能遇到的数据类型的整体特征不够敏感——至少不如我们希望的那样敏感。换句话说，它们可能在训练集上表现良好，但在其他所有数据上表现较差。
- en: Let’s learn how we can curtail the decision tree’s penchant for overfitting
    by introducing randomness.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何通过引入随机性来遏制决策树过拟合的倾向。
- en: '***Additional Randomness***'
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***额外的随机性***'
- en: Three techniques turn a solitary decision tree into a forest of decision trees,
    or a random forest. The first is *bagging*, which generates many new datasets
    by sampling the original dataset with replacement (called *bootstrapping*). The
    second uses random subsets of the available features for each new, bootstrapped
    dataset, and the third is *ensembling*, the creation of multiple models that somehow
    vote or otherwise combine their output. Let’s review these techniques before putting
    them together to grow random forests.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种技术将一个孤立的决策树转变为一个决策树森林，或称随机森林。第一种是*袋装法*（bagging），它通过对原始数据集进行有放回抽样（称为*自助抽样*）生成多个新的数据集。第二种是对于每个新的自助抽样数据集，使用可用特征的随机子集，第三种是*集成法*（ensembling），即创建多个模型，这些模型以某种方式投票或以其他方式组合它们的输出。让我们在将这些技术结合起来构建随机森林之前先回顾一下它们。
- en: '**Creating Datasets with Bagging**'
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**通过袋装法创建数据集**'
- en: I have a dataset with 100 samples, feature 0 from the iris dataset. The values
    are measurements from a parent distribution, a population from which this particular
    collection of values is a sample. The sample has a mean value of 5.85, which we
    can take as an *estimate* of the population mean, but not necessarily the actual
    value. Over what range of values do we expect, with 95 percent confidence, to
    find the actual population mean?
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个包含100个样本的数据集，其中特征0来自鸢尾花数据集。这些值是来自一个父分布的测量值，特定的这些值集合是该父分布的一个样本。样本的平均值为5.85，我们可以将其视为对总体均值的*估计*，但不一定是实际的值。我们预计，在95%的置信度下，实际的总体均值将在什么范围内？
- en: We have only one set of 100 values, but we’ll use bootstrapping to generate
    another set, taken randomly from the first and allowing for the possibility of
    selecting the same sample more than once. This is known as “sampling with replacement.”
    We now have two collections of samples, both plausibly from the parent population.
    We can repeat this process many times to generate many sets, each of which has
    a mean value. Then, with the collection of mean values, we use NumPy’s `quantile`
    function to estimate the 95 percent confidence range.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只有一组 100 个值，但我们将使用引导法生成另一组数据，该组数据是从第一组中随机抽取的，并允许同一个样本被选择多次。这被称为“带替换的抽样”。现在我们有两组样本，它们都可以合理地认为来自母体总体。我们可以多次重复这个过程，生成多个数据集，每个数据集都有一个均值。然后，利用这些均值集合，我们使用
    NumPy 的 `quantile` 函数来估算 95% 的置信范围。
- en: The file *bootstrap.py* implements this process. It loads the iris training
    set, keeping only feature 0, and then generates 10,000 bootstrap datasets, keeping
    the mean value of each. To get the 95 percent confidence interval, we need to
    know the 2.5 percent and 97.5 percent quantile values. A quantile provides the
    percentiles for a dataset. The 50th percentile is the median, the middle value
    once the data has been sorted. The 25th percentile means that 25 percent of the
    data is below this value, while the 75th percentile means that 75 percent of the
    data is under this value.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 文件 *bootstrap.py* 实现了这个过程。它加载鸢尾花训练集，保留特征 0，然后生成 10,000 个引导数据集，保留每个数据集的均值。为了获得
    95% 的置信区间，我们需要知道 2.5% 和 97.5% 的分位数值。分位数提供了数据集的百分位数。第 50 百分位数是中位数，即排序后的数据的中间值。第
    25 百分位数表示 25% 的数据低于该值，而第 75 百分位数则表示 75% 的数据低于该值。
- en: 'To get the 95 percent confidence range, we want the values where 95 percent
    of the data is between them, implying we need to exclude 5 percent of the data:
    the bottom and top 2.5 percents. For that, we need the 2.5th and 97.5th percentiles,
    both of which we find courtesy of NumPy. Let’s review the code, shown in [Listing
    6-19](ch06.xhtml#ch06list019).'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得 95% 的置信范围，我们需要找出 95% 数据落在其中的值，这意味着我们需要排除 5% 的数据：底部和顶部各 2.5%。为此，我们需要找出第
    2.5 百分位数和 97.5 百分位数，这两个值可以通过 NumPy 获得。让我们回顾一下代码，见 [清单 6-19](ch06.xhtml#ch06list019)。
- en: '[PRE28]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '*Listing 6-19: Bootstrapping confidence intervals*'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 6-19: 引导法置信区间*'
- en: First, load the iris training data, keeping feature 0 (`x`). Then generate 10,000
    bootstrap versions of `x` tracking the mean of each. Use `quantile` to find the
    lower (`L`) and upper (`U`) confidence interval bounds before reporting them.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载鸢尾花训练数据，保留特征 0（`x`）。然后生成 10,000 个 `x` 的引导版本，并跟踪每个版本的均值。使用 `quantile` 查找下界（`L`）和上界（`U`）置信区间，再报告它们。
- en: The `bootstrap` function creates `n`, a vector of the same length as `x` where
    each value is an integer in the range 0 through `len(x)-1`. These are indices
    into `x` where it’s possible the same index appears more than once—a sample with
    replacement.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '`bootstrap` 函数创建了一个与 `x` 长度相同的向量 `n`，其中每个值是范围从 0 到 `len(x)-1` 的整数。这些是 `x` 中的索引，可能会出现相同的索引多次——一种带有替换的抽样。'
- en: 'Each run of *bootstrap.py* produces a slightly different range:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 每次运行 *bootstrap.py* 都会产生略有不同的范围：
- en: '[PRE29]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This output indicates that the true population mean is, with 95 percent confidence,
    between 5.694 and 6.009\. Without the bootstrap process, we couldn’t know this
    range from a single set of measurements. If we made assumptions about the shape
    of the distribution, that it’s normal or follows a t-distribution, then we could
    make an estimate; by bootstrapping, we don’t need to assume normally distributed
    data.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出表示，真实的总体均值在 95% 的置信度下介于 5.694 和 6.009 之间。没有引导法过程，我们无法仅凭一个测量集知道这个范围。如果我们对分布的形状做出假设，认为它是正态分布或遵循
    t 分布，那么我们可以进行估算；而通过引导法，我们不需要假设数据是正态分布的。
- en: Bootstrapping is a helpful technique to get confidence intervals when building
    random forests because each bootstrapped dataset is a plausible collection of
    measurements. In that sense, the bootstrapped datasets are newly acquired datasets
    for training a model. Bagging is the process of using bootstrapped datasets to
    train multiple decision trees.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 引导法是一种在构建随机森林时获取置信区间的有用技术，因为每个引导数据集都是一组合理的测量数据。从这个角度来看，引导数据集是用于训练模型的新增数据集。袋装法（Bagging）是利用引导数据集训练多个决策树的过程。
- en: '**Combining Models with Ensembling**'
  id: totrans-305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**模型集成的组合**'
- en: Bagging helps because each decision tree is trained on a slightly different
    dataset; anything causing overfitting in one training set is hopefully compensated
    for in another.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋方法有帮助，因为每棵决策树都是在稍微不同的数据集上训练的；任何导致某个训练集过拟合的因素，希望能在另一个训练集中得到补偿。
- en: We’ll make an ensemble of decision trees with bagging, in which we train multiple
    models using bootstrapped datasets and average their predictions to see if it
    improves results over ordinary decision trees. We’ll use the histology dataset
    from the beginning of this chapter. The file *bagging.py* trains a user-supplied
    number of decision trees, each with a different bootstrapped version of the histology
    dataset. We’ll then apply each model to the histology test data and average the
    resulting predictions to produce an ensemble output. Averaging model output is
    one approach to ensembling; we’ll use another, voting, later in this chapter.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用装袋方法创建一个决策树集成，其中我们使用自助采样数据集训练多个模型，并平均它们的预测结果，看看是否能改善普通决策树的结果。我们将使用本章开头的组织学数据集。文件*bagging.py*训练一个用户指定数量的决策树，每棵树都使用组织学数据集的不同自助采样版本。然后，我们将每个模型应用于组织学测试数据，并平均得到的预测结果，生成集成输出。平均模型输出是一种集成方法；我们将在本章后面使用另一种方法：投票。
- en: Consider the relevant code in [Listing 6-20](ch06.xhtml#ch06list020).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[列表6-20](ch06.xhtml#ch06list020)中的相关代码。
- en: '[PRE30]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '*Listing 6-20: Using bagging to build an ensemble of decision trees*'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表6-20：使用装袋方法构建决策树集成*'
- en: The code defines a `Bootstrap` function, loads the histology train and test
    datasets, creates and trains multiple decision trees using bootstrapped training
    sets, and makes predictions on the test data before averaging the results to create
    a final, aggregate set of predictions.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 代码定义了一个`Bootstrap`函数，加载组织学训练和测试数据集，使用自助采样训练集创建并训练多棵决策树，然后在测试数据上做出预测，最后将结果求平均，生成最终的预测集。
- en: The first `for` loop creates a decision tree object (that is, an instance of
    `DecisionTreeClassifier`). If `bag` is true, it trains the tree using a bootstrapped
    version of the training data. If `bag` is false, it uses the entire dataset each
    time (no bagging). The `Bootstrap` function needs to select both the feature vectors
    and the proper label.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`for`循环创建了一个决策树对象（即`DecisionTreeClassifier`的实例）。如果`bag`为真，它使用自助采样版本的训练数据来训练树；如果`bag`为假，它每次使用整个数据集（没有装袋）。`Bootstrap`函数需要选择特征向量和正确的标签。
- en: The next loop creates `preds`, a list of predictions for each decision tree.
    Each tree was trained on a different bootstrapped dataset, so if `bag` is true,
    the predictions will have slightly different errors. Turning `preds` into a NumPy
    array lets us average down the rows to get a single vector that’s the average
    of all tree outputs for each test sample (the columns). We want to assign a class
    label, 0 or 1, to the average, so we add 0.5 and then use `floor` to round to
    the nearest integer (0 or 1).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的循环创建了`preds`，这是每棵决策树的预测列表。每棵树都是在不同的自助采样数据集上训练的，因此如果`bag`为真，预测结果将有些微差异。将`preds`转为NumPy数组后，我们可以对每一行求平均，得到一个单一的向量，表示每个测试样本（列）的所有树输出的平均值。我们希望给这个平均值分配一个类别标签，0或1，因此我们加上0.5，然后使用`floor`函数将其舍入到最接近的整数（0或1）。
- en: Finally, a call to `Confusion` builds the confusion matrix and overall accuracy,
    which later code (not shown) displays.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，调用`Confusion`构建混淆矩阵和总体准确率，后续代码（未展示）将显示结果。
- en: 'One run of *bagging.py*, using a collection of 60 trees and bagging, returned:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 运行一次*bagging.py*，使用60棵树和装袋方法，返回结果为：
- en: '[PRE31]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The overall ensemble accuracy was 92.98 percent. The accuracies of the first
    six decision trees are also shown. Your runs will produce different output.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 总体集成准确率为92.98%。前六棵决策树的准确率也显示在内。你的运行结果可能会有所不同。
- en: 'Running the code again with no bagging returns:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 再次运行代码且不使用装袋方法时，返回的结果是：
- en: '[PRE32]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This is a significantly worse outcome.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个显著较差的结果。
- en: I ran *bagging.py* 30 times, first with bagging and again without. The respective
    mean accuracies were 92.73 percent and 89.77 percent, with a p-value of less than
    10^(–10) using the Mann-Whitney U test. Bagging has a dramatic effect on the quality
    of the models.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我运行了*bagging.py* 30次，第一次使用了装袋方法，第二次没有使用。各自的平均准确率为92.73%和89.77%，使用Mann-Whitney
    U检验的p值小于10^(-10)。装袋对模型质量有显著影响。
- en: 'Bootstrapped training sets and ensembling make up two-thirds of a random forest.
    Now let’s add the remaining third: random feature sets.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 自助采样训练集和集成方法构成了随机森林的三分之二。现在让我们添加剩下的三分之一：随机特征集。
- en: '**Using Random Feature Sets**'
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**使用随机特征集**'
- en: The final ingredient in a random forest is to use random subsets of the available
    features. Traditionally, the number of features we’ll use is the square root of
    the available features. For example, the histology dataset has 30 features, so
    we’ll use 5, selected randomly, whenever we want a bootstrapped dataset.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的最终要素是使用可用特征的随机子集。传统上，我们使用的特征数量是可用特征的平方根。例如，组织学数据集有30个特征，所以我们将在每次需要自助数据集时，随机选择5个特征。
- en: 'The formula is as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 公式如下：
- en: Select a bootstrapped dataset using five randomly selected features.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用五个随机选择的特征选择一个自助数据集。
- en: Train a decision tree using this dataset. Keep it and the specific set of features
    selected (for testing).
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此数据集训练决策树。保持它和选择的特定特征集（用于测试）。
- en: Repeat step 1 and step 2 for each tree in the forest.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对森林中的每棵树重复步骤1和步骤2。
- en: Apply each tree to the test data using only the features that the tree expects.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试数据应用每棵树，只使用树所期望的特征。
- en: Average the results across the test set to get the final predictions from the
    random forest.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试集的结果取平均，以获得随机森林的最终预测。
- en: In code, adding random feature selection is only a minor tweak to the bagging
    example; take a look at *forest.py*. [Listing 6-21](ch06.xhtml#ch06list021) shows
    the relevant changes from *bagging.py*.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，添加随机特征选择只是对装袋示例的一个小调整；查看*forest.py*。 [列表 6-21](ch06.xhtml#ch06list021)显示了与*bagging.py*的相关更改。
- en: '[PRE33]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '*Listing 6-21: Implementing a random forest*'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6-21：实现一个随机森林*'
- en: First, we must modify `Bootstrap` to select not only a random sampling of the
    training set (`n`) but also a random set of features (`nf` leading to `m`). The
    particular features extracted must be returned so we can use them at test time.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须修改`Bootstrap`，不仅要选择训练集的随机采样（`n`），还要选择一个随机的特征集（`nf`，最终得到`m`）。必须返回提取的特定特征，以便在测试时使用。
- en: The second paragraph trains the trees as before, but drags `m` along for the
    ride. Finally, at test time, we apply each tree to `xtst` after keeping only the
    proper subset of features. The output of *forest.py* is identical to that of *bagging.py*,
    minus the first six individual tree accuracies.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 第二段像之前一样训练树，但同时带上`m`。最后，在测试时，我们对`xtst`应用每棵树，只保留适当的特征子集。*forest.py*的输出与*bagging.py*的输出相同，除了前六个单独树的准确率。
- en: I ran the code 30 times using 60 trees, as before, to get the mean accuracies
    in [Table 6-3](ch06.xhtml#ch06tab03). I’ve included previous mean accuracies to
    show the improvement as each phase of the random forest is added.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我按照之前的方法运行了代码30次，使用60棵树，以获得[表 6-3](ch06.xhtml#ch06tab03)中的平均准确率。我包含了之前的平均准确率，以展示随着每个阶段的随机森林添加，准确率的提升。
- en: '**Table 6-3:** Mean Accuracy by Model Type'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 6-3：** 按模型类型的平均准确率'
- en: '| **Option** | **Mean accuracy** |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| **选项** | **平均准确率** |'
- en: '| --- | --- |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| No bagging | 89.47% |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 无装袋 | 89.47% |'
- en: '| Bagging, ensembling | 92.98% |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 装袋，集成 | 92.98% |'
- en: '| Bagging, ensembling, random features | 95.25% |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 装袋，集成，随机特征 | 95.25% |'
- en: All random forest steps combined led to a significant improvement over a simple
    decision tree.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 所有随机森林步骤结合起来，相比简单的决策树，显著提高了性能。
- en: 'As you might expect, scikit-learn also supports random forests via the `RandomForestClassifier`
    class:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所料，scikit-learn也通过`RandomForestClassifier`类支持随机森林：
- en: '[PRE34]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `RandomForestClassifier` class supports all three tricks plus additional
    tricks; see the scikit-learn documentation. Training 30 instances of `RandomForestClassifier`
    using 60 trees and all defaults results in a mean accuracy of 96.55 percent, which
    is even better than *forest.py*.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomForestClassifier`类支持所有三种技巧以及其他技巧；请参阅scikit-learn文档。使用60棵树和所有默认设置训练30个`RandomForestClassifier`实例，结果的平均准确率为96.55%，比*forest.py*的结果还要好。'
- en: '***Models Combined with Voting***'
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***与投票结合的模型***'
- en: Before we leave this section, let’s investigate how the size of the forest affects
    performance. For this experiment, we’ll switch to the MNIST digits dataset. Also,
    instead of averaging each model’s output, we’ll vote to assign class labels.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们离开这一部分之前，让我们研究森林的大小如何影响性能。为了进行此实验，我们将切换到MNIST数字数据集。此外，我们将不再对每个模型的输出取平均，而是通过投票来分配类别标签。
- en: 'The code we want is in *forest_mnist.py*. Built on *forest.py*, it loops over
    different sized forests to determine the mean accuracy for 20 models with that
    many trees. The output is a plot, but before we examine it, let’s review how to
    implement voting:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的代码在*forest_mnist.py*中。它基于*forest.py*，通过不同大小的森林循环，以确定使用那么多棵树的20个模型的平均准确率。输出是一个图形，但在我们检查它之前，让我们回顾一下如何实现投票：
- en: '[PRE35]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The first code paragraph captures the predictions for each tree in the forest.
    The second creates `pred`, a vector that holds the most commonly selected class
    label across each model for each test sample. To get the winner for test sample
    `i`, we first use `bincount` to count how often each label appears for all models
    (the rows of `preds`) and then use `argmax` to return the index of the class label
    with the highest count. Ties go to the first occurrence of the highest value,
    that is, the lower index. Breaking ties this way might introduce a slight bias
    toward lower class labels, but we can live with that—consider it a systematic
    error as all forest sizes are likewise affected.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个代码段捕捉了森林中每棵树的预测结果。第二个代码段创建了`pred`，一个向量，存储每个模型在每个测试样本上最常选择的类别标签。为了获取测试样本`i`的获胜者，我们首先使用`bincount`统计每个标签在所有模型中出现的频率（`preds`的行），然后使用`argmax`返回出现次数最多的类别标签的索引。若出现平局，则选择首次出现最大值的标签，即较小的索引。通过这种方式打破平局可能会对较低的类别标签引入轻微偏差，但我们可以接受这一点——可以将其视为一种系统性误差，因为所有森林大小都受到相同影响。
- en: '[Figure 6-7](ch06.xhtml#ch06fig07) illustrates how the accuracy changes with
    forest size.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-7](ch06.xhtml#ch06fig07)展示了准确率如何随着森林大小的变化而变化。'
- en: '![Image](../images/06fig07.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/06fig07.jpg)'
- en: '*Figure 6-7: Mean accuracy on the digits dataset as a function of forest size*'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-7：数字数据集的平均准确率与森林大小的关系*'
- en: At first, increasing the number of trees helps, but eventually, saturation sets
    in, producing diminishing returns as the forest grows.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，增加树木数量有助于提高性能，但最终，随着森林规模的增大，会出现饱和现象，回报递减。
- en: '**Exercises**'
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**练习**'
- en: 'Machine learning is a vast and critically important field. Here are some exercises
    related to the topics of this chapter to help you enhance your machine learning
    expertise and intuition:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个广泛且至关重要的领域。以下是与本章主题相关的一些练习，帮助你提升机器学习的专业知识和直觉：
- en: We augmented the digits dataset rather conservatively, with small rotations
    and zooming. Other image processing options might help improve the performance
    of the models in this chapter. Experiment with them by adding other options to
    the `augment` function in *build_mnist_dataset.py*. The `Image` and `ImageFilter`
    classes in Python’s `PIL` module might be helpful. Use `Image.fromarray` to convert
    a NumPy array of `dtype` uint8 to a PIL image. To go the other way, pass the `Image`
    object to `np.array`.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对数字数据集进行了相对保守的数据增强，进行了小幅旋转和缩放。其他图像处理选项可能有助于提高本章模型的性能。通过向`augment`函数中添加其他选项来进行实验，文件位置为*build_mnist_dataset.py*。Python中`PIL`模块的`Image`和`ImageFilter`类可能会有所帮助。使用`Image.fromarray`将一个`dtype`为uint8的NumPy数组转换为PIL图像。反向操作时，将`Image`对象传递给`np.array`即可。
- en: The `Classifier` subclass of `MLPClassifier`, used by *init_test.py*, defines
    multiple approaches to initializing a neural network. Add new ones to see how
    they affect results. What happens if all weight matrices are initially zero or
    some constant value? What about when the bias vectors are zero? Consider experimenting
    with the beta distribution (`np.random.beta`), as adjusting its two parameters
    can generate samples with a wide range of shapes.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MLPClassifier`的`Classifier`子类在*init_test.py*中定义了多种初始化神经网络的方法。可以添加新的方法，看看它们如何影响结果。如果所有权重矩阵初始值为零或某个常数值，会发生什么？如果偏置向量为零呢？考虑实验beta分布（`np.random.beta`），因为调整其两个参数可以生成形状各异的样本。'
- en: The extreme learning machine uses a randomly generated weight matrix and bias
    vector to map the input to the first hidden layer. The selection method we used
    in this chapter is commonly found in the literature. What happens if you alter
    this method and select random values according to a nonuniform distribution? Consider
    `np.random.normal` (or the `RE`-based version) along with the beta distribution.
    Is there much of an effect?
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极限学习机使用一个随机生成的权重矩阵和偏置向量将输入映射到第一个隐藏层。我们在本章中使用的选择方法在文献中比较常见。如果改变该方法，按非均匀分布选择随机值会发生什么呢？可以考虑使用`np.random.normal`（或者基于`RE`的版本）以及beta分布。会产生很大影响吗？
- en: Create a two-layer extreme learning machine where `w0`, `w1`, `b0`, and `b1`
    are randomly selected. The final weight matrix, now `w2`, will be learned as before
    from the output of the second hidden layer. Compare the performance to the single-layer
    version.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个两层极限学习机，其中`w0`、`w1`、`b0`和`b1`是随机选择的。最终的权重矩阵`w2`将像以前一样从第二层隐藏层的输出中学习。将其性能与单层版本进行比较。
- en: You’ll find files in the *datasets* directory beginning with *mnist_14x14*.
    They contain 14×14-pixel versions of all MNIST digits, [0, 9]. Try them in place
    of the four-digit version used throughout the chapter. How well do the various
    models perform?
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*datasets*目录中，你会找到以*mnist_14x14*开头的文件。它们包含了所有MNIST数字[0, 9]的14×14像素版本。尝试将它们替换为本章中使用的四位数字版本。各种模型的表现如何？
- en: Modify *elm_brute.py* to track the number of all tested models over many runs
    for the same accuracy. Then use `np.histogram` and Matplotlib to plot histograms
    for a fixed accuracy (like 0.92). What shape do they have? Does the shape make
    sense to you?
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改*elm_brute.py*，以跟踪相同准确度下经过多次运行测试的所有模型的数量。然后使用`np.histogram`和Matplotlib绘制固定准确度（例如0.92）的直方图。它们的形状是什么样的？这种形状对你来说有意义吗？
- en: We largely ignored scikit-learn’s `RandomForestClassifier` in favor of our homegrown
    version. Explore scikit-learn’s approach in more detail by reading through the
    documentation page for the class and experimenting with the different options.
    Consider using both the histology and digits datasets.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们主要忽略了scikit-learn的`RandomForestClassifier`，转而使用我们自己开发的版本。通过阅读该类的文档页面并尝试不同选项，深入探索scikit-learn的方法。考虑使用组织学数据集和数字数据集。
- en: Run *rf_vs_mlp.py* followed by *rf_vs_mlp_results.py*. (Some patience is required.)
    Consider the output, which demonstrates how scaling the input feature vectors
    affects model performance. Which type of model is sensitive to the relative ranges
    of the features, the neural network or the random forest? Why might that be? Think
    about what a neural network is trying to do and compare that to the individual
    decision trees of the random forest. Why might one type of model care about the
    feature ranges while the other might not?
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行*rf_vs_mlp.py*，然后运行*rf_vs_mlp_results.py*。（需要一些耐心。）考虑输出结果，这展示了输入特征向量的缩放如何影响模型性能。哪种类型的模型对特征的相对范围敏感，是神经网络还是随机森林？为什么会这样？思考一下神经网络的目标，并与随机森林中的单棵决策树进行比较。为什么一种模型会关注特征范围，而另一种则可能不会？
- en: '**Summary**'
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: This chapter explored the importance of randomness in machine learning when
    building datasets, both in terms of ordering the samples during training and in
    augmenting existing samples with plausible new ones to enlarge the type of data
    from which models learn.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了在机器学习中构建数据集时随机性的重要性，既包括在训练过程中对样本的排序，也包括通过合理的新样本增强现有样本，以扩大模型学习的数据类型。
- en: Next, we considered the initialization of neural networks. We subclassed scikit-learn’s
    `MLPClassifier` to override its initialization method, allowing us to add alternate
    initialization approaches. We then examined their effect on model performance.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论了神经网络的初始化。我们对子类化了scikit-learn的`MLPClassifier`，重写了其初始化方法，允许我们添加备用的初始化方式。然后，我们检查了这些方法对模型性能的影响。
- en: Following this, we explored extreme learning machines, a subtype of neural networks
    that employs randomness as one of its essential components. We learned how these
    machines perform on our datasets, then considered the effect of hidden layer size
    and activation function. We concluded the section by replacing the random weight
    matrix and bias vector with ones learned by a swarm optimization exercise. We
    discovered that swarm algorithms could generate models with performance beyond
    what most extreme learning machines provide (for the same architecture).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们探讨了极限学习机，这是一种将随机性作为其核心组成部分的神经网络子类型。我们了解了这些机器在我们的数据集上的表现，然后考虑了隐藏层大小和激活函数的影响。我们通过用一个群体优化练习学习到的随机权重矩阵和偏置向量替换原有的权重和偏置，来结束这一部分内容。我们发现，群体算法能够生成超越大多数极限学习机（在相同架构下）表现的模型。
- en: Lastly, we experimented with random forests, a collection of decision trees.
    We learned what decision trees are and how to build a random forest from collections
    of decision trees employing bagging, ensemble voting, and random feature selection.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实验了随机森林，它是由一组决策树组成的。我们了解了什么是决策树，以及如何通过集成法、投票法和随机特征选择来构建随机森林。
- en: The next chapter takes a break from the practical to enhance our lives through
    generative art.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将从实践中抽离，带来通过生成艺术来提升我们的生活。
