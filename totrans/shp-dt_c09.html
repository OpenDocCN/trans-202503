<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
<head>
<title>Chapter 9: Final Project: Analyzing Text Data</title>
<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:ea5eeba6-9dea-4463-bfe4-b91d6c6b5861" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter chapter">
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_179" title="179"/><a class="XrefDestination" id="9"/><span class="XrefDestination" id="xref-503083c09-001"/>9</span><br/>
<span class="ChapterTitle"><a class="XrefDestination" id="WorkingwithLanguage"/><span class="XrefDestination" id="xref-503083c09-002"/>Final Project: Analyzing Text Data</span></h1>
</header>
<figure class="opener">
<img alt="" src="image_fi/book_art/chapterart.png"/>
</figure>
<p class="ChapterIntro">In this chapter, we’ll put together some of the tools developed in previous chapters by working on a project related to linguistics and psychology. Many important data projects today deal with text data, from text matching to chatbots to customer sentiment analysis to authorship discernment and linguistic analysis. We’ll look at a small dataset of linguistic features derived from different creative writing samples to see how language usage varies over genres; some genres encourage a different writing process that reflects a different author mindset, such as writing a personal essay versus writing a spontaneous haiku.</p>
<p>Specifically, we’ll look at cluster overlap using <em>k</em>-NN and our distance metric of choice, then visualize the feature space by reducing the dimensionality of the dataset, then use dgLARS to create a cross-validated model distinguishing poetry types by features, and finally examine a predictive model based on language embedding. By completing a whole project, we’ll see how these tools can fit together to derive insight from data.</p>
<h2 id="h1-503083c09-0001"><span epub:type="pagebreak" id="Page_180" title="180"/><a class="XrefDestination" id="IntroducingNaturalLanguageProcessing"/><span class="XrefDestination" id="xref-503083c09-003"/>Building a Natural Language Processing Pipeline</h2>
<p class="BodyFirst">In <span class="xref" itemid="xref_target_Chapter 1"><a href="c01.xhtml">Chapter 1</a></span>, we briefly discussed the importance of text data and how natural language processing (NLP) pipelines can transform text data into model features that fit well with supervised learning methods. This is the approach we’ll take with our upcoming project. We’ll use Python to transform the text data into features and then use R to analyze those features.</p>
<p>The first step is to <em>parse</em> the data; we have to break the blocks of text into more manageable chunks—either sentences within a paragraph or words and punctuation within a sentence. This allows us to analyze small pieces of text and combine the results into a sentence, a document, or even a set of documents. For instance, in this chapter, you might want to parse each section, then each paragraph, and then each word in each paragraph to understand how the language usage varies between introductory material and application examples.</p>
<p>Sometimes, you’ll want to take out punctuation, certain types of filler words, or additions to root words. <em>Root words </em>exclude endings like <em>ing</em>, which change the tense of a verb or turn one part of speech into another. Consider the differences between <em>dribble</em>, <em>dribbled</em>, and <em>dribbling</em> with respect to a basketball practice. If the point guard is dribbling the ball, the action is occurring now. If they’ve already dribbled for an entire game, they are probably tired and have put the ball back on the rack. However, the point guard’s action, whether past or present, is the same. It doesn’t matter much if they’re doing it now or did it yesterday. Stemming and lemmatizing are two approaches that break words into root words: <em>stemming</em> does this by reducing words to their roots regardless of whether the root is still a word; <em>lemmatizing</em> reduces words to roots in a way that ensures the root is still a word.</p>
<p>Once text is parsed to the extent necessary for your specific application, you can start analyzing each piece. <em>Sentiment analysis</em> aims to understand the emotions behind the words and phrases of a given piece of text. For instance, “terrible product!!! never buy” has a fairly negative tone compared to “some users may not like how the product smells.” Sentiment analysis quantifies emotions within the text by tallying up totals for each emotion within the text, such that each receives a score and can be rolled into a final score, if preferred.</p>
<p>Once parsing is done, we can apply <em>named entity recognition</em>, which matches words to lists of important people, places, or terms of interest in a field. For instance, when processing medical notes related to patient discharge and outcome, you might want to match words to a list of diagnoses.</p>
<p>In other applications, it may be important to tag parts of speech, including pronouns, verbs, prepositions, adjectives, and more. Some people might use certain parts of speech at higher rates than others, and understanding these patterns can give insight into the text source’s personality, temporary state of mind, or even truthfulness. For some NLP applications, it’s possible to load these factors onto a given outcome or set of outcomes to create entirely new metrics from text data.</p>
<p><span epub:type="pagebreak" id="Page_181" title="181"/>Each of these analysis types can be integrated into a relational database as additional features for downstream models. For instance, as we’ll see later in the chapter, we can tag parts of speech, normalize them by the length of that particular document, and feed those features into models. You can also vectorize the words that exist in the document or set of documents to count frequencies of each word that exists in the set of documents for a given document. This often precedes deep learning models and visualization methods within NLP applications.</p>
<p>Again, because R has limited NLP capabilities compared to Python, we’ve done this first step—parsing the text data into features—in Python and provided the resulting data in the files for the book. We’ll then do the analysis using R. We used Python’s NLTK toolkit to parse the text data, and while these steps are beyond the scope of this book, we have included the scripts in our Python downloadable repository (<a class="LinkURL" href="https://nostarch.com/download/ShapeofData_PythonCode.zip">https://nostarch.com/download/ShapeofData_PythonCode.zip</a>), and we encourage you to take the raw data provided and see if you can build a similar NLP pipeline.</p>
<h2 id="h1-503083c09-0002"><a class="XrefDestination" id="DistinguishingTypesofPoetryfromParts-of-SpeechAnalysis"/><span class="XrefDestination" id="xref-503083c09-004"/>The Project: Analyzing Language in Poetry</h2>
<p class="BodyFirst">Modern poetry includes many types of poems with different structures, literary devices, and subject matters. For instance, formal-verse poems, such as sonnets or villanelles, have a defined number of stressed and total syllables in each line and a defined rhyme scheme. These types of poems often make heavy use of other literary devices, such as allusion or conceit (reference to other works as a juxtaposition of ideas), alliteration or assonance (repetition of a certain sound), or meter (patterns of stressed syllables within a line of poetry). This is an example of formal verse (a sonnet):</p>
<blockquote class="blockquote">
<p class="Blockquote">Ever After</p>
<p class="Blockquote"> </p>
<p class="Blockquote">Her glass slipper turns to his M-16,</p>
<p class="Blockquote">her elegant dress to faded fatigues.</p>
<p class="Blockquote">He’s a shell of the man from their intrigues,</p>
<p class="Blockquote">served five months patrol away from his queen.</p>
<p class="Blockquote"> </p>
<p class="Blockquote">Cinders to palace, her dreams now rubble,</p>
<p class="Blockquote">she watches her carriage morph to Abrams tank,</p>
<p class="Blockquote">as if her fairy tale were some cruel prank.</p>
<p class="Blockquote">He shouts, “Hurry, men! March on the double!”</p>
<p class="Blockquote"> </p>
<p class="Blockquote">Her clock strikes twelve, his tank an IED,</p>
<p class="Blockquote">and widower’s daughter is left widow.</p>
<p class="Blockquote">She has but memories, now as shadows,</p>
<p class="Blockquote">to comfort her dark days of misery.</p>
<p class="Blockquote">Her ever after has no tomorrow,</p>
<p class="Blockquote">leaving Cinderella grief and sorrow.</p></blockquote>
<p>In contrast, free-verse poetry doesn’t have a defined rhyme scheme for the end of each line, may have varying line lengths (or consistently long, <span epub:type="pagebreak" id="Page_182" title="182"/>short, or medium lines), and tends to use literary devices such as meter or rhyme for emphasis of a particular piece of the poem. This is an example of a free-verse poem with short line lengths:</p>
<blockquote class="blockquote">
<p class="Blockquote">Anya</p>
<p class="Blockquote"> </p>
<p class="Blockquote">Gaunt,</p>
<p class="Blockquote">made-up,</p>
<p class="Blockquote">wobbling in heels</p>
<p class="Blockquote">too big for tiny feet,</p>
<p class="Blockquote">heels</p>
<p class="Blockquote">that sparkle and clack</p>
<p class="Blockquote">against cold concrete</p>
<p class="Blockquote"> </p>
<p class="Blockquote">as wind</p>
<p class="Blockquote">whips her teased hair</p>
<p class="Blockquote">like a lasso</p>
<p class="Blockquote">roping a steer</p>
<p class="Blockquote"> </p>
<p class="Blockquote">as snow</p>
<p class="Blockquote">binds to tight jeans</p>
<p class="Blockquote">like the shackles</p>
<p class="Blockquote">she wore on the ship to her Shangri La</p>
<p class="Blockquote"> </p>
<p class="Blockquote">as streetlight</p>
<p class="Blockquote">catches a gleam in her eyes,</p>
<p class="Blockquote">a glimpse as she stares into</p>
<p class="Blockquote">tonight</p>
<p class="Blockquote"> </p>
<p class="Blockquote">her fifteenth birthday.</p></blockquote>
<p>Some poems don’t fit neatly into free verse or formal verse, such as prose poetry (where line breaks don’t exist) or haibun (a Japanese form that incorporates prose poetry with different types of haiku). Modern haiku and its related forms juxtapose two images or thoughts with a turn, such as a dash, that connects the two images or thoughts in a moment of insight. Typically, modern haiku doesn’t conform to the Japanese syllable count requirements, but it usually includes some reference to season and nature (or human nature in the case of the related form, senryu). Haibun knits a story together through the poem title, the haiku, and the prose pieces. Here’s one example of a modern haiku:</p>
<blockquote class="blockquote">
<p class="Blockquote">shooting stars—</p>
<p class="Blockquote">the dash between</p>
<p class="Blockquote">born and died</p></blockquote>
<p>From prior research, we know that different authors can be identified by their preferred word choices and their unique usage of different parts of speech (this is a core feature of antiplagiarism software); we also know that language usage varies by the author’s state of mind. However, it’s unknown <span epub:type="pagebreak" id="Page_183" title="183"/>if the same author constructs poetry differently depending on the type of poem they are writing. Because haiku and haibun originated as a spark of insight juxtaposing ideas, it’s possible that the different genesis of the poem influences the use of language and grammar within the poem.</p>
<p>Let’s dive into the dataset a bit before we start visualizing it. We have eight haiku, eight haibun, eight free-verse poems, and eight formal-verse poems in the dataset. We’ll group them into haiku-based poems and other poems to simplify the analyses by how poems are typically generated (free association versus crafting). The features we’ll consider are punctuation fraction, noun fraction, verb fraction, personal pronoun fraction, adjective fraction, adverb fraction, and preposition/conjunction fraction. Given the construction of each poem type, it’s likely some of these factors will vary. With a larger sample of poems, you could use other parts of speech or break categories, such as verbs, into their individual components.</p>
<p>We’ll be completing these steps in Python, so we’ll overview only the steps used rather than dive into code. You can find the processed data in the files for this book.</p>
<h3 id="h2-503083c09-0001"><a class="XrefDestination" id="TokenizingTextData"/><span class="XrefDestination" id="xref-503083c09-005"/>Tokenizing Text Data</h3>
<p class="BodyFirst">The first step in processing the poems for analysis is <em>tokenizing</em> the text data, meaning we need to parse our poems into individual words and punctuation marks. The Treebank tokenizer uses regular expressions to parse words in sentences, handle contractions and other combinations of words and punctuation, and splice quotes. These are important steps for parsing poem text data, as punctuation is interspersed with words and phrases at relatively high rates. Haiku, in particular, tends to use punctuation to create a cut in the poem to link two different images or ideas.</p>
<p>Because the Treebank tokenizer often splits contractions and other words that connect with punctuation, it’s useful to the regex tokenizer to count the number of actual words that exist in the text and parse them into words that can contain punctuation. Given how short some poems are, we want to make sure we aren’t inflating word counts. The regex tokenizer results give us an accurate word count to normalize parts of speech or punctuation proportions.</p>
<p>After obtaining the lengths of tokenizer results, we can subtract the number of words from the number of tokens to derive a length of punctuation in the text. This allows us to compare fractions of words and fractions of punctuation for different poem types, which likely varies by poem type (and by poem author, according to prior research on linguistic differences in text passages by author).</p>
<h3 id="h2-503083c09-0002"><a class="XrefDestination" id="AnalyzingPartsofSpeech"/><span class="XrefDestination" id="xref-503083c09-006"/>Tagging Parts of Speech</h3>
<p class="BodyFirst">To tag relevant parts of speech, we’ll use the <em>averaged perceptron tagger</em>, a supervised learning algorithm that tends to have pretty good accuracy across text types and has been pretrained for the NLTK package. While it’s a bit slow on large volumes of text, our text samples are fairly small, allowing the application to tag words without much processing power required. <span epub:type="pagebreak" id="Page_184" title="184"/>It’s possible to scale NLTK’s averaged perceptron tagger application to very large datasets using the big data technologies that we’ll consider in the next chapter.</p>
<p>We’ll parse out nouns, verbs, personal pronouns, adverbs, adjectives, and prepositions and conjunctions and count the numbers of each category that exist in each text sample.</p>
<p>Nouns include singular, plural, common, and proper noun combinations. Verbs include all type and tense combinations. Personal pronouns include pronouns and possessive pronouns. Adverbs and adjectives include comparative and superlative forms of adverbs and adjectives. Prepositions and coordinating conjunctions are also tagged and counted.</p>
<p>Some other tagged parts of speech exist in the averaged perceptron tagger, and other taggers may include further divisions of parts of speech. If you want to explore how parts of speech can be used to distinguish text types, text authorship, or demographics of the text author, you may want to use another tagger or disaggregate nouns, verbs, and so on, from their individual components. However, this will result in more columns within your dataset, so we recommend you collect more samples if you’re doing that type of nuanced analysis of text attributes and parts-of-speech analysis.</p>
<h3 id="h2-503083c09-0003"><a class="XrefDestination" id="NormalizingVectors"/><span class="XrefDestination" id="xref-503083c09-007"/>Normalizing Vectors</h3>
<p class="BodyFirst">Because our text samples include some short samples and some long samples according to poem type, we’ll want to standardize the parts-of-speech counts before we work with the data. Our approach includes normalization of punctuation by token count (punctuation and word count totals) and normalization of parts-of-speech count by word count, summarized in this chapter’s files. This should give good enough features to demonstrate poem type differences and still allow for good dimensionality reduction results to visualize our dataset.</p>
<p>For a more nuanced approach that parses out types of verbs, nouns, and so on, you could derive fractions of part of speech category or break down fractions within parts of speech to engineer more detailed features for your analysis. If you’re familiar with Python, we encourage you to play around with the NLP pipeline and customize your analyses for more insight into poem-type linguistic differences.</p>
<p>For now, let’s move onto the analysis in R.</p>
<h2 id="h1-503083c09-0003"><a class="XrefDestination" id="AnalyzingPoemDifferences"/><span class="XrefDestination" id="xref-503083c09-008"/>Analyzing the Poem Dataset in R</h2>
<p class="BodyFirst">We’ll start by loading the processed poem dataset and exploring the features we’ve derived using the code in <a href="#listing9-1" id="listinganchor9-1">Listing 9-1</a>.</p>
<pre><code>#load poem data and set session seed
mydata&lt;-read.csv("PoemData.csv")
summary(mydata)</code></pre>
<p class="CodeListingCaption"><a id="listing9-1">Listing 9-1</a>: A script that reads in the processed poem data</p>
<p><span epub:type="pagebreak" id="Page_185" title="185"/>Adverbs tend to be the least-represented features, accounting for only 0–10.5 percent of words used in any given poem. Nouns tend to be the most frequent words appearing in this set of poems, accounting for 12.5–53.5 percent of words in a given poem. Personal pronouns are rare, with more than a quarter of the poems not containing a personal pronoun (likely due to the haiku, which tend not to use them). Punctuation usage varies quite a bit, from no representation in a haiku to nearly half of a free-verse poem being composed of punctuation (a list poem of medical diagnoses at a hospital). Given this variation, it’s likely we have good features to use in our analyses.</p>
<p>Let’s set the seed for our analyses and visualize the parts-of-speech features with t-SNE, using the shape of each visualized point as a designation of the poem type; add the following code to <a href="#listing9-1">Listing 9-1</a>:</p>
<pre><code>#grab relevant pieces of data
mydata1&lt;-mydata[,-1]
haiku&lt;-which(mydata1$Type=="haiku")
mydata1$Type[haiku]&lt;-1
mydata1$Type[-haiku]&lt;-2
set.seed(1)

#create and plot t-SNE projections of poem data
library(dimRed)
t1&lt;-getDimRedData(embed(mydata1[,-1],"tSNE",ndim=2,perplexity=5))
plot(as.data.frame(t1),xlab="Coordinate 1",ylab="Coordinate 2",main="Perplexity=5 t-SNE Results",pch=ifelse(mydata1$Type==1,1,2))</code></pre>
<figure>
<img alt="" class="" src="image_fi/503083c09/f09001.png"/>
<figcaption><p><a id="figure9-1">Figure 9-1</a>: A t-SNE plot of poem features by poem type, with type represented by either circles or triangles (<span class="LiteralInCaption"><code>perplexity=5</code></span>)</p></figcaption>
</figure>
<p>From the plot in <a href="#figure9-1" id="figureanchor9-1">Figure 9-1</a>, we can see that our poems tend to separate out into clusters where most points are of the same type. This means <span epub:type="pagebreak" id="Page_186" title="186"/>a kernel-based model or nearest neighbor model is probably sufficient for classifying poem type by features.</p>
<p>Given some separation of points by features, it’s likely we can apply algorithms to cluster our data and use supervised learning to understand which differences exist between haiku-type poems and other poems.</p>
<p>Let’s divide our sample into training and test fractions and then apply a Euclidean-distance-based <em>k</em>-NN algorithm to classify poem types based on a poem’s five nearest neighbors by adding to our code so far:</p>
<pre><code>#create Canberra KNN models with different distances and five nearest neighbors
library(knnGarden)
s&lt;-sample(1:32,24)
train&lt;-mydata1[s,]
test&lt;-mydata1[-s,]
kc&lt;-knnVCN(TrnX=train[,-1],OrigTrnG=train[,1],TstX=test[,-1],
K=5,method="euclidean")</code></pre>
<p>Since our t-SNE plot (<a href="#figure9-1">Figure 9-1</a>) suggests some separation and points generally near poems of a similar type, it’s likely that this model has worked well. Let’s examine the predicted and true labels for our test set under the <em>k</em>-NN model through this addition to our code:</p>
<pre><code>#examine performance
which(kc==test$Type)
&gt; <b>1, 2, 3, 4, 5, 6, 7, 8</b></code></pre>
<p>It looks like this model correctly classifies all test poems, suggesting a high-quality model that can separate types of poems based on features.</p>
<p>We’ll examine these potential type differences by feature further using a 10-fold cross-validated dgLARS model in an additional step to our code:</p>
<pre><code>#create a cross-validated dgLARS model
library(dglars)
dg1&lt;-cvdglars(factor(Type)~.,data=mydata1,
family="binomial",control=list(nfold=10))
dg1</code></pre>
<p>Because we are working with a small dataset, it’s possible that one or more of your folds will have issues, giving a slightly different model than the results shown here:</p>
<pre><code>#examine the dgLARS model
&gt; <b>dg1</b>

Call:  cvdglars(formula=factor(Type) ~ .,family="binomial",data=mydata1,
    control=list(nfold=10))

Coefficients:
                   Estimate
Int.                -1.4782
<span epub:type="pagebreak" id="Page_187" title="187"/>Punctuation_Length   0.3113
Adverb_Count        -0.2767

dispersion parameter for binomial family taken to be 1

Details:
   number of non-zero estimates: 3
      cross-validation deviance: 2.687
                              g: 0.4573
                        n. fold: 10</code></pre>
<p>Your model should show differences in punctuation fraction and adverb fraction. Free-verse and formal-verse poems have higher rates of punctuation usage. Given that these types of poems are more likely to use full sentences rather than connected phrases, the differences in punctuation usage are consistent with expected differences.</p>
<p>The differences in adverb fractions aren’t as expected. However, adverb usage is linked to many different transient and fixed personality traits. It’s possible that haiku taps into a different transient mood or trait, creating style differences reflected in adverb usage. Subject matter may also influence this difference.</p>
<p>Importantly, this analysis shows that parts-of-speech and punctuation patterns vary within samples of writing by the same author in the same type of writing (poetry). Given that prior research suggests that some of these input features can be used to identify the likely author of a text, it may be prudent to rethink authorship prediction based on parts-of-speech analysis. Different subject matters, different types of writing, and different life stages of the author may influence word choice, sentence structure, and usage of punctuation.</p>
<p>To see how the poems group together within each poem type, let’s visualize the persistence diagrams for each poem type through adding to our code:</p>
<pre><code>#create the Vietoris-Rips complexes
library(TDAstats)
set1&lt;-mydata1[mydata1$Type==1,-1]
set2&lt;-mydata1[mydata1$Type==2,-1]

#compute Manhattan distance for set 1, compute Rips diagram, and plot
mm1&lt;-dist(set1,"manhattan",diag=T,upper=T)
d1&lt;-calculate_homology(as.matrix(mm1),dim=1,threshold=0,format="cloud")
plot_persist(d1)

#compute Manhattan distance for set 2, compute Rips diagram, and plot
mm2&lt;-dist(set2,"manhattan",diag=T,upper=T)
d2&lt;-calculate_homology(as.matrix(mm2),dim=1,threshold=0,format="cloud")
plot_persist(d2)</code></pre>
<p><span epub:type="pagebreak" id="Page_188" title="188"/><a href="#figure9-2" id="figureanchor9-2">Figure 9-2</a> shows the persistence diagrams for haiku and non-haiku samples, highlighting some differences in poem clustering within type:</p>
<figure>
<img alt="" class="" src="image_fi/503083c09/f09002.png"/>
<figcaption><p><a id="figure9-2">Figure 9-2</a>: Persistence diagrams by poem type</p></figcaption>
</figure>
<p>As we can see in <a href="#figure9-2">Figure 9-2</a>, non-haiku poems tend to cluster more consistently, while haiku poems spread out without a lot of separation involving multiple poems grouped together. This suggests that haiku features vary from poem to poem, while non-haiku poems show more consistency in features. The spontaneity of the composition may result in a wider variety of poem structures found with haiku poems. Data related to spontaneousness of poem creation and time spent crafting the poem may shed light on conscious language usage differences between poems (such as haiku) that arise from a single moment and those that are created with more intent behind their creation.</p>
<h2 id="h1-503083c09-0004"><a class="XrefDestination" id="UsingTopology-BasedNLPTools"/><span class="XrefDestination" id="xref-503083c09-009"/>Using Topology-Based NLP Tools</h2>
<p class="BodyFirst">NLP as a field has evolved over the past years, and some common tools in NLP leverage topology to solve important problems. Early embedding tools tended to tally word frequencies within and across documents of interest to parse text data into numeric data that works well in machine learning algorithms. However, words often don’t contain all the semantic information needed to make sense of a sentence or paragraph or entire document. Negatives, such as <em>no</em> or <em>not</em>, modify actions or actors within a sentence. For instance, “she did let him in the house” is a very different statement semantically than “she did not let him in the house.” Depending on what you’re trying to predict or classify, simple mappings from individual words to a matrix of numbers don’t work well.</p>
<p>In addition, a document or collection of documents might have 30,000+ words that occur at least once. Most common words will occur often with little value added by their presence. Important words might occur only once or twice. This leads us into the problem of dimensionality again when we try to sift through the data for important trends.</p>
<p>Fortunately, recent years have seen some great developments in low-dimensional embeddings via topological mapping with special types of <span epub:type="pagebreak" id="Page_189" title="189"/>neural networks. <em>Pretrained transformer models </em>are neural networks that can pass information forward and backward through their hidden layers to obtain optimal topological mappings for text data. Pretrained transformer models learn low-dimensional embeddings of text data from massive training sets in the language or languages of interest. <em>BERT</em> (Bidirectional Encoder Representations from Transformers) and its sentence-based embedding cousin, <em>SBERT</em>, are two of the most common open source pretrained transformer models used to embed text data into lower-dimensional, dense matrices for use in machine learning tasks. BERT models can be extended to languages that are not currently supported, such as embeddings of text in Hausa or Lingala or Rushani; this has the potential to accelerate language translation services and preserve endangered languages. <em>GPT-3</em>, trained on a similar premise as BERT, has created accurate translations and chatbots that can parse meaning from input text rather than match keywords or eat up computing resources trying to process high-dimensional matrices within machine learning algorithms.</p>
<p>Using Python, we’ve built a BERT model on our poem set based on their serious tone or humorous tone to show how BERT embeddings can fit with our supervised learning tools in this book. (Note that the order of poems has changed from the original set with the data munging to get BERT embeddings and pass them back to a <em>.csv</em> file.) You can consult Python’s transformer package to learn more about how BERT models are imported and leveraged in text embedding or refer to the Python scripts for this chapter (<a class="LinkURL" href="https://nostarch.com/download/ShapeofData_PythonCode.zip">https://nostarch.com/download/ShapeofData_PythonCode.zip</a>). However, we’ll stick to importing the results into R and visualizing the data in a smaller dimension, as we did with our poem linguistic features earlier in the chapter. Let’s create a t-SNE embedding and plot it with <a href="#listing9-2" id="listinganchor9-2">Listing 9-2</a>.</p>
<pre><code>#load poem data
#load data
mydata&lt;-read.csv("BertSet.csv")

#create the embedding
library(dimRed)
t1&lt;-getDimRedData(embed(mydata[,-1],"tSNE",ndim=2,perplexity=5))

#plot the results
plot(as.data.frame(t1),xlab="Coordinate 1",ylab="Coordinate 2",main="Perplexity=5 t-SNE Results",pch=ifelse(mydata$Type=="serious",1,2))</code></pre>
<p class="CodeListingCaption"><a id="listing9-2">Listing 9-2</a>: A script that loads BERT data for serious and humorous poems, embeds the data with t-SNE, and plots the results</p>
<p><a href="#figure9-3" id="figureanchor9-3">Figure 9-3</a> shows the embedding, which demonstrates that poems separate into clusters by poem type. This mirrors our haiku versus non-haiku poem results in <a href="#figure9-1">Figure 9-1</a>, where we saw features separating out by type in the t-SNE embedding. Again, this suggests that a machine learning classifier should work well with our dataset.</p>
<span epub:type="pagebreak" id="Page_190" title="190"/><figure>
<img alt="" class="" src="image_fi/503083c09/f09003.png"/>
<figcaption><p><a id="figure9-3">Figure 9-3</a>: A plot of t-SNE embedding results, with serious or humorous types of poems denoted by circles and triangles, respectively (<span class="LiteralInCaption"><code>perplexity=5</code></span>)</p></figcaption>
</figure>
<p>Now that we know a machine learning model may do well at classifying this data, let’s fit a Lasso model with homotopy continuation to handle the small sample size by adding <a href="#listing9-3" id="listinganchor9-3">Listing 9-3</a>:</p>
<pre><code>#get training data split
set.seed(1)
s&lt;-sample(1:25,0.85*25)
train&lt;-mydata[s,]
test&lt;-mydata[-s,]

#build Lasso model
library(lasso2)
etastart&lt;-NULL
las1&lt;-gl1ce(factor(Type)~.,train,family=binomial(link=probit),bound=5,standardize=F)
lpred1&lt;-round(predict(las1,test,type="response"))

#examine results
lpred1
<b>&gt;</b><b> 1 1 1 0 1</b>
test$Type
<b>&gt;</b><b> [1] serious serious humor serious</b></code></pre>
<p class="CodeListingCaption"><a id="listing9-3">Listing 9-3</a>: A script that loads BERT data for serious and humorous poems, splits the data into training and test sets, builds a Lasso model with homotopy continuation fitting, and shows prediction and actual values for test data</p>
<p>The dataset has 384 components from the embedded BERT model and 25 poems. Your version of R may split the data and fit the model differently, but in our version of R, the test data has three serious poems followed by a humorous one followed by a serious one. The model predicts a serious poem, a serious one, a serious one, a humorous one, and a serious one <span epub:type="pagebreak" id="Page_191" title="191"/>(giving a model accuracy of 100 percent). Given how small this training set is compared to the 384 predictors fed into our model, this is great performance. Note that the selected features have little meaning semantically, as they are simply embeddings. Coupling topology-based methods into pipelines to process and model small datasets can provide decent prediction where other models will fail entirely.</p>
<h2 id="h1-503083c09-0005"><a class="XrefDestination" id="Summary"/><span class="XrefDestination" id="xref-503083c09-010"/>Summary</h2>
<p class="BodyFirst">In this chapter, we applied several of the methods overviewed in the book on a linguistics question involving a dataset of features derived from NLP-processed poems; we also embedded our data and created a model to predict tone differences in our poem set.</p>
<p>For the first problem, we reduced the dimensionality of the dataset to visualize group differences, applied two supervised learning models to understand classification accuracy and important features distinguishing the poetry types, and visualized topological features that exist in both sets of poems. This showed us that language usage, particularly punctuation, varies across poem types.</p>
<p>We then looked at context-aware embeddings and predicting the tone of our poem set using a topology-based embedding method and a topology-based classification model, which showed that we could get fairly accurate prediction building a model from 384 embedded components and a training set of 21 poems.</p>
<p>In the next chapter, we’ll wrap up the book by looking at ways to scale topological data analysis algorithms with distributed and quantum computing approaches.</p>
</section>
</body>
</html>