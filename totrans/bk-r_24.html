<html><head></head><body>
<h2 class="h2" id="ch20"><span epub:type="pagebreak" id="page_451"/><span class="big"><strong>20</strong></span><br/><strong>SIMPLE LINEAR REGRESSION</strong></h2>&#13;
<div class="image"><img src="../images/common-01.jpg" alt="image"/></div>&#13;
<p class="noindent">Though straightforward comparative tests of individual statistics are useful in their own right, you’ll often want to learn more from your data. In this chapter, you’ll look at <em>linear regression</em> models: a suite of methods used to evaluate precisely <em>how</em> variables relate to each other.</p>&#13;
<p class="indent">Simple linear regression models describe the effect that a particular variable, called the <em>explanatory variable</em>, might have on the value of a continuous outcome variable, called the <em>response variable</em>. The explanatory variable may be continuous, discrete, or categorical, but to introduce the key concepts, I’ll concentrate on continuous explanatory variables for the first several sections in this chapter. Then, I’ll cover how the representation of the model changes if the explanatory variable is categorical.</p>&#13;
<h3 class="h3" id="ch20lev1sec62"><strong>20.1 An Example of a Linear Relationship</strong></h3>&#13;
<p class="noindent">As an example to start with, let’s continue with the data used in <a href="ch19.xhtml#ch19lev1sec61">Section 19.3</a> and look at the student survey data (the <code>survey</code> data frame in the package <code>MASS</code>) a little more closely. If you haven’t already done so, with the required package loaded (call <code>library("MASS")</code>), you can read the help file <code>?survey</code> for details on the variables present.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_452"/>Plot the student heights on the <em>y</em>-axis and their handspans (of their writing hand) on the <em>x</em>-axis.</p>&#13;
<pre>R&gt; plot(survey$Height~survey$Wr.Hnd,xlab="Writing handspan (cm)",<br/>        ylab="Height (cm)")</pre>&#13;
<p class="indent"><a href="ch20.xhtml#ch20fig1">Figure 20-1</a> shows the result.</p>&#13;
<div class="image"><img src="../images/f20-01.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch20fig1"/>Figure 20-1: A scatterplot of height against writing handspan for a sample of first-year statistics students</em></p>&#13;
<p class="indent">Note that the call to <code>plot</code> uses formula notation (also referred to as <em>symbolic notation</em>) to specify “height <em>on</em> handspan.” You can produce the same scatterplot by using the coordinate vector form of (<em>x</em>, <em>y</em>), that is, <code>plot(survey$Wr.Hnd,survey$Height,...)</code>, but I’m using the symbolic notation here because it nicely reflects how you’ll fit the linear model in a moment.</p>&#13;
<p class="indent">As you might expect, there’s a positive association between a student’s handspan and their height. That relationship appears to be linear in nature. To assess the strength of the linear relationship (refer to <a href="ch13.xhtml#ch13lev2sec120">Section 13.2.5</a>), you can find the estimated correlation coefficient.</p>&#13;
<pre>R&gt; cor(survey$Wr.Hnd,survey$Height,use="complete.obs")<br/>[1] 0.6009909</pre>&#13;
<p class="indent">Though there are 237 records in the data frame, the plot doesn’t actually show 237 points. This is because there are missing observations (coded <code>NA</code>; see <a href="ch06.xhtml#ch06lev2sec57">Section 6.1.3</a>). By default, R removes any “incomplete” pairs when producing a plot like this. To find out how many offending observations have been deleted, you can use the short-form logical operator <code>|</code> <span epub:type="pagebreak" id="page_453"/>(<a href="ch04.xhtml#ch04lev2sec39">Section 4.1.3</a>) in conjunction with <code>is.na</code> (<a href="ch06.xhtml#ch06lev2sec57">Section 6.1.3</a>) and <code>which</code> (<a href="ch04.xhtml#ch04lev2sec41">Section 4.1.5</a>). You then use <code>length</code> to discover there are 29 missing observation pairs.</p>&#13;
<pre>R&gt; incomplete.obs &lt;- which(is.na(survey$Height)|is.na(survey$Wr.Hnd))<br/>R&gt; length(incomplete.obs)<br/>[1] 29</pre>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>Because there are</em> <code><span class="codeitalic">NA</code></span><em>s in the vectors supplied to the correlation coefficient function</em> <code><span class="codeitalic">cor</code></span><em>, you must also specify the optional argument</em> <code><span class="codeitalic">use="complete.obs"</code></span><em>. This means that the calculated statistic takes into account only those observation pairs in the</em> <code><span class="codeitalic">Wr.Hnd</code></span> <em>and</em> <code><span class="codeitalic">Height</code></span> <em>vectors where</em> neither <em>element is</em> <code><span class="codeitalic">NA</code></span><em>. You can think of this argument as doing much the same thing as</em> <code><span class="codeitalic">na.rm=TRUE</code></span> <em>in univariate summary statistic functions such as</em> <code><span class="codeitalic">mean</code></span> <em>and</em> <code><span class="codeitalic">sd</code></span>.</p>&#13;
</div>&#13;
<h3 class="h3" id="ch20lev1sec63"><strong>20.2 General Concepts</strong></h3>&#13;
<p class="noindent">The purpose of a linear regression model is to come up with a function that estimates the <em>mean</em> of one variable given a particular value of another variable. These variables are known as the <em>response variable</em> (the “outcome” variable whose mean you are attempting to find) and the <em>explanatory variable</em> (the “predictor” variable whose value you already have).</p>&#13;
<p class="indent">In terms of the student survey example, you might ask something like “What’s the expected height of a student if their handspan is 14.5 cm?” Here the response variable is the height, and the explanatory variable is the handspan.</p>&#13;
<h4 class="h4" id="ch20lev2sec173"><strong><em>20.2.1 Definition of the Model</em></strong></h4>&#13;
<p class="noindent">Assume you’re looking to determine the value of response variable <em>Y</em> given the value of an explanatory variable <em>X</em>. The <em>simple linear regression model</em> states that the value of a response is expressed as the following equation:</p>&#13;
<div class="imagec"><a id="ch20eq1"/><img src="../images/e20-1.jpg" alt="image"/></div>&#13;
<p class="indent">On the left side of <a href="ch20.xhtml#ch20eq1">Equation (20.1)</a>, the notation <em>Y</em>|<em>X</em> reads as “the value of <em>Y</em> conditional upon the value of <em>X</em>.”</p>&#13;
<h5 class="h5" id="ch20lev3sec88"><strong>Residual Assumptions</strong></h5>&#13;
<p class="noindentb">The validity of the conclusions you can draw based on the model in (20.1) is critically dependent on the assumptions made about <em><span class="ent">∊</span></em>, which are defined as follows:</p>&#13;
<p class="bull">• The value of <em><span class="ent">∊</span></em> is assumed to be normally distributed such that <em><span class="ent">∊</span></em> ~ N(0,<em>σ</em>).</p>&#13;
<p class="bull">• That <em><span class="ent">∊</span></em> is centered (that is, has a mean of) zero.</p>&#13;
<p class="bull">• The variance of <em><span class="ent">∊</span></em>, <em>σ</em><sup>2</sup>, is constant.</p>&#13;
<p class="indentt"><span epub:type="pagebreak" id="page_454"/>The <em><span class="ent">∊</span></em> term represents random error. In other words, you assume that any raw value of the response is owed to a linear change in a given value of <em>X</em>, plus or minus some random, <em>residual</em> variation or normally distributed <em>noise</em>.</p>&#13;
<h5 class="h5" id="ch20lev3sec89"><strong>Parameters</strong></h5>&#13;
<p class="noindentb">The value denoted by <em>β</em><sub>0</sub> is called the <em>intercept</em>, and that of <em>β</em><sub>1</sub> is called the <em>slope</em>. Together, they are also referred to as the <em>regression coefficients</em> and are interpreted as follows:</p>&#13;
<p class="bull">• The intercept, <em>β</em><sub>0</sub>, is interpreted as the expected value of the response variable when the predictor is zero.</p>&#13;
<p class="bull">• Generally, the slope, <em>β</em><sub>1</sub>, is the focus of interest. This is interpreted as the change in the mean response for each one-unit increase in the predictor. When the slope is positive, the regression line increases from left to right (the mean response is higher when the predictor is higher); when the slope is negative, the line decreases from left to right (the mean response is lower when the predictor is higher). When the slope is zero, this implies that the predictor has no effect on the value of the response. The more extreme the value of <em>β</em><sub>1</sub> (that is, away from zero), the steeper the increasing or decreasing line becomes.</p>&#13;
<h4 class="h4" id="ch20lev2sec174"><strong><em>20.2.2 Estimating the Intercept and Slope Parameters</em></strong></h4>&#13;
<p class="noindent">The goal is to use your data to estimate the regression parameters, yielding the estimates <img class="middle" src="../images/b0.jpg" alt="image"/> and <img class="middle" src="../images/b1.jpg" alt="image"/>; this is referred to as <em>fitting</em> the linear model. In this case, the data comprise <em>n</em> pairs of observations for each individual. The fitted model of interest concerns the mean response value, denoted <span class="ent">ŷ</span>, for a specific value of the predictor, <em>x</em>, and is written as follows:</p>&#13;
<div class="imagec"><a id="ch20eq2"/><img src="../images/e20-2.jpg" alt="image"/></div>&#13;
<p class="indent">Sometimes, alternative notation such as <img class="middle" src="../images/e.jpg" alt="image"/>[<em>Y</em>] or <img class="middle" src="../images/e.jpg" alt="image"/>[<em>Y</em>|<em>X</em> = <em>x</em>] is used on the left side of (20.2) to emphasize the fact that the model gives the mean (that is, the expected value) of the response. For compactness, many simply use something like <span class="ent">ŷ</span>, as shown here.</p>&#13;
<p class="indent">Let your <em>n</em> observed data pairs be denoted <em>x<sub>i</sub></em> and <em>y<sub>i</sub></em> for the predictor and response variables, respectively; <em>i</em> = 1, . . . , <em>n</em>. Then, the parameter estimates for the simple linear regression function are</p>&#13;
<div class="imagec"><a id="ch20eq3"/><img src="../images/e20-3.jpg" alt="image"/></div>&#13;
<p class="noindentb">where</p>&#13;
<p class="bull">• <em><span class="ent">x̄</span></em> and <em><span class="ent">ȳ</span></em> are the sample means of the <em>x<sub>i</sub></em>s and <em>y<sub>i</sub></em>s.</p>&#13;
<p class="bull">• <em>s<sub>x</sub></em> and <em>s<sub>y</sub></em> are the sample standard deviations of the <em>x<sub>i</sub></em>s and <em>y<sub>i</sub></em>s.</p>&#13;
<p class="bull">• <em>ρ<sub>xy</sub></em> is the estimate of correlation between <em>X</em> and <em>Y</em> based on the data (see <a href="ch13.xhtml#ch13lev2sec120">Section 13.2.5</a>).</p>&#13;
<p class="indentt"><span epub:type="pagebreak" id="page_455"/>Estimating the model parameters in this way is referred to as <em>least-squares regression</em>; the reason for this will become clear in a moment.</p>&#13;
<h4 class="h4" id="ch20lev2sec175"><strong><em>20.2.3 Fitting Linear Models with lm</em></strong></h4>&#13;
<p class="noindent">In R, the command <code>lm</code> performs the estimation for you. For example, the following line creates a fitted linear model object of the mean student height by handspan and stores it in your global environment as <code>survfit</code>:</p>&#13;
<pre>R&gt; survfit &lt;- lm(Height~Wr.Hnd,data=survey)</pre>&#13;
<p class="indent">The first argument is the now-familiar <code><span class="codeitalic">response</code> ~ <span class="codeitalic">predictor</span></span> formula, which specifies the desired model. You don’t have to use the <code>survey$</code> prefix to extract the vectors from the data frame because you specifically instruct <code>lm</code> to look in the object supplied to the <code>data</code> argument.</p>&#13;
<p class="indent">The fitted linear model object itself, <code>survfit</code>, has a special class in R—one of <code>"lm"</code>. An object of class <code>"lm"</code> can essentially be thought of as a list containing several components that describe the model. You’ll look at these in a moment.</p>&#13;
<p class="indent">If you simply enter the name of the <code>"lm"</code> object at the prompt, it will provide the most basic output: a repeat of your call and the estimates of the intercept (<img src="../images/b0.jpg" alt="image"/>) and slope (<img src="../images/b1.jpg" alt="image"/>).</p>&#13;
<pre>R&gt; survfit<br/><br/>Call:<br/>lm(formula = Height ~ Wr.Hnd, data = survey)<br/><br/>Coefficients:<br/>(Intercept)       Wr.Hnd<br/>    113.954        3.117</pre>&#13;
<p class="indent">This reveals that the linear model for this example is estimated as follows:</p>&#13;
<div class="imagec"><a id="ch20eq4"/><img src="../images/e20-4.jpg" alt="image"/></div>&#13;
<p class="indent">If you evaluate the mathematical function for <span class="ent">ŷ</span>—<a href="ch20.xhtml#ch20eq2">Equation (20.2)</a>—at a range of different values for <em>x</em>, you end up with a straight line when you plot the results. Considering the definition of intercept given earlier as the expected value of the response variable when the predictor is zero, in the current example, this would imply that the mean height of a student with a handspan of 0 cm is 113.954 cm (an arguably less-than-useful statement since a value of zero for the explanatory variable doesn’t make sense; you’ll consider these and related issues in <a href="ch20.xhtml#ch20lev1sec65">Section 20.4</a>). The slope, the change in the mean response for each one-unit increase in the predictor, is 3.117. This states that, on average, for every 1 cm increase in handspan, a student’s height is estimated to increase by 3.117 cm.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_456"/>With all this in mind, once more run the line to plot the raw data as given in <a href="ch20.xhtml#ch20lev1sec62">Section 20.1</a> and shown in <a href="ch20.xhtml#ch20fig1">Figure 20-1</a>, but now add the fitted regression line using <code>abline</code>. So far, you’ve only used the <code>abline</code> command to add perfectly horizontal and vertical lines to an existing plot, but when passed an object of class <code>"lm"</code> that represents a simple linear model, like <code>survfit</code>, the fitted regression line will be added instead.</p>&#13;
<pre>R&gt; abline(survfit,lwd=2)</pre>&#13;
<p class="indent">This adds the slightly thickened diagonally increasing line shown in <a href="ch20.xhtml#ch20fig2">Figure 20-2</a>.</p>&#13;
<div class="image"><img src="../images/f20-02.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch20fig2"/>Figure 20-2: The simple linear regression line (solid, bold) fitted to the observed data. Two dashed vertical line segments provide examples of a positive (leftmost) and negative (rightmost)</em> residual.</p>&#13;
<h4 class="h4" id="ch20lev2sec176"><strong><em>20.2.4 Illustrating Residuals</em></strong></h4>&#13;
<p class="noindent">When the parameters are estimated as shown here, using (20.3), the fitted line is referred to as an implementation of <em>least-squares regression</em> because it’s the line that minimizes the average squared difference between the observed data and itself. This concept is easier to understand by drawing the distances between the observations and the fitted line, formally called <em>residuals</em>, for a couple of individual observations within <a href="ch20.xhtml#ch20fig2">Figure 20-2</a>.</p>&#13;
<p class="indent">First, let’s extract two specific records from the <code>Wr.Hnd</code> and <code>Height</code> data vectors and call the resulting vectors <code>obsA</code> and <code>obsB</code>.</p>&#13;
<pre>R&gt; obsA &lt;- c(survey$Wr.Hnd[197],survey$Height[197])<br/>R&gt; obsA<br/>[1]  15.00 170.18<br/>R&gt; obsB &lt;- c(survey$Wr.Hnd[154],survey$Height[154])<br/>R&gt; obsB<br/>[1]  21.50 172.72</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_457"/>Next, briefly inspect the names of the members of the <code>survfit</code> object.</p>&#13;
<pre>R&gt; names(survfit)<br/> [1] "coefficients"  "residuals"   "effects"    "rank"<br/> [5] "fitted.values" "assign"      "qr"         "df.residual"<br/> [9] "na.action"     "xlevels"     "call"       "terms"<br/>[13] "model"</pre>&#13;
<p class="indent">These members are the components that automatically make up a fitted model object of class <code>"lm"</code>, mentioned briefly earlier. Note that there’s a component called <code>"coefficients"</code>. This contains a numeric vector of the estimates of the intercept and slope.</p>&#13;
<p class="indent">You can extract this component (and indeed any of the other ones listed here) in the same way you would perform a member reference on a named list: by entering <code>survfit$coefficients</code> at the prompt. Where possible, though, it’s technically preferable for programming purposes to extract such components using a “direct-access” function. For the <code>coefficients</code> component of an <code>"lm"</code> object, the function you use is <code>coef</code>.</p>&#13;
<pre>R&gt; mycoefs &lt;- coef(survfit)<br/>R&gt; mycoefs<br/>(Intercept)      Wr.Hnd<br/> 113.953623    3.116617<br/>R&gt; beta0.hat &lt;- mycoefs[1]<br/>R&gt; beta1.hat &lt;- mycoefs[2]</pre>&#13;
<p class="indent">Here, the regression coefficients are extracted from the object and then separately assigned to the objects <code>beta0.hat</code> and <code>beta1.hat</code>. Other common direct-access functions include <code>resid</code> and <code>fitted</code>; these two pertain to the <code>"residuals"</code> and <code>"fitted.values"</code> components, respectively.</p>&#13;
<p class="indent">Finally, I use <code>segments</code> to draw the vertical dashed lines present in <a href="ch20.xhtml#ch20fig2">Figure 20-2</a>.</p>&#13;
<pre>R&gt; segments(x0=c(obsA[1],obsB[1]),y0=beta0.hat+beta1.hat*c(obsA[1],obsB[1]),<br/>            x1=c(obsA[1],obsB[1]),y1=c(obsA[2],obsB[2]),lty=2)</pre>&#13;
<p class="indent">Note that the dashed lines meet the fitted line at the vertical axis locations passed to <code>y0</code>, which, with the use of the regression coefficients <code>beta0.hat</code> and <code>beta1.hat</code>, reflects <a href="ch20.xhtml#ch20eq4">Equation (20.4)</a>.</p>&#13;
<p class="indent">Now, imagine a collection of alternative regression lines drawn through the data (achieved by altering the value of the intercept and slope). Then, for each of the alternative regression lines, imagine you calculate the residuals (vertical distances) between the response value of every observation and <span epub:type="pagebreak" id="page_458"/>the <em>fitted value</em> of that line. The simple linear regression line estimated as per (20.3) is the line that lies “closest to all observations.” By this, it is meant that the fitted regression model is represented by the estimated line that passes through the coordinate provided by the variable means (<em><span class="ent">x̄</span></em>, <em><span class="ent">ȳ</span></em>), and it’s the line that yields the smallest overall measure of the squared residual distances. For this reason, another name for a least-squares-estimated regression equation like this is the <em>line of best fit</em>.</p>&#13;
<h3 class="h3" id="ch20lev1sec64"><strong>20.3 Statistical Inference</strong></h3>&#13;
<p class="noindent">The estimation of a regression equation is relatively straightforward, but this is merely the beginning. You should now think about what can be inferred from your result. In simple linear regression, there’s a natural question that should always be asked: Is there statistical evidence to support the presence of a relationship between the predictor and the response? To put it another way, is there evidence that a change in the explanatory variable affects the mean outcome? You investigate this following the same ideas that were introduced in <a href="ch17.xhtml#ch17">Chapter 17</a> when you began thinking about the variability present in estimated statistics and then continued to infer from your results using confidence intervals and, in <a href="ch18.xhtml#ch18">Chapter 18</a>, hypothesis testing.</p>&#13;
<h4 class="h4" id="ch20lev2sec177"><strong><em>20.3.1 Summarizing the Fitted Model</em></strong></h4>&#13;
<p class="noindent">This kind of <em>model-based inference</em> is automatically carried out by R when <code>lm</code> objects are processed. Using the <code>summary</code> function on an object created by <code>lm</code> provides you with output far more detailed than simply printing the object to the console. For the moment, you’ll focus on just two aspects of the information presented in <code>summary</code>: the significance tests associated with the regression coefficients and the interpretation of the so-called <em>coefficient of determination</em> (labeled <code>R-squared</code> in the output), which I’ll explain shortly.</p>&#13;
<p class="indent">Use <code>summary</code> on the current model object <code>survfit</code>, and you’ll see the following:</p>&#13;
<pre>R&gt; summary(survfit)<br/><br/>Call:<br/>lm(formula = Height ~ Wr.Hnd, data = survey)<br/><br/>Residuals:<br/>     Min       1Q   Median      3Q       Max<br/>-19.7276  -5.0706  -0.8269  4.9473   25.8704<br/><br/>Coefficients:<br/>            Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept) 113.9536     5.4416   20.94   &lt;2e-16 ***<br/>Wr.Hnd        3.1166     0.2888   10.79   &lt;2e-16 ***<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/>Residual standard error: 7.909 on 206 degrees of freedom<br/>  (29 observations deleted due to missingness)<br/>Multiple R-squared:  0.3612,   Adjusted R-squared:  0.3581<br/>F-statistic: 116.5 on 1 and 206 DF,  p-value: &lt; 2.2e-16</pre>&#13;
<h4 class="h4" id="ch20lev2sec178"><span epub:type="pagebreak" id="page_459"/><strong><em>20.3.2 Regression Coefficient Significance Tests</em></strong></h4>&#13;
<p class="noindentb">Let’s begin by focusing on the way the estimated regression coefficients are reported. The first column of the coefficients table contains the point estimates of the intercept and slope (the intercept is labeled as such, and the slope is labeled after the name of the predictor variable in the data frame); the table also includes estimates of the standard errors of these statistics. It can be shown that simple linear regression coefficients, when estimated using least-squares, follow a <em>t</em>-distribution with <em>n</em> − 2 degrees of freedom (when given the number of observations, <em>n</em>, used in the model fit). The standardized <em>t</em> value and a <em>p</em>-value are reported for each parameter. These represent the results of a two-tailed hypothesis test formally defined as</p>&#13;
<p class="center1">H<sub>0</sub> : <em>β<sub>j</sub></em> = 0</p>&#13;
<p class="center1">H<sub>A</sub> : <em>β<sub>j</sub></em> ≠ 0</p>&#13;
<p class="noindentt1">where <em>j</em> = 0 for the intercept and <em>j</em> = 1 for the slope, using the notation in <a href="ch20.xhtml#ch20eq1">Equation (20.1)</a>.</p>&#13;
<p class="indent">Focus on the row of results for the predictor. With a null value of zero, truth of H<sub>0</sub> implies that the predictor has no effect on the response. The claim here is interested in whether there is <em>any effect</em> of the covariate, not the direction of this effect, so H<sub>A</sub> is two-sided (via ≠). As with any hypothesis test, the smaller the <em>p</em>-value, the stronger the evidence against H<sub>0</sub>. With a small <em>p</em>-value (&lt; 2 × 10<sup>−</sup><sup>16</sup>) attached to this particular test statistic (which you can confirm using the formula in <a href="ch18.xhtml#ch18">Chapter 18</a>: <em>T</em> = (3.116 − 0)/0.2888 = 10.79), you’d therefore conclude there is strong evidence <em>against</em> the claim that the predictor has no effect on the mean level of the response.</p>&#13;
<p class="indent">The same test is carried out for the intercept, but the test for the slope parameter <em>β</em><sub>1</sub> is typically more interesting (since rejection of the null hypothesis for <em>β</em><sub>0</sub> simply indicates evidence that the regression line does not strike the vertical axis at zero), especially when the observed data don’t include <em>x</em> = 0, as is the case here.</p>&#13;
<p class="indent">From this, you can conclude that the fitted model suggests there is evidence that an increase in handspan is associated with an increase in height among the population being studied. For each additional centimeter of handspan, the average increase in height is approximately 3.12 cm.</p>&#13;
<p class="indent">You could also produce confidence intervals for your estimates using <a href="ch17.xhtml#ch17eq2">Equation (17.2)</a> on <a href="ch17.xhtml#page_378">page 378</a> and knowledge of the sampling distributions of the regression parameters; however, yet again, R provides a convenient function for an object of class <code>"lm"</code> to do this for you.</p>&#13;
<pre><span epub:type="pagebreak" id="page_460"/>R&gt; confint(survfit,level=0.95)<br/>                 2.5 %     97.5 %<br/>(Intercept) 103.225178 124.682069<br/>Wr.Hnd        2.547273   3.685961</pre>&#13;
<p class="indent">To the <code>confint</code> function you pass your model object as the first argument and your desired level of confidence as <code>level</code>. This indicates that you should be 95 percent confident the true value of <em>β</em><sub>1</sub> lies somewhere between 2.55 and 3.69 (to 2 d.p.). As usual, the exclusion of the null value of zero reflects the statistically significant result from earlier.</p>&#13;
<h4 class="h4" id="ch20lev2sec179"><strong><em>20.3.3 Coefficient of Determination</em></strong></h4>&#13;
<p class="noindent">The output of <code>summary</code> also provides you with the values of <code>Multiple R-squared</code> and <code>Adjusted R-squared</code>, which are particularly interesting. Both of these are referred to as the <em>coefficient of determination</em>; they describe the proportion of the variation in the response that can be attributed to the predictor.</p>&#13;
<p class="indent">For simple linear regression, the first (unadjusted) measure is simply obtained as the square of the estimated correlation coefficient (refer to <a href="ch13.xhtml#ch13lev2sec120">Section 13.2.5</a>). For the student height example, first store the estimated correlation between <code>Wr.Hnd</code> and <code>Height</code> as <code>rho.xy</code>, and then square it:</p>&#13;
<pre>R&gt; rho.xy &lt;- cor(survey$Wr.Hnd,survey$Height,use="complete.obs")<br/>R&gt; rho.xy^2<br/>[1] 0.3611901</pre>&#13;
<p class="indent">You get the same result as the <code>Multiple R-squared</code> value (usually written mathematically as <em>R</em><sup>2</sup>). This tells you that about 36.1 percent of the variation in the student heights can be attributed to handspan.</p>&#13;
<p class="indent">The adjusted measure is an alternative estimate that takes into account the number of parameters that require estimation. The adjusted measure is generally important only if you’re using the coefficient of determination to assess the overall “quality” of the fitted model in terms of a balance between goodness of fit and complexity. I’ll cover this in <a href="ch22.xhtml#ch22">Chapter 22</a>, so I won’t go into any more detail just yet.</p>&#13;
<h4 class="h4" id="ch20lev2sec180"><strong><em>20.3.4 Other summary Output</em></strong></h4>&#13;
<p class="noindent">The <code>summary</code> of the model object provides you with even more useful information. The “residual standard error” is the estimated standard error of the <em><span class="ent">∊</span></em> term (in other words, the square root of the estimated variance of <em><span class="ent">∊</span></em>, namely, <em>σ</em><sup>2</sup>); below that it also reports any missing values. (The 29 observation pairs “deleted due to missingness” here matches the number of incomplete observations determined in <a href="ch20.xhtml#ch20lev1sec62">Section 20.1</a>.)</p>&#13;
<p class="indent">The output also provides a five-number summary (<a href="ch13.xhtml#ch13lev2sec118">Section 13.2.3</a>) for the residual distances—I’ll cover this further in <a href="ch22.xhtml#ch22lev1sec74">Section 22.3</a>. As the final <span epub:type="pagebreak" id="page_461"/>result, you’re provided with a certain hypothesis test performed using the <em>F</em>-distribution. This is a global test of the impact of your predictor(s) on the response; this will be explored alongside multiple linear regression in <a href="ch21.xhtml#ch21lev2sec197">Section 21.3.5</a>.</p>&#13;
<p class="indent">You can access all the output provided by <code>summary</code> directly, as individual R objects, rather than having to read them off the screen from the entire printed summary. Just as <code>names(survfit)</code> provides you with an indication of the contents of the stand-alone <code>survfit</code> object, the following code gives you the names of all the components accessible <em>after</em> <code>summary</code> is used to process <code>survfit</code>.</p>&#13;
<pre>R&gt; names(summary(survfit))<br/> [1] "call"          "terms"        "residuals"      "coefficients"<br/> [5] "aliased"       "sigma"        "df"             "r.squared"<br/> [9] "adj.r.squared" "fstatistic"   "cov.unscaled"   "na.action"</pre>&#13;
<p class="indent">It’s fairly easy to match most of the components with the printed <code>summary</code> output, and they can be extracted using the dollar operator as usual. The residual standard error, for example, can be retrieved directly with this:</p>&#13;
<pre>R&gt; summary(survfit)$sigma<br/>[1] 7.90878</pre>&#13;
<p class="indent">There are further details on this in the <code>?summary.lm</code> help file.</p>&#13;
<h3 class="h3" id="ch20lev1sec65"><strong>20.4 Prediction</strong></h3>&#13;
<p class="noindent">To wrap up these preliminary details of linear regression, you’ll now look at using your fitted model for predictive purposes. The ability to fit a statistical model means that you not only can understand and quantify the nature of relationships in your data (like the estimated 3.1166 cm increase in mean height per 1 cm increase in handspan for the student example) but can also <em>predict</em> values of the outcome of interest, even where you haven’t actually observed the values of any explanatory variables in the original data set. As with any statistic, though, there is always a need to accompany any point estimates or predictions with a measure of spread.</p>&#13;
<h4 class="h4" id="ch20lev2sec181"><strong><em>20.4.1 Confidence Interval or Prediction Interval?</em></strong></h4>&#13;
<p class="noindent">With a fitted simple linear model you’re able to calculate a point estimate of the <em>mean response</em> value, conditional upon the value of an explanatory variable. To do this, you simply plug in (to the fitted model equation) the value of <em>x</em> you’re interested in. A statistic like this is always subject to variation, so just as with sample statistics explored in earlier chapters, you use a <em>confidence interval for the mean response (CI)</em> to gauge this uncertainty.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_462"/>Assume a simple linear regression line has been fitted to <em>n</em> observations such that <img class="middle" src="../images/f0462-01.jpg" alt="image"/> percent confidence interval for the mean response given a value of <em>x</em> is calculated with</p>&#13;
<div class="imagec"><a id="ch20eq5"/><img src="../images/e20-5.jpg" alt="image"/></div>&#13;
<p class="noindent">where you obtain the lower limit by subtraction, the upper limit by addition.</p>&#13;
<p class="indent">Here, <span class="ent">ŷ</span> is the fitted value (from the regression line) at <em>x</em>; <em>t</em><sub>(</sub><sub>1</sub><sub>−</sub> <sub>/</sub><sub>2</sub><sub>,</sub><sub>n</sub><sub>−</sub><sub>2</sub><sub>)</sub> is the appropriate critical value from a <em>t</em>-distribution with <em>n</em> − 2 degrees of freedom (in other words, resulting in an upper-tail area of exactly /2); <em>s<sub><span class="ent">ɛ</span></sub></em> is the estimated residual standard error; and <em><span class="ent">x̄</span></em> and <img class="middle" src="../images/common-05.jpg" alt="image"/> represent the sample mean and the variance of the observations of the predictor, respectively.</p>&#13;
<p class="indent">A <em>prediction interval (PI)</em> for an observed response is different from the confidence interval in terms of context. Where CIs are used to describe the variability of the <em>mean</em> response, a PI is used to provide the possible range of values that an <em>individual realization</em> of the response variable might take, given <em>x</em>. This distinction is subtle but important: the CI corresponds to a mean, and the PI corresponds to an individual observation.</p>&#13;
<p class="indent">Let’s remain with the previous notation. It can be shown that 100(1 − <em>α</em>) percent prediction interval for an individual response given a value of <em>x</em> is calculated with the following:</p>&#13;
<div class="imagec"><a id="ch20eq6"/><img src="../images/e20-6.jpg" alt="image"/></div>&#13;
<p class="indent">It turns out that the only difference from (20.5) is the 1+ that appears in the square root. As such, a PI at <em>x</em> is wider than a CI at <em>x</em>.</p>&#13;
<h4 class="h4" id="ch20lev2sec182"><strong><em>20.4.2 Interpreting Intervals</em></strong></h4>&#13;
<p class="noindent">Continuing with our example, let’s say you want to determine the mean height for students with a handspan of 14.5 cm and for students with a handspan of 24 cm. The point estimates themselves are easy—just plug the desired <em>x</em> values into the regression <a href="ch20.xhtml#ch20eq4">equation (20.4)</a>.</p>&#13;
<pre>R&gt; as.numeric(beta0.hat+beta1.hat*14.5)<br/>[1] 159.1446<br/>R&gt; as.numeric(beta0.hat+beta1.hat*24)<br/>[1] 188.7524</pre>&#13;
<p class="indent">According to the model, you can expect mean heights to be around 159.14 and 188.75 cm for handspans of 14.5 and 24 cm, respectively. The <code>as.numeric</code> coercion function (first encountered in <a href="ch06.xhtml#ch06lev2sec62">Section 6.2.4</a>) is used simply to strip the result of the annotative names that are otherwise present from the <code>beta0.hat</code> and <code>beta1.hat</code> objects.</p>&#13;
<h5 class="h5" id="ch20lev3sec90"><span epub:type="pagebreak" id="page_463"/><strong>Confidence Intervals for Mean Heights</strong></h5>&#13;
<p class="noindent">To find confidence intervals for these estimates, you could calculate them manually using (20.5), but of course R has a built-in <code>predict</code> command to do it for you. To use <code>predict</code>, you first need to store your <em>x</em> values in a particular way: as a column in a new data frame. The name of the column must match the predictor used in the original call to create the fitted model object. In this example, I’ll create a new data frame, <code>xvals</code>, with the column named <code>Wr.Hnd</code>, which contains only two values of interest—the handspans of 14.5 and 24 cm.</p>&#13;
<pre>R&gt; xvals &lt;- data.frame(Wr.Hnd=c(14.5,24))<br/>R&gt; xvals<br/>  Wr.Hnd<br/>1   14.5<br/>2   24.0</pre>&#13;
<p class="indent">Now, when <code>predict</code> is called, the first argument must be the fitted model object of interest, <code>survfit</code> for this example. Next, in the argument <code>newdata</code>, you pass the specially constructed data frame containing the specified predictor values. To the <code>interval</code> argument you must specify <code>"confidence"</code> as a character string value. The confidence level, here set for 95 percent, is passed (on the scale of a probability) to <code>level</code>.</p>&#13;
<pre>R&gt; mypred.ci &lt;- predict(survfit,newdata=xvals,interval="confidence",level=0.95)<br/>R&gt; mypred.ci<br/>       fit      lwr      upr<br/>1 159.1446 156.4956 161.7936<br/>2 188.7524 185.5726 191.9323</pre>&#13;
<p class="indent">This call will return a matrix with three columns, whose number (and order) of rows correspond to the predictor values you supplied in the <code>newdata</code> data frame. The first column, with a heading of <code>fit</code>, is the point estimate on the regression line; you can see that these numbers match the values you worked out earlier. The other columns provide the lower and upper CI limits as the <code>lwr</code> and <code>upr</code> columns, respectively. In this case, you’d interpret this as 95 percent confidence that the mean height of a student with a handspan of 14.5 cm lies somewhere between 156.5 cm and 161.8 cm and lies between 185.6 cm and 191.9 cm for a handspan of 24 cm (when rounded to 1 d.p.). Remember, these CIs, calculated as per (20.5) through <code>predict</code>, are for the <em>mean</em> response value.</p>&#13;
<h5 class="h5" id="ch20lev3sec91"><strong>Prediction Intervals for Individual Observations</strong></h5>&#13;
<p class="noindent">The <code>predict</code> function will also provide your prediction intervals. To find the prediction interval for possible individual observations with a certain probability, you simply need to change the <code>interval</code> argument to <code>"prediction"</code>.</p>&#13;
<pre>R&gt; mypred.pi &lt;- predict(survfit,newdata=xvals,interval="prediction",level=0.95)<br/>R&gt; mypred.pi<br/>       fit      lwr      upr<br/>1 159.1446 143.3286 174.9605<br/>2 188.7524 172.8390 204.6659</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_464"/>Notice that the fitted values remain the same, as Equations (20.5) and (20.6) indicate. The widths of the PIs, however, are significantly larger than those of the corresponding CIs—this is because raw observations themselves, at a specific <em>x</em> value, will naturally be more variable than their mean.</p>&#13;
<p class="indent">Interpretation changes accordingly. The intervals describe where raw student heights are predicted to lie “95 percent of the time.” For a handspan of 14.5 cm, the model predicts individual observations to lie somewhere between 143.3 cm and 175.0 cm with a probability of 0.95; for a handspan of 24 cm, the same PI is estimated at 172.8 cm and 204.7 cm (when rounded to 1 d.p.).</p>&#13;
<h4 class="h4" id="ch20lev2sec183"><strong><em>20.4.3 Plotting Intervals</em></strong></h4>&#13;
<p class="noindent">Both CIs and PIs are well suited to visualization for simple linear regression models. With the following code, you can start off <a href="ch20.xhtml#ch20fig3">Figure 20-3</a> by plotting the data and estimated regression line just as for <a href="ch20.xhtml#ch20fig2">Figure 20-2</a>, but this time using <code>xlim</code> and <code>ylim</code> in <code>plot</code> to widen the <em>x</em>- and <em>y</em>-limits a little in order to accommodate the full length and breadth of the CI and PI.</p>&#13;
<pre>R&gt; plot(survey$Height~survey$Wr.Hnd,xlim=c(13,24),ylim=c(140,205),<br/>        xlab="Writing handspan (cm)",ylab="Height (cm)")<br/>R&gt; abline(survfit,lwd=2)</pre>&#13;
<p class="indent">To this you add the locations of the fitted values for <em>x</em> = 14.5 and <em>x</em> = 24, as well as two sets of vertical lines showing the CIs and PIs.</p>&#13;
<pre>R&gt; points(xvals[,1],mypred.ci[,1],pch=8)<br/>R&gt; segments(x0=c(14.5,24),y0=c(mypred.pi[1,2],mypred.pi[2,2]),<br/>            x1=c(14.5,24),y1=c(mypred.pi[1,3],mypred.pi[2,3]),col="gray",lwd=3)<br/>R&gt; segments(x0=c(14.5,24),y0=c(mypred.ci[1,2],mypred.ci[2,2]),<br/>            x1=c(14.5,24),y1=c(mypred.ci[1,3],mypred.ci[2,3]),lwd=2)</pre>&#13;
<p class="indent">The call to <code>points</code> marks the fitted values for these two particular values of <em>x</em>. The first call to <code>segments</code> lays down the PIs as thickened vertical gray lines, and the second lays down the CIs as the shorter vertical black lines. The coordinates for these plotted line segments are taken directly from the <code>mypred.pi</code> and <code>mypred.ci</code> objects, respectively.</p>&#13;
<p class="indent">You can also produce “bands” around the fitted regression line that mark one or both of these intervals over <em>all</em> values of the predictor. From a programming standpoint, this isn’t technically possible for a continuous variable, but you can achieve it practically by defining a fine sequence of values along the <em>x</em>-axis (using <code>seq</code> with a high <code>length</code> value) and evaluating the CI and PI at every point in this fine sequence. Then you just join resulting points as lines when plotting.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_465"/><img src="../images/f20-03.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch20fig3"/>Figure 20-3: The student height regression example, with a fitted regression line and point estimates at</em> x <em>= 14.5 and</em> x <em>= 24 and with corresponding 95 percent CIs (black vertical lines) and PIs (gray vertical lines). The dashed black and dashed gray lines provide 95 percent confidence and prediction bands for the response variable over the visible range of</em> x <em>values.</em></p>&#13;
<p class="indent">In R, this requires you to rerun the <code>predict</code> command as follows:</p>&#13;
<pre>R&gt; xseq &lt;- data.frame(Wr.Hnd=seq(12,25,length=100))<br/>R&gt; ci.band &lt;- predict(survfit,newdata=xseq,interval="confidence",level=0.95)<br/>R&gt; pi.band &lt;- predict(survfit,newdata=xseq,interval="prediction",level=0.95)</pre>&#13;
<p class="indent">The first line in this code creates the fine sequence of predictor values and stores it in the format required by the <code>newdata</code> argument. The <em>y</em>-axis coordinates for CI and PI bands are stored as the second and third columns of the matrix objects <code>ci.band</code> and <code>pi.band</code>. Finally, <code>lines</code> is used to add each of the four dashed lines corresponding to the upper and lower limits of the two intervals, and a legend adds a final touch.</p>&#13;
<pre>R&gt; lines(xseq[,1],ci.band[,2],lty=2)<br/>R&gt; lines(xseq[,1],ci.band[,3],lty=2)<br/>R&gt; lines(xseq[,1],pi.band[,2],lty=2,col="gray")<br/>R&gt; lines(xseq[,1],pi.band[,3],lty=2,col="gray")<br/>R&gt; legend("topleft",legend=c("Fit","95% CI","95% PI"),lty=c(1,2,2),<br/>          col=c("black","black","gray"),lwd=c(2,1,1))</pre>&#13;
<p class="indent">Note that the black dashed CI bands meet the vertical black lines and the gray dashed PI bands meet the vertical gray lines for the two individual <em>x</em> values from earlier, just as you’d expect.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_466"/><a href="ch20.xhtml#ch20fig3">Figure 20-3</a> shows the end result of all these additions to the plot. The “bowing inwards” curvature of the intervals is characteristic of this kind of plot and is especially visible in the CI. This curve occurs because there is naturally less variation if you’re predicting where there are more data. For more information on <code>predict</code> for linear model objects, take a look at the <code>?predict.lm</code> help file.</p>&#13;
<h4 class="h4" id="ch20lev2sec184"><strong><em>20.4.4 Interpolation vs. Extrapolation</em></strong></h4>&#13;
<p class="noindent">Before finishing this introduction to prediction, it’s important to clarify the definitions of two key terms: <em>interpolation</em> and <em>extrapolation</em>. These terms describe the nature of a given prediction. A prediction is referred to as interpolation if the <em>x</em> value you specify falls within the range of your observed data; extrapolation is when the <em>x</em> value of interest lies outside this range. From the point-predictions you just made, you can see that the location <em>x</em> = 14.5 is an example of interpolation, and <em>x</em> = 24 is an example of extrapolation.</p>&#13;
<p class="indent">In general, interpolation is preferable to extrapolation—it makes more sense to use a fitted model for prediction in the vicinity of data that have already been observed. Extrapolation that isn’t too far out of that vicinity may still be considered reliable, though. The extrapolation for the student height example at <em>x</em> = 24 is a case in point. This is outside the range of the observed data, but not by much in terms of scale, and the estimated intervals for the expected value of <span class="ent">ŷ</span> = 188.75 cm appear, at least visually, not unreasonable given the distribution of the other observations. In contrast, it would make less sense to use the fitted model to predict student height at a handspan of, say, 50 cm:</p>&#13;
<pre>R&gt; predict(survfit,newdata=data.frame(Wr.Hnd=50),interval="confidence",<br/>           level=0.95)<br/>       fit      lwr      upr<br/>1 269.7845 251.9583 287.6106</pre>&#13;
<p class="indent">Such an extreme extrapolation suggests that the mean height of an individual with a handspan of 50 cm is almost 270 cm, both being fairly unrealistic measurements. The same is true in the other direction; the intercept <img class="middle" src="../images/b0.jpg" alt="image"/> doesn’t have a particularly useful practical interpretation, indicating that the mean height of a student with a handspan of 0 cm is around 114 cm.</p>&#13;
<p class="indent">The main message here is to use common sense when making any prediction from a linear model fit. In terms of the reliability of the results, predictions made at values within an appropriate proximity of the observed data are preferable.</p>&#13;
<div class="ex">&#13;
<p class="ext"><span epub:type="pagebreak" id="page_467"/><a id="ch20exc1"/><strong>Exercise 20.1</strong></p>&#13;
<p class="noindentz">Continue to use the <code>survey</code> data frame from the package <code>MASS</code> for the next few exercises.</p>&#13;
<ol type="a">&#13;
<li><p class="noindents">Using your fitted model of student height on writing handspan, <code>survfit</code>, provide point estimates and 99 percent confidence intervals for the mean student height for handspans of 12, 15.2, 17, and 19.9 cm.</p></li>&#13;
<li><p class="noindents">In <a href="ch20.xhtml#ch20lev1sec62">Section 20.1</a>, you defined the object <code>incomplete.obs</code>, a numeric vector that provides the records of <code>survey</code> that were automatically removed from consideration when estimating the model parameters. Now, use the <code>incomplete.obs</code> vector along with <code>survey</code> and <a href="ch20.xhtml#ch20eq3">Equation (20.3)</a> to calculate <img class="middle" src="../images/b0.jpg" alt="image"/> and <img class="middle" src="../images/b1.jpg" alt="image"/> in R. (Remember the functions <code>mean</code>, <code>sd</code>, and <code>cor</code>. Ensure your answers match the output from <code>survfit</code>.)</p></li>&#13;
<li><p class="noindents">The <code>survey</code> data frame has a number of other variables present aside from <code>Height</code> and <code>Wr.Hnd</code>. For this exercise, the end aim is to fit a simple linear model to predict the mean student height, but this time from their pulse rate, given in <code>Pulse</code> (continue to assume the conditions listed in <a href="ch20.xhtml#ch20lev1sec63">Section 20.2</a> are satisfied).</p>&#13;
<ol type="i">&#13;
<li><p class="noindents">Fit the regression model and produce a scatterplot with the fitted line superimposed upon the data. Make sure you can write down the fitted model equation and keep the plot open.</p></li>&#13;
<li><p class="noindents">Identify and interpret the point estimate of the slope, as well as the outcome of the test associated with the hypotheses H<sub>0</sub> : <em>β</em><sub>1</sub> = 0; H<sub>A</sub> : <em>β</em><sub>1</sub> ≠ 0. Also find a 90 percent CI for the slope parameter.</p></li>&#13;
<li><p class="noindents">Using your model, add lines for 90 percent confidence and prediction interval bands on the plot from (i) and add a legend to differentiate between the lines.</p></li>&#13;
<li><p class="noindents">Create an <code>incomplete.obs</code> vector for the current “height on pulse” data. Use that vector to calculate the sample mean of the height observations that were used for the model fitted in (i). Then add a perfectly horizontal line to the plot at this mean (use color or line type options to avoid confusion with the other lines present). What do you notice? Does the plot support your conclusions from (ii)?</p></li>&#13;
</ol></li>&#13;
</ol>&#13;
<p class="noindentz"><span epub:type="pagebreak" id="page_468"/>Next, examine the help file for the <code>mtcars</code> data set, which you first saw in <a href="ch13.xhtml#ch13exc4">Exercise 13.4</a> on <a href="ch13.xhtml#page_287">page 287</a>. For this exercise, the goal is to model fuel efficiency, measured in miles per gallon (MPG), in terms of the overall weight of the vehicle (in thousands of pounds).</p>&#13;
<ol type="a" start="4">&#13;
<li><p class="noindents">Plot the data—<code>mpg</code> on the <em>y</em>-axis and <code>wt</code> on the <em>x</em>-axis.</p></li>&#13;
<li><p class="noindents">Fit the simple linear regression model. Add the fitted line to the plot from (d).</p></li>&#13;
<li><p class="noindents">Write down the regression equation and interpret the point estimate of the slope. Is the effect of <code>wt</code> on mean <code>mpg</code> estimated to be statistically significant?</p></li>&#13;
<li><p class="noindents">Produce a point estimate and associated 95 percent PI for a car that weighs 6,000 lbs. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?</p></li>&#13;
</ol>&#13;
</div>&#13;
<h3 class="h3" id="ch20lev1sec66"><strong>20.5 Understanding Categorical Predictors</strong></h3>&#13;
<p class="noindent">So far, you’ve looked at simple linear regression models that rely on continuous explanatory variables, but it’s also possible to use a discrete or categorical explanatory variable, made up of <em>k</em> distinct groups or levels, to model the mean response. You must be able to make the same assumptions noted in <a href="ch20.xhtml#ch20lev1sec63">Section 20.2</a>: that observations are all independent of one another and residuals are normally distributed with an equal variance. To begin with, you’ll look at the simplest case in which <em>k</em> = 2 (a binary-valued predictor), which forms the basis of the slightly more complicated situation in which the categorical predictor has more than two levels (a multilevel predictor: <em>k</em> &gt; 2).</p>&#13;
<h4 class="h4" id="ch20lev2sec185"><strong><em>20.5.1 Binary Variables: k = 2</em></strong></h4>&#13;
<p class="noindent">Turn your attention back to <a href="ch20.xhtml#ch20eq1">Equation (20.1)</a>, where the regression model is specified as <em>Y</em>|<em>X</em> = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>X</em> + <em><span class="ent">∊</span></em> for a response variable <em>Y</em> and predictor <em>X</em>, and <em><span class="ent">∊</span></em> ~ N(0,<em>σ</em><sup>2</sup>). Now, suppose your predictor variable is categorical, with only two possible levels (binary; <em>k</em> = 2) and observations coded either 0 or 1. For this case, (20.1) still holds, but the interpretation of the model parameters, <em>β</em><sub>0</sub> and <em>β</em><sub>1</sub>, isn’t really one of an “intercept” and a “slope” anymore. Instead, it’s better to think of them as being something like two intercepts, where <em>β</em><sub>0</sub> provides the <em>baseline</em> or <em>reference</em> value of the response when <em>X</em> = 0 and <em>β</em><sub>1</sub> represents the <em>additive effect</em> on the mean response if <em>X</em> = 1. In other words, if <em>X</em> = 0, then <em>Y</em> = <em>β</em><sub>0</sub> + <em><span class="ent">∊</span></em>; if <em>X</em> = 1, then <em>Y</em> = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub> + <em><span class="ent">∊</span></em>. As usual, estimation is in terms of finding the <em>mean</em> response <span class="ent">ŷ</span> <img class="middle" src="../images/e.jpg" alt="image"/>[<em>Y</em> |<em>X</em> = <em>x</em>] as per <a href="ch20.xhtml#ch20eq2">Equation (20.2)</a>, so the equation becomes <img class="middle" src="../images/f0468-01.jpg" alt="image"/>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_469"/>Go back to the <code>survey</code> data frame and note that you have a <code>Sex</code> variable, where the students recorded their gender. Look at the documentation on the help page <code>?survey</code> or enter something like this:</p>&#13;
<pre>R&gt; class(survey$Sex)<br/>[1] "factor"<br/>R&gt; table(survey$Sex)<br/><br/>Female   Male<br/>   118    118</pre>&#13;
<p class="indent">You’ll see that the sex data column is a factor vector with two levels, <code>Female</code> and <code>Male</code>, and that there happens to be an equal number of the two (one of the 237 records has a missing value for this variable).</p>&#13;
<p class="indent">You’re going to determine whether there is statistical evidence that the height of a student is affected by sex. This means that you’re again interested in modeling height as the response variable, but this time, it’s with the categorical sex variable as the predictor.</p>&#13;
<p class="indent">To visualize the data, if you make a call to <code>plot</code> as follows, you’ll get a pair of boxplots.</p>&#13;
<pre>R&gt; plot(survey$Height~survey$Sex)</pre>&#13;
<p class="indent">This is because the response variable specified to the left of the <code>~</code> is numeric and the explanatory variable to the right is a factor, and the default behavior of R in that situation is to produce side-by-side boxplots.</p>&#13;
<p class="indent">To further emphasize the categorical nature of the explanatory variable, you can superimpose the raw height and sex observations on top of the boxplots. To do this, just convert the factor vector to numeric with a call to <code>as.numeric</code>; this can be done directly in a call to <code>points</code>.</p>&#13;
<pre>R&gt; points(survey$Height~as.numeric(survey$Sex),cex=0.5)</pre>&#13;
<p class="indent">Remember that boxplots mark off the median as the central bold line but that least-squares linear regressions are defined by the mean outcome, so it’s useful to also display the mean heights according to sex.</p>&#13;
<pre>R&gt; means.sex &lt;- tapply(survey$Height,INDEX=survey$Sex,FUN=mean,na.rm=TRUE)<br/>R&gt; means.sex<br/>  Female     Male<br/>165.6867 178.8260<br/>R&gt; points(1:2,means.sex,pch=4,cex=3)</pre>&#13;
<p class="indent">You were introduced to <code>tapply</code> in <a href="ch10.xhtml#ch10lev2sec94">Section 10.2.3</a>; in this call, the argument <code>na.rm=TRUE</code> is matched to the ellipsis in the definition of <code>tapply</code> and is passed to <code>mean</code> (you need it to ensure the missing values present in the data <span epub:type="pagebreak" id="page_470"/>do not end up producing <code>NA</code>s as the results). A further call to <code>points</code> adds those coordinates (as × symbols) to the image; <a href="ch20.xhtml#ch20fig4">Figure 20-4</a> gives the final result.</p>&#13;
<div class="image"><img src="../images/f20-04.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch20fig4"/>Figure 20-4: Boxplots of the student heights split by sex, with the raw observations and sample means (small <span class="ent">○</span> and large × symbols, respectively) superimposed</em></p>&#13;
<p class="indent">The plot indicates, overall, that males tend to be taller than females—but is there statistical evidence of a difference to back this up?</p>&#13;
<h5 class="h5" id="ch20lev3sec92"><strong>Linear Regression Model of Binary Variables</strong></h5>&#13;
<p class="noindent">To answer this with a simple linear regression model, you can use <code>lm</code> to produce least-squares estimates just like with every other model you’ve fitted so far.</p>&#13;
<pre>R&gt; survfit2 &lt;- lm(Height~Sex,data=survey)<br/>R&gt; summary(survfit2)<br/><br/>Call:<br/>lm(formula = Height ~ Sex, data = survey)<br/><br/>Residuals:<br/>    Min      1Q  Median      3Q    Max<br/>-23.886  -5.667   1.174   4.358 21.174<br/><br/>Coefficients:<br/>            Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept)  165.687      0.730  226.98   &lt;2e-16 ***<br/>SexMale       13.139      1.022   12.85   &lt;2e-16 ***<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 7.372 on 206 degrees of freedom<br/>  (29 observations deleted due to missingness)<br/>Multiple R-squared:  0.4449, Adjusted R-squared:  0.4422<br/>F-statistic: 165.1 on 1 and 206 DF,  p-value: &lt; 2.2e-16</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_471"/>However, because the predictor is a factor vector instead of a numeric vector, the reporting of the coefficients is slightly different. The estimate of <sub>0</sub> is again reported as <code>(Intercept)</code>; this is the estimate of the mean height if a student is female. The estimate of <em>β</em><sub>1</sub> is reported as <code>SexMale</code>. The corresponding regression coefficient of 13.139 is the estimated difference that is imparted upon the mean height of a student if male. If you look at the corresponding regression equation</p>&#13;
<div class="imagec"><a id="ch20eq7"/><img src="../images/e20-7.jpg" alt="image"/></div>&#13;
<p class="noindent">you can see that the model has been fitted assuming the variable <em>x</em> is defined as “the individual is male”—0 for no/false, 1 for yes/true. In other words, the level of “female” for the sex variable is assumed as a reference, and it is the effect of “being male” on mean height that is explicitly estimated. The hypothesis test for <em>β</em><sub>0</sub> and <em>β</em><sub>1</sub> is performed with the same hypotheses defined in <a href="ch20.xhtml#ch20lev2sec178">Section 20.3.2</a>:</p>&#13;
<p class="center">H<sub>0</sub> : <em>β<sub>j</sub></em> = 0</p>&#13;
<p class="center">H<sub>A</sub> : <em>β<sub>j</sub></em> ≠ 0</p>&#13;
<p class="indent">Again, it’s the test for <em>β</em><sub>1</sub> that’s generally of the most interest since it’s this value that tells you whether there is statistical evidence that the mean response variable is affected by the explanatory variable, that is, if <em>β</em><sub>1</sub> is significantly different from zero.</p>&#13;
<h5 class="h5" id="ch20lev3sec93"><strong>Predictions from a Binary Categorical Variable</strong></h5>&#13;
<p class="noindent">Because there are only two possible values for <em>x</em>, prediction is straightforward here. When you evaluate the equation, the only decision that needs to be made is whether <img class="middle" src="../images/b1.jpg" alt="image"/> needs to be used (in other words, if an individual is male) or not (if an individual is female). For example, you can enter the following code to create a factor of five extra observations with the same level names as the original data and store the new data in <code>extra.obs</code>:</p>&#13;
<pre>R&gt; extra.obs &lt;- factor(c("Female","Male","Male","Male","Female"))<br/>R&gt; extra.obs<br/>[1] Female Male   Male   Male   Female<br/>Levels: Female Male</pre>&#13;
<p class="indent">Then, use <code>predict</code> in the now-familiar fashion to find the mean heights at those extra values of the predictor. (Remember that when you pass in new data to <code>predict</code> using the <code>newdata</code> argument, the predictors must be in the same form as the data that were used to fit the model in the first place.)</p>&#13;
<pre><span epub:type="pagebreak" id="page_472"/>R&gt; predict(survfit2,newdata=data.frame(Sex=extra.obs),interval="confidence",<br/>           level=0.9)<br/>       fit      lwr      upr<br/>1 165.6867 164.4806 166.8928<br/>2 178.8260 177.6429 180.0092<br/>3 178.8260 177.6429 180.0092<br/>4 178.8260 177.6429 180.0092<br/>5 165.6867 164.4806 166.8928</pre>&#13;
<p class="indent">You can see from the output that the predictions are different only between the two sets of values—the point estimates of the two instances of <code>Female</code> are identical, simply <img class="middle" src="../images/b0.jpg" alt="image"/> with 90 percent CIs. The point estimates and CIs for the instances of <code>Male</code> are also all the same as each other, based on a point estimate of <img class="middle" src="../images/f0472-01.jpg" alt="image"/>.</p>&#13;
<p class="indent">On its own, admittedly, this example isn’t too exciting. However, it’s critical to understand how R presents regression results when using categorical predictors, especially when considering multiple regression in <a href="ch21.xhtml#ch21">Chapter 21</a>.</p>&#13;
<h4 class="h4" id="ch20lev2sec186"><strong><em>20.5.2 Multilevel Variables: k &gt; 2</em></strong></h4>&#13;
<p class="noindent">It’s common to work with data where the categorical predictor variables have more than two levels so that (<em>k</em> &gt; 2). These can also be referred to as <em>multilevel</em> categorical variables. To deal with this more complicated situation while retaining interpretability of your parameters, you must first dummy code your predictor into <em>k</em> − 1 binary variables.</p>&#13;
<h5 class="h5" id="ch20lev3sec94"><strong>Dummy Coding Multilevel Variables</strong></h5>&#13;
<p class="noindent">To see how this is done, assume that you want to find the value of response variable <em>Y</em> when given the value of a categorical variable <em>X</em>, where <em>X</em> has <em>k</em> &gt; 2 levels (also assume the conditions for validity of the linear regression model—<a href="ch20.xhtml#ch20lev1sec63">Section 20.2</a>—are satisfied).</p>&#13;
<p class="indent">In regression modeling, <em>dummy coding</em> is the procedure used to create several binary variables from a categorical variable like <em>X</em>. Instead of the single categorical variable with possible realizations</p>&#13;
<p class="center"><em>X</em> = 1,2,3, . . . , <em>k</em></p>&#13;
<p class="noindent">you recode it into several yes/no variables—one for each level—with possible realizations:</p>&#13;
<p class="center"><em>X</em><sub>(</sub><sub>1</sub><sub>)</sub> = 0,1; <em>X</em><sub>(</sub><sub>2</sub><sub>)</sub> = 0,1; <em>X</em><sub>(</sub><sub>3</sub><sub>)</sub> = 0,1; . . . ; <em>X</em><sub>(</sub><sub>k</sub><sub>)</sub> = 0,1</p>&#13;
<p class="indent">As you can see, <em>X</em><sub>(</sub><sub>i</sub><sub>)</sub> represents a binary variable for the <em>i</em>th level of the original <em>X</em>. For example, if an individual has <em>X</em> = 2 in the original categorical variable, then <em>X</em><sub>(</sub><sub>2</sub><sub>)</sub> = 1 (yes) and all of the others (<em>X</em><sub>(</sub><sub>1</sub><sub>)</sub>, <em>X</em><sub>(</sub><sub>3</sub><sub>)</sub>, . . . , <em>X</em><sub>(</sub><sub>k</sub><sub>)</sub>) will be zero (no).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_473"/>Suppose <em>X</em> is a variable that can take any one of the <em>k</em> = 4 values 1, 2, 3, or 4, and you’ve made six observations of this variable: 1, 2, 2, 1, 4, 3. <a href="ch20.xhtml#ch20tab1">Table 20-1</a> shows these observations and their dummy-coded equivalents <em>X</em><sub>(</sub><sub>1</sub><sub>)</sub>, <em>X</em><sub>(</sub><sub>2</sub><sub>)</sub>, <em>X</em><sub>(</sub><sub>3</sub><sub>)</sub>, and <em>X</em><sub>(</sub><sub>4</sub><sub>)</sub>.</p>&#13;
<p class="tabt"><a id="ch20tab1"/><strong>Table 20-1:</strong> Illustrative Example of Dummy Coding for Six Observations of a Categorical Variable with <em>k</em> = 4 Groups</p>&#13;
<table class="topbot">&#13;
<thead>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="tableth2"><p class="table"><em>X</em></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="table"><em>X</em><sub>(</sub><sub>1</sub><sub>)</sub></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="table"><em>X</em><sub>(</sub><sub>2</sub><sub>)</sub></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="table"><em>X</em><sub>(</sub><sub>3</sub><sub>)</sub></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="table"><em>X</em><sub>(</sub><sub>4</sub><sub>)</sub></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table3"><p class="table">1</p></td>&#13;
<td style="vertical-align: top;"><p class="table">1</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table3"><p class="table">2</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
<td style="vertical-align: top;"><p class="table">1</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table3"><p class="table">2</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
<td style="vertical-align: top;"><p class="table">1</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table3"><p class="table">1</p></td>&#13;
<td style="vertical-align: top;"><p class="table">1</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table3"><p class="table">4</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
<td style="vertical-align: top;"><p class="table">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table3"><p class="table">3</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
<td style="vertical-align: top;"><p class="table">1</p></td>&#13;
<td style="vertical-align: top;"><p class="table">0</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">In fitting the subsequent model, you usually only use <em>k</em> − 1 of the dummy binary variables—one of the variables acts as a <em>reference</em> or <em>baseline</em> level, and it’s incorporated into the overall intercept of the model. In practice, you would end up with an estimated model like this,</p>&#13;
<div class="imagec"><a id="ch20eq8"/><img src="../images/e20-8.jpg" alt="image"/></div>&#13;
<p class="noindent">assuming 1 is the reference level. As you can see, in addition to the overall intercept term <img class="middle" src="../images/b0.jpg" alt="image"/>, you have <em>k</em> −1 other estimated intercept terms that modify the baseline coefficient <img class="middle" src="../images/b0.jpg" alt="image"/>, depending on which of the original categories an observation takes on. For example, in light of the coding imposed in (20.8), if an observation has <em>X</em><sub>(</sub><sub>3</sub><sub>)</sub> = 1 and all other binary values are therefore zero (so that observation would’ve had a value of <em>X</em> = 3 for the original categorical variable), the predicted mean value of the response would be <img class="middle" src="../images/f0473-01.jpg" alt="image"/>. On the other hand, because the reference level is defined as 1, if an observation has values of zero for all the binary variables, it implies the observation originally had <em>X</em> = 1, and the prediction would be simply <img class="middle" src="../images/f0473-02.jpg" alt="image"/>.</p>&#13;
<p class="indent">The reason it’s necessary to dummy code for categorical variables of this nature is that, in general, categories cannot be related to each other in the same numeric sense as continuous variables. It’s often not appropriate, for example, to think that an observation in category 4 is “twice as much” as one in category 2, which is what the estimation methods would assume. Binary presence/absence variables are valid, however, and can be easily incorporated into the modeling framework. Choosing the reference level is generally of secondary importance—the specific values of the estimated coefficients will change accordingly, but any overall interpretations you make based on the fitted model will be the same regardless.</p>&#13;
<div class="note">&#13;
<p class="notet"><span epub:type="pagebreak" id="page_474"/><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>Implementing this dummy-coding approach is technically a form of multiple regression since you’re now including several binary variables in the model. It’s important, however, to be aware of the somewhat artificial nature of dummy coding—you should still think of the multiple coefficients as representing a single categorical variable since the binary variables X</em><sub>(</sub><sub>1</sub><sub>)</sub><em>, . . . , X</em><sub>(</sub><sub>k</sub><sub>)</sub> <em>are not independent of one another. This is why I’ve chosen to define these models in this chapter; multiple regression will be formally discussed in <a href="ch21.xhtml#ch21">Chapter 21</a>.</em></p>&#13;
</div>&#13;
<h5 class="h5" id="ch20lev3sec95"><strong>Linear Regression Model of Multilevel Variables</strong></h5>&#13;
<p class="noindent">R makes working with categorical predictors in this way quite simple since it automatically dummy codes for any such explanatory variable when you call <code>lm</code>. There are two things you should check before fitting your model, though.</p>&#13;
<ol>&#13;
<li><p class="noindents">The categorical variable of interest should be stored as a (formally unordered) factor.</p></li>&#13;
<li><p class="noindents">You should check that you’re happy with the category assigned as the reference level (for interpretative purposes—see <a href="ch20.xhtml#ch20lev2sec187">Section 20.5.3</a>).</p></li>&#13;
</ol>&#13;
<p class="indent">You must also of course be happy with the validity of the familiar assumptions of normality and independence of <em><span class="ent">∊</span></em>.</p>&#13;
<p class="indent">To demonstrate all these definitions and ideas, let’s return to the student survey data from the <code>MASS</code> package and keep “student height” as the response variable of interest. Among the data is the variable <code>Smoke</code>. This variable describes the kind of smoker each student reports themselves as, defined by frequency and split into four categories: “heavy,” “never,” “occasional,” and “regular.”</p>&#13;
<pre>R&gt; is.factor(survey$Smoke)<br/>[1] TRUE<br/>R&gt; table(survey$Smoke)<br/><br/>Heavy Never Occas Regul<br/>   11   189    19    17<br/>R&gt; levels(survey$Smoke)<br/>[1] "Heavy" "Never" "Occas" "Regul"</pre>&#13;
<p class="indent">Here, the result from <code>is.factor(survey$Smoke)</code> indicates that you do indeed have a factor vector at hand, the call to <code>table</code> yields the number of students in each of the four categories, and as per <a href="ch05.xhtml#ch05">Chapter 5</a>, you can explicitly request the levels attribute of any R factor via <code>levels</code>.</p>&#13;
<p class="indent">Let’s ask whether there’s statistical evidence to support a difference in mean student height according to smoking frequency. You can create a set of boxplots of these data with the following two lines; <a href="ch20.xhtml#ch20fig5">Figure 20-5</a> shows the result.</p>&#13;
<pre>R&gt; boxplot(Height~Smoke,data=survey)<br/>R&gt; points(1:4,tapply(survey$Height,survey$Smoke,mean,na.rm=TRUE),pch=4)</pre>&#13;
<div class="image"><span epub:type="pagebreak" id="page_475"/><img src="../images/f20-05.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch20fig5"/>Figure 20-5: Boxplots of the observed student heights split by smoking frequency; respective sample means marked with</em> ×</p>&#13;
<p class="indent">Note from earlier R output that unless explicitly defined at creation, the levels of a factor appear in alphabetical order by default—as is the case for <code>Smoke</code>—and R will automatically set the first one (as shown in the output of a call to <code>levels</code>) as the reference level when that factor is used as a predictor in subsequent model fitting. Fitting the linear model in mind using <code>lm</code>, you can see from a subsequent call to <code>summary</code> that indeed the first level of <code>Smoke</code>, for “heavy”, has been used as the reference:</p>&#13;
<pre>R&gt; survfit3 &lt;- lm(Height~Smoke,data=survey)<br/>R&gt; summary(survfit3)<br/><br/>Call:<br/>lm(formula = Height ~ Smoke, data = survey)<br/><br/>Residuals:<br/>   Min     1Q Median     3Q    Max<br/>-25.02  -6.82  -1.64   8.18  28.18<br/><br/>Coefficients:<br/><br/>            Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept) 173.7720     3.1028  56.005   &lt;2e-16 ***<br/>SmokeNever   -1.9520     3.1933  -0.611    0.542<br/>SmokeOccas   -0.7433     3.9553  -0.188    0.851<br/>SmokeRegul    3.6451     4.0625   0.897    0.371<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 9.812 on 205 degrees of freedom<br/>  (28 observations deleted due to missingness)<br/>Multiple R-squared:  0.02153, Adjusted R-squared:  0.007214<br/>F-statistic: 1.504 on 3 and 205 DF,  p-value: 0.2147</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_476"/>As outlined in <a href="ch20.xhtml#ch20eq8">Equation (20.8)</a>, you get estimates of coefficients corresponding to the dummy binary variables for three of the four possible categories in this example—the three nonreference levels. The observation in the reference category <code>Heavy</code> is represented solely by <img class="middle" src="../images/b0.jpg" alt="image"/>, designated first as the overall <code>(Intercept)</code>, with the other coefficients providing the effects associated with an observation in one of the other categories.</p>&#13;
<h5 class="h5" id="ch20lev3sec96"><strong>Predictions from a Multilevel Categorical Variable</strong></h5>&#13;
<p class="noindent">You find point estimates through prediction, as usual.</p>&#13;
<pre>R&gt; one.of.each &lt;- factor(levels(survey$Smoke))<br/>R&gt; one.of.each<br/>[1] Heavy Never Occas Regul<br/>Levels: Heavy Never Occas Regul<br/>R&gt; predict(survfit3,newdata=data.frame(Smoke=one.of.each),<br/>           interval="confidence",level=0.95)<br/>       fit      lwr      upr<br/>1 173.7720 167.6545 179.8895<br/>2 171.8200 170.3319 173.3081<br/>3 173.0287 168.1924 177.8651<br/>4 177.4171 172.2469 182.5874</pre>&#13;
<p class="indent">Here, I’ve created the object <code>one.of.each</code> for illustrative purposes; it represents one observation in each of the four categories, stored as an object matching the class (and levels) of the original <code>Smoke</code> data. A student in the <code>Occas</code> category, for example, is predicted to have a mean height of 173.772 − 0.7433 = 173.0287.</p>&#13;
<p class="indent">The output from the model summary earlier, however, shows that none of the binary dummy variable coefficients are considered statistically significant from zero (because all the <em>p</em>-values are too large). The results indicate, as you might have suspected, that there’s no evidence that smoking frequency (or more specifically, having a smoking frequency that’s different from the reference level) affects mean student heights based on this sample of individuals. As is common, the baseline coefficient <img class="middle" src="../images/b0.jpg" alt="image"/> is highly statistically significant—but that only suggests that the overall intercept probably isn’t zero. (Because your response variable is a measurement of height and will clearly not be centered anywhere near 0 cm, that result makes sense.) The confidence intervals supplied are calculated in the usual <em>t</em>-based fashion.</p>&#13;
<p class="indent">The small <code>R-Squared</code> value reinforces this conclusion, indicating that barely any of the variation in the response can be explained by changing the category of smoking frequency. Furthermore, the overall <em>F</em> -test <em>p</em>-value is rather large at around 0.215, suggesting an overall nonsignificant effect of the predictor on the response; you’ll look at this in more detail in a moment in <a href="ch20.xhtml#ch20lev2sec189">Section 20.5.5</a> and later on in <a href="ch21.xhtml#ch21lev2sec197">Section 21.3.5</a>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_477"/>As noted earlier, it’s important that you interpret these results—indeed any based on a <em>k</em>-level categorical variable in regression—in a <em>collective</em> fashion. You can claim only that there is <em>no</em> discernible effect of smoking on height because <em>all</em> the <em>p</em>-values for the binary dummy coefficients are nonsignificant. If one of the levels was in fact highly significant (through a small <em>p</em>-value), it would imply that the smoking factor as defined here, as a whole, <em>does</em> have a statistically detectable effect on the response (even if the other two levels were still associated with very high <em>p</em>-values). This will be discussed further in several more examples in <a href="ch21.xhtml#ch21">Chapter 21</a>.</p>&#13;
<h4 class="h4" id="ch20lev2sec187"><strong><em>20.5.3 Changing the Reference Level</em></strong></h4>&#13;
<p class="noindent">Sometimes you might decide to change the automatically selected reference level, compared to which the effects of taking on any of the other levels are estimated. Changing the baseline will result in the estimation of different coefficients, meaning that individual <em>p</em>-values are subject to change, but the overall result (in terms of global significance of the factor) will not be affected. Because of this, altering the reference level is only done for interpretative purposes—sometimes there’s an intuitively natural baseline of the predictor (for example, “Placebo” versus “Drug A” and “Drug B” as a treatment variable in the analysis of some clinical trial) from which you want to estimate deviation in the mean response with respect to the other possible categories.</p>&#13;
<p class="indent">Redefining the reference level can be achieved quickly using the built-in <code>relevel</code> function in R. This function allows you to choose which level comes first in the definition of a given factor vector object and will therefore be designated as the reference level in subsequent model fitting. In the current example, let’s say you’d rather have the nonsmokers as the reference level.</p>&#13;
<pre>R&gt; SmokeReordered &lt;- relevel(survey$Smoke,ref="Never")<br/>R&gt; levels(SmokeReordered)<br/>[1] "Never" "Heavy" "Occas" "Regul"</pre>&#13;
<p class="indent">The <code>relevel</code> function has moved the <code>Never</code> category into the first position in the new factor vector. If you go ahead fit the model again using <code>SmokeReordered</code> instead of the original <code>Smoke</code> column of <code>survey</code>, it’ll provide estimates of coefficients associated with the three different levels of smokers.</p>&#13;
<p class="indent">It’s worth noting the differences in the treatment of unordered versus ordered factor vectors in regression applications. It might seem sensible to formally order the smoking variable by, for example, increasing the frequency of smoking when creating a new factor vector. However, when an ordered factor vector is supplied in a call to <code>lm</code>, R reacts in a different way—it doesn’t perform the relatively simple dummy coding discussed here, where an effect is associated with each optional level to the baseline (technically referred to as <em>orthogonal contrasts</em>). Instead, the default behavior is to fit the model based on something called <em>polynomial contrasts</em>, where the effect of the ordered categorical variable on the response is defined in a more complicated functional form. That discussion is beyond the scope of this text, but <span epub:type="pagebreak" id="page_478"/>it suffices to say that this approach can be beneficial when your interest lies in the specific functional nature of “moving up” through an ordered set of categories. For more on the technical details, see Kuhn and Johnson (<a href="ref.xhtml#ref37">2013</a>). For all relevant regression examples in this book, we’ll work exclusively with unordered factor vectors.</p>&#13;
<h4 class="h4" id="ch20lev2sec188"><strong><em>20.5.4 Treating Categorical Variables as Numeric</em></strong></h4>&#13;
<p class="noindent">The way in which <code>lm</code> decides to define the parameters of the fitted model depends primarily on the kind of data you pass to the function. As discussed, <code>lm</code> imposes dummy coding only if the explanatory variable is an unordered factor vector.</p>&#13;
<p class="indent">Sometimes the categorical data you want to analyze haven’t been stored as a factor in your data object. If the categorical variable is a character vector, <code>lm</code> will implicitly coerce it into a factor. If, however, the intended categorical variable is numeric, then <code>lm</code> performs linear regression exactly as if it were a continuous numeric predictor; it estimates a single regression coefficient, which is interpreted as a “per-one-unit-change” in the mean response.</p>&#13;
<p class="indent">This may seem inappropriate if the original explanatory variable is supposed to be made up of distinct groups. In some settings, however, especially when the variable can be naturally treated as numeric-discrete, this treatment is not only valid statistically but also helps with interpretation.</p>&#13;
<p class="indent">Let’s take a break from the <code>survey</code> data and go back to the ready-to-use <code>mtcars</code> data set. Say you’re interested in the variables mileage, <code>mpg</code> (continuous), and number of cylinders, <code>cyl</code> (discrete; the data set contains cars with either 4, 6, or 8 cylinders). Now, it’s perfectly sensible to automatically think of <code>cyl</code> as a categorical variable. Taking <code>mpg</code> to be the response variable, box-plots are well suited to reflect the grouped nature of <code>cyl</code> as a predictor; the result of the following line is given on the left of <a href="ch20.xhtml#ch20fig6">Figure 20-6</a>:</p>&#13;
<pre>R&gt; boxplot(mtcars$mpg~mtcars$cyl,xlab="Cylinders",ylab="MPG")</pre>&#13;
<p class="indent">When fitting the associated regression model, you must be aware of what you’re instructing R to do. Since the <code>cyl</code> column of <code>mtcars</code> is numeric, and not a factor vector per se, <code>lm</code> will treat it as continuous if you just directly access the data frame.</p>&#13;
<pre>R&gt; class(mtcars$cyl)<br/>[1] "numeric"<br/>R&gt; carfit &lt;- lm(mpg~cyl,data=mtcars)<br/>R&gt; summary(carfit)<br/><br/>Call:<br/>lm(formula = mpg ~ cyl, data = mtcars)<br/><br/>Residuals:<br/>    Min      1Q  Median      3Q     Max<br/>-4.9814 -2.1185  0.2217  1.0717  7.5186<br/><br/>Coefficients:<br/>            Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept)  37.8846     2.0738   18.27  &lt; 2e-16 ***<br/>cyl          -2.8758     0.3224   -8.92 6.11e-10 ***<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 3.206 on 30 degrees of freedom<br/>Multiple R-squared:  0.7262, Adjusted R-squared:  0.7171<br/>F-statistic: 79.56 on 1 and 30 DF,  p-value: 6.113e-10</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_479"/>Just as in earlier sections, you’ve received an intercept and a slope estimate; the latter is highly statistically significant, indicating that there is evidence against the true value of the slope being zero. Your fitted regression line is</p>&#13;
<div class="imagec"><img src="../images/f0479-01.jpg" alt="image"/></div>&#13;
<p class="noindent">where <span class="ent">ŷ</span> is the average mileage and <em>x</em> is numeric—the number of cylinders. For each single additional cylinder, the model says your mileage will decrease by 2.88 MPG, on average.</p>&#13;
<p class="indent">It’s important to recognize the fact that you’ve fitted a continuous line to what is effectively categorical data. The right panel of <a href="ch20.xhtml#ch20fig6">Figure 20-6</a>, created with the following lines, highlights this fact:</p>&#13;
<pre>R&gt; plot(mtcars$mpg~mtcars$cyl,xlab="Cylinders",ylab="MPG")<br/>R&gt; abline(carfit,lwd=2)</pre>&#13;
<div class="image"><img src="../images/f20-06.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch20fig6"/>Figure 20-6: Left: Boxplots of mileage split by cylinders for the</em> <code>mtcars</code> <em>data set. Right: Scatterplot of the same data with fitted regression line (treating</em> <code>cyl</code> <em>as numeric-continuous) superimposed.</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_480"/>Some researchers fit categorical or discrete predictors as continuous variables purposefully. First, it allows interpolation; for example, you could use this model to evaluate the average MPG for a 5-cylinder car. Second, it means there are fewer parameters that require estimation; in other words, instead of <em>k</em> − 1 intercepts for a categorical variable with <em>k</em> groups, you need only one parameter for the slope. Finally, it can be a convenient way to control for so-called nuisance variables; this will become clearer in <a href="ch21.xhtml#ch21">Chapter 21</a>. On the other hand, it means that you no longer get group-specific information. It can be misleading to proceed in this way if any differences in the mean response according to the predictor category of an observation are not well represented linearly—detection of significant effects can be lost altogether.</p>&#13;
<p class="indent">At the very least, it’s important to recognize this distinction when fitting models. If you had only just now recognized that R had fitted the <code>cyl</code> variable as continuous and wanted to actually fit the model with <code>cyl</code> as categorical, you’d have to explicitly convert it into a factor vector beforehand or in the actual call to <code>lm</code>.</p>&#13;
<pre>R&gt; carfit &lt;- lm(mpg~factor(cyl),data=mtcars)<br/>R&gt; summary(carfit)<br/><br/>Call:<br/>lm(formula = mpg ~ factor(cyl), data = mtcars)<br/><br/>Residuals:<br/>    Min      1Q  Median      3Q     Max<br/>-5.2636 -1.8357  0.0286  1.3893  7.2364<br/><br/>Coefficients:<br/>             Estimate Std. Error t  value Pr(&gt;|t|)<br/>(Intercept)   26.6636      0.9718  27.437  &lt; 2e-16 ***<br/>factor(cyl)6  -6.9208      1.5583  -4.441 0.000119 ***<br/>factor(cyl)8 -11.5636      1.2986  -8.905 8.57e-10 ***<br/>---<br/>Signif. codes:  0 '***'  0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 3.223 on 29 degrees of freedom<br/>Multiple R-squared:  0.7325, Adjusted R-squared:  0.714<br/>F-statistic:  39.7 on 2 and 29 DF,  p-value: 4.979e-09</pre>&#13;
<p class="indent">Here, by wrapping <code>cyl</code> in a call to <code>factor</code> when specifying the formula for <code>lm</code>, you can see you’ve obtained regression coefficient estimates for the levels of <code>cyl</code> corresponding to 6- and 8-cylinder cars (with the reference level automatically set to 4-cylinder cars).</p>&#13;
<h4 class="h4" id="ch20lev2sec189"><span epub:type="pagebreak" id="page_481"/><strong><em>20.5.5 Equivalence with One-Way ANOVA</em></strong></h4>&#13;
<p class="noindent">There’s one final observation to make about regression models with a single nominal categorical predictor. Think about the fact that these models describe a mean response value for the <em>k</em> different groups. Does this remind you of anything? In this particular setting, you’re actually doing the same thing as in one-way ANOVA (<a href="ch19.xhtml#ch19lev1sec59">Section 19.1</a>): comparing more than two means and determining whether there is statistical evidence that at least one mean is different from the others. You need to be able to make the same key assumptions of independence and normality for both techniques.</p>&#13;
<p class="indent">In fact, simple linear regression with a single categorical predictor, implemented using least-squares estimation, is just another way to perform one-way ANOVA. Or, perhaps more concisely, ANOVA is a special case of least-squares regression. The outcome of a one-way ANOVA test is a single <em>p</em>-value quantifying a level of statistical evidence against the null hypothesis that states that group means are equal. When you have one categorical predictor in a regression, it’s exactly that <em>p</em>-value that’s reported at the end of the <code>summary</code> of an <code>lm</code> object—something I’ve referred to a couple of times now as the “overall” or “global” significance test (for example, in <a href="ch20.xhtml#ch20lev2sec179">Section 20.3.3</a>).</p>&#13;
<p class="indent">Look back to the final result of that global significance test for the student height modeled by smoking status example—you had a <em>p</em>-value of 0.2147. This came from an <em>F</em> test statistic of 1.504 with df<sub>1</sub> = 3 and df<sub>2</sub> = 205. Now, suppose you were just handed the data and asked to perform a one-way ANOVA of height on smoking. Using the <code>aov</code> function as introduced in <a href="ch19.xhtml#ch19lev1sec59">Section 19.1</a>, you’d call something like this:</p>&#13;
<pre>R&gt; summary(aov(Height~Smoke,data=survey))<br/>             Df Sum Sq Mean Sq F value Pr(&gt;F)<br/>Smoke         3    434  144.78   1.504  0.215<br/>Residuals   205  19736   96.27<br/>28 observations deleted due to missingness</pre>&#13;
<p class="indent">Those same values are returned here; you can also find the square root of the MSE:</p>&#13;
<pre>R&gt; sqrt(96.27)<br/>[1] 9.811728</pre>&#13;
<p class="indent">This is in fact the “residual standard error” given in the <code>lm</code> summary. The two conclusions you’d draw about the impact of smoking status on height (one for the <code>lm</code> output, the other for the ANOVA test) are of course also the same.</p>&#13;
<p class="indent">The global test that <code>lm</code> provides isn’t just there for the benefit of confirming ANOVA results. As a generalization of ANOVA, least-squares regression models provide more than just coefficient-specific tests. That global test <span epub:type="pagebreak" id="page_482"/>is formally referred to as the <em>omnibus</em> F-<em>test</em>, and while it is indeed equivalent to one-way ANOVA in the “single categorical predictor” setting, it’s also a useful overall, stand-alone test of the statistical contribution of several predictors to the outcome value. You’ll explore this further in <a href="ch21.xhtml#ch21lev2sec197">Section 21.3.5</a> after you’ve begun modeling your response variable using multiple explanatory variables.</p>&#13;
<div class="ex">&#13;
<p class="ext"><a id="ch20exc2"/><strong>Exercise 20.2</strong></p>&#13;
<p class="noindentz">Continue using the <code>survey</code> data frame from the package <code>MASS</code> for the next few exercises.</p>&#13;
<ol type="a">&#13;
<li><p class="noindents">The <code>survey</code> data set has a variable named <code>Exer</code>, a factor with <em>k</em> = 3 levels describing the amount of physical exercise time each student gets: none, some, or frequent. Obtain a count of the number of students in each category and produce side-by-side boxplots of student height split by exercise.</p></li>&#13;
<li><p class="noindents">Assuming independence of the observations and normality as usual, fit a linear regression model with height as the response variable and exercise as the explanatory variable (dummy coding). What’s the default reference level of the predictor? Produce a model summary.</p></li>&#13;
<li><p class="noindents">Draw a conclusion based on the fitted model from (b)—does it appear that exercise frequency has any impact on mean height? What is the nature of the estimated effect?</p></li>&#13;
<li><p class="noindents">Predict the mean heights of one individual in each of the three exercise categories, accompanied by 95 percent prediction intervals.</p></li>&#13;
<li><p class="noindents">Do you arrive at the same result and interpretation for the height-by-exercise model if you construct an ANOVA table using <code>aov</code>?</p></li>&#13;
<li><p class="noindents">Is there any change to the outcome of (e) if you alter the model so that the reference level of the exercise variable is “none”? Would you expect there to be?</p></li>&#13;
</ol>&#13;
<p class="noindentz">Now, turn back to the ready-to-use <code>mtcars</code> data set. One of the variables in this data frame is <code>qsec</code>, described as the time in seconds it takes to race a quarter mile; another is <code>gear</code>, the number of forward gears (cars in this data set have either 3, 4, or 5 gears).</p>&#13;
<ol type="a" start="7">&#13;
<li><p class="noindents">Using the vectors straight from the data frame, fit a simple linear regression model with <code>qsec</code> as the response variable and <code>gear</code> as the explanatory variable and interpret the model summary.</p></li>&#13;
<li><p class="noindents"><span epub:type="pagebreak" id="page_483"/>Explicitly convert <code>gear</code> to a factor vector and refit the model. Compare the model summary with that from (g). What do you find?</p></li>&#13;
<li><p class="noindents">Explain, with the aid of a relevant plot in the same style as the right image of <a href="ch20.xhtml#ch20fig6">Figure 20-6</a>, why you think there is a difference between the two models (g) and (h).</p></li>&#13;
</ol>&#13;
</div>&#13;
<h5 class="h5" id="ch20lev3sec97"><strong>Important Code in This Chapter</strong></h5>&#13;
<table class="topbot">&#13;
<thead>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table2r"><p class="table"><strong>Function/operator</strong></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="table"><strong>Brief description</strong></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="table"><strong>First occurrence</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>lm</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Fit linear model</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch20.xhtml#ch20lev2sec175">Section 20.2.3</a>, <a href="ch20.xhtml#page_455">p. 455</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>coef</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Get estimated coefficients</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch20.xhtml#ch20lev2sec176">Section 20.2.4</a>, <a href="ch20.xhtml#page_457">p. 457</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>summary</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Summarize linear model</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch20.xhtml#ch20lev2sec177">Section 20.3.1</a>, <a href="ch20.xhtml#page_458">p. 458</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>confint</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Get CIs for estimated coefficients</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch20.xhtml#ch20lev2sec178">Section 20.3.2</a>, <a href="ch20.xhtml#page_460">p. 460</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>predict</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Predict from linear model</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch20.xhtml#ch20lev2sec182">Section 20.4.2</a>, <a href="ch20.xhtml#page_463">p. 463</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>relevel</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Change factor reference level</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch20.xhtml#ch20lev2sec187">Section 20.5.3</a>, <a href="ch20.xhtml#page_477">p. 477</a><span epub:type="pagebreak" id="page_484"/></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
</body></html>