<html><head></head><body>
<div id="sbo-rt-content">
<section epub:type="backmatter" aria-labelledby="appendixB">
<hgroup>
<h1 class="CHAPTER" id="appendixB"><span class="AppendixNumber"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_483" aria-label=" Page 483. "/><samp class="SANS_Futura_Std_Bold_Condensed_B_11">B</samp></span>
<span class="AppendixTitle"><samp class="SANS_Dogma_OT_Bold_B_11">SCRAPING THE WEB</samp></span>
</h1>
</hgroup>
<p class="COS">Sometimes, in order to research important data publicly available online, you’ll need to download a local copy. When websites don’t provide this data in structured downloadable formats like spreadsheets, JSON files, or databases, you can make your own copy using <i>web scraping</i> (or <i>screen scraping</i>): writing code that loads web pages for you and extracts their contents. These might include social media posts, court documents, or any other online data. You can use web scraping to download either full datasets or the same web page again and again on a regular basis to see if its content changes over time.</p>

<p class="TX"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_484" aria-label=" Page 484. "/>For example, consider the Parler dataset discussed in <span class="Xref"><a href="chapter11.xhtml">Chapter 11</a></span>. Before Parler was kicked offline by its hosting provider for refusing to moderate content that encourages and incites violence, the archivist <span class="LinkTwitter">@donk_enby</span> wrote code to scrape all 32TB of videos—over a million of them—to distribute to researchers and journalists. This appendix teaches you how to do something similar, if the occasion arises.</p>

<p class="TX">I’ll discuss legal considerations around web scraping and give a brief overview of HTTP, the protocol that web browsers use to communicate with websites. Finally, I describe three different techniques that allow you to scrape different types of websites. Complete <span class="Xref"><a href="chapter7.xhtml">Chapters 7</a></span>, <span class="Xref"><a href="chapter8.xhtml">8</a></span>, <span class="Xref"><a href="chapter9.xhtml">9</a></span>, and <span class="Xref"><a href="chapter11.xhtml">11</a></span> before following along, since you’ll need the basic knowledge of Python programming, CSVs, HTML, and JSON covered there.</p>

<section epub:type="division" aria-labelledby="sec1">
<h2 class="H1" id="sec1"><span id="h-368"/><samp class="SANS_Futura_Std_Bold_B_11">Legal Considerations</samp></h2>
<p class="TNI">Web scraping isn’t a crime, but its legality is still murky. In general, using computer systems with permission (like visiting a public website) is perfectly fine, but accessing them without authorization (like logging in to someone else’s account) is illegal hacking.</p>

<p class="TX">In the US, unauthorized access is a violation of the extremely outdated hacking law known as the Computer Fraud and Abuse Act of 1986, or CFAA. Web scraping shouldn’t fall under unauthorized access because it entails writing code simply to load public web pages that everyone can already access, rather than loading those pages the normal way (using a web browser). The problem is that scraping may violate a website’s terms of service, and there’s no legal consensus on whether this could constitute a violation of the CFAA—courts have ruled both ways.</p>

<p class="TX">Despite this, web scraping is an extremely common practice. Search engines like Google are essentially massive web scraping operations, as are archive sites like the Internet Archive’s Wayback Machine at <a href="https://web.archive.org"><i>https://<wbr/>web<wbr/>.archive<wbr/>.org</i></a>. Companies often use web scraping to keep track of airline ticket prices, job listings, and other public data. It’s also a critical tool for investigative reporting.</p>
<blockquote>
<p class="NOTE"><span class="NoteHead"><samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp></span></p>
</blockquote>
<p class="NOTE-TXT"><i>The CFAA was originally passed, at least in part, in response to the 1983 film</i> <span class="note_Italic">WarGames</span><i>. In the film, a teenage hacker, played by Matthew Broderick, breaks into a military supercomputer and almost starts World War III by mistake. At the time, there weren’t any laws against hacking computers. The wildly popular film scared Congress into passing such laws.</i></p>

<p class="TX">The Markup, a nonprofit newsroom that investigates the tech industry, summed up the case for web scraping in an article that includes several examples of investigative journalism that relied on it. For example, the newsroom Reveal scraped content from extremist groups on Facebook, as well as law enforcement groups, and found significant overlap in membership. Reuters also scraped social media and message boards and uncovered <span role="doc-pagebreak" epub:type="pagebreak" id="pg_485" aria-label=" Page 485. "/>an underground market for adopted kids; that investigation led to a kidnapping conviction. You can read the full article at <a href="https://themarkup.org/news/2020/12/03/why-web-scraping-is-vital-to-democracy"><i>https://<wbr/>themarkup<wbr/>.org<wbr/>/news<wbr/>/2020<wbr/>/12<wbr/>/03<wbr/>/why<wbr/>-web<wbr/>-scraping<wbr/>-is<wbr/>-vital<wbr/>-to<wbr/>-democracy</i></a>.</p>

<p class="TX">Before you can start writing code to scrape the web yourself, you’ll need to understand what HTTP requests are.</p>
</section>

<section epub:type="division" aria-labelledby="sec2">
<h2 class="H1" id="sec2"><span id="h-369"/><samp class="SANS_Futura_Std_Bold_B_11">HTTP Requests</samp></h2>
<p class="TNI">When you load a web page, your web browser makes an <i>HTTP request</i>. You can think of this as the browser sending a message to the website’s server, saying, “I’d like to download the content for the page at this URL so I can look at it on my computer,” to which the server replies with an <i>HTTP response</i> that contains the content, typically HTML code. Your browser parses this HTML to figure out what else it needs to download to show you the full web page: images, fonts, Cascading Style Sheets (CSS) files that define how the web page looks, and JavaScript files that tell the website how to act. The browser makes another HTTP request for each of these resources, getting the content for them all. Websites also tend to make lots of HTTP requests while you’re using them, such as to check for updates and display them on the page in real time.</p>

<p class="TX">HTTP requests and responses have <i>headers</i>, or metadata about the request or response. You might need to send specific headers for your scraping to work properly, depending on the website you’re trying to scrape. You might also need your code to keep track of <i>cookies</i>, which are required for any site with a login option. There are many types of requests you can incorporate into your web scraping code, such as <i>POST</i> requests, which are used to submit forms. However, the code in this appendix will make only <i>GET</i> requests, the simplest and most common type of request, which download the content from a URL.</p>

<p class="TX">Many sites don’t like web scrapers for a variety of reasons, including the fact that if a script is hammering a site with HTTP requests, this increases the site’s bandwidth costs and could even cause it to crash. Sometimes sites will add roadblocks, such as limiting the number of requests you can make in a short amount of time or requiring that the user (or bot) fill out a CAPTCHA, in an effort to hinder or prevent scraping.</p>
<blockquote>
<p class="NOTE"><span class="NoteHead"><samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp></span></p>
</blockquote>
<p class="NOTE-TXT"><i>Some time around 2002, when I was in high school, my friends and I decided to make a song lyrics website. Similar sites existed, but they were incomplete. I thought it would be simple to scrape the lyrics from those other sites and make a single site that had</i> <span class="note_Italic">all</span> <i>of the lyrics. I wrote a script to scrape thousands of lyrics from one particular site, but my script crashed while it was running. I realized it was because the source website had gone down. A few days later, the site came back online with a message: the owner was overjoyed to learn how much traffic the site was getting, but to keep up with it, they had to raise money to keep the site online. I felt bad about it, and we never ended up launching that lyrics site.</i></p>
</section>

<section epub:type="division" aria-labelledby="sec3">
<h2 class="H1" id="sec3"><span id="h-370"/><span role="doc-pagebreak" epub:type="pagebreak" id="pg_486" aria-label=" Page 486. "/><samp class="SANS_Futura_Std_Bold_B_11">Scraping Techniques</samp></h2>
<p class="TNI">This section describes three different techniques for web scraping, each introducing a different Python module. You’ll use a Python package called HTTPX to make HTTP requests, then use another called Beautiful Soup to help you select the data that you care about from a soup of messy HTML code. Finally, you’ll use a package called Selenium to write code that launches a web browser and controls what it does.</p>

<p class="TX">Web scraping requires a lot of trial and error as well as a thorough understanding of the layout of the website that you’re scraping data from. This appendix gives you just a few examples, not a comprehensive overview, but they should give you a head start on writing your own web scraping scripts in the future.</p>

<section epub:type="division" aria-labelledby="sec4">
<h3 class="H2" id="sec4"><span id="h-371"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Loading Pages with HTTPX</samp></h3>

<p class="TNI">HTTPX is a third-party Python package that lets you make your own HTTP requests with Python. In this section, you’ll learn how to use it to scrape the most recent posts from any given user on the far-right social media site Gab, which you read about in <span class="Xref"><a href="chapter1.xhtml">Chapters 1</a></span>, <span class="Xref"><a href="chapter12.xhtml">12</a></span>, and <span class="Xref"><a href="chapter13.xhtml">13</a></span>.</p>

<p class="TX">Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> module with pip by running <samp class="SANS_TheSansMonoCd_W7Bold_B_11">python3 -m pip install</samp> <samp class="SANS_TheSansMonoCd_W7Bold_B_11">httpx</samp>. After importing <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> into your code, you should be able to load a web page by running the <samp class="SANS_TheSansMonoCd_W7Bold_B_11">httpx.get()</samp> function and passing in a URL. This function returns a request object, and you can access the request’s content with <samp class="SANS_TheSansMonoCd_W5Regular_11">.content</samp> for binary data or <samp class="SANS_TheSansMonoCd_W5Regular_11">.text</samp> for text data. For example, Listing B-1 shows Python code to make an HTTP request to <i>https://<wbr/>example<wbr/>.com</i> and view its content.</p>

<pre id="pre-650"><code>&gt;&gt;&gt; <b>import </b><b>httpx</b>
&gt;&gt;&gt; <b>r = </b><b>httpx.get("https://example.com")</b>
&gt;&gt;&gt; <b>print(r.text)</b>
&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Example Domain&lt;/title&gt;
<var>--snip--</var>
&lt;/head&gt;

&lt;body&gt;
&lt;div&gt;
    &lt;h1&gt;Example Domain&lt;/h1&gt;
    &lt;p&gt;This domain is for use in illustrative examples in documents. You may use this
    domain in literature without prior coordination or asking for permission.&lt;/p&gt;
    &lt;p&gt;&lt;a href="https://www.iana.org/domains/example"&gt;More information...&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
<p class="CodeListingCaptionWide"><samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing B-1: Scraping the HTML from</samp> <samp class="SANS_Futura_Std_Book_11">https://example.com</samp></p>

<p class="TX"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_487" aria-label=" Page 487. "/>First, this code imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> module. It then calls the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp> function, passing in a URL as an argument, and saves the response in the variable <samp class="SANS_TheSansMonoCd_W5Regular_11">r</samp>. Finally, it displays the <samp class="SANS_TheSansMonoCd_W5Regular_11">r.text</samp> variable, which is all of the HTML code that makes up <i>https://<wbr/>example<wbr/>.com</i>. (If you’re loading a binary file, like an image, then you can get the binary data in the <samp class="SANS_TheSansMonoCd_W5Regular_11">r.content</samp> variable.) This simple <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp> function is often all you need to scrape entire databases of information from the web. The script I’ll show you in this section that scrapes posts from Gab relies on this function.</p>

<p class="TX">Since web scraping means writing code that loads URLs, your first step should be to determine which URLs you need to load. The easiest way to do this is to use the built-in developer tools in your web browser. You can open them in most browsers by pressing the F12 key. In both Firefox and Chrome, you can see the HTTP requests your browser is making, and what the responses look like, in the Network tab of the developer tools. For example, if you open your browser’s developer tools and load the profile page of a Gab user, you can see what HTTP requests it makes to gather that user’s most recent posts. Once you have that information, you can write a script that makes the same HTTP requests for you.</p>
<blockquote>
<p class="NOTE"><span class="NoteHead"><samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp></span></p>
</blockquote>
<p class="NOTE-TXT"><samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp><i>The developer tools built in to Firefox, Chrome, and other browsers are a great way to learn what data your web browser is sending back and forth on the websites you’re visiting, to see exactly how web pages are laid out, and more. For more about Firefox’s developer tools, see</i> <a href="https://firefox-source-docs.mozilla.org/devtools-user/index.html"><span class="note_LinkURL_Italic">https://firefox-source-docs.mozilla.org/devtools-user/index.html</span></a><i>;</i> <i>for Chrome, see</i> <a href="https://developer.chrome.com/docs/devtools"><span class="note_LinkURL_Italic">https://developer.chrome.com/docs/devtools</span></a><i>.</i></p>

<p class="TNI1">For example, the Gab page for Marjorie Taylor Greene, the US congressperson who’s also a Christian nationalist and QAnon conspiracy theorist, is located at <a href="https://gab.com/RealMarjorieGreene"><i>https://<wbr/>gab<wbr/>.com<wbr/>/RealMarjorieGreene</i></a>. In a web browser, load that URL and then open the developer tools. Refresh the page to get all of the HTTP requests to show up in the Network tab.</p>

<p class="TNI1">In the Network tab, you should see several HTTP requests listed on the left half of the developer tools panel. When you click a request, the right half of the panel displays information about it. The right half has its own tabs that you can switch through to see details like the request’s headers, cookies, and the body of the request and its response.</p>

<p class="TNI1">When I loaded this page and looked through my browser’s HTTP requests and their responses, I decided I was most interested in the following URLs:</p>

<p class="RunInPara"><a href="https://gab.com/api/v1/account_by_username/RealMarjorieGreene"><b><i>https://<wbr/>gab<wbr/>.com<wbr/>/api<wbr/>/v1<wbr/>/account<wbr/>_by<wbr/>_username<wbr/>/RealMarjorieGreene</i></b></a> The response to this request includes a JSON object containing information about Greene’s Gab profile, including her Gab account ID, 3155503.</p>

<p class="RunInPara"><a href="https://gab.com/api/v1/accounts/3155503/statuses?sort_by=newest"><b><i>https://<wbr/>gab<wbr/>.com<wbr/>/api<wbr/>/v1<wbr/>/accounts<wbr/>/3155503<wbr/>/statuses<wbr/>?sort<wbr/>_by<wbr/>=newest</i></b></a> The response to this request includes a JSON array of Greene’s most recent Gab posts. Her account ID is in the URL itself.</p>

<p class="TNI1">The first URL let me look up the Gab ID of any account, and the second URL let me look up the recent posts from an account, based on its Gab ID. <a href="#figB-1">Figure B-1</a> shows Firefox’s developer tools in action while loading this page.</p>
<span role="doc-pagebreak" epub:type="pagebreak" id="pg_488" aria-label=" Page 488. "/>
<figure class="IMG"><img class="img100" id="figB-1" src="Images/FigureB-1.png" alt="A screenshot of Firefox with Marjorie Taylor Greene’s Gab account loaded. The developer tools are open, the Network tab is selected, and a list of all of the HTTP requests from the browser is shown." width="695" height="464"/>
<figcaption><p class="CAP"><samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure B-1: Viewing the JSON response to a specific request in the Firefox developer tools Network tab</samp></p></figcaption>
</figure>
<p class="TNI1">As you can see, this response is in JSON format. I wanted to write a script that, given a Gab username, would download the latest posts from that user. In order to write it, I had to spend some time looking at the JSON in these responses to understand how it was structured and what information I was interested in. For example, since I wanted to start with a Gab username, my script would first need to load the URL <i>https://<wbr/>gab<wbr/>.com<wbr/>/api<wbr/>/v1<wbr/>/account<wbr/>_by<wbr/>_username<wbr/>/&lt;username&gt;</i>, replacing <i>&lt;username&gt;</i> with my target username. It would then need to parse the JSON it receives to extract this Gab user’s ID. Then, using that ID, it would need to load the URL <i>https://<wbr/>gab<wbr/>.com<wbr/>/api<wbr/>/v1<wbr/>/accounts<wbr/>/&lt;id&gt;<wbr/>/statuses<wbr/>?sort<wbr/>_by<wbr/>=newest</i>, replacing <i>&lt;id&gt;</i> with the Gab ID of the target account. Finally, it would need to parse that JSON response to display the latest Gab posts.</p>

<p class="TNI1">Based on this research, I wrote the following script to scrape the latest posts from any target Gab account. Here’s the code for this web scraping script, <i>httpx<wbr/>-example<wbr/>.py</i>:</p>

<pre id="pre-651"><code>import httpx
import click

@click.command()
@click.argument("gab_username") <span class="CodeAnnotationCode-1" aria-label="annotation1">❶</span>
def main(gab_username):
    """Download a user's posts from Gab"""
    
    # Get information about the user
    r = httpx.get(f"https://gab.com/api/v1/account_by_username/{gab_username}") <span class="CodeAnnotationCode-1" aria-label="annotation2">❷</span>
    user_info = r.json()<span role="doc-pagebreak" epub:type="pagebreak" id="pg_489" aria-label=" Page 489. "/>
    if "error" in user_info:
        print(user_info["error"])
        return

    # Display some user info
    click.echo(f"Display name: {user_info['display_name']}") 
    click.echo(f"{user_info['followers_count']:,} followers, {user_info['following_count']:,} following, {user_info['statuses_count']:,} posts") <span class="CodeAnnotationCode-1" aria-label="annotation3">❸</span>
    print()

    # Get this user's posts
    r = httpx.get(f"https://gab.com/api/v1/accounts/{user_info['id']}/statuses") <span class="CodeAnnotationCode-1" aria-label="annotation4">❹</span>
    posts = r.json()
    for post in posts:
        if post["reblog"]:
            print(f"repost @{post['reblog']['account']['username']}: {post['reblog']['created_at']}: {post['reblog']['content']}")
        else:
            print(f"{post['created_at']}: {post['content']}")

if __name__ == "__main__":
    main()</code></pre>
<p class="TNI1">This script first imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> module, since it will need that module to make HTTP requests. Like many Python scripts throughout this book, it uses the <samp class="SANS_TheSansMonoCd_W5Regular_11">click</samp> module to accept CLI arguments. In this case, it accepts an argument called <samp class="SANS_TheSansMonoCd_W5Regular_11">gab_username</samp>, the username of the target Gab user <span class="CodeAnnotation" aria-label="annotation1">❶</span>.</p>

<p class="TNI1">When the <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> function runs, it downloads information about the target user by calling the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp> function and passing in the URL <i>https://<wbr/>gab<wbr/>.com<wbr/>/api<wbr/>/v1<wbr/>/account<wbr/>_by<wbr/>_username<wbr/>/&lt;gab<wbr/>_username&gt;</i>, replacing <i>&lt;gab_username&gt;</i> with the value of the CLI argument and storing the result in the variable <samp class="SANS_TheSansMonoCd_W5Regular_11">r</samp> <span class="CodeAnnotation" aria-label="annotation2">❷</span>. As my browser’s developer tools made clear, the response should be a JSON object, so the script next calls <samp class="SANS_TheSansMonoCd_W5Regular_11">r.json()</samp> on it to make HTTPX convert it into a dictionary called <samp class="SANS_TheSansMonoCd_W5Regular_11">user_info</samp>. It then checks to see if <samp class="SANS_TheSansMonoCd_W5Regular_11">user_info</samp> has an <samp class="SANS_TheSansMonoCd_W5Regular_11">error</samp> key; if so, it displays the error message and quits early. If you try loading that URL with an invalid username, you’ll see the error message in the <samp class="SANS_TheSansMonoCd_W5Regular_11">error</samp> key: the string <samp class="SANS_TheSansMonoCd_W5Regular_11">Record not found</samp>.</p>

<p class="TNI1">Once the script has successfully retrieved information about a Gab user, it displays some of that information—the display name, number of followers, number of follows, and number of posts—in the terminal <span class="CodeAnnotation" aria-label="annotation3">❸</span>. The script then uses HTTPX to make another HTTP request, this time to load the user’s posts. Note that this URL includes <samp class="SANS_TheSansMonoCd_W5Regular_11">user_info['id']</samp>, which is the ID of the user discovered from the previous HTTP request <span class="CodeAnnotation" aria-label="annotation4">❹</span>. As before, it calls <samp class="SANS_TheSansMonoCd_W5Regular_11">r.json()</samp> to convert the JSON into a Python object, this time a list called <samp class="SANS_TheSansMonoCd_W5Regular_11">posts</samp>. In the following <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop, the script loops through the list of posts, displaying them one at a time.</p>

<p class="TNI1">You can find a complete copy of this code in the book’s GitHub repo at <a href="https://github.com/micahflee/hacks-leaks-and-revelations/blob/main/appendix-b/httpx-example.py"><i>https://<wbr/>github<wbr/>.com<wbr/>/micahflee<wbr/>/hacks<wbr/>-leaks<wbr/>-and<wbr/>-revelations<wbr/>/blob<wbr/>/main<wbr/>/appendix<wbr/>-b<wbr/>/httpx<wbr/>-example<wbr/>.py</i></a>.</p>

<p class="TNI1"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_490" aria-label=" Page 490. "/>At the time of writing, I could use this script to download the recent posts of any Gab user by including their username as an argument. For example, here’s what it looked like when I ran this script on the account of Andrew Torba, Gab’s founder and owner and the author of the book <i>Christian Nationalism</i>, whose Gab username is <samp class="SANS_TheSansMonoCd_W5Regular_11">a</samp>:</p>

<pre id="pre-652"><code>micah@trapdoor appendix-b % <b>python3 </b><b>httpx-example.py</b><b> a</b>
Display name: Andrew Torba
3,803,381 followers, 2,662 following, 67,317 posts

2022-12-07T04:56:56.989Z: Is it really so crazy to think that I care nothing
at all about a particular historical atrocity that happened on another
continent 80 years ago when there is a genocide of babies happening right
here, right now, today?
repost @ScipioAmericanus: 2022-12-07T04:50:37.560Z: Jews literally believe
that they can reject God because they're justified according to the flesh and
their own laws. Wicked stuff.
<var>--snip--</var></code></pre>
<p class="TNI1">The output shows Torba’s display name, statistics about his account, and several of his latest posts to Gab. As you can see, they’re on the fascist side. Torba has 3.8 million followers, because every Gab user automatically follows him when they create an account.</p>
<blockquote>
<p class="NOTE"><span class="NoteHead"><samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp></span></p>
</blockquote>
<p class="NOTE-TXT"><i>While 3.8 million followers sounds like a lot, most of those accounts aren’t active. In 2021, I analyzed hacked Gab data and discovered that of the roughly 4 million accounts, only 1.5 million of them had posted any content at all, only 400,000 had posted more than 10 times, and only 100,000 of those had posted anything recently. You can read my analysis at</i> <a href="https://theintercept.com/2021/03/15/gab-hack-donald-trump-parler-extremists/"><span class="note_LinkURL_Italic">https://theintercept.com/2021/03/15/gab-hack-donald-trump-parler-extremists/</span></a><i>.</i></p>

<p class="TNI1">Try running <i>httpx<wbr/>-example<wbr/>.py</i> on any Gab account you’d like. Unless Gab’s website has changed, this should download the recent posts from that user. However, it’s possible that by the time you run this script, the site may have changed so that the script doesn’t work anymore. This is the unfortunate nature of web scraping. Every script you write that scrapes the web relies on websites acting one specific way; if they don’t, your script might break. It’s often a simple matter to update a script so it works again, though. To do so, you’d need to use your browser’s developer tools to figure out how the website changed, and then update your script to match its new URLs and behavior—basically, repeat what you just did. In the worst case, if the website has changed a lot, you may need to rewrite your scraping script from scratch.</p>

<p class="TNI1">Using Python logic and HTTPX, you can also modify the script to get <i>all</i> of the posts for a given account, rather than just the recent ones. You could write a script that finds a target Gab user and downloads the list of accounts they follow. Or you can take a target Gab post and download a list of accounts that liked it. You’d just need to learn exactly which HTTP requests to make to get the information you’re interested in, and then have Python make those requests for you. Some of these tasks would be more <span role="doc-pagebreak" epub:type="pagebreak" id="pg_491" aria-label=" Page 491. "/>complicated than others—for example, to get the data you’re looking for, you may need to create a Gab account and have your scraper make requests while you’re logged in. The more web scraping scripts like these you write, the better at it you’ll get.</p>

<p class="TNI1">To learn more about using the HTTPX package, check out its documentation at <a href="https://www.python-httpx.org"><i>https://<wbr/>www<wbr/>.python<wbr/>-httpx<wbr/>.org</i></a>.</p>
</section>

<section epub:type="division" aria-labelledby="sec5">
<h3 class="H2" id="sec5"><span id="h-372"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Parsing HTML with Beautiful Soup</samp></h3>

<p class="BodyFirst">Scraping Gab was simple because the responses to the HTTP requests were in JSON format, but pulling specific information out of the HTML in a web page is more challenging. The easiest way to parse HTML in Python is to use a package aptly called Beautiful Soup (BS4 for short). Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">bs4</samp> module by running <samp class="SANS_TheSansMonoCd_W7Bold_B_11">python3 -m pip install bs4</samp>.</p>

<p class="TNI1">For example, here’s some code that uses the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> module to download the HTML from <i>https://<wbr/>example<wbr/>.com</i>, like you did in the last section:</p>

<pre id="pre-653"><code>&gt;&gt;&gt; <b>import </b><b>httpx</b>
&gt;&gt;&gt; <b>from bs4 import BeautifulSoup</b>
&gt;&gt;&gt; <b>r = </b><b>httpx.get("https://example.com")</b></code></pre>
<p class="TNI1">This code imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> module, then imports Beautiful Soup from the <samp class="SANS_TheSansMonoCd_W5Regular_11">bs4</samp> module. Next, it uses <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp> to make an HTTP request to <i>https://<wbr/>example<wbr/>.com</i> and stores the result in <samp class="SANS_TheSansMonoCd_W5Regular_11">r</samp>, allowing you to access the HTML string itself using the <samp class="SANS_TheSansMonoCd_W5Regular_11">r.text</samp> variable. As you saw in Listing B-1, this HTTP response is in HTML format and includes the page’s title inside the <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;title&gt;</samp> tag, as well as two paragraphs of text within <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;p</samp><samp class="SANS_TheSansMonoCd_W5Regular_11">&gt;</samp> tags inside the <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;body&gt;</samp> tag.</p>

<p class="TNI1">Using BS4, you can parse this HTML to select specific pieces of content—in this case, the page title and the content of the first paragraph:</p>

<pre id="pre-654"><code>&gt;&gt;&gt; <b>soup = BeautifulSoup(r.text, "html.parser")</b>
&gt;&gt;&gt; <b>print(soup.title.text)</b>
Example Domain
&gt;&gt;&gt; <b>paragraph = soup.find("p")</b>
&gt;&gt;&gt; <b>print(paragraph.text)</b>
This domain is for use in illustrative examples in documents. You may use this
    domain in literature without prior coordination or asking for permission.
&gt;&gt;&gt; <b>for link in soup.find_all("a"):</b>
...      <b>print(link.get("href"))</b>
...
https://www.iana.org/domains/example</code></pre>
<p class="TNI1">This code parses the HTML string (<samp class="SANS_TheSansMonoCd_W5Regular_11">r.text</samp>) using BS4, storing the resulting <samp class="SANS_TheSansMonoCd_W5Regular_11">BeautifulSoup</samp> object in the <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp> variable defined in the first line of code. This allows you to use <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp> to extract whatever information you’re interested in from the HTML. The code then displays the page title by printing the value of <samp class="SANS_TheSansMonoCd_W5Regular_11">soup.title.text</samp>.</p>

<p class="TNI1">Next, the script searches for the first paragraph on the HTML page and displays its text by printing the value of <samp class="SANS_TheSansMonoCd_W5Regular_11">paragraph.text</samp>. Finally, it finds all of the links on the page (which are <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;a&gt;</samp> tags), loops through them in a <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> <span role="doc-pagebreak" epub:type="pagebreak" id="pg_492" aria-label=" Page 492. "/>loop, and prints the URL for each link (the URL is defined in the <samp class="SANS_TheSansMonoCd_W5Regular_11">href</samp> attribute of <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;a&gt;</samp> tags). The <i>https://<wbr/>example<wbr/>.com</i> web page has only one link, so the code displays just that.</p>

<p class="TNI1">For practice, next we’ll explore a script that scrapes content from Hacker News (<a href="https://news.ycombinator.com"><i>https://<wbr/>news<wbr/>.ycombinator<wbr/>.com</i></a>), a news aggregator site about tech startups and computer science. Hacker News is similar to Reddit in that anyone can post links, and users then upvote and downvote those links, with the most popular ones rising to the top. Its web design is simple and has remained the same for many years, making it a good choice for web scraping practice.</p>

<p class="TNI1">Your practice script will download the title and URL from the first five pages of popular links. The front page of Hacker News displays the 30 most popular recent posts. If you scroll to the bottom and click More, you’ll see the second page of results, showing the next 30 most popular recent posts, at the <a href="https://news.ycombinator.com/?p=2"><i>https://news.ycombinator.com/?p</i><span class="symbol_LinkURL_Italic">=</span><i>2</i></a> URL. Likewise, the third page of results has the URL <a href="https://news.ycombinator.com/?p=3"><i>https://news.ycombinator.com/?p</i><span class="symbol_LinkURL_Italic">=</span><i>3</i></a>, and so on.</p>

<p class="TNI1"><a href="#figB-2">Figure B-2</a> shows a Firefox window with Hacker News loaded and the developer tools open. This time, I’ve switched to the Inspector tab, which allows you to inspect how the HTML of the page is laid out. The Inspector tab shows all of the HTML tags that make up the page, and when you mouse over an individual tag, your browser highlights the corresponding design element on the web page. In this example, I moused over an <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;a&gt;</samp> tag, and the browser highlighted that element.</p>
<figure class="IMG"><img class="img100" id="figB-2" src="Images/FigureB-2.png" alt="A screenshot of Firefox, with Hacker News loaded and the developer tools open. The Inspector tab is selected in developer tools, showing the HTML that makes up the post." width="695" height="559"/>
<figcaption><p class="CAP"><samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure B-2: Using Firefox’s developer tools to inspect the HTML that makes up a Hacker News post</samp></p></figcaption>
</figure>
<p class="TNI1"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_493" aria-label=" Page 493. "/>The developer tools show that all posts in the Hacker News site are laid out in an HTML table. In HTML, tables are defined within <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;table&gt;</samp> tags. Each row is a <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;tr&gt;</samp> tag, and each cell within it has a <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;td&gt;</samp> tag. Here’s the HTML code from a typical Hacker News post:</p>

<pre id="pre-655"><code>&lt;tr <b>class="athing"</b> id="34466985"&gt;
  &lt;td class="title" valign="top" align="right"&gt;&lt;span class="rank"&gt;4.&lt;/span&gt;&lt;/td&gt;
  &lt;td class="votelinks" valign="top"&gt;
    &lt;center&gt;
      &lt;a id="up_34466985" href="vote?id=34466985&amp;amp;how=up&amp;amp;goto=news"&gt;
      &lt;div class="votearrow" title="upvote"&gt;&lt;/div&gt;&lt;/a&gt;
    &lt;/center&gt;
  &lt;/td&gt;
  &lt;td class="title"&gt;
    &lt;span class="titleline"&gt;
      &lt;a href="https://people.ece.cornell.edu/land/courses/ece4760/RP2040/C_SDK_DMA_machine/DMA_machine_rp2040.html"&gt;
        Direct Memory Access computing machine RP2040
      &lt;/a&gt;
      &lt;span class="sitebit comhead"&gt; (&lt;a href="from?site=cornell.edu"&gt;
        &lt;span class="sitestr"&gt;cornell.edu&lt;/span&gt;&lt;/a&gt;)
      &lt;/span&gt;
    &lt;/span&gt;
  &lt;/td&gt;
&lt;/tr&gt;</code></pre>
<p class="TNI1">The rows with <samp class="SANS_TheSansMonoCd_W5Regular_11">class="athing"</samp>, or the value of the attribute <samp class="SANS_TheSansMonoCd_W5Regular_11">class</samp> set to <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>, contain links that users have posted. Inside each <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp> row, there are three cells (that is, three <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;</samp><samp class="SANS_TheSansMonoCd_W5Regular_11">td&gt;</samp> tags). The last of these cells contains the actual link, the <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;a&gt;</samp> tag.</p>

<p class="TNI1">The following script, <i>bs4-example.py</i>, scrapes the titles and URLs of the first five pages of the most popular posts recently posted on Hacker News, saving them in a CSV spreadsheet and also displaying them to the terminal:</p>

<pre id="pre-656"><code>import csv
import time
import httpx
from bs4 import BeautifulSoup

def main():
    with open("output.csv", "w") as f:
        writer = csv.DictWriter(f, fieldnames=["Title", "URL"])
        writer.writeheader()

        for page_number in range(1, 6):
            print(f"LOADING PAGE {page_number}")
            r = httpx.get(f"https://news.ycombinator.com/?p={page_number}")
            print("")

            soup = BeautifulSoup(r.text, "html.parser")
            for table_row in soup.find_all("tr", class_="athing"):
                table_cells = table_row.find_all("td")
                last_cell = table_cells[-1]<span role="doc-pagebreak" epub:type="pagebreak" id="pg_494" aria-label=" Page 494. "/>
                link = last_cell.find("a")
                link_url = link.get("href")

                print(link.text)
                print(link_url)
                print("")

                writer.writerow({"Title": link.text, "URL": link_url})

            time.sleep(1)
if __name__ == "__main__":
    main()</code></pre>
<p class="TNI1">First, the script imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">csv</samp>, <samp class="SANS_TheSansMonoCd_W5Regular_11">time</samp>, <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp>, and <samp class="SANS_TheSansMonoCd_W5Regular_11">bs4</samp> modules. In the <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> function, it opens a new CSV for writing called <i>output.csv</i>, creates a <samp class="SANS_TheSansMonoCd_W5Regular_11">csv.DictWriter()</samp> object, and uses that object to write the CSV headers (<samp class="SANS_TheSansMonoCd_W5Regular_11">Title</samp> and <samp class="SANS_TheSansMonoCd_W5Regular_11">URL</samp>, in this case), as you learned in <span class="Xref"><a href="chapter9.xhtml">Chapter 9</a></span>.</p>

<p class="TNI1">The following <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop loops through the results of <samp class="SANS_TheSansMonoCd_W5Regular_11">range(1, 6)</samp>, saving each item as <samp class="SANS_TheSansMonoCd_W5Regular_11">page</samp>. The <samp class="SANS_TheSansMonoCd_W5Regular_11">range()</samp> function is useful for looping through a list of numbers; in this case, it starts with 1, then 2, and so on until it hits 6 and then stops, meaning it returns the numbers 1 through 5. The code displays the page number that it’s about to load, then makes the HTTP request to load that page using <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp>, creating a different URL for the current page on each loop. After making each HTTP request that gets a page of results, the code parses all of the HTML from that page using BS4, storing it as <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp>.</p>

<p class="TNI1">Now things get slightly trickier. As noted earlier, all of the HTML table rows that have the class <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp> contain links that users posted. The script gets a list of all of these rows by calling <samp class="SANS_TheSansMonoCd_W5Regular_11">soup.find_all("tr", class_="athing")</samp>. The <samp class="SANS_TheSansMonoCd_W5Regular_11">find_all()</samp> method searches the BS4 object <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp> for all instances of the HTML tag <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;tr&gt;</samp> and returns a list of matches. In this case, the code also includes <samp class="SANS_TheSansMonoCd_W5Regular_11">class_="athing"</samp>, which tells BS4 to include only tags that have the <samp class="SANS_TheSansMonoCd_W5Regular_11">class</samp> attribute set to <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>. The <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop loops through them, saving each item in the <samp class="SANS_TheSansMonoCd_W5Regular_11">table_row</samp> variable.</p>

<p class="TNI1">Now that the code is looping through each table row that contains a link posted by a user, it goes on to find that link tag. There are several links in each table row, so it figures out which one is the link a user posted. First, it calls <samp class="SANS_TheSansMonoCd_W5Regular_11">table_row.find_all("td")</samp> to get a list of all of the table cells inside <samp class="SANS_TheSansMonoCd_W5Regular_11">table_row</samp>, storing that list in <samp class="SANS_TheSansMonoCd_W5Regular_11">table_cells</samp>. As noted earlier, the last cell contains the link that we care about. Therefore, the code pulls out just the last cell in this list, storing it in the variable <samp class="SANS_TheSansMonoCd_W5Regular_11">last_cell</samp> (the <samp class="SANS_TheSansMonoCd_W5Regular_11">−1</samp> index is the last item in a list). The code searches just <samp class="SANS_TheSansMonoCd_W5Regular_11">last_cell</samp> for the link it contains (the <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;a&gt;</samp> tag), and uses <samp class="SANS_TheSansMonoCd_W5Regular_11">print()</samp> to display the link’s title and URL. Finally, it calls <samp class="SANS_TheSansMonoCd_W5Regular_11">writer.writerow()</samp> to also save this row into the CSV.</p>

<p class="TNI1">The code does this once for each of the page’s 30 rows. It then waits one second, using <samp class="SANS_TheSansMonoCd_W5Regular_11">time.sleep(1)</samp>, and moves on to the next page, until it has extracted all the links from the first five pages. When the script is finished running, it creates a file called <i>output.csv</i> that should contain the 150 most recent popular links posted to Hacker News. Most of the time when you’re <span role="doc-pagebreak" epub:type="pagebreak" id="pg_495" aria-label=" Page 495. "/>scraping real data for an investigation, you’ll save it to a CSV spreadsheet, like this script did, or to a SQL database (as discussed in <span class="Xref"><a href="chapter12.xhtml">Chapter 12</a></span>) so that you can work with it later.</p>
<blockquote>
<p class="NOTE"><span class="NoteHead"><samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp></span></p>
</blockquote>
<p class="NOTE-TXT"><i>The <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">find_all()</samp> method in this code passes an argument called <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">class_</samp> instead of <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">class</samp>. This is because <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">class</samp> is a Python keyword and can’t be used as a variable name. If you want to use <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">find_all()</samp> to select tags using any other attribute, then the argument name will be the same as the attribute name. For example, <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">soup.find _all("a", href="https://<wbr/>example<wbr/>.com")</samp></i> <i>will find all link tags in <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">soup</samp> that have an href attribute set to</i> <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">https://<wbr/>example<wbr/>.com<wbr/>.</samp></p>

<p class="TNI1">You can also find a copy of this code in the book’s GitHub repo at <a href="https://github.com/micahflee/hacks-leaks-and-revelations/blob/main/appendix-b/bs4-example.py"><i>https://<wbr/>github<wbr/>.com<wbr/>/micahflee<wbr/>/hacks<wbr/>-leaks<wbr/>-and<wbr/>-revelations<wbr/>/blob<wbr/>/main<wbr/>/appendix<wbr/>-b<wbr/>/bs4<wbr/>-example<wbr/>.py</i></a>.</p>

<p class="TNI1">Here’s what it looked like when I ran this script:</p>

<pre id="pre-657"><code>micah@trapdoor appendix-b % <b>python3 bs4-example.py</b>
LOADING PAGE 1

Buy Hi-Resolution Satellite Images of Any Place on Earth
https://www.skyfi.com/pricing

The McMurdo Webcams
https://www.usap.gov/videoclipsandmaps/mcmwebcam.cfm

<var>--snip--</var>
LOADING PAGE 2

Thoughts on the Python Packaging Ecosystem
https://pradyunsg.me/blog/2023/01/21/thoughts-on-python-packaging/
<var>--snip--</var></code></pre>
<p class="TNI1">Try running it yourself now. Assuming Hacker News hasn’t updated its web design, it should work fine; however, the URLs will differ because the most popular recent links on Hacker News are constantly changing.</p>

<p class="TNI1">This script scrapes only the first five pages of content on Hacker News. In theory, you could scrape <i>all</i> the content on the site since its founding in 2007. To do so, you’d have to modify the script to stop not after page 5 but when it gets to the very last page, presumably one that doesn’t have any links on it. This assumes that the site will actually show you content that old and that you could make those millions of HTTP requests without it blocking your IP address. I don’t know if this is true or not with Hacker News—I haven’t attempted to scrape everything from this site myself.</p>

<p class="TNI1">I mentioned in the <span class="Xref">“HTTP Requests”</span> section that some websites add roadblocks to make scraping more difficult, and this turned out to be true with Hacker News. When I first wrote this script, it didn’t include the <samp class="SANS_TheSansMonoCd_W5Regular_11">time .sleep(1)</samp> code, which waits one second between each HTTP request. I found that Hacker News limits how quickly you can make HTTP requests, and the fifth request in quick succession responded with an HTML page with the error message <samp class="SANS_TheSansMonoCd_W5Regular_11">Sorry, we're not able to serve your requests this quickly</samp>. I <span role="doc-pagebreak" epub:type="pagebreak" id="pg_496" aria-label=" Page 496. "/>solved this problem by waiting one second between HTTP requests. It’s common to run into hurdles like this while you’re writing scrapers, but it’s also often a simple matter of modifying your script like this to get around these roadblocks.</p>
<aside class="box" aria-labelledby="box-1">
<h4 class="BH" id="box-1"><samp class="SANS_Dogma_OT_Bold_B_11">BS4 WEB SCRAPING FOR TRAVEL</samp></h4>
<p class="BoxBodyFirst"><samp class="SANS_Futura_Std_Book_11">In 2014, shortly after</samp> <samp class="SANS_Futura_Std_Book_11">Edward Snowden leaked his massive dataset of top-secret NSA documents to Laura Poitras and Glenn Greenwald, I scheduled a trip to Rio de Janeiro, where Greenwald lived, to help him with computer security and to look through the Snowden docs myself. (This was several years before Greenwald unfortunately became a far-right pundit who openly supports the American fascist movement.) At the time, Americans could travel to Brazil only if they had a visa, and they needed to visit a consulate in person to get one. However, the San Francisco consulate’s website told me that all appointments were booked for the next two months.</samp></p>

<p class="BoxBodyLast"><samp class="SANS_Futura_Std_Book_11">To solve this problem, I wrote a simple Python script that scraped the consulate’s availability calendar web page, using BS4 to loop through each cell in the calendar and see if any appointment slots were open. If it found an opening, my script would send me a text message. I then configured a VPS to run this script every 10 minutes so I’d be the first to know if someone canceled their appointment. In less than two days, I got a text, snagged an appointment, and got my visa. I flew to Rio a few days later.</samp></p>
</aside>
<p class="TNI1">To learn more about using the BS4 package, check out its documentation at <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/"><i>https://<wbr/>www<wbr/>.crummy<wbr/>.com<wbr/>/software<wbr/>/BeautifulSoup<wbr/>/bs4<wbr/>/doc<wbr/>/</i></a>.</p>
</section>

<section epub:type="division" aria-labelledby="sec6">
<h3 class="H2" id="sec6"><span id="h-373"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Automating Web Browsers with Selenium</samp></h3>

<p class="BodyFirst">Sometimes scraping websites is too challenging for Beautiful Soup alone. This is often the case with sites that are JavaScript-heavy, where viewing the HTML source doesn’t result in much information you’re interested in. This is true for sites like Facebook, YouTube, and Google Maps. It’s much simpler to get information from this sort of site by using a web browser than by untangling the complicated web of HTTP requests that you’d need to make to get the same information. Some websites also put up barriers to scraping. They might add JavaScript code that ensures visitors are using real web browsers before showing them content, preventing users from just making HTTP requests using cURL (discussed in <span class="Xref"><a href="chapter4.xhtml">Chapter 4</a></span>) or a Python package like HTTPX.</p>

<p class="TNI1">You can control a real web browser for scraping purposes by using software called Selenium. Scripts that just make HTTP requests are more efficient and run much quicker than using Selenium because they don’t require running a whole web browser and downloading all of the resources <span role="doc-pagebreak" epub:type="pagebreak" id="pg_497" aria-label=" Page 497. "/>of the target website. When I’m writing a scraper, I generally start by attempting to scrape the site using HTTPX, but if this technique turns out to be too complicated, I switch to Selenium.</p>

<p class="TNI1">To use the Selenium Python package, you must also install a <i>web driver</i>, software that Selenium uses to control a web browser. Selenium supports Chrome, Firefox, Safari, and Edge. The example in this section uses the Firefox driver, which is called geckodriver.</p>

<p class="TNI1">To continue, follow the instructions for your operating system, then skip to the <span class="Xref">“Testing Selenium in the Python Interpreter”</span> section.</p>

<section epub:type="division" aria-labelledby="sec7">
<h4 class="H3" id="sec7"><samp class="SANS_Futura_Std_Bold_Condensed_B_11">Installing Selenium and geckodriver on Windows</samp></h4>
<p class="BodyFirst">For this task, Windows users should work with native Windows tools rather than WSL. Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp> Python module by opening PowerShell and running the following command:</p>

<pre id="pre-658"><code><b>python -m pip install selenium</b></code></pre>
<p class="BodyContinued">Also make sure you have Firefox installed (see <a href="https://www.mozilla.org/en-US/firefox/new/"><i>https://<wbr/>www<wbr/>.mozilla<wbr/>.org<wbr/>/en<wbr/>-US<wbr/>/firefox<wbr/>/new<wbr/>/</i></a>).</p>

<p class="TNI1">To install geckodriver, go to <i><a href="https://github.com/mozilla/geckodriver/releases">https://<wbr/>github<wbr/>.com<wbr/>/mozilla<wbr/>/geckodriver<wbr/>/releases<wbr/></a>.</i> You’ll see several ZIP files for the latest version of geckodriver that you can download. Download the appropriate Windows version and unzip it. You should end up with a single file called <i>geckodriver.exe</i>. In File Explorer, copy this file and paste it into <i>C:\Windows\System32</i>. This will allow you to run geckodriver from PowerShell no matter what your working directory is.</p>
</section>

<section epub:type="division" aria-labelledby="sec8">
<h4 class="H3" id="sec8"><samp class="SANS_Futura_Std_Bold_Condensed_B_11">Installing Selenium and geckodriver on macOS</samp></h4>
<p class="BodyFirst">If you’re using macOS, open a terminal. Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp> Python module by running the following:</p>

<pre id="pre-659"><code><b>python3 -m pip install selenium</b></code></pre>
<p class="BodyContinued">Then install geckodriver by running the following:</p>

<pre id="pre-660"><code><b>brew install geckodriver</b></code></pre>
<p class="BodyContinued">This should give you everything you need to use Selenium in Python.</p>
</section>

<section epub:type="division" aria-labelledby="sec9">
<h4 class="H3" id="sec9"><samp class="SANS_Futura_Std_Bold_Condensed_B_11">Installing Selenium and geckodriver on Linux</samp></h4>
<p class="BodyFirst">If you’re using Linux, open a terminal. Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp> Python module by running the following:</p>

<pre id="pre-661"><code><b>python3 -m pip install selenium</b></code></pre>
<p class="BodyContinued"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_498" aria-label=" Page 498. "/>Install geckodriver by running the following:</p>

<pre id="pre-662"><code><b>sudo apt update</b>
<b>sudo apt install firefox-geckodriver</b></code></pre>
<p class="BodyContinued">This should give you everything you need to use Selenium in Python.</p>
</section>

<section epub:type="division" aria-labelledby="sec10">
<h4 class="H3" id="sec10"><samp class="SANS_Futura_Std_Bold_Condensed_B_11">Testing Selenium in the Python Interpreter</samp></h4>
<p class="BodyFirst">Now that you have Selenium and geckodriver installed, test them out in the Python interpreter by loading this book’s git repo website on GitHub to get a feel for how Selenium allows you to control a web browser:</p>

<pre id="pre-663"><code>&gt;&gt;&gt; <b>from selenium import webdriver</b>
&gt;&gt;&gt; <b>driver = webdriver.Firefox()</b>
&gt;&gt;&gt; <b>driver.get("</b><b>https://github.com/micahflee/hacks-leaks-and-revelations")</b>
&gt;&gt;&gt; <b>print(driver.title)</b>
GitHub - micahflee/hacks-leaks-and-revelations: Code that goes along with the Hacks, Leaks, and Revelations book
&gt;&gt;&gt; <b>driver.quit()</b></code></pre>
<p class="TNI1">This code first imports <samp class="SANS_TheSansMonoCd_W5Regular_11">webdriver</samp> from the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp> module. It then creates a new Firefox driver by calling <samp class="SANS_TheSansMonoCd_W5Regular_11">webdriver.Firefox()</samp> and saves it in the variable <samp class="SANS_TheSansMonoCd_W5Regular_11">driver</samp>. When you create the Selenium driver, a new Firefox window should open on your computer, and a robot icon should appear in the address bar—this is how you know that this browser is being controlled by Selenium.</p>

<p class="TNI1">The code then instructs the browser to load the URL <a href="https://github.com/micahflee/hacks-leaks-and-revelations"><i>https://<wbr/>github<wbr/>.com<wbr/>/micahflee<wbr/>/hacks<wbr/>-leaks<wbr/>-and<wbr/>-revelations</i></a>. After running the command, you should see Firefox load that GitHub page. Once the page is loaded, including all of its JavaScript or other complicated components, you can write code to control it. In this case, the code just displays the title of the page in the terminal with <samp class="SANS_TheSansMonoCd_W5Regular_11">print(driver.title)</samp>. Finally, it quits Firefox.</p>
</section>

<section epub:type="division" aria-labelledby="sec11">
<h4 class="H3" id="sec11"><samp class="SANS_Futura_Std_Bold_Condensed_B_11">Automating Screenshots with Selenium</samp></h4>
<p class="BodyFirst">Now let’s try something slightly more complicated. In this section, we’ll go over a script that will take two arguments: a location name and the filename of a screenshot to save. Using Selenium, the script will load Google Maps at <a href="https://maps.google.com"><i>https://<wbr/>maps<wbr/>.google<wbr/>.com</i></a>, search for the location, zoom in a little, turn on the satellite images layer, and take a screenshot of the satellite image of the location, saving it to disk.</p>

<p class="TNI1">While I’m programming web scrapers, I find it helpful to have an interactive Python interpreter open in a terminal where I can test out Selenium or BS4 commands, allowing me to see if they work in real time without having to start my script over. When I’m writing a Selenium script, I open developer tools inside the browser I’m driving to inspect all of the HTML tags, which helps me figure out which commands to run. Once I get something working, I copy the working code into the script that I’m writing in my text editor.</p>

<p class="TNI1">For example, to search for the location in Google Maps, I needed to make the Selenium browser select the search box, type the location, and <span role="doc-pagebreak" epub:type="pagebreak" id="pg_499" aria-label=" Page 499. "/>press <small>ENTER</small>. In HTML, tags often have <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp> attributes. By using the Firefox developer tools, I discovered that the search box in Google Maps, which is an <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;input&gt;</samp> tag, includes the <samp class="SANS_TheSansMonoCd_W5Regular_11">id="searchboxinput"</samp> attribute, meaning the search box has an <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp> of <samp class="SANS_TheSansMonoCd_W5Regular_11">searchboxinput</samp>. That allowed me to enter code into the Python interpreter that would select the search box, type a search query into it, and press <small>ENTER</small> in the browser it was controlling. I didn’t always get it right on the first try, but after some trial and error, I wrote some working code. At this point, I added that code to my script.</p>

<p class="TNI1">I also used developer tools to figure out how to turn on the satellite image layer. In the bottom-left corner of Google Maps is an icon called the <i>minimap</i> that lets you toggle different layers on and off. The developer tools showed me that this icon had an <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp> of <samp class="SANS_TheSansMonoCd_W5Regular_11">minimap</samp> and that I could click one of the buttons in the minimap element to turn on the satellite layer; just like with the search box, I tested clicking this icon in the Python interpreter until I got it working.</p>

<p class="TNI1">The following script, <i>selenium-example.py,</i> uses Selenium to take satellite image screenshots from Google Maps for you:</p>

<pre id="pre-664"><code>import click
import time
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By

@click.command()
@click.argument("location")
@click.argument("screenshot_filename", type=click.Path(exists=False))
def main(location, screenshot_filename):
    driver = webdriver.Firefox()
    driver.implicitly_wait(10)

    driver.get("https://maps.google.com")
    search_box = driver.find_element(By.ID, "searchboxinput")
    search_box.clear()
    search_box.send_keys(location)
    search_box.send_keys(Keys.RETURN)

    body = driver.find_element(By.TAG_NAME, "body")
    body.send_keys(Keys.ADD)
    body.send_keys(Keys.ADD)

    minimap = driver.find_element(By.ID, "minimap")
    buttons = minimap.find_elements(By.TAG_NAME, "button")
    buttons[2].click()

    time.sleep(2)
    driver.save_screenshot(screenshot_filename)
    driver.quit()

if __name__ == "__main__":
    main()</code></pre>
<p class="TNI1"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_500" aria-label=" Page 500. "/>First, the script imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">click</samp> and <samp class="SANS_TheSansMonoCd_W5Regular_11">time</samp> modules and then several components from the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp> module. Specifically, it imports <samp class="SANS_TheSansMonoCd_W5Regular_11">webdriver</samp>, the component required to actually launch and control a web browser. It also imports <samp class="SANS_TheSansMonoCd_W5Regular_11">Keys</samp> and <samp class="SANS_TheSansMonoCd_W5Regular_11">By</samp> to automate pressing <small>ENTER</small> after searching and to search for HTML elements by their <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp> attribute.</p>
<blockquote>
<p class="NOTE"><span class="NoteHead"><samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp></span></p>
</blockquote>
<p class="NOTE-TXT"><i>Exactly what you need to import from <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">selenium</samp> depends on what you’re trying to do. Consult the Selenium for Python documentation to learn exactly what you need and when—that’s how I figured it out.</i></p>

<p class="TNI1">The code includes Click decorators before the <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> function, making this a command line program that takes two arguments, <samp class="SANS_TheSansMonoCd_W5Regular_11">location</samp> and <samp class="SANS_TheSansMonoCd_W5Regular_11">screenshot_filename</samp>. The <samp class="SANS_TheSansMonoCd_W5Regular_11">location</samp> variable is a Google Maps search query, like <i>Manhattan, NY</i> or <i>The Great Pyramid of Giza</i>, and <samp class="SANS_TheSansMonoCd_W5Regular_11">screenshot_filename</samp> is the path to save the final screenshot.</p>

<p class="TNI1">When the <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> function runs, the code starts by creating a Selenium web driver, which should open a Firefox window that the script will then control. The <samp class="SANS_TheSansMonoCd_W5Regular_11">driver.implicitly_wait(10)</samp> function tells Selenium to wait up to 10 seconds for page elements to load. The code loads <a href="https://maps.google.com"><i>https://<wbr/>maps<wbr/>.google<wbr/>.com</i></a> in Firefox with the <samp class="SANS_TheSansMonoCd_W5Regular_11">driver.get()</samp> function, then finds the search box element on the page, storing it in the variable <samp class="SANS_TheSansMonoCd_W5Regular_11">search_box</samp>. It finds the search box by running <samp class="SANS_TheSansMonoCd_W5Regular_11">driver.find_element(By.ID, "searchboxinput")</samp>. Once the code has this search box object stored in <samp class="SANS_TheSansMonoCd_W5Regular_11">search_box</samp>, it clears any text in the text box by calling the <samp class="SANS_TheSansMonoCd_W5Regular_11">clear()</samp> method on it, and then it types the text in the <samp class="SANS_TheSansMonoCd_W5Regular_11">location</samp> string by calling <samp class="SANS_TheSansMonoCd_W5Regular_11">send_keys(location)</samp>. Finally, it presses <small>ENTER</small> to search for this location by calling <samp class="SANS_TheSansMonoCd_W5Regular_11">send_keys(Keys.RETURN)</samp>. At this point, Google Maps should search for the location.</p>

<p class="TNI1">The code then zooms in by selecting the <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;body&gt;</samp> tag, the main HTML tag that contains all other tags, then telling Firefox to press the + key twice, which is the Google Maps keyboard shortcut to zoom in.</p>

<p class="TNI1">At this point, Firefox has loaded Google Maps, searched for a location, and zoomed in on that location. The code then turns on the satellite image layer by locating the minimap in the corner of the screen. Once it finds this, it locates all of the <samp class="SANS_TheSansMonoCd_W5Regular_11">&lt;button&gt;</samp> tags inside the minimap by calling the <samp class="SANS_TheSansMonoCd_W5Regular_11">find_elements(By.TAG_NAME, "button")</samp> method, and then it clicks the third button, calling the <samp class="SANS_TheSansMonoCd_W5Regular_11">click()</samp> method on the third element (which has an index of <samp class="SANS_TheSansMonoCd_W5Regular_11">2</samp>) on the list of buttons. This turns on the satellite images layer.</p>

<p class="TNI1">Finally, the script waits two seconds, just to make sure the satellite images have finished loading, and then saves a screenshot of the web page to <samp class="SANS_TheSansMonoCd_W5Regular_11">screenshot_filename</samp>. When it’s done, it quits Firefox.</p>

<p class="TNI1">You can find a complete copy of this code in the book’s GitHub repo at <a href="https://github.com/micahflee/hacks-leaks-and-revelations/blob/main/appendix-b/selenium-example.py"><i>https://<wbr/>github<wbr/>.com<wbr/>/micahflee<wbr/>/hacks<wbr/>-leaks<wbr/>-and<wbr/>-revelations<wbr/>/blob<wbr/>/main<wbr/>/appendix<wbr/>-b<wbr/>/selenium<wbr/>-example<wbr/>.py</i></a>.</p>

<p class="TNI1">You can use <i>selenium-example.py</i> to generate Google Maps screenshots of any location you like. For example, I ran the following command:</p>

<pre id="pre-665"><code><b>python3 selenium-example.py "great pyramid of giza" giza.png</b></code></pre>
<p class="TNI1"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_501" aria-label=" Page 501. "/>This opened a Firefox window that was controlled by Selenium. It loaded Google Maps, searched for <i>great pyramid of giza</i>, zoomed in, turned on the satellite images layer, and saved a screenshot of the window in the file <i>giza.png</i>. <a href="#figB-3">Figure B-3</a> shows <i>giza.png</i>, scraped from Google Maps.</p>
<figure class="IMG"><img class="img100" id="figB-3" src="Images/FigureB-3.jpg" alt="A screenshot that Selenium created showing a satellite image of the Great Pyramid of Giza. The Google Maps interface is displayed on the left, including a photo of the pyramid, information reviews, its address, and so on." width="696" height="535"/>
<figcaption><p class="CAP"><samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure B-3: A satellite image of the Great Pyramid of Giza from Selenium</samp></p></figcaption>
</figure>
<p class="TNI1">On your own, it might also be fun to try searching for <i>US Capitol</i>; <i>Washington, DC</i>; <i>Kremlin, Moscow</i>; or <i>Tokyo, Japan</i>.</p>

<p class="TNI1">This example script used Selenium to take screenshots. You could modify it so that Selenium automatically takes a screenshot each time a public figure posts to social media, so you’ll have a record of it in case they delete it. You’re not limited to cataloging information in this way, though; you can also use Selenium to extract information from web pages and store them in CSV spreadsheets or any other format you’d like, just like you can with BS4.</p>

<p class="TNI1">To learn more about Selenium for Python, check out its documentation at <a href="https://selenium-python.readthedocs.io"><i>https://<wbr/>selenium<wbr/>-python<wbr/>.readthedocs<wbr/>.io</i></a>.</p>
</section>

</section>

</section>

<section epub:type="division" aria-labelledby="sec12">
<h2 class="H1" id="sec12"><span id="h-374"/><samp class="SANS_Futura_Std_Bold_B_11">Next Steps</samp></h2>
<p class="BodyFirst">In this appendix, I’ve gone over a few techniques for web scraping and provided some simple example scripts to show off the basics of how they work. However, in order to write code for your future web scraping projects, you’ll probably need to learn more about web development than is covered in this book, depending on what site you’re trying to scrape. For example, <span role="doc-pagebreak" epub:type="pagebreak" id="pg_502" aria-label=" Page 502. "/>your HTTPX and BS4 scraper might need to first log in to a website and then make all of its future requests as that logged-in user in order to access the content you’re after. This would require making HTTP POST requests instead of just GET requests and keeping track of cookies, neither of which I’ve covered here.</p>

<p class="TNI1">As a next step, I recommend getting more comfortable with the developer tools built into browsers. This will help familiarize you with the HTTP requests your browser makes and what their responses include. Spend more time browsing the layout of HTML elements, as you did in this appendix. The more you learn about web development, including more complex topics like HTTP headers and cookies, the easier it will be for you to scrape the web. If you can access information in a web browser, you can write a script that automates accessing that information.</p>
</section>

</section>

</div></body></html>