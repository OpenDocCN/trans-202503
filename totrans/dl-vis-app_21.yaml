- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Convnets in Practice
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 实际中的卷积神经网络（ConvNets）
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: In the last chapter we discussed convolution, and we wrapped up with a simplified
    example of a convolutional network, or convnet.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了卷积，并且通过一个简化的卷积网络（convnet）示例做了总结。
- en: In this chapter, we look at two real convnets designed for image classification.
    The first identifies grayscale handwritten digits, and the second identifies what
    object is dominant in a color photograph, choosing from 1,000 different categories.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将介绍两个用于图像分类的实际卷积神经网络。第一个用于识别灰度手写数字，第二个用于识别彩色照片中占主导地位的物体，从 1,000 个不同类别中进行选择。
- en: Categorizing Handwritten Digits
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手写数字分类
- en: Categorizing handwritten digits is a famous problem in machine learning (LeCun
    et al. 1989), thanks to a freely available dataset called MNIST (pronounced em´-nist).
    It contains 60,000 hand-drawn digits from 0 to 9, each a grayscale picture rendered
    in white on a 28 by 28 black background, with a label identifying the digit. The
    drawings were collected from census takers and students. Our job is to identify
    the digit in each image.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 手写数字分类是机器学习中的一个著名问题（LeCun 等，1989），这要感谢一个免费提供的数据集，称为 MNIST（发音为 em´-nist）。它包含
    60,000 个手绘数字，从 0 到 9，每个数字都是白色的灰度图像，背景是 28 x 28 的黑色，标签标识数字。图像是由人口普查员和学生收集的。我们的任务是识别每张图像中的数字。
- en: We will use a simple convnet designed for this job that is included with the
    Keras machine learning library (Chollet 2017). [Figure 17-1](#figure17-1) shows
    the architecture in our schematic form and in the traditional box-and-label form.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个为此任务设计的简单卷积神经网络（convnet），它包含在 Keras 机器学习库中（Chollet 2017）。[图 17-1](#figure17-1)展示了我们示意图中的架构以及传统的框和标签形式。
- en: '![F17001](Images/F17001.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![F17001](Images/F17001.png)'
- en: 'Figure 17-1: A convnet for classifying MNIST digits. The input images are 28
    by 28 by 1 channel. Two convolution layers are followed by pooling, dropout, and
    flatten, then a dense (or fully connected) layer, another dropout, and a final
    dense layer with 10 outputs followed by softmax. Top: Our schematic version. Bottom:
    Traditional box-and-label form.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17-1：用于分类 MNIST 数字的卷积神经网络。输入图像是 28 x 28 x 1 的通道。两个卷积层后跟池化、丢弃法、扁平化，然后是一个密集层（或全连接层），另一个丢弃法层，最后是一个包含
    10 个输出的密集层，后跟 softmax。上图：我们的示意图。下图：传统的框和标签形式。
- en: The input to the net is the MNIST image, provided as a 3D tensor of shape 28
    by 28 by 1 (the 1 refers to the single grayscale channel). Though there are two
    fully connected layers at the end, and various helper layers (such as dropout,
    flatten, and pooling), we still refer to this as a convolutional network, or convnet,
    because the convolution layers dominate the classification work. The first convolution
    layer runs 32 filters, each of size 3 by 3, over the input. Each filter’s output
    is run through a ReLU activation function before it leaves the layer.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的输入是 MNIST 图像，作为形状为 28 x 28 x 1 的 3D 张量提供（1 表示单一的灰度通道）。尽管在最后有两个全连接层，以及各种辅助层（如丢弃法、扁平化和池化），但我们仍然称其为卷积神经网络（convnet），因为卷积层主导了分类工作。第一个卷积层使用
    32 个大小为 3 x 3 的滤波器对输入进行操作。每个滤波器的输出都经过 ReLU 激活函数处理后才离开该层。
- en: By not specifying a stride, the filters will move by one element in each direction.
    We’re also not applying any padding. As we saw in [Figure 16-10](c16.xhtml#figure16-10),
    this means that we lose a ring of elements after each convolution. That’s okay
    in this case because all MNIST images are supposed to have a border of four black
    pixels around the digit (not all images actually have this border, but most do).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由于未指定步幅，滤波器将在每个方向上移动一个元素。我们也没有应用任何填充。正如我们在[图 16-10](c16.xhtml#figure16-10)中看到的，这意味着每次卷积后我们会丢失一圈元素。这样做在本例中是可以接受的，因为所有
    MNIST 图像应该在数字周围有一个四个黑色像素的边框（并非所有图像都有这个边框，但大多数都有）。
- en: The first layer’s input tensor is 28 by 28 by 1, so each filter in the first
    convolution layer is one channel deep. Because we have 32 filters, we don’t have
    any padding on the input, and the filters have a 3 by 3 footprint, the output
    of the first convolution layer is 26 by 26 by 32\. The second convolution layer
    contains 64 filters with 3 by 3 footprints. The system knows that the input has
    32 channels (because the previous layer had 32 filters), so each filter is created
    as a tensor of shape 3 by 3 by 32\. Because we’re still not using padding, we
    again lose a ring around the outside of the input, producing an output tensor
    that’s 24 by 24 by 64.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层的输入张量是 28x28x1，因此第一卷积层中的每个滤波器深度为一个通道。由于我们有 32 个滤波器，并且输入没有填充，且滤波器的尺寸为 3x3，第一个卷积层的输出为
    26x26x32。第二个卷积层包含 64 个滤波器，尺寸为 3x3。系统知道输入有 32 个通道（因为上一层有 32 个滤波器），因此每个滤波器创建为一个形状为
    3x3x32 的张量。由于我们仍然没有使用填充，输入的外围再次丢失一圈，生成的输出张量是 24x24x64。
- en: We could have used striding to reduce the size of the output, but here we use
    an explicit max pooling layer with blocks of size 2 by 2\. That means for every
    nonoverlapping 2 by 2 block in the input, the layer outputs just one value containing
    the largest value in the block. Thus, the output of this layer is a tensor of
    size 12 by 12 by 64 (the pooling doesn’t change the number of channels).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以使用步幅（striding）来减少输出的大小，但在这里我们使用了一个显式的最大池化层，池化块的大小为 2x2。这意味着，对于输入中每个不重叠的
    2x2 块，层输出一个值，该值包含该块中的最大值。因此，这一层的输出是一个 12x12x64 的张量（池化层不会改变通道的数量）。
- en: Next, we come to a dropout layer, represented by a diagonal slash. As we saw
    in Chapter 15, the dropout layer itself doesn’t actually do any processing. Instead,
    it instructs the system to apply dropout to the nearest preceding layer that contains
    neurons. The nearest layer preceding the dropout is pooling, but that has no neurons.
    As we continue to work backward, we find a convolution layer, which does have
    neurons. During training, the dropout algorithm is applied to this convolution
    layer (recall that dropout is only applied during training, and is otherwise ignored).
    Before each epoch of training, one-quarter of the neurons in this convolution
    layer are temporarily disabled. This should help hold off overfitting. By convention,
    we usually treat dropout as a layer, even though it does no computation. Note
    that since the dropout layer looks backward for the nearest layer with neurons,
    we could have placed it to the left of the pooling layer and nothing about the
    network would have changed. By convention, when we pool after convolution, we
    usually place those two layers together.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍一个 dropout 层，用斜线表示。正如我们在第 15 章中看到的，dropout 层本身并不执行任何处理。相反，它指示系统对最近的包含神经元的前置层应用
    dropout。dropout 前最近的层是池化层，但池化层没有神经元。当我们继续向后工作时，发现一个卷积层，它确实包含神经元。在训练过程中，dropout
    算法会应用到这个卷积层（请记住，dropout 仅在训练期间应用，其他时候会被忽略）。在每个训练的周期前，卷积层中四分之一的神经元会被暂时禁用。这有助于防止过拟合。根据惯例，我们通常将
    dropout 视为一个层，即使它不执行任何计算。请注意，由于 dropout 层会向后查找最近的具有神经元的层，我们本可以将其放置在池化层的左侧，网络的行为不会改变。根据惯例，当我们在卷积后进行池化时，我们通常将这两个层放在一起。
- en: Now we leave the convolutional part of the network and prepare the values for
    output. We typically find these steps, or something like them, at the end of classification
    convnets. The output of the second convolution layer is a 3D tensor, but we want
    to feed that into a fully connected layer, which expects a list (or 1D tensor).
    A *flatten* layer, shown as two parallel lines, takes an input tensor of any number
    of dimensions and reorganizes it into a 1D tensor by placing all the elements
    together end-to-end. The list is made up starting with the first row in the tensor.
    We take the first element, and place its 64 values at the head of our list. Then
    we move to the second element, and place its 64 values at the end of the list.
    We continue doing this for every element in the row, and then we do it for the
    next row, and so on. [Figure 17-2](#figure17-2) shows the process. None of the
    values in the tensor are lost in this rearrangement.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们离开网络的卷积部分，为输出做准备。我们通常会在分类卷积神经网络的末端找到这些步骤，或者类似的步骤。第二个卷积层的输出是一个3D张量，但我们希望将其输入到一个全连接层，该层期望接收一个列表（或1D张量）。一个*flatten*层，如图所示，将任何维度的输入张量重组织成一个1D张量，通过将所有元素按顺序放在一起。这个列表从张量的第一行开始。我们取出第一个元素，并将它的64个值放在列表的开头。然后我们移到第二个元素，并将它的64个值放在列表的末尾。我们继续对每个元素执行此操作，然后对下一行重复此过程，以此类推。[图17-2](#figure17-2)展示了这个过程。在这个重排过程中，张量中的值不会丢失。
- en: '![F17002](Images/F17002.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![F17002](Images/F17002.png)'
- en: 'Figure 17-2: The action of a flatten layer. Top: The input tensor. Middle:
    Turning each channel into a list. Bottom: Placing the lists one after the other
    to make one large list.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-2：flatten层的作用。顶部：输入张量。中间：将每个通道转换为一个列表。底部：将这些列表一个接一个地放在一起，形成一个大的列表。
- en: Returning to [Figure 17-1](#figure17-1), the flatten layer produces a list of
    12 × 12 × 64 = 9,216 numbers. That list goes into a fully connected, or dense,
    layer of 128 neurons. That layer gets affected by dropout, where a quarter of
    the neurons are temporarily disconnected at the start of each batch during training.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到[图17-1](#figure17-1)，flatten层生成一个12 × 12 × 64 = 9,216个数字的列表。这个列表进入一个包含128个神经元的全连接层。这个层受到dropout的影响，在训练的每个批次开始时，四分之一的神经元会被暂时断开。
- en: The 128 outputs of this layer go into a final dense layer with 10 neurons. The
    10 outputs of this layer go into a softmax step so that they’re converted to probabilities.
    The 10 numbers that come out of this last layer give us the network’s prediction
    of the probability that the input image belongs to each of the 10 possible classes,
    corresponding to the digits 0 through 9.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该层的128个输出进入一个最终的全连接层，该层有10个神经元。这一层的10个输出进入一个softmax步骤，将它们转换为概率。最后一层输出的10个数字给出了网络对输入图像属于0到9这10个可能类别的概率预测。
- en: We trained this network for 12 epochs using the standard MNIST training data.
    Its accuracy on the training and validation data sets is shown in [Figure 17-3](#figure17-3).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用标准的MNIST训练数据训练了该网络12个周期。其在训练集和验证集上的准确率如[图17-3](#figure17-3)所示。
- en: The curves show we’ve achieved about 99 percent accuracy on both the training
    and validation data sets. Since the curves aren’t diverging, we’ve successfully
    avoided overfitting.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线显示我们在训练和验证数据集上都达到了约99%的准确率。由于曲线没有发散，我们成功避免了过拟合。
- en: '![f17002](Images/f17003.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![f17002](Images/f17003.png)'
- en: 'Figure 17-3: The training performance of our convnet in [Figure 17-2](#figure17-2).
    We trained for 12 epochs, and since the training and validation curves are not
    diverging, we’ve successfully avoided overfitting, while reaching about 99 percent
    accuracy on both data sets.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-3：我们在[图17-2](#figure17-2)中的卷积神经网络的训练表现。我们训练了12个周期，由于训练和验证曲线没有发散，我们成功避免了过拟合，并且在两个数据集上达到了约99%的准确率。
- en: Let’s look at some predictions. [Figure 17-4](#figure17-4) shows some images
    from the MNIST validation set, labeled by the digit that the network gave the
    largest probability to. On this little set of examples, it did a perfect job.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些预测。图[17-4](#figure17-4)显示了来自MNIST验证集的一些图像，标记了网络给出的最大概率对应的数字。在这个小的例子集中，网络做得非常完美。
- en: '![f17004](Images/f17004.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![f17004](Images/f17004.png)'
- en: 'Figure 17-4: These are 24 randomly chosen images from the MNIST validation
    set. Each image is labeled with the output of the network, showing the digit with
    the highest probability. The network classified all 24 of these digits correctly.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Just two convolution layers gave this system enough power to achieve 99 percent
    accuracy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: VGG16
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at a bigger and more powerful convnet, called *VGG16*. It was trained
    to analyze color photographs and identify the dominant object in each photo by
    assigning probabilities to 1,000 different classes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: VGG16 was trained on a famous dataset that was used as part of a contest. The
    ILSVRC2014 competition was a public challenge in 2014\. The goal was to build
    a neural network for classifying photos in a provided database of images (Russakovsky
    et al. 2015). The acronym ILSVRC stands for ImageNet Large Scale Visual Recognition
    Challenge, so the database of pictures is often called the ImageNet database.
    The ImageNet photo database is freely available online and is still widely used
    for training and testing new networks (newer, bigger versions of ImageNet are
    also available [Fei-Fei et al. 2020]).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The original ImageNet database contained 1.2 million images, each manually labeled
    with one of 1,000 labels, describing the object most prominent in the photo. The
    challenge actually included several subchallenges, each with its own winners (ImageNet
    2020). The winner of one of the classification tasks was VGG16 (Simonyan and Zisserman
    2014). VGG is an acronym for the Visual Geometry Group, who developed the system.
    The 16 refers to the network’s 16 computational layers (there are also some utility
    layers, such as dropout and flatten, that don’t do computation).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: VGG16 broke records for accuracy when it won the contest, and even though years
    have passed, it remains popular. This is largely because it still does very well
    at classifying images (even compared to newer, more sophisticated systems), and
    it has a simple structure that’s easy to modify and experiment with. The authors
    have released all the weights and how they preprocessed the training data. Even
    better, every deep learning library makes it easy to create a fully trained instance
    of VGG16 in our own code. Thanks to all of these qualities, VGG16 is a frequent
    starting point for projects that involve image classification.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the VGG16 architecture. Most of the work is done by a series of
    convolution layers. Utility layers appear along the way, and some flattening and
    fully connected layers appear at the very end, as they did in [Figure 17-1](#figure17-1).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Before we feed any data to our model, we must preprocess it in the same way
    that the authors preprocessed their training data. That involves making sure that
    each channel has been adjusted by subtracting a specific value from all of its
    pixels (Simonyan and Zisserman 2014). To better discuss the shapes of the tensors
    flowing through the network, let’s assume each input image has a height and width
    of 224 to match the dimensions of the Imagenet data the network was trained on,
    and its colors have been correctly pre-processed. Once that’s done, we’re ready
    to feed our image to the network.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在将任何数据输入到我们的模型之前，我们必须以与作者处理训练数据相同的方式预处理数据。这涉及到确保每个通道通过从所有像素中减去特定值进行调整（Simonyan
    和 Zisserman 2014）。为了更好地讨论网络中流动的张量形状，假设每张输入图片的高度和宽度为224，以匹配网络训练时使用的Imagenet数据的维度，并且其颜色已正确预处理。完成这些步骤后，我们就可以将图像输入到网络中。
- en: 'We will present the VGG16 architecture as a series of six groups of layers.
    These groups are strictly conceptual and are just a way of gathering together
    related layers for our discussion. The first few groups have the same structure:
    two or three layers of convolution followed by a pooling layer.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将VGG16架构呈现为六组层的系列。这些组是严格概念性的，仅仅是将相关层聚集在一起进行讨论的一种方式。前几组具有相同的结构：两层或三层卷积，后跟一个池化层。
- en: Group 1 is shown in [Figure 17-5](#figure17-5).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第1组如[图17-5](#figure17-5)所示。
- en: '![f17005](Images/f17005.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![f17005](Images/f17005.png)'
- en: 'Figure 17-5: Group 1 of VGG16\. We convolve the input tensor with 64 filters
    each of size 3 by 3\. Then we convolve again with 64 new filters. Finally, we
    use max pooling to reduce the output tensor’s height and width by half.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-5：VGG16的第1组。我们使用64个大小为3x3的滤波器对输入张量进行卷积。然后，我们使用64个新的滤波器再次进行卷积。最后，我们使用最大池化将输出张量的高度和宽度减半。
- en: The convolutions both apply zero padding to their inputs so there’s no loss
    in width or height. The max pooling step uses nonoverlapping blocks of size 2
    by 2.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层对其输入应用了零填充，因此不会丢失宽度或高度。最大池化步骤使用大小为2x2的不重叠块。
- en: All of the convolution layers in VGG16 use the default ReLU activation function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16中的所有卷积层都使用默认的ReLU激活函数。
- en: We’ve seen how useful pooling is for helping our filters recognize patterns
    even if they’ve been displaced. For the same reasons that we used pooling when
    matching masks in Chapter 16, we apply pooling here, too.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，池化对于帮助我们的滤波器识别即使被位移的模式是多么有用。出于与第16章中匹配掩模时相同的原因，我们在这里也应用了池化。
- en: The output of the group in [Figure 17-5](#figure17-5) is a tensor of dimensions
    112 by 112 by 64\. The values of 112 come from the input dimensions of 224 by
    224 that have been halved, and the 64 results from the 64 filters in the second
    convolution layer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-5](#figure17-5)中的组输出的是一个尺寸为112x112x64的张量。112的数值来自于输入的224x224尺寸经过减半后的结果，而64来自于第二个卷积层中的64个滤波器。'
- en: Group 2 is just like the first, only now we apply 128 filters in each convolution
    layer. [Figure 17-6](#figure17-6) shows the layers. The output of this group has
    size 56 by 56 by 128.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第2组与第一组类似，只不过现在我们在每个卷积层中应用128个滤波器。[图17-6](#figure17-6)展示了各层。该组的输出尺寸为56x56x128。
- en: '![f17006](Images/f17006.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![f17006](Images/f17006.png)'
- en: 'Figure 17-6: Group 2 of VGG16 is just like the first block in [Figure 17-5](#figure17-5),
    except that we use 128 filters in each convolution layer rather than 64.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-6：VGG16的第2组就像[图17-5](#figure17-5)中的第一个块，只不过我们在每个卷积层中使用128个滤波器，而不是64个。
- en: Group 3 continues the pattern of doubling the number of filters in each convolution
    layer, but it repeats the convolution step three times instead of twice. [Figure
    17-7](#figure17-7) shows Group 3\. The tensor after the max pooling step has size
    28 by 28 by 256.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第3组继续每个卷积层中滤波器数量翻倍的模式，但它将卷积步骤重复三次，而不是两次。[图17-7](#figure17-7)展示了第3组。最大池化步骤后的张量尺寸为28x28x256。
- en: '![f17007](Images/f17007.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![f17007](Images/f17007.png)'
- en: 'Figure 17-7: Group 3 of VGG16 doubles the number of filters again to 256 and
    repeats the convolution step three times rather than two as before.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-7：VGG16的第3组再次将滤波器的数量翻倍，达到256个，并且将卷积步骤重复三次，而不是像之前那样重复两次。
- en: Groups 4 and 5 of the network are the same. Each group is built from three steps
    of convolution with 512 filters, followed by a max pooling layer. The structure
    of these layers is shown in [Figure 17-8](#figure17-8). The tensor coming out
    of Group 4 has size 28 by 28 by 512, and the tensor after the max pooling layer
    in Group 5 has dimensions 14 by 14 by 512.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的第 4 组和第 5 组是相同的。每组由三次 512 个滤波器的卷积操作组成，后跟一个最大池化层。这些层的结构见于[图 17-8](#figure17-8)。来自第
    4 组的张量尺寸为 28 x 28 x 512，第 5 组最大池化层后的张量尺寸为 14 x 14 x 512。
- en: '![f17008](Images/f17008.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![f17008](Images/f17008.png)'
- en: 'Figure 17-8: Groups 4 and 5 of VGG16 are the same. They each have three convolution
    layers, followed by a two by two max pooling layer.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17-8：VGG16 的第 4 组和第 5 组是相同的。每组包含三个卷积层，后跟一个 2x2 的最大池化层。
- en: This ends the convolution part of the network, and now we come to the wrap-up.
    As with the MNIST classifier we saw in [Figure 17-1](#figure17-1), we first flatten
    the tensor coming out of Group 5\. We then run it through two dense layers of
    4,096 neurons, each using ReLU, and each followed by dropout with an aggressive
    setting of 50 percent. Finally, the output goes into a dense layer with 1,000
    neurons. The results are fed to softmax, which produces our output of 1,000 probabilities,
    one for each class that VGG16 was trained to recognize. These final steps, which
    are typical for classification networks of this style, are shown in [Figure 17-9](#figure17-9).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了网络的卷积部分，现在进入总结部分。与我们在[图 17-1](#figure17-1)中看到的 MNIST 分类器相同，我们首先将来自第 5 组的张量展平。然后将其通过两个拥有
    4,096 个神经元的全连接层，每个使用 ReLU，并且都经过 50% 设置的 dropout。最后，输出进入一个拥有 1,000 个神经元的全连接层。结果送入
    softmax，产生 1,000 个概率值，分别对应 VGG16 被训练识别的每个类别。这些最终步骤，典型于这种风格的分类网络，见于[图 17-9](#figure17-9)。
- en: '![f17009](Images/f17009.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![f17009](Images/f17009.png)'
- en: 'Figure 17-9: The final steps of processing in VGG16\. We flatten the image,
    then run it through two dense layers each using ReLU, followed by dropout, then
    through a dense layer with softmax.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17-9：VGG16 的最终处理步骤。我们将图像展平，然后通过两个使用 ReLU 的全连接层，接着是 dropout，再通过一个带有 softmax
    的全连接层。
- en: '[Figure 17-10](#figure17-10) shows the whole architecture in one place.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 17-10](#figure17-10)展示了整个架构。'
- en: '![f17010](Images/f17010.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![f17010](Images/f17010.png)'
- en: 'Figure 17-10: The VGG16 architecture in one place'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17-10：VGG16 架构汇总
- en: This network works very well. [Figure 17-11](#figure17-11) shows four pictures
    shot around Seattle on a phone’s camera.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络效果非常好。[图 17-11](#figure17-11)展示了四张使用手机相机在西雅图拍摄的图片。
- en: '![f17011](Images/f17011.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![f17011](Images/f17011.png)'
- en: 'Figure 17-11: Four photos shot around Seattle on a sunny day. The convnet of
    [Figure 17-10](#figure17-10) does a great job of identifying each image.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17-11：四张在晴天时在西雅图拍摄的照片。[图 17-10](#figure17-10)中的卷积网络很好地识别了每张图像。
- en: The convnet has never seen these images, but it does a great job with them.
    Even the ambiguous round object in the upper right is assigned sensible labels.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 该卷积网络从未见过这些图像，但它处理得非常好。即使是右上角那个模糊的圆形物体，也被分配了合理的标签。
- en: Let’s take a closer look at what’s going on inside VGG16 by looking at its filters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看 VGG16 的滤波器，来更仔细地了解其内部工作原理。
- en: Visualizing Filters, Part 1
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滤波器可视化，第一部分
- en: VGG16’s success in classifying is due to the filters that were learned by its
    convolution layers. It’s tempting to look at the filters and see what they’ve
    learned, but the filters themselves are big blocks of numbers, which are hard
    for us to interpret. Instead of trying to somehow make sense of a block of numbers,
    we can visualize our filters indirectly by creating images that trigger them.
    In other words, once we’ve selected a filter we want to visualize, we can find
    a picture that causes that filter to output its biggest value. That picture shows
    us what that filter is looking for.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16 在分类中的成功归功于它的卷积层所学习到的滤波器。尽管很想看一下滤波器，并了解它们学到了什么，但滤波器本身是由大量数字组成的块，难以解读。我们不需要试图直接理解这些数字块，而是可以通过创建触发它们的图像来间接可视化滤波器。换句话说，一旦选择了一个要可视化的滤波器，我们可以找到一张图像，使该滤波器输出其最大值。那张图像向我们展示了这个滤波器在寻找什么。
- en: We can do this with a little trick based on gradient descent, the algorithm
    that we saw in Chapter 14 as part of backpropagation. We flip gradient descent
    around to create gradient *ascent*, which we use to climb up the gradient and
    increase the system’s error. Remember from Chapter 14 that during training, we
    use the system’s error to create gradients that we push backward through the network
    with backprop, enabling us to change the weights in order to reduce that error.
    For filter visualization, we’re going to ignore the network’s output and its error
    entirely. The only output we care about is the feature map that comes out of the
    particular filter (or neuron) we want to visualize. We know that when the filter
    sees information that it’s looking for, it produces a big output, so if we add
    up all of the output values of that filter for a given input image, it tells us
    how much of what the filter is looking for is in that image. We can use the sum
    of all the values in the feature map as a replacement for the network’s error.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一点基于梯度下降的小技巧来实现这一点，梯度下降算法是我们在第14章中作为反向传播的一部分看到的。我们将梯度下降颠倒过来，创建梯度*上升*，用于沿着梯度上升并增加系统的误差。请记住，在第14章的训练过程中，我们使用系统的误差创建梯度，通过反向传播将其向网络推送，从而使我们能够改变权重以减少该误差。对于滤波器可视化，我们将忽略网络的输出及其误差。我们唯一关心的输出是特定滤波器（或神经元）生成的特征图。我们知道，当滤波器看到它正在寻找的信息时，它会产生一个大的输出，因此，如果我们将该滤波器对给定输入图像的所有输出值相加起来，就可以知道该图像中滤波器所寻找的信息有多少。我们可以使用特征图中所有值的总和来替代网络的误差。
- en: '[Figure 17-12](#figure17-12) shows the idea.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-12](#figure17-12)展示了这个概念。'
- en: '![f17012](Images/f17012.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![f17012](Images/f17012.png)'
- en: 'Figure 17-12: Visualizing a filter. The sum of all the values in the feature
    map serves as the network’s error.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-12：可视化滤波器。特征图中所有值的总和作为网络的误差。
- en: 'We’re using VGG16, but for this visualization process we leave off the layers
    after the last convolution. We feed in a grid of random numbers and extract the
    filter map for the filter we want to visualize. That becomes our error. Now comes
    the tricky part: we use this error to compute the gradients, but we don’t adjust
    the weights at all. The network itself and all of its weights are *frozen*. We
    just keep computing the gradients and pushing them back until we reach the input
    layer, which holds the pixel values of the input image. The gradients that arrive
    at this layer tell us how to change those pixel values to decrease the error,
    which we know is the filter’s output. Since we want to stimulate the neuron as
    much as we can, we want the “error” to be as big as possible, so we change the
    pixel values to increase, rather than decrease, this error. That makes the picture
    stimulate our selected neuron a little more than it did before.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用VGG16，但在此可视化过程中，我们跳过最后卷积层之后的所有层。我们输入一个随机数网格，并提取我们要可视化的滤波器的滤波器图。这成为我们的误差。现在来到棘手的部分：我们使用这个误差来计算梯度，但我们根本不调整权重。网络本身和所有权重都是*冻结*的。我们只是不断计算梯度并将其推回，直到达到输入层，该层保存输入图像的像素值。到达这一层的梯度告诉我们如何更改这些像素值以减少我们知道的滤波器输出的误差。因为我们希望尽可能地激活神经元，所以我们希望“误差”尽可能大，因此我们改变像素值以增加而不是减少这个误差。这使得图片比之前更多地激活我们选择的神经元。
- en: After doing this over and over, we will have adjusted our initially random pixel
    values so that they’re making the filter output the biggest values we can get
    it to produce. When we look at the input after it’s been modified in this way,
    we see a picture that makes that neuron produce a huge output, so the picture
    shows us what the filter is looking for (or at least gives us a general idea)
    (Zeiler and Fergus 2013). We will use this visualization process again in Chapter
    23 when we look at the deep dreaming algorithm.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 反复进行此操作后，我们将调整我们最初的随机像素值，使它们使滤波器输出尽可能大的值。当我们查看经过这种方式修改后的输入时，我们会看到一幅图片，使得该神经元产生巨大的输出，因此图片展示了滤波器正在寻找的内容（或至少给了我们一个大致的想法）（Zeiler和Fergus
    2013）。我们将在第23章再次使用这个可视化过程，当我们看深度梦境算法时。
- en: Because we start with random values in the input image, we get a different final
    image every time we run this algorithm. But each image we make is roughly like
    the others, since they’re all based on maximizing the output of the same filter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在输入图像中从随机值开始，每次运行此算法时都会得到不同的最终图像。但我们制作的每幅图像大致相似，因为它们都是基于最大化同一滤波器输出而生成的。
- en: Let’s look at some images produced by this method. [Figure 17-13](#figure17-13)
    shows pictures produced for the 64 filters in the second convolution layer in
    the first block, or group, of VGG16 (we use the label `block1_conv2` for this
    layer and similar names for the other layers we look at). In [Figure 17-13](#figure17-13)
    and the others like it to come, we’ve enhanced the color saturation to make the
    results easier to interpret.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们看看使用这种方法生成的图像。[图17-13](#figure17-13)展示了VGG16第一个块或组中的第二个卷积层的64个滤波器生成的图像（我们用标签`block1_conv2`表示这一层，其他层也有类似的名称）。在[图17-13](#figure17-13)及之后的类似图像中，我们增强了色彩饱和度，使得结果更容易解读。
- en: '![f17013](Images/f17013.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![f17013](Images/f17013.png)'
- en: 'Figure 17-13: Images that get the biggest response from each of the 64 filters
    in the `block1_conv2` layer of VGG16'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-13：VGG16中`block1_conv2`层的每64个滤波器产生最大响应的图像
- en: It seems that a lot of these layers are looking for edges in different orientations.
    Some have values that are too subtle for us to interpret easily.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很多这些层正在寻找不同方向的边缘。有些值太微妙，难以让我们轻易解读。
- en: Let’s move forward to block 3, and look at the first 64 filters from the first
    convolution layer there. [Figure 17-14](#figure17-14) shows images that stimulate
    these filters the most.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们来看第3块，观察该块第一个卷积层的前64个滤波器。[图17-14](#figure17-14)展示了最能激发这些滤波器的图像。
- en: '![f17014](Images/f17014.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![f17014](Images/f17014.png)'
- en: 'Figure 17-14: Images that get the biggest response from the first 64 filters
    in the `block3_conv1` layer of VGG16'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-14：VGG16中`block3_conv1`层的前64个滤波器产生最大响应的图像
- en: Now we’re talking! As we’d expect, the filters here are looking for more complex
    textures, combining the simpler patterns found by prior layers. Let’s move farther
    along and look at the first 64 filters from the first convolution layer of block
    4, in [Figure 17-15](#figure17-15).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始有趣了！正如我们预期的那样，这里的滤波器正在寻找更复杂的纹理，结合了之前层所发现的简单模式。接着我们继续，看看第4块的第一个卷积层中的前64个滤波器，如[图17-15](#figure17-15)所示。
- en: '![f17015](Images/f17015.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![f17015](Images/f17015.png)'
- en: 'Figure 17-15: Images that get the biggest response from the first 64 filters
    in the `block4_conv1` layer of VGG16'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-15：VGG16中`block4_conv1`层的前64个滤波器产生最大响应的图像
- en: These are fascinating glimpses into what VGG16 has learned. We can see some
    of the structures it has found to be useful in order to classify the object in
    an image. The filters seem to be hunting for patterns that involve a lot of different
    kinds of flowing and interlocking textures like those we’d find on animals and
    other surfaces in the world around us.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是对VGG16学习过程的迷人一瞥。我们可以看到它在分类图像中的物体时发现的一些有用结构。这些滤波器似乎在寻找包含多种不同类型流动和交织纹理的模式，就像我们在动物和我们周围世界的其他表面上看到的那样。
- en: We can really see the value of the convolution hierarchy here. Each layer of
    convolution looks for patterns in the output of the previous layer, letting us
    work our way up from low-level details like stripes and edges to complex and rich
    geometrical structures.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里真正看到了卷积层级的价值。每一层卷积都在寻找前一层输出中的模式，帮助我们从低级别的细节（如条纹和边缘）逐渐过渡到复杂且丰富的几何结构。
- en: Just for fun, let’s look at close-ups of a few of these filters. [Figure 17-16](#figure17-16)
    shows larger views of nine patterns from the first few layers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了好玩，我们来看一下其中几个滤波器的特写。[图17-16](#figure17-16)展示了从前几层选取的九种模式的更大视图。
- en: '![f17016](Images/f17016.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![f17016](Images/f17016.png)'
- en: 'Figure 17-16: Close-ups of some manually selected images that triggered the
    largest filter responses from the first few layers of VGG16'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-16：VGG16前几层中触发最大滤波器响应的一些手动选取图像的特写
- en: '[Figure 17-17](#figure17-17) shows patterns that triggered big responses from
    filters in the last few layers.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-17](#figure17-17)展示了触发最后几层滤波器产生强烈响应的模式。'
- en: These patterns are exciting and beautiful. They also have an organic feeling
    about them, probably because the ImageNet database contains many images of animals.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模式令人兴奋而美丽。它们也有一种有机的感觉，可能是因为ImageNet数据库包含了许多动物的图像。
- en: '![f17017](Images/f17017.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![f17017](Images/f17017.png)'
- en: 'Figure 17-17: Close-ups of some manually selected images that triggered the
    largest filter responses from the last few layers of VGG16'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-17：VGG16最后几层中触发最大滤波器响应的一些手动选取图像的特写
- en: Visualizing Filters, Part 2
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化滤波器，第2部分
- en: Another way to visualize a filter is to run an image through VGG16, and look
    at the feature map produced by that filter. That is, we feed an image to VGG16
    and let it run through the network, but as before, we ignore the network’s output.
    Instead, we extract the feature map for the filter we’re interested in, and draw
    it like a picture. This is possible because each feature map always has a single
    channel, so we can draw it as a grayscale image.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可视化过滤器的方法是通过VGG16运行图像，查看该过滤器生成的特征图。也就是说，我们将图像输入VGG16并让其通过网络，但像之前一样，我们忽略网络的输出。相反，我们提取我们感兴趣的过滤器的特征图，并将其像图像一样绘制出来。这是可能的，因为每个特征图总是只有一个通道，因此我们可以将其绘制为灰度图。
- en: Let’s give it a spin. [Figure 17-18](#figure17-18) shows our input image of
    a drake, or male duck. This is the starting image for all of our visualizations
    in this section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试看。[图17-18](#figure17-18)展示了我们的输入图像，一只雄鸭。这是我们本节所有可视化的起始图像。
- en: '![f17018](Images/f17018.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![f17018](Images/f17018.png)'
- en: Figure 17-18 The drake image that we use to visualize filter outputs
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-18 我们用来可视化过滤器输出的雄鸭图像
- en: To get a feeling for things, [Figure 17-19](#figure17-19) shows the response
    from the very first filter on the very first convolution layer of the network.
    Since the output of a filter has just one channel, we can draw it in grayscale.
    We’ve chosen to instead use a heatmap from black to reds to yellow.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，[图17-19](#figure17-19)展示了网络第一层卷积层中第一个过滤器的响应。由于过滤器的输出只有一个通道，我们可以用灰度图来表示它。我们选择使用从黑色到红色再到黄色的热图来展示。
- en: '![f17019](Images/f17019.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![f17019](Images/f17019.png)'
- en: 'Figure 17-19: The response of filter 0 in layer `block1_conv1` in VGG16 to
    the duck image in [Figure 17-18](#figure17-18)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-19：VGG16中`block1_conv1`层第0个过滤器对[图17-18](#figure17-18)中雄鸭图像的响应
- en: This filter is looking for edges. Consider the tail in the lower right. An edge
    that’s light on top and darker below gets a very large output from the filter,
    whereas an edge in the other direction gets a very low output. Less extreme changes
    cause smaller outputs, and regions of constant color have middling outputs.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过滤器在寻找边缘。考虑一下右下角的尾部。一个上面浅色下面深色的边缘会产生很大的输出，而另一个方向的边缘则输出非常低。变化较小的区域产生较小的输出，而颜色恒定的区域输出中等。
- en: '[Figure 17-20](#figure17-20) shows the responses from the first 32 filters
    in the first convolution layer of the first block.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-20](#figure17-20)展示了第一个卷积层中前32个过滤器的响应。'
- en: '![f17020](Images/f17020.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![f17020](Images/f17020.png)'
- en: 'Figure 17-20: The responses of the first 32 filters in VGG convolution layer
    `block1_conv1`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-20：VGG卷积层`block1_conv1`中前32个过滤器的响应
- en: A lot of these filters seem to be looking for edges, but others seem to be looking
    for particular features of the image. Let’s look at close-ups of 8 manually selected
    filters chosen from all 64 of the filters on this layer, shown in [Figure 17-21](#figure17-21).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些过滤器中的很多似乎在寻找边缘，但也有些似乎在寻找图像的特定特征。我们来看一下从这层64个过滤器中手动选出的8个过滤器的特写，见[图17-21](#figure17-21)。
- en: '![f17021](Images/f17021.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![f17021](Images/f17021.png)'
- en: 'Figure 17-21: Close-ups of eight manually chosen filter responses from VGG16’s
    first convolution layer, `block1_``conv1`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-21：VGG16第一个卷积层`block1_conv1`中八个手动选择的过滤器响应特写
- en: The third image in the top row seems to be looking for the duck’s feet, or maybe
    it’s just interested in bright orange things. The left-most image in the bottom
    row looks like it’s searching for the waves and sand behind the duck, though the
    image to its right appears to be responding most to the blue waves. Some more
    experimentation with other inputs would help us nail down these interpretations,
    but it’s fun to see how much we can guess from a single image.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 顶排中的第三张图似乎在寻找鸭子的脚，或者它可能只对明亮的橙色物体感兴趣。底排最左边的图像看起来像是在寻找鸭子背后的波浪和沙滩，而其右边的图像似乎最强烈地响应蓝色波浪。通过对其他输入进行更多实验，我们可以更准确地确认这些解释，但看到我们从一张图像中能猜测出这么多内容，确实很有趣。
- en: Let’s move farther into the network, out to the third block of convolution layers.
    The outputs here are smaller by a factor of four on each side than those coming
    out of the first block because they’ve gone through two pooling layers. We expect
    that they are looking for clusters of features. [Figure 17-22](#figure17-22) shows
    the responses for the first convolution layer in block 3.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步深入网络，进入第三个卷积层块。这里的输出比第一个块的输出小了四倍，因为它们已经通过了两层池化层。我们预计它们正在寻找特征的聚类。[图17-22](#figure17-22)展示了第三块中第一个卷积层的响应。
- en: '![f17022](Images/f17022.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![f17022](Images/f17022.png)'
- en: 'Figure 17-22: The responses of the first 32 filters in the VGG convolution
    layer `block3_conv1`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-22：VGG卷积层`block3_conv1`中前32个滤波器的响应
- en: It’s interesting that a lot of edge finding still seems to be going on. This
    suggests that strong edges are an important cue for VGG16 as it works to figure
    out what an image is showing, even in the third set of convolutions. But lots
    of other regions are also bright.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 很有趣的是，很多边缘检测似乎仍在进行中。这表明，强边缘对于VGG16来说是一个重要的线索，它在尝试识别图像内容时，即便是在第三组卷积中，边缘依然发挥着作用。但其他许多区域也在变亮。
- en: Let’s jump all the way to the last block. [Figure 17-23](#figure17-23) shows
    the responses for the first 32 filters for the first convolution layer in block
    5.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们跳到最后一个块。[图17-23](#figure17-23)展示了第五块卷积层中第一个卷积层的前32个滤波器的响应。
- en: '![f17023](Images/f17023.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![f17023](Images/f17023.png)'
- en: 'Figure 17-23: Filter responses for the first 32 filters in VGG convolution
    layer `block5_conv1`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图17-23：VGG卷积层`block5_conv1`中前32个滤波器的响应
- en: As we’d expect, these images are even smaller, having passed through two more
    pooling layers that each reduce the size by a factor of two on each side. At this
    point, the duck is hardly visible because the system is combining features from
    the previous layers. Some of the filters are barely responding. They are probably
    responsible for finding high-level features that aren’t present in the duck image.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所预期的，这些图像变得更小，因为它们经过了两层池化层，每层都会将大小缩小一半。此时，鸭子几乎看不见，因为系统正在结合前面层的特征。一些滤波器几乎没有响应。它们可能负责寻找在鸭子图像中没有出现的高层次特征。
- en: In Chapter 23 we’ll look at a couple of creative applications that use the filter
    responses in a convnet.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在第23章，我们将探讨一些利用卷积神经网络（convnet）滤波器响应的创意应用。
- en: Adversaries
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗样本
- en: Although VGG16 does very well at predicting the correct label for many images,
    we can change an image in ways so small that they’re undetectable to the human
    eye, but that fools the classifier into assigning the wrong label. In fact, this
    process can mess up the results of any convolution-based classifier.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管VGG16在预测许多图像的正确标签时表现非常好，我们仍然可以通过一些微小到人眼无法察觉的方式修改图像，进而欺骗分类器，使其分配错误的标签。事实上，这个过程可以干扰任何基于卷积的分类器的结果。
- en: The trick to fooling a convnet involves creating a new image called an *adversary*.
    This image is created from the starting image by adding an *adversarial perturbation*
    (or more simply, a *perturbation*). The perturbation is another image, the same
    size as the image we want to classify, typically with very small values. If we
    add the perturbation to our original image, the changes are usually so small that
    most people can’t detect any difference, even in the finest details. But if we
    ask VGG16 to classify the perturbed image, it gives us the wrong answer. Sometimes
    we can find a single perturbation that messes up the results for every image we
    give to a particular classifier, which we call a *universal perturbation* (Moosavi-Dezfooli
    et al. 2016).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 欺骗卷积神经网络的技巧是创建一个被称为*对抗样本*的新图像。这个图像是通过在原始图像上添加*对抗扰动*（或更简单地说，*扰动*）来创建的。扰动是另一张图像，大小与我们要分类的图像相同，通常具有非常小的值。如果我们将扰动加到原始图像中，变化通常非常小，以至于大多数人无法察觉任何不同，哪怕是在最细微的细节上。但是，如果我们让VGG16去分类这个扰动后的图像，它会给出错误的答案。有时，我们可以找到一个单一的扰动，使得每张图像在输入到特定分类器时都会产生错误的结果，这种扰动我们称之为*通用扰动*（Moosavi-Dezfooli等，2016年）。
- en: Let’s see this in action. On the left of [Figure 17-24](#figure17-24) we see
    an image of a tiger. All of the pixel values in this image are between 0 and 255\.
    The system correctly classifies it as a tiger with about 80 percent confidence,
    with smaller confidences for related animals such as a tiger cat and a jaguar.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个过程的实际表现。在[图17-24](#figure17-24)的左侧，我们看到一张老虎的图像。图像中的所有像素值都在0到255之间。系统正确地将其分类为老虎，置信度约为80%，对于相关动物如虎猫和美洲豹的置信度较小。
- en: '![f17024](Images/f17024.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![f17024](Images/f17024.png)'
- en: 'Figure 17-24: An adversarial attack on an image. Left: The input and VGG16’s
    top five classes. Middle: The adversarial image, where the pixel values are in
    the range of about [–2, 2], but shown here scaled to the range [0, 255] so they
    can be seen. Right: The result of adding the image and the original (unscaled)
    adversary together, and the new top five classes.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17-24：对图像的对抗性攻击。左侧：输入图像及 VGG16 的前五个分类结果。中间：对抗性图像，其中像素值的范围大约为 [–2, 2]，但此处为了便于观察，已将其缩放到
    [0, 255] 范围。右侧：将图像与原始（未缩放）对抗性图像相加后的结果，以及新的前五个分类结果。
- en: In the middle of [Figure 17-24](#figure17-24) we show an image computed by an
    algorithm designed to find adversaries. All of the values in this image are about
    in the range [–2, 2], but for this figure, we scaled the values to the range [0,
    255] so they’d be easier to see. In the top right of [Figure 17-23](#figure17-23)
    we show the result of adding the tiger and the adversary, so each of the original
    tiger’s pixels is changed by a value within the range [–2, 2]. To our eyes, the
    tiger seems unchanged. Even the thin whiskers look the same. Below that image
    are VGG16’s top five predictions for this new image. The system comes up with
    completely different predictions for the image, none of which come anywhere close
    to the correct class. Except for the low-probability class of brain coral, the
    system doesn’t even think this image is an animal.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 17-24](#figure17-24) 中间，我们展示了一张通过旨在寻找对抗样本的算法计算得出的图像。图像中的所有值大约都在 [–2, 2]
    范围内，但为了便于观察，我们将这些值缩放到了 [0, 255] 范围。在 [图 17-23](#figure17-23) 的右上角，我们展示了将老虎和对抗样本相加后的结果，因此原始老虎的每个像素都被改变了一个
    [–2, 2] 范围内的值。对我们来说，老虎似乎没有变化，连细小的胡须看起来也没有改变。在该图像下方是 VGG16 对这张新图像的前五个预测结果。系统给出的预测完全不同，其中没有任何一个接近正确的类别。除了低概率的脑珊瑚类，系统甚至认为这张图像不是动物。
- en: The perturbation image in [Figure 17-24](#figure17-24) may look random to our
    eyes, but it’s not. This picture was specifically computed to throw off VGG16’s
    prediction for the image of the tiger.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 17-24](#figure17-24) 中的扰动图像可能对我们眼睛看起来是随机的，但其实并非如此。这张图像是专门计算出来的，目的是扰乱 VGG16
    对老虎图像的预测。'
- en: There are many different ways to compute adversarial images (Rauber, Brendel,
    and Bethge 2018). The range of values in the perturbations these methods create
    for a given image can vary considerably, so to find the smallest perturbation,
    it’s often worth trying a few different methods, also called *attacks*. We can
    compute adversaries to achieve different goals (Rauber and Brendel 2017b). For
    example, we can ask for a perturbation that simply causes the input to be misclassified.
    Another option asks for a perturbation that causes the input to be classified
    as a specific, desired class. To make [Figure 17-24](#figure17-24), we used an
    algorithm that is designed to make the classifier’s top seven predictions much
    more unlikely. That is, it takes in the starting image and the top seven predictions
    from the classifier and produces an adversary. When we add the adversary to the
    input and hand that to the classifier, none of its new top seven predictions contain
    any of the previous top seven predictions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 计算对抗性图像有很多不同的方法（Rauber、Brendel 和 Bethge 2018）。这些方法为给定图像创建的扰动值的范围可能会有显著差异，因此为了找到最小的扰动，通常值得尝试几种不同的方法，这些方法也被称为
    *攻击*。我们可以计算对抗样本以实现不同的目标（Rauber 和 Brendel 2017b）。例如，我们可以要求产生一个扰动，简单地使输入被错误分类。另一种选择是要求产生一个扰动，使得输入被分类为特定的目标类别。为了生成
    [图 17-24](#figure17-24)，我们使用了一种旨在使分类器的前七个预测结果不太可能出现的算法。也就是说，它接收起始图像和分类器的前七个预测结果，并生成一个对抗样本。当我们将对抗样本与输入图像相加，并交给分类器时，分类器的新前七个预测中不包含任何先前的前七个预测。
- en: We have to carefully construct adversarial perturbations, which suggests that
    they’re exploiting something subtle in our convnets.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须小心构建对抗扰动，这表明它们正在利用我们卷积神经网络中的某些微妙之处。
- en: We may find a way to build convnets that resist these attacks, but convolutional
    networks may be inherently vulnerable to these subtle image manipulations (Gilmer
    et al. 2018). The existence of adversaries suggests that convnets still hold surprises
    for us, and they shouldn’t be considered foolproof. There’s more to be learned
    about what’s going on inside of convolutional networks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会找到构建抗攻击的卷积神经网络（convnets）的方法，但卷积神经网络可能本质上容易受到这些微妙图像操作的影响（Gilmer 等，2018）。对抗样本的存在表明，卷积神经网络仍然充满未知，我们不应将其视为万无一失的。关于卷积神经网络内部发生了什么，仍有许多值得探索的内容。
- en: Summary
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter we looked at a couple of real convnets: a small one for classifying
    handwritten MNIST digits and the larger VGG16 network for classifying photos.
    Though our MNIST network was quite small, it was able to classify digits with
    about 99 percent accuracy.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们看了几个实际的卷积神经网络：一个小型的用于分类手写MNIST数字的网络和一个较大的VGG16网络，用于分类照片。尽管我们的MNIST网络相当小，但它能够以约99%的准确率对数字进行分类。
- en: We looked at the structure of VGG16, and then two different types of visualizations
    of its filters. We saw that the filters in this network start by looking for simple
    structures like edges and build up to complex and beautiful organic patterns.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们查看了VGG16的结构，接着看了两种不同类型的其滤波器可视化。我们发现该网络中的滤波器首先寻找简单的结构，如边缘，然后逐渐构建出复杂且美丽的有机图案。
- en: Finally, we saw that convolutional networks used as image classifiers are susceptible
    to being fooled by adjusting the pixel values by tiny amounts that are imperceptible
    to a human observer.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看到作为图像分类器使用的卷积神经网络容易受到欺骗，通过微小的像素值调整，这些调整对人类观察者是难以察觉的。
- en: In the next chapter we’ll look at how to build networks that figure out how
    to compress an input into a much smaller representation and then expand that again
    to produce something close to the original.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何构建能够将输入压缩成更小表示的网络，然后再将其扩展以生成接近原始输入的结果。
