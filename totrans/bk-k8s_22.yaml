- en: '19'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '19'
- en: TUNING QUALITY OF SERVICE
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 调优服务质量
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Ideally, our applications would use minimal or highly predictable processing,
    memory, storage, and network resources. In the real world, though, applications
    are “bursty,” with changes in load driven by user demand, large amounts of data,
    or complex processing. In a Kubernetes cluster, where application components are
    deployed dynamically to various nodes in the cluster, uneven distribution of load
    across those nodes can cause performance bottlenecks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们的应用程序应使用最小或高度可预测的处理、内存、存储和网络资源。然而，在现实世界中，应用程序是“突发性的”，其负载变化由用户需求、大量数据或复杂处理驱动。在Kubernetes集群中，应用组件动态部署到集群中不同的节点上，如果负载在这些节点间分布不均，可能会造成性能瓶颈。
- en: From an application architecture standpoint, the more we can make the application
    components small and scalable, the more we can evenly distribute load across the
    cluster. Unfortunately, it’s not always possible to solve performance issues with
    horizontal scaling. In this chapter, we’ll look at how we can use resource specifications
    to provide hints to the cluster about how to schedule our Pods, with the goal
    of making application performance more predictable.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从应用架构的角度来看，越是将应用组件做得小巧且可扩展，我们就能越均匀地分配负载到集群中。不幸的是，性能问题并不总是能够通过水平扩展来解决。在本章中，我们将探讨如何使用资源规格来向集群提供有关如何调度我们的Pod的提示，目的是使应用性能更加可预测。
- en: Achieving Predictability
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现可预测性
- en: In normal, everyday language, the term *real time* has the sense of something
    that happens quickly and continuously. But in computer science, we make a distinction
    between *real time* and *real fast* to such a degree that they are thought of
    as opposites. This is due to the importance of predictability.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在日常语言中，“实时”一词通常指某些迅速且持续发生的事情。但在计算机科学中，我们区分“实时”和“实时快速”，它们甚至被认为是对立的。这是因为可预测性的重要性。
- en: Real-time processing is simply processing that needs to keep up with some activity
    that is happening in the real world. It could be anything from airplane cockpit
    software that needs to keep up with sensor data input and maintain up-to-date
    electronic flight displays, to a video streaming application that needs to receive
    and decode each frame of video in time to display it. In real-time systems, it
    is critical that we can guarantee that processing will be “fast enough” to keep
    up with the real-world requirement.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 实时处理指的是需要跟上现实世界某些活动的处理。它可以是任何需要跟上传感器数据输入并保持最新电子飞行显示的飞机驾驶舱软件，也可以是需要及时接收并解码每一帧视频以便显示的视频流应用程序。在实时系统中，至关重要的是我们能够保证处理“足够快”，以跟上现实世界的需求。
- en: Fast enough is all we need. It’s not necessary for the processing to go any
    faster than the real world, as there isn’t anything else for the application to
    do. But even a single time interval when the processing is slower than the real
    world means we fall behind our inputs or outputs, leading to annoyed movie watchers—or
    even to crashed airplanes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 只要“足够快”就好。处理速度不需要快过现实世界，因为应用程序没有其他事情可做。但即便是一个处理速度慢于现实世界的时间间隔，也意味着我们落后于输入或输出，导致观影者的不满——甚至可能导致飞机坠毁。
- en: For this reason, the main goal in real-time systems is predictability. Resources
    are allocated based on the worst-case scenario the system will encounter, and
    we’re willing to provide significantly more processing than necessary to have
    plenty of margin on that worst case. Indeed, it’s common to require these types
    of systems to stay under 50 percent utilization of the available processing and
    memory, even at maximum expected load.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，实时系统中的主要目标是可预测性。资源是根据系统可能遇到的最坏情况进行分配的，我们愿意提供比实际需要更多的处理能力，以确保在最坏情况下有足够的余地。实际上，要求这类系统在最大预期负载下，即使在可用处理和内存资源上，也要保持低于50%的利用率是很常见的。
- en: But whereas responsiveness is always important, most applications don’t operate
    in a real-time environment, and this additional resource margin is expensive.
    For that reason, most systems try to find a balance between predictability and
    efficiency, which means that we are often willing to tolerate a bit of slower
    performance from our application components as long as it is temporary.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但尽管响应性始终很重要，大多数应用程序并不在实时环境中运行，而这种额外的资源余量是昂贵的。出于这个原因，大多数系统试图在可预测性和效率之间找到平衡，这意味着我们通常愿意容忍应用组件略微的性能下降，只要它是暂时的。
- en: Quality of Service Classes
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务质量类别
- en: 'To help us balance predictability and efficiency for the containers in a cluster,
    Kubernetes allocates Pods to one of three different Quality of Service classes:
    `BestEffort`, `Burstable`, and `Guaranteed`. In a way, we can think of these as
    descriptive. `BestEffort` is used when we don’t provide Kubernetes with any resource
    requirements, and it can only do its best to provide enough resources for the
    Pod. `Burstable` is used when a Pod might exceed its resource request. `Guaranteed`
    is used when we provide consistent resource requirements and our Pod is expected
    to stay within them. Because these classes are descriptive and are based solely
    on how the containers in the Pod specify their resource requirements, there is
    no way to specify the QoS for a Pod manually.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们平衡集群中容器的可预测性和效率，Kubernetes 将 Pods 分配到三种不同的服务质量类别：`BestEffort`、`Burstable`
    和 `Guaranteed`。从某种意义上讲，我们可以将这些类别看作是描述性的。`BestEffort` 用于我们没有提供任何资源要求时，它只能尽最大努力为
    Pod 提供足够的资源。`Burstable` 用于 Pod 可能超过其资源请求的情况。`Guaranteed` 用于我们提供一致的资源要求，并且期望 Pod
    始终保持在这些要求内。因为这些类别是描述性的，并且仅基于容器在 Pod 中指定的资源要求，因此没有办法手动指定 Pod 的 QoS。
- en: The QoS class is used in two ways. First, Pods in a QoS class are grouped together
    for Linux control groups (cgroups) configuration. As we saw in [Chapter 3](ch03.xhtml#ch03),
    cgroups are used to control resource utilization, especially processing and memory,
    for a group of processes, so a Pod’s cgroup affects its priority in use of processing
    time when the system load is high. Second, if the node needs to start evicting
    Pods due to lack of memory resources, the QoS class affects which Pods are evicted
    first.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: QoS 类别有两种使用方式。首先，属于同一 QoS 类别的 Pods 会被分组，以便进行 Linux 控制组（cgroups）配置。正如我们在[第 3
    章](ch03.xhtml#ch03)中看到的，cgroups 用于控制一组进程的资源使用，特别是处理能力和内存，因此，Pod 的 cgroup 会影响其在系统负载较高时的处理时间优先级。其次，如果节点因内存资源不足需要开始逐出
    Pods，QoS 类别会影响哪些 Pods 会首先被逐出。
- en: BestEffort
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BestEffort
- en: The simplest case is one in which we declare a Pod with no `limits`. In that
    case, the Pod is assigned to the `BestEffort` class. Let’s create an example Pod
    to explore what that means.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的情况是我们声明一个没有 `limits` 的 Pod。在这种情况下，Pod 被分配到 `BestEffort` 类别。让我们创建一个示例 Pod
    来探索这意味着什么。
- en: '**NOTE**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例代码库位于* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。*有关如何设置的详细信息，请参见[第
    xx 页](ch00.xhtml#ch00lev1sec2)中的“运行示例”。*'
- en: 'Here’s the Pod definition:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Pod 的定义：
- en: '*best-effort.yaml*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*best-effort.yaml*'
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This definition includes no `resources` field at all, but the QoS class would
    be the same if we included a `resources` field with `requests` but no `limits`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义完全没有`resources`字段，但如果我们包含一个带有`requests`但没有`limits`的`resources`字段，QoS 类别会是一样的。
- en: 'We use `nodeName` to force this Pod onto `host01` so that we can observe how
    its resource use is configured. Let’s apply it to to the cluster:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `nodeName` 强制将该 Pod 部署到 `host01`，以便观察其资源使用配置。让我们将其应用到集群中：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After the Pod is running, we can look at its details to see that it has been
    allocated to the `BestEffort` QoS class:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pod 启动后，我们可以查看它的详细信息，看到它已分配到 `BestEffort` QoS 类别：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can use the `cgroup-info` script we saw in [Chapter 14](ch14.xhtml#ch14)
    to see how the QoS class affects the cgroup configuration for containers in the
    Pod:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用在[第 14 章](ch14.xhtml#ch14)中看到的 `cgroup-info` 脚本，查看 QoS 类别如何影响 Pod 中容器的
    cgroup 配置：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The Pod is effectively unlimited in CPU and memory usage. However, the Pod’s
    cgroup is under the *kubepods-besteffort.slice* path, reflecting its allocation
    to the `BestEffort` QoS class. This allocation has an immediate effect on its
    CPU priority, as we can see when we compare the `cpu.shares` allocated to the
    `BestEffort` class compared to the `Burstable` class:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该Pod在CPU和内存使用上实际上没有限制。然而，Pod的cgroup位于*kubepods-besteffort.slice*路径下，反映了它被分配到`BestEffort`
    QoS类别中。这种分配直接影响了它的CPU优先级，正如我们在比较`BestEffort`类别和`Burstable`类别的`cpu.shares`时所看到的那样：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we saw in [Chapter 14](ch14.xhtml#ch14), these values are relative, so this
    configuration means that when our system’s processing load is high, containers
    in `Burstable` Pods are going to be allocated more than 500 times the processor
    share that containers in `BestEffort` Pods receive. This value is based on the
    number of Pods that are already in the `BestEffort` and `Burstable` QoS classes,
    including the various cluster infrastructure components already running on *host01*,
    thus you might see a slightly different value.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第14章](ch14.xhtml#ch14)中看到的，这些值是相对的，因此这一配置意味着，当系统的处理负载很高时，`Burstable` Pods中的容器将被分配比`BestEffort`
    Pods中容器超过500倍的处理器份额。这个值是基于已经在`BestEffort`和`Burstable` QoS类别中的Pod数量，包括在*host01*上运行的各种集群基础设施组件，因此你可能会看到略有不同的值。
- en: The *kubepods.slice* cgroup sits at the same level as cgroups for user and system
    processes, so when the system is loaded it gets an approximately equal share of
    processing time as those other cgroups. Based on the *cpu.shares* identified within
    the *kubepods.slice* cgroup, `BestEffort` Pods are receiving less than 1 percent
    of the total share of processing compared to `Burstable` Pods, even without considering
    any processor time allocated to `Guaranteed` Pods. This means that `BestEffort`
    Pods receive almost no processor time when the system is loaded, so they should
    be used only for background processing that can run when the cluster is idle.
    In addition, because Pods are placed in the `BestEffort` class only if they have
    no `limits` specified, they cannot be created in a Namespace with limit quotas.
    So most of our application Pods will be in one of the other two QoS classes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*kubepods.slice* cgroup与用户和系统进程的cgroup处于同一级别，因此当系统负载较高时，它会获得与其他cgroup几乎相等的处理时间份额。基于在*kubepods.slice*
    cgroup中识别到的*cpu.shares*，`BestEffort` Pods相对于`Burstable` Pods，获得的处理器时间份额不到总份额的1％，即使不考虑分配给`Guaranteed`
    Pods的处理器时间。这意味着当系统负载高时，`BestEffort` Pods几乎没有处理器时间，因此它们应该仅用于在集群空闲时运行的后台处理。此外，由于只有在未指定`limits`时才将Pods放置在`BestEffort`类别中，因此它们无法在具有限制配额的命名空间中创建。因此，我们的大多数应用程序Pods将位于其他两个QoS类别之一。'
- en: Burstable
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Burstable
- en: Pods are placed in the `Burstable` class if they specify both `requests` and
    `limits` and if those two specifications are different. As we saw in [Chapter
    14](ch14.xhtml#ch14), the `requests` specification is used for scheduling purposes,
    whereas the `limits` specification is used for runtime enforcement. In other words,
    Pods in this situation can have “bursts” of resource utilization above their `requests`
    level, but they cannot exceed their `limits`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Pod同时指定了`requests`和`limits`，并且这两个规格不同，则Pod会被放置在`Burstable`类别中。正如我们在[第14章](ch14.xhtml#ch14)中看到的，`requests`规格用于调度目的，而`limits`规格用于运行时强制执行。换句话说，这种情况下的Pods可以在其`requests`级别之上有“突发”的资源使用，但不能超过其`limits`。
- en: 'Let’s look at an example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子：
- en: '*burstable.yaml*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*burstable.yaml*'
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This Pod definition supplies both `requests` and `limits` resource requirements,
    and they are different, so we should expect this Pod to be placed in the `Burstable`
    class.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Pod定义提供了`requests`和`limits`资源要求，并且它们是不同的，因此我们可以预期这个Pod将被放置在`Burstable`类别中。
- en: 'Let’s apply this Pod to the cluster:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个Pod应用到集群中：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, let’s verify that it was assigned to the `Burstable` QoS class:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们验证它是否已分配到`Burstable` QoS类别：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Indeed, the cgroup configuration follows the QoS class and the `limits` we
    specified:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，cgroup配置遵循了我们指定的QoS类别和`limits`：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `limits` specified for this Pod were used to set both a CPU limit and a
    memory limit. Also, as we expect, this Pod’s cgroup is placed within *kubepods-burstable.slice*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 该`limits`为此Pod指定的值用于设置CPU限制和内存限制。此外，正如我们预期的，这个Pod的cgroup被放置在*kubepods-burstable.slice*中。
- en: 'Adding another Pod to the `Burstable` QoS class has caused Kubernetes to rebalance
    the allocation of processor time:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 向`Burstable` QoS类别添加另一个Pod，导致Kubernetes重新平衡了处理器时间的分配：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The result is that Pods in the `Burstable` QoS class now show a value of 1413
    for *cpu.shares*, whereas Pods in the `BestEffort` class still show 2\. This means
    that the relative processor share under load is now 700 to 1 in favor of Pods
    in the `Burstable` class. Again, you may see slightly different values based on
    how many infrastructure Pods Kubernetes has allocated to `host01`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，`Burstable` QoS 类别下的 Pod 显示 *cpu.shares* 的值为 1413，而 `BestEffort` 类别下的 Pod
    仍然显示 2。这意味着在负载下，`Burstable` 类别 Pod 的相对处理器份额是 700 比 1。再一次，你可能会看到略有不同的值，取决于 Kubernetes
    为 `host01` 分配了多少基础设施 Pod。
- en: Because `Burstable` Pods are scheduled based on `requests` but cgroup runtime
    enforcement is based on `limits`, a node’s processor and memory resources can
    be overcommitted. It works fine as long as the Pods on a node balance out one
    another so that the average utilization matches the `requests`. It becomes a problem
    if the average utilization exceeds the `requests`. In that case, Pods will see
    their CPU throttled and may even be evicted if memory becomes scarce, as we saw
    in [Chapter 10](ch10.xhtml#ch10).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 `Burstable` 类 Pod 是根据 `requests` 调度的，但 cgroup 运行时强制执行是基于 `limits` 的，所以节点的处理器和内存资源可能会超额分配。只要节点上的
    Pod 彼此之间平衡，平均利用率与 `requests` 匹配，就没有问题。如果平均利用率超过了 `requests`，就会出现问题。在这种情况下，Pod
    会看到其 CPU 被限速，如果内存变得紧张，可能会被驱逐，就像我们在[第 10 章](ch10.xhtml#ch10)中看到的那样。
- en: Guaranteed
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 保证类
- en: 'If we want to increase predictability for the processing and memory available
    to a Pod, we can place it in the `Guaranteed` QoS class by giving the `requests`
    and `limits` equal settings. Here’s an example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望提高 Pod 可用处理能力和内存的可预测性，可以通过设置相同的 `requests` 和 `limits` 来将 Pod 放入 `Guaranteed`
    QoS 类别。以下是一个示例：
- en: '*guaranteed.yaml*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*guaranteed.yaml*'
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this example, only `limits` is specified given that Kubernetes automatically
    sets the `requests` to match the `limits` if `requests` is missing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，只有 `limits` 被指定，因为如果 `requests` 缺失，Kubernetes 会自动将 `requests` 设置为与 `limits`
    匹配。
- en: 'Let’s apply this to the cluster:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将此应用于集群：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After the Pod is running, verify the QoS class:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pod 运行后，验证其 QoS 类别：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The cgroups configuration looks a little different:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: cgroup 配置看起来有点不同：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Rather than place these containers into a separate directory, containers in
    the `Guaranteed` QoS class are placed directly in *kubepods.slice*. Putting them
    in this location has the effect of privileging containers in `Guaranteed` Pods
    when the system is loaded because those containers receive their CPU shares individually
    rather than as a class.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将这些容器放入单独的目录中，`Guaranteed` QoS 类别下的容器直接放入 *kubepods.slice* 中。将它们放置在这个位置的效果是，当系统负载时，会优先考虑
    `Guaranteed` 类 Pod 中的容器，因为这些容器按个别处理器份额接收 CPU 分配，而不是按类接收。
- en: QoS Class Eviction
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: QoS 类别驱逐
- en: The privileged treatment of Pods in the `Guaranteed` QoS class extends to Pod
    eviction as well. As described in [Chapter 3](ch03.xhtml#ch03), cgroup enforcement
    of memory limits is handled by the OOM killer. The OOM killer also runs when a
    node is completely out of memory. To help the OOM killer choose which containers
    to terminate, Kubernetes sets the `oom_score_adj` parameter based on the QoS class
    of the Pod. This parameter can have a value from –1000 to 1000\. The higher the
    number, the more likely the OOM killer will choose a process to be killed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`Guaranteed` QoS 类别 Pod 的优先处理也扩展到了 Pod 驱逐。如[第 3 章](ch03.xhtml#ch03)中所述，cgroup
    对内存限制的强制执行是由 OOM killer 处理的。当节点完全耗尽内存时，OOM killer 也会运行。为了帮助 OOM killer 选择要终止的容器，Kubernetes
    会根据 Pod 的 QoS 类别设置 `oom_score_adj` 参数。此参数的值范围从 -1000 到 1000。数值越高，OOM killer 选择终止进程的可能性就越大。'
- en: 'The `oom_score_adj` value is recorded in */proc* for each process. The automation
    has added a script called *oom-info* to retrieve it for a given Pod. Let’s check
    the values for the Pods in each QoS class:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`oom_score_adj` 值会为每个进程记录在 */proc* 中。自动化系统已添加一个名为 *oom-info* 的脚本，用于获取特定 Pod
    的该值。让我们检查每个 QoS 类别下 Pod 的值：'
- en: '[PRE14]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Pods in the `BestEffort` QoS class have the maximum adjustment of 1000, so they
    would be targeted first by the OOM killer. Pods in the `Burstable` QoS class have
    a score calculated based on the amount of memory specified in the `requests` field,
    as a percentage of the node’s total memory capacity. This value will therefore
    be different for every Pod but will always be between 2 and 999\. Thus, Pods in
    the `Burstable` QoS class will always be second in priority for the OOM killer.
    Meanwhile, Pods in the `Guaranteed` QoS class are set close to the minimum value,
    in this case –997, so they are protected from the OOM killer as much as possible.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`BestEffort` QoS 类中的 Pods 具有最大调整值为 1000，因此它们会首先成为 OOM 杀手的目标。`Burstable` QoS
    类中的 Pods 其得分是基于 `requests` 字段中指定的内存量计算的，作为节点总内存容量的百分比。因此，这个值对于每个 Pod 都会有所不同，但始终介于
    2 和 999 之间。因此，`Burstable` QoS 类中的 Pods 在 OOM 杀手的优先级中始终排在第二位。与此同时，`Guaranteed`
    QoS 类中的 Pods 被设置为接近最小值，在本例中为 -997，因此它们会尽可能避免被 OOM 杀手终止。'
- en: Of course, as mentioned in [Chapter 3](ch03.xhtml#ch03), the OOM killer terminates
    a process immediately, so it is an extreme measure. When memory on a node is low
    but not yet exhausted, Kubernetes attempts to evict Pods to reclaim memory. This
    eviction is also prioritized based on the QoS class. Pods in the `BestEffort`
    class and Pods in the `Burstable` class that are using more than their `requests`
    value (high-use `Burstable`) are the first to be evicted, followed by Pods in
    the `Burstable` class that are using less than their `requests` value (low-use
    `Burstable`) and Pods in the `Guaranteed` class.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，正如 [第 3 章](ch03.xhtml#ch03) 中提到的，OOM 杀手会立即终止一个进程，因此它是一种极端的措施。当节点上的内存不足但尚未耗尽时，Kubernetes
    会尝试驱逐 Pods 以回收内存。这个驱逐过程也根据 QoS 类进行优先级排序。`BestEffort` 类中的 Pods 和使用超过其 `requests`
    值的 `Burstable` 类 Pods（高使用 `Burstable`）是最先被驱逐的，其次是使用低于其 `requests` 值的 `Burstable`
    类 Pods（低使用 `Burstable`）和 `Guaranteed` 类中的 Pods。
- en: 'Before moving on, let’s do some cleanup:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们做一些清理：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now we can have a fresh start when we look at Pod priorities later in this chapter.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在本章稍后再看一下 Pod 优先级时从头开始。
- en: Choosing a QoS Class
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择 QoS 类
- en: Given this prioritization in processing time and eviction priority, it might
    be tempting to place all Pods in the `Guaranteed` QoS class. And there are application
    components for which this is a viable strategy. As described in [Chapter 7](ch07.xhtml#ch07),
    we can configure a HorizontalPodAutoscaler to make new Pod instances automatically
    if the existing instances are consuming a significant percentage of their allocated
    resources. This means that we can request a reasonable `limits` value for Pods
    in a Deployment and allow the cluster to automatically scale the Deployment if
    we’re getting too close to the limit across those Pods. If the cluster is running
    in a cloud environment, we can even extend autoscaling to the node level, dynamically
    creating new cluster nodes when load is high and reducing the number of nodes
    when the cluster is idle.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于处理时间和驱逐优先级的这一优先顺序，可能会想将所有 Pods 都放在 `Guaranteed` QoS 类中。对于某些应用组件来说，这是一个可行的策略。如
    [第 7 章](ch07.xhtml#ch07) 所述，我们可以配置一个 HorizontalPodAutoscaler，当现有实例消耗了它们分配资源的显著比例时，自动创建新的
    Pod 实例。这意味着我们可以为 Deployment 中的 Pods 请求一个合理的 `limits` 值，并允许集群在这些 Pods 接近限制时自动扩展
    Deployment。如果集群运行在云环境中，我们甚至可以将自动扩展扩展到节点级别，在负载高时动态创建新的集群节点，在集群空闲时减少节点数量。
- en: Using only `Guaranteed` Pods together with autoscaling sounds great, but it
    assumes that our application components are easily scalable. It also only works
    well when our application load consists of many small requests, so that an increase
    in load primarily means we are handing similar-sized requests from more users.
    If we have application components that periodically handle large or complex requests,
    we must set the `limits` for those components to accommodate the worst-case scenario.
    Given that Pods in the `Guaranteed` QoS class have `requests` equal to `limits`,
    our cluster will need enough resources to handle this worst-case scenario, or
    we won’t even be able to schedule our Pods. This results in a cluster that is
    largely idle unless the system is under its maximum load. Similarly, if we have
    scalability limitations such as dependency on specialized hardware, we might have
    a natural limit on the number of Pods we can create for a component, forcing each
    Pod to have more resources to handle its share of the overall load.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用`Guaranteed` Pod配合自动扩展听起来不错，但这假设我们的应用组件是容易扩展的。它也只有在我们的应用负载由许多小请求组成时才有效，这样负载增加主要意味着我们正在处理来自更多用户的类似大小的请求。如果我们的应用组件周期性地处理大或复杂的请求，我们必须为这些组件设置`limits`，以应对最坏情况。考虑到`Guaranteed`
    QoS类中的Pod具有`requests`等于`limits`，我们的集群需要足够的资源来处理这个最坏情况，否则我们甚至无法调度我们的Pod。这将导致集群在没有达到最大负载时大部分处于空闲状态。同样，如果我们有扩展性限制，如依赖于专业硬件，我们可能会对可以为某个组件创建的Pod数量有自然限制，从而迫使每个Pod拥有更多资源来处理其在整体负载中的份额。
- en: For this reason, it makes sense to balance the use of the `Guaranteed` and `Burstable`
    QoS classes for our Pods. Any Pods that have consistent load, or that can feasibly
    be scaled horizontally to meet additional demand, should be in the `Guaranteed`
    class. Pods that are harder to scale, or need to handle a mix of large and small
    workloads, should be in the `Burstable` class. These Pods should specify their
    `requests` based on their average utilization, and specify `limits` based on their
    worst-case scenario. Specifying resource requirements in this way will ensure
    that the cluster’s expected performance margin can be monitored by simply comparing
    the allocated resources to the cluster capacity. Finally, if a large request causes
    multiple application components to run at their worst-case utilization simultaneously,
    it may be worth running performance tests and exploring anti-affinity, as described
    in [Chapter 18](ch18.xhtml#ch18), to avoid overloading a single node.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，平衡使用`Guaranteed`和`Burstable` QoS类对我们的Pod来说是有意义的。任何负载稳定，或者可以通过水平扩展来满足额外需求的Pod，应该使用`Guaranteed`类。那些更难以扩展，或者需要处理大负载和小负载混合的Pod，应该使用`Burstable`类。这些Pod应该根据其平均利用率来指定`requests`，并根据其最坏情况来指定`limits`。以这种方式指定资源需求，将确保集群的预期性能边际可以通过简单地将分配的资源与集群容量进行比较来进行监控。最后，如果一个大请求导致多个应用组件同时以最坏情况的利用率运行，那么可能值得进行性能测试，并探索反亲和性，如[第18章](ch18.xhtml#ch18)所述，以避免过载单个节点。
- en: Pod Priority
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pod 优先级
- en: In addition to using hints to help the Kubernetes cluster understand how to
    manage Pods when the system is highly loaded, it is possible to tell the cluster
    directly to give some Pods a higher priority than others. This higher priority
    applies during Pod eviction, as Pods will be evicted in priority order within
    their QoS class. It also applies during scheduling because the Kubernetes scheduler
    will evict Pods if necessary to be able to schedule a higher-priority Pod.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用提示帮助Kubernetes集群理解在系统高度负载时如何管理Pods，还可以直接告诉集群为某些Pods分配比其他Pods更高的优先级。在Pod驱逐时，这种更高的优先级适用，因为Pods会根据其QoS类内的优先级顺序被驱逐。它在调度时也适用，因为Kubernetes调度器会在必要时驱逐Pod，以便调度一个优先级更高的Pod。
- en: 'Pod priority is a simple numeric field; higher numbers are higher priority.
    Numbers greater than one billion are reserved for critical system Pods. To assign
    a priority to a Pod, we must create a *PriorityClass* resource first. Here’s an
    example:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Pod优先级是一个简单的数字字段；数字越大，优先级越高。大于十亿的数字保留给关键系统Pod。为了为Pod分配优先级，我们必须首先创建一个*PriorityClass*资源。以下是一个示例：
- en: '*essential.yaml*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*essential.yaml*'
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s apply this to the cluster:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其应用到集群中：
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that this PriorityClass has been defined, we can apply it to Pods. However,
    let’s first create a large number of low-priority Pods through which we can see
    Pods being preempted. We’ll use this Deployment:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个 PriorityClass 已经定义完毕，我们可以将其应用到 Pods。不过，首先让我们创建大量低优先级的 Pods，通过这些 Pods，我们可以看到
    Pods 被抢占。我们将使用这个 Deployment：
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This is a basic Deployment that runs `sleep` and doesn’t request very much
    memory or CPU, but it does set `replicas` to `1000`, so we’re asking our Kubernetes
    cluster to create 1,000 Pods. The example cluster isn’t large enough to deploy
    1,000 Pods, both because we don’t have sufficient resources to meet the specification
    and because a node is limited to 110 Pods by default. Still, let’s apply it to
    the cluster, as shown in [Listing 19-1](ch19.xhtml#ch19list1), and the scheduler
    will create as many Pods as it can:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基本的 Deployment，运行 `sleep`，并且没有请求太多内存或 CPU，但它将 `replicas` 设置为 `1000`，所以我们要求
    Kubernetes 集群创建 1,000 个 Pods。示例集群的规模不足以部署 1,000 个 Pods，因为我们没有足够的资源来满足这个规格，而且每个节点默认最多只能调度
    110 个 Pods。不过，还是让我们将它应用到集群中，如[清单 19-1](ch19.xhtml#ch19list1)所示，调度器会创建尽可能多的 Pods：
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Listing 19-1: Deploy lots of Pods*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 19-1：部署大量 Pods*'
- en: 'Let’s describe the Deployment to see how things are going:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述一下 Deployment，看看情况如何：
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We managed to get only seven Pods in our example cluster, given the number
    of Pods already running for cluster infrastructure components. Unfortunately,
    that’s all the Pods we’ll get:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于集群基础设施组件已经运行了一些 Pods，我们的示例集群仅能容纳七个 Pods。不幸的是，这就是我们能得到的所有 Pods：
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The data for `host01` shows that we’ve allocated 94 percent of the available
    CPU ➊. But each of our Pods is requesting 250 millicores, so there isn’t enough
    capacity remaining to schedule another Pod on this node. The other two nodes are
    in a similar situation, with insufficient CPU room to schedule any more Pods.
    Still, the cluster is performing just fine. We’ve theoretically allocated all
    of the processing power, but those containers are just running `sleep`, and as
    such, they aren’t actually using much CPU.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`host01` 的数据表明，我们已经分配了 94% 的可用 CPU ➊。但是我们的每个 Pod 请求 250 毫核心，所以没有足够的容量来调度另一个
    Pod 到这个节点。其他两个节点也处于类似情况，没有足够的 CPU 容量来调度更多 Pods。不过，集群的运行状况非常良好。理论上，我们已经分配了所有的处理能力，但那些容器仅仅在运行
    `sleep`，因此它们实际上并没有使用很多 CPU。'
- en: Also, it’s important to remember that the `requests` field is used for scheduling,
    so even though we have a number of infrastructure `BestEffort` Pods that specify
    `requests` but no `limits` and we have plenty of `Limits` capacity on this node,
    we still don’t have any room for scheduling new Pods. Only `Limits` can be overcommitted,
    not `Requests`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，重要的是要记住，`requests` 字段用于调度，因此尽管我们有一些基础设施 `BestEffort` Pods，它们指定了 `requests`
    但没有 `limits`，而且我们这个节点上有足够的 `Limits` 容量，但我们依然没有空间调度新的 Pods。只有 `Limits` 可以超配，`Requests`
    不能。
- en: 'Because we have no more CPU to allocate to Pods, the rest of the Pods in our
    Deployment are stuck in a Pending state:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有更多的 CPU 来分配给 Pods，Deployment 中剩余的 Pods 都卡在了 Pending 状态：
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: All 993 of these Pods have the default pod priority of 0\. As a result, when
    we create a new Pod using the `essential` PriorityClass, it will jump to the front
    of the scheduling queue. Not only that, but the cluster will evict Pods as necessary
    to enable it to be scheduled.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这 993 个 Pods 都有默认的 pod 优先级 0。因此，当我们使用 `essential` PriorityClass 创建一个新 Pod 时，它将排到调度队列的前面。不仅如此，集群还会根据需要驱逐
    Pods，以便让它能够被调度。
- en: 'Here’s the Pod definition:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Pod 定义：
- en: '*needed.yaml*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*needed.yaml*'
- en: '[PRE23]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The key difference here is the specification of the `priorityClassName`, matching
    the PriorityClass we created. Let’s apply this to the cluster:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键区别是 `priorityClassName` 的指定，它与我们创建的 PriorityClass 匹配。让我们将其应用到集群中：
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It will take the cluster a little time to evict another Pod so that this one
    can be scheduled, but after a minute or so it will start running:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 集群需要一些时间来驱逐另一个 Pod，以便为这个 Pod 调度，但大约一分钟后它将开始运行：
- en: '[PRE25]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To allow this to happen, one of the Pods from the `lots` Deployment we created
    in [Listing 19-1](ch19.xhtml#ch19list1) had to be evicted:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这一切发生，我们在[清单 19-1](ch19.xhtml#ch19list1)中创建的 `lots` Deployment 中的一个 Pod 必须被驱逐：
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We’re now down to only six Pods available in the Deployment ➊, as one Pod was
    evicted. It’s worth noting that being in the `Guaranteed` QoS class did not prevent
    this Pod from being evicted. The `Guaranteed` QoS class gets priority for evictions
    caused by node resource usage, but not for eviction caused by the scheduler finding
    room for a higher-priority Pod.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在部署中只剩下六个 Pod ➊，因为有一个 Pod 被驱逐。值得注意的是，处于`Guaranteed` QoS 类别并没有防止该 Pod 被驱逐。`Guaranteed`
    QoS 类别在节点资源使用导致的驱逐中有优先权，但在调度器为更高优先级的 Pod 找到空间时，不能阻止驱逐。
- en: Of course, the ability to specify a higher priority for a Pod, resulting in
    the eviction of other Pods, is powerful and should be used sparingly. Normal users
    do not have the ability to create a new PriorityClass, and administrators can
    apply a quota to limit the use of a PriorityClass in a given Namespace, effectively
    limiting normal users from creating high-priority Pods.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，指定 Pod 的更高优先级，从而驱逐其他 Pod 的能力是非常强大的，应该谨慎使用。普通用户没有能力创建新的 PriorityClass，管理员可以为给定的命名空间应用配额，以限制
    PriorityClass 的使用，实质上限制普通用户创建高优先级的 Pod。
- en: Final Thoughts
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最后的思考
- en: Deploying an application to Kubernetes so that it is performant and reliable
    requires an understanding of the application architecture and of the normal and
    worst-case load for each component. Kubernetes QoS classes allow us to shape the
    way that Pods are deployed to nodes to achieve a balance of predictability and
    efficiency in the use of resources. Additionally, both QoS classes and Pod priorities
    allow us to provide hints to the Kubernetes cluster so the deployed applications
    degrade gracefully as the load on the cluster becomes too high.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 将应用部署到 Kubernetes 上，使其既高效又可靠，需要理解应用架构以及每个组件的正常负载和最坏情况下的负载。Kubernetes QoS 类别允许我们塑造
    Pod 部署到节点的方式，以在资源使用的可预测性和效率之间实现平衡。此外，QoS 类别和 Pod 优先级都可以为 Kubernetes 集群提供提示，以便在集群负载过高时，部署的应用能够优雅降级。
- en: In the next chapter, we’ll bring together the ideas we’ve seen on how to best
    use the features of a Kubernetes cluster to deploy performant, resilient applications.
    We’ll also explore how we can monitor those applications and respond automatically
    to changes in behavior.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将整合如何最好地利用 Kubernetes 集群的特性来部署高性能、具韧性的应用的想法。我们还将探讨如何监控这些应用，并自动响应行为变化。
