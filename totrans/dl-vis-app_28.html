<html><head></head><body><div id="sbo-rt-content"><section><header><h1 class="BackmatterTitle"><span epub:type="pagebreak" title="693" id="Page_693"/>References</h1></header>
<p class="BodyFirst">Whenever possible, I have preferred to use references that are available online so that you can immediately access them. The exceptions are usually books and other printed matter, but occasionally I’ve included an important online reference even if it’s behind a paywall. Every link here was live as of the time of writing. But the Internet is nothing if not volatile, and some of these links are sure to change, or simply stop working. If you find a link doesn’t work, I suggest using a search engine to look for the title and/or author of the reference you want to locate. Often you’ll find that it’s only moved to a new location. If it’s gone, you can try finding a saved copy on the Wayback Machine at <a href="https://archive.org/web/" class="LinkURL">https://archive.org/web/</a>. You can also try searching for the reference using the Google search engine, which will sometimes offer a cached version of a page that is no longer live.</p>
<p>If you’re reading this as an ebook, you can simply click on any link to follow it. If you’re reading this on paper, you can type any of these links into your favorite browser, but a better approach is to go to the online copy of this chapter at No Starch Press, where the links are live: <a href="https://nostarch.com/deep-learning-visual-approach/" class="LinkURL">https://nostarch.com/deep-learning-visual-approach/</a>.</p>
<h2 id="h1-500723b01-0001">Chapter 1</h2>
<p class="Reference">Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. New York: Springer-Verlag. Available at <a href="https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=aWFtYW5kaS5ldXxpc2N8Z3g6MjViZDk1NGI1NjQzOWZiYQ" class="LinkURL">https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=aWFtYW5kaS5ldXxpc2N8Z3g6MjViZDk1NGI1NjQzOWZiYQ</a>.</p>
<p class="Reference">Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2017. <em>Deep Learning</em>. Cambridge, MA: MIT Press. Available at <a href="https://www.deeplearningbook.org/" class="LinkURL">https://www.deeplearningbook.org/</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="694" id="Page_694"/>Saba, Luca, Mainak Biswas, Venkatanareshbabu Kuppili, Elisa Cuadrado Godia, Harman S. Suri, Damodar Reddy Edla, Tomaž Omerzu, John R. Laird, Narendra N. Khanna, Sophie Mavrogeni, et al. 2019. “The Present and Future of Deep Learning in Radiology.” <em>European Journal of Radiology</em> 114 (May): 14–24.</p>
<h2 id="h1-500723b01-0002">Chapter 2</h2>
<p class="Reference">Anscombe, F. J. 1973. “Graphs in Statistical Analysis.” <em>American Statistician</em> 27, no. 1 (February): 17–21.</p>
<p class="Reference">Banchoff, Thomas F. 1990. <em>Beyond the Third Dimension: Geometry, Computer Graphics, and Higher Dimensions</em>. Scientific American Library Series. New York: W. H. Freeman.</p>
<p class="Reference">Brownlee, Jason. 2017. “How to Calculate Bootstrap Confidence Intervals for Machine Learning Results in Python.” <em>Machine Learning Mastery</em> (blog). Last updated on August 14, 2020. <a href="https://machinelearningmastery.com/calculate-bootstrap-confidence-intervals-machine-learning-results-python/" class="LinkURL">https://machinelearningmastery.com/calculate-bootstrap-confidence-intervals-machine-learning-results-python/</a>.</p>
<p class="Reference">Efron, Bradley, and Robert J. Tibshirani. 1993. <em>An Introduction to the Bootstrap</em>. Boca Raton, FL: Chapman and Hall/CRC, Taylor and Francis Group. </p>
<p class="Reference">Matejka, Justin, and George Fitzmaurice. 2017. “Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics Through Simulated Annealing.” In <em>Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</em> (Denver, CO, May 6–11): 1290–94. <a href="https://www.autodesk.com/research/publications/same-stats-different-graphs" class="LinkURL">https://www.autodesk.com/research/publications/same-stats-different-graphs</a>.  </p>
<p class="Reference">Norton, John D. 2014. “What Is a Four Dimensional Space Like?” Lecture notes. Department of History and Philosophy of Science, University of Pittsburgh, PA. <a href="https://www.pitt.edu/~jdnorton/teaching/HPS_0410/chapters/four_dimensions/index.html" class="LinkURL">https://www.pitt.edu/~jdnorton/teaching/HPS_0410/chapters/four_dimensions/index.html</a>.</p>
<p class="Reference">Teknomo, Kardi. 2015. “Bootstrap Sampling Tutorial.” Revoledu. <a href="https://people.revoledu.com/kardi/tutorial/Bootstrap/index.html" class="LinkURL">https://people.revoledu.com/kardi/tutorial/Bootstrap/index.html</a>. </p>
<p class="Reference">ten Bosch, Marc. 2020. “<em>N</em>-Dimensional Rigid Body Dynamics.” <em>ACM Transactions on Graphics</em> 39, no. 4 (July). <a href="https://marctenbosch.com/ndphysics/NDrigidbody.pdf" class="LinkURL">https://marctenbosch.com/ndphysics/NDrigidbody.pdf</a>.</p>
<p class="Reference">Wikipedia. 2017a. “Anscombe’s Quartet.” Last modified June 21, 2020. <a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet" class="LinkURL">https://en.wikipedia.org/wiki/Anscombe%27s_quartet</a>.</p>
<p class="Reference">Wikipedia. 2017b. “Random Variable.” Last modified August 21, 2020. <a href="https://en.wikipedia.org/wiki/Random_variable" class="LinkURL">https://en.wikipedia.org/wiki/Random_variable</a>.</p>
<h2 id="h1-500723b01-0003">Chapter 3</h2>
<p class="Reference">Glen, Stephanie. 2014. “Marginal Distribution.” Statisticshowto.com. February 6, 2014. <a href="http://www.statisticshowto.com/marginal-distribution/" class="LinkURL">http://www.statisticshowto.com/marginal-distribution/</a>.</p>
<p class="Reference">Jaynes, Edwin Thompson. 2003. <em>Probability Theory: The Logic of Science</em>. Cambridge, UK: Cambridge University Press.</p>
<p class="Reference">Kirby, Roger. 2011. <em>Small Gland, Big Problem</em>. London, UK: Health Press.</p>
<p class="Reference">Kunin, Daniel, Jingru Guo, Tyler Dae Devlin, and Daniel Xiang. 2020. “Seeing Theory.” Seeing Theory. Accessed September 16, 2020. <a href="https://seeing-theory.brown.edu/#firstPage" class="LinkURL">https://seeing-theory.brown.edu/#firstPage</a>.</p>
<p class="Reference">Levitin, Daniel J. 2016.<em> A Field Guide to Lies: Critical Thinking in the Information Age</em>. New York: Viking Press.</p>
<p class="Reference">Walpole, Ronald E., Raymond H. Myers, Sharon L. Myers, and Keying E. Ye. 2011. <em>Probability and Statistics for Engineers and Scientists</em>, 9th ed. New York: Pearson.</p>
<p class="Reference"><span epub:type="pagebreak" title="695" id="Page_695"/>Wikipedia. 2020. “Sensitivity and Specificity.” Last updated October 20, 2020. <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" class="LinkURL">https://en.wikipedia.org/wiki/Sensitivity_and_specificity</a>. </p>
<h2 id="h1-500723b01-0004">Chapter 4</h2>
<p class="Reference">Cthaeh, The. 2016a. “Bayes’ Theorem: An Informal Derivation.” <em>Probabilistic World</em>. February 28, 2016. <a href="http://www.probabilisticworld.com/anatomy-bayes-theorem/" class="LinkURL">http://www.probabilisticworld.com/anatomy-bayes-theorem/</a>.</p>
<p class="Reference">Cthaeh, The. 2016b. “Calculating Coin Bias with Bayes’ Theorem.” <em>Probabilistic World</em>. March 21, 2016. <a href="http://www.probabilisticworld.com/calculating-coin-bias-bayes-theorem/" class="LinkURL">http://www.probabilisticworld.com/calculating-coin-bias-bayes-theorem/</a>.</p>
<p class="Reference">Genovese, Christopher R. 2004. “Tutorial on Bayesian Analysis (in Neuroimaging).” Paper presented at the Institute for Pure and Applied Mathematics Conference, University of California, Los Angeles, July 20, 2004. <a href="http://www.stat.cmu.edu/~genovese/talks/ipam-04.pdf" class="LinkURL">http://www.stat.cmu.edu/~genovese/talks/ipam-04.pdf</a>.</p>
<p class="Reference">Kruschke, John K. 2014. <em>Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan</em>, 2nd ed. Cambridge, MA: Academic Press.</p>
<p class="Reference">Stark, P. B., and D. A. Freedman. 2016. “What Is the Chance of an Earthquake?” UC Berkeley Department of Statistics, Technical Report 611, October 2016. <a href="https://www.stat.berkeley.edu/~stark/Preprints/611.pdf" class="LinkURL">https://www.stat.berkeley.edu/~stark/Preprints/611.pdf</a>.</p>
<p class="Reference">VanderPlas, Jake. 2014. “Frequentism and Bayesianism: A Python-Driven Primer.” Cornell University, Astrophysics, arXiv:1411.5018, November 18, 2014. <a href="https://arxiv.org/abs/1411.5018" class="LinkURL">https://arxiv.org/abs/1411.5018</a>.</p>
<h2 id="h1-500723b01-0005">Chapter 5</h2>
<p class="Reference">3Blue1Brown. 2020. 3Blue1Brown home page. Accessed September 1, 2020. <a href="https://www.3blue1brown.com" class="LinkURL">https://www.3blue1brown.com</a>.</p>
<p class="Reference">Apostol, Tom M. 1991. <em>Calculus, Vol. 1: One-Variable Calculus, with an Introduction to Linear Algebra</em>, 2nd ed. New York: Wiley.</p>
<p class="Reference">Berkey, Dennis D., and Paul Blanchard. 1992. <em>Calculus</em>. Boston: Houghton Mifflin Harcourt School.</p>
<h2 id="h1-500723b01-0006">Chapter 6</h2>
<p class="Reference">Bellizzi, Courtney. 2011. “A Forgotten History: Alfred Vail and Samuel Morse.” Smithsonian Institution Archives. May 24, 2011. <a href="http://siarchives.si.edu/blog/forgotten-history-alfred-vail-and-samuel-morse" class="LinkURL">http://siarchives.si.edu/blog/forgotten-history-alfred-vail-and-samuel-morse</a>.</p>
<p class="Reference">Ferrier, Andrew. 2020. “A Quick Tutorial on Generating a Huffman Tree.” <em>Andrew Ferrier </em>(tutorial). Accessed November 12, 2020. <a href="https://www.andrewferrier.com/oldpages/huffman_tutorial.html" class="LinkURL">https://www.andrewferrier.com/oldpages/huffman_tutorial.html</a>.</p>
<p class="Reference">Huffman, David A. 1952. “A Method for the Construction of Minimum-Redundancy Codes.” In <em>Proceedings of the IRE</em> 40, no. 9. <a href="https://web.stanford.edu/class/ee398a/handouts/papers/Huffman%20-%20Min%20Redundancy%20Codes%20-%20IRE52.pdf" class="LinkURL">https://web.stanford.edu/class/ee398a/handouts/papers/Huffman%20-%20Min%20Redundancy%20Codes%20-%20IRE52.pdf</a>.</p>
<p class="Reference">Kurt, Will. 2017. “Kullback-Leibler Divergence Explained.” <em>Probably a Probability</em> (blog), Count Bayesie. May 10, 2017. <a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" class="LinkURL">https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained</a>.</p>
<p class="Reference">Longden, George. 1987. “G3ZQS’ Explanation of How FISTS Got Its Name.” FISTS CW Club. Accessed September 1, 2020. <a href="https://fists.co.uk/g3zqsintroduction.html" class="LinkURL">https://fists.co.uk/g3zqsintroduction.html</a>.</p>
<p class="Reference">McEwen, Neal. 1997. “Morse Code or Vail Code?” The Telegraph Office. http://www.telegraph-office.com/pages/vail.html. </p>
<p class="Reference"><span epub:type="pagebreak" title="696" id="Page_696"/>Pope, Alfred. 1887. “The American Inventors of the Telegraph, with Special References to the Services of Alfred Vail.” <em>The Century Illustrated Monthly Magazine </em>35, no. 1 (November). <a href="http://tinyurl.com/jobhn2b" class="LinkURL">http://tinyurl.com/jobhn2b</a>. </p>
<p class="Reference">Serrano, Luis. 2017. “Shannon Entropy, Information Gain, and Picking Balls from Buckets.” <em>Medium</em>. November 5, 2017. <a href="https://medium.com/udacity/shannon-entropy-information-gain-and-picking-balls-from-buckets-5810d35d54b4" class="LinkURL">https://medium.com/udacity/shannon-entropy-information-gain-and-picking- balls-from-buckets-5810d35d54b4</a>.</p>
<p class="Reference">Seuss, Dr. 1960. <em>Green Eggs and Ham</em>. Beginner Books. </p>
<p class="Reference">Shannon, Claude E. 1948. “A Mathematical Theory of Communication.” <em>Bell Labs Technical Journal</em> (July). <a href="http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf" class="LinkURL">http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf</a>.</p>
<p class="Reference">Stevenson, Robert Louis. 1883. <em>Treasure Island</em>. Project Gutenberg.<em> </em>Available at <a href="https://www.gutenberg.org/ebooks/120" class="LinkURL">https://www.gutenberg.org/ebooks/120</a>.</p>
<p class="Reference">Thomas, Andrew. 2017. “An Introduction to Entropy, Cross Entropy and KL Divergence in Machine Learning.” <em>Adventures in Machine Learning</em> (blog). March 29, 2017.</p>
<p class="Reference">Twain, Mark. 1885. <em>Adventures of Huckleberry Finn (Tom Sawyer’s Comrade)</em>. Charles L. Webster and Company. Available at <a href="https://www.gutenberg.org/ebooks/32325" class="LinkURL">https://www.gutenberg.org/ebooks/32325</a>. </p>
<p class="Reference">Wikipedia. 2020. “Letter Frequency.” Wikipedia. Last modified August 31, 2020. <a href="https://en.wikipedia.org/wiki/Letter_frequency" class="LinkURL">https://en.wikipedia.org/wiki/Letter_frequency</a>.</p>
<h2 id="h1-500723b01-0007">Chapter 7</h2>
<p class="Reference">Aggarwal, Charu C., Alexander Hinneburg, and Daniel A. Keim. 2001. “On the Surprising Behavior of Distance Metrics in High Dimensional Space.” Conference paper presented at the International Conference on Database Theory 2001 in London, UK, January 4–6, 2001. <a href="https://bib.dbvis.de/uploadedFiles/155.pdf" class="LinkURL">https://bib.dbvis.de/uploadedFiles/155.pdf</a>.</p>
<p class="Reference">Arcuri, Lauren. 2019. “How to Candle an Egg.” <em>The Spruce</em> (blog). April 20, 2019. <a href="https://www.thespruce.com/definition-of-candling-3016955" class="LinkURL">https://www.thespruce.com/definition-of-candling-3016955</a>.</p>
<p class="Reference">Bellman, Richard Ernest. 1957. <em>Dynamic Programming</em>. Princeton, NJ: Princeton University Press.</p>
<p class="Reference">Domingos, Pedro. 2012. “A Few Useful Things to Know About Machine Learning.” <em>Communications of the ACM</em> 55, no. 10 (October). <a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf" class="LinkURL">https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf</a>. </p>
<p class="Reference">Hughes, G. F. 1968. “On the Mean Accuracy of Statistical Pattern Recognizers.” <em>IEEE Transactions on Information Theory</em> 14, no. 1: 55–63.</p>
<p class="Reference">Nebraska Extension. 2017. “Candling Eggs.” Nebraska Extension in Lancaster County, University of Nebraska-Lincoln. <a href="http://lancaster.unl.edu/4h/embryology/candling" class="LinkURL">http://lancaster.unl.edu/4h/embryology/candling</a>.</p>
<p class="Reference">Numberphile. 2017. “Strange Spheres in Higher Dimensions.” YouTube. September 18, 2017. <a href="https://www.youtube.com/watch?v=mceaM2_zQd8" class="LinkURL">https://www.youtube.com/watch?v=mceaM2_zQd8</a>.</p>
<p class="Reference">Spruyt, Vincent. 2014. “The Curse of Dimensionality.” <em>Computer Vision for Dummies</em> (blog). April 16, 2014. <a href="http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/" class="LinkURL">http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/</a>.</p>
<h2 id="h1-500723b01-0008">Chapter 8</h2>
<p class="Reference">Muehlhauser, Luke. 2011. “Machine Learning and Unintended Consequences.” <em>LessWrong</em> (blog). September 22, 2011. <a href="http://lesswrong.com/lw/7qz/machine_learning_and_unintended_consequences/" class="LinkURL">http://lesswrong.com/lw/7qz/machine_learning_and_unintended_consequences/</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="697" id="Page_697"/>Schneider, Jeff, and Andrew W. Moore. 1997. “Cross Validation.” In <em>A Locally Weighted Learning Tutorial Using Vizier 1.0.</em> Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA, February 1, 1997. <a href="https://www.cs.cmu.edu/~schneide/tut5/node42.html" class="LinkURL">https://www.cs.cmu.edu/~schneide/tut5/node42.html</a>.</p>
<h2 id="h1-500723b01-0009">Chapter 9</h2>
<p class="Reference">Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. New York: Springer-Verlag.</p>
<p class="Reference">Bullinaria, John A. 2015. “Bias and Variance, Under-Fitting and Over-Fitting.” <em>Neural Computation, Lecture 9</em> (lecture notes), University of Birmingham, United Kingdom. <a href="http://www.cs.bham.ac.uk/~jxb/INC/l9.pdf" class="LinkURL">http://www.cs.bham.ac.uk/~jxb/INC/l9.pdf</a>.</p>
<p class="Reference">Domke, Justin. 2008. “Why Does Regularization Work?” <em>Justin Domke’s Weblog</em> (blog). December 12, 2008. <a href="https://justindomke.wordpress.com/2008/12/12/why-does-regularization-work/" class="LinkURL">https://justindomke.wordpress.com/2008/12/12/why-does-regularization-work/</a>.</p>
<p class="Reference">Domingos, Pedro. 2015. <em>The Master Algorithm</em>. New York: Basic Books.</p>
<p class="Reference">Foer, Joshua. 2012. <em>Moonwalking with Einstein: The Art and Science of Remembering Everything</em>. New York: Penguin Books.</p>
<p class="Reference">Macskassy, Sofus A. 2008. “Machine Learning (CS 567) Notes.” (PowerPoint presentation, Bias-Variance. Fall 2008. <a href="http://www-scf.usc.edu/~csci567/17-18-bias-variance.pdf" class="LinkURL">http://www-scf.usc.edu/~csci567/17-18-bias-variance.pdf</a>.</p>
<p class="Reference">Proctor, Philip, and Peter Bergman. 1978. “Brainduster Memory School,” from <em>Give Us A Break</em>, Mercury Records. <a href="https://www.youtube.com/watch?v=PD2Uh_TJ9X0" class="LinkURL">https://www.youtube.com/watch?v=PD2Uh_TJ9X0</a>.</p>
<h2 id="h1-500723b01-0010">Chapter 10</h2>
<p class="Reference">Centers for Disease Control and Prevention. 2020. “Body Mass Index (BMI).” CDC.gov. June 30, 2020. <a href="https://www.cdc.gov/healthyweight/assessing/bmi/" class="LinkURL">https://www.cdc.gov/healthyweight/assessing/bmi/</a>.</p>
<p class="Reference">Crayola. 2020. “What Were the Original Eight (8) Colors in the 1903 Box of Crayola Crayons?” Accessed on September 10, 2020. <a href="http://www.crayola.com/faq/your-history/what-were-the-original-eight-8-colors-in-the-1903-box-of-crayola-crayons/" class="LinkURL">http://www.crayola.com/faq/your-history/what-were-the-original-eight-8-colors-in-the-1903-box-of-crayola-crayons/</a>.</p>
<p class="Reference">Turk, Matthew, and Alex Pentland. 1991. “Eigenfaces for Recognition.” <em>Journal of Cognitive Neuroscience</em> 3, no. 1. <a href="http://www.face-rec.org/algorithms/pca/jcn.pdf" class="LinkURL">http://www.face-rec.org/algorithms/pca/jcn.pdf</a>.</p>
<h2 id="h1-500723b01-0011">Chapter 11</h2>
<p class="Reference">Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. New York: Springer.</p>
<p class="Reference">Raschka, Sebastian. 2015. <em>Python Machine Learning</em>. Birmingham, UK: Packt Publishing.</p>
<p class="Reference">Steinwart, Ingo, and Andreas Christmann. 2008. <em>Support Vector Machines</em>. New York: Springer.</p>
<p class="Reference">VanderPlas, Jake. 2016. <em>Python Data Science Handbook</em>. Sebastopol, CA: O’Reilly.</p>
<h2 id="h1-500723b01-0012">Chapter 12</h2>
<p class="Reference">Bonab, Hamed, R., and Fazli Can. 2016. “A Theoretical Framework on the Ideal Number of Classifiers for Online Ensembles in Data Streams.” In <em>Proceedings of the 25th ACM International Conference on Information and Knowledge Management (CIKM), 2016</em> (October): 2053–56.</p>
<p class="Reference"><span epub:type="pagebreak" title="698" id="Page_698"/>Ceruzzi, Paul. 2015. “Apollo Guidance Computer and the First Silicon Chips.” Smithsonian National Air and Space Museum, October 14, 2015. <a href="https://airandspace.si.edu/stories/editorial/apollo-guidance-computer-and-first-silicon-chips" class="LinkURL">https://airandspace.si.edu/stories/editorial/apollo-guidance-computer-and-first-silicon-chips</a>.</p>
<p class="Reference">Freund, Y., and R. E. Schapire. 1997. “A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting.” <em>Journal of Computer and System Sciences</em> 55 (1): 119–39. </p>
<p class="Reference">Fumera, Giorgio, Roli Fabio, and Serrau Alessandra. 2008. “A Theoretical Analysis of Bagging as a Linear Combination of Classifiers.” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 30, no. 7: 1293–99.</p>
<p class="Reference">Kak, Avinash. 2017. “Decision Trees: How to Construct Them and How to Use Them for Classifying New Data.” RVL Tutorial Presentation at Purdue University, August 28, 2017. <a href="https://engineering.purdue.edu/kak/Tutorials/DecisionTreeClassifiers.pdf" class="LinkURL">https://engineering.purdue.edu/kak/Tutorials/DecisionTreeClassifiers.pdf</a>.</p>
<p class="Reference">RangeVoting.org. 2020. “Glossary of Voting-Related Terms.” Accessed on September 16, 2020. <a href="http://rangevoting.org/Glossary.html" class="LinkURL">http://rangevoting.org/Glossary.html</a>. </p>
<p class="Reference">Schapire, Robert E., and Yoav Freund. 2012. <em>Boosting Foundations and Algorithms</em>. Cambridge, MA: MIT Press.</p>
<p class="Reference">Schapire, Robert E. 2013. “Explaining Adaboost.” in <em>Empirical Inference: Festschrift in Honor of Vladimir N. Vapnik</em>. Berlin, Germany: Springer-Verlag. Available at <a href="http://rob.schapire.net/papers/explaining-adaboost.pdf" class="LinkURL">http://rob.schapire.net/papers/explaining-adaboost.pdf</a>.</p>
<h2 id="h1-500723b01-0013">Chapter 13</h2>
<p class="Reference">Clevert, Djork-Arné, Thomas Unterthiner, and Sepp Hochreiter. 2016. “Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).” Cornell University, Computer Science, arXiv:1511.07289, February 22, 2016. <a href="https://arxiv.org/abs/1511.07289" class="LinkURL">https://arxiv.org/abs/1511.07289</a>.</p>
<p class="Reference">Estebon, Michele D. 1997. “Perceptrons: An Associative Learning Network,” Virginia Institute of Technology. <a href="http://ei.cs.vt.edu/~history/Perceptrons.Estebon.html" class="LinkURL">http://ei.cs.vt.edu/~history/Perceptrons.Estebon.html</a>.</p>
<p class="Reference">Furber, Steve. 2012. “Low-Power Chips to Model a Billion Neurons.” <em>IEEE Spectrum</em> (July 31). <a href="http://spectrum.ieee.org/computing/hardware/lowpower-chips-to-model-a-billion-neurons" class="LinkURL">http://spectrum.ieee.org/computing/hardware/lowpower-chips-to-model-a-billion-neurons</a>.</p>
<p class="Reference">Glorot, Xavier, and Yoshua Bengio. 2010. “Understanding the Difficulty of Training Deep Feedforward Neural Networks,” In <em>Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS), 2010</em> (Chia Laguna Resort, Sardinia, Italy): 249–56. <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf" class="LinkURL">http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf</a>.</p>
<p class="Reference">Goldberg, Joseph. 2015. “How Different Antidepressants Work.” WebMD Medical Reference (August). <a href="http://www.webmd.com/depression/how-different-antidepressants-work" class="LinkURL">http://www.webmd.com/depression/how-different-antidepressants-work</a>. </p>
<p class="Reference">Goodfellow, Ian J., David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. 2013. “Maxout Networks.” In <em>Proceedings of the 30th International Conference on Machine Learning (PMLR)</em> 28, no. 3: 1319–27. <a href="http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf" class="LinkURL">http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf</a>.</p>
<p class="Reference">He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.” Cornell University, Computer Science, arXiv:1502.01852, February 6, 2015. <a href="https://arxiv.org/abs/1502.01852" class="LinkURL">https://arxiv.org/abs/1502.01852</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="699" id="Page_699"/>Julien, Robert M. 2011. <em>A Primer of Drug Action</em>, 12th ed. New York: Worth Publishers.</p>
<p class="Reference">Khanna, Asrushi. 2018. “Cells of the Nervous System.” <em>Teach Me Physiology</em> (blog). Last modified August 2, 2018. <a href="https://teachmephysiology.com/nervous-system/components/cells-nervous-system" class="LinkURL">https://teachmephysiology.com/nervous-system/components/cells-nervous-system</a>.</p>
<p class="Reference">Kuphaldt, Tony R. 2017. “Introduction to Diodes and Rectifiers, Chapter 3 - Diodes and Rectifiers.” In <em>Lessons in Electric Circuits, Volume III. All About Circuits</em>. Accessed on September 18, 2020. <a href="https://www.allaboutcircuits.com/textbook/semiconductors/chpt-3/introduction-to-diodes-and-rectifiers/" class="LinkURL">https://www.allaboutcircuits.com/textbook/semiconductors/chpt-3/introduction-to-diodes-and-rectifiers/</a>.</p>
<p class="Reference">LeCun, Yann, Leon Bottou, Genevieve B. Orr, and Klaus-Rober Müller. 1998. “Efficient BackProp.” In <em>Neural Networks: Tricks of the Trade</em>, edited by Grégoire Montavon, Genevieve B. Orr, and Klaus-Rober Müller. Berlin: Springer-Verlag. 9–48. <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" class="LinkURL">http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf</a>.</p>
<p class="Reference">Limmer, Steffen, and Slawomir Stanczak. 2017. “Optimal Deep Neural Networks for Sparse Recovery via Laplace Techniques.” Cornell University, Computer Science, arXiv:1709.01112, September 26, 2017. <a href="https://arxiv.org/abs/1709.01112" class="LinkURL">https://arxiv.org/abs/1709.01112</a>.</p>
<p class="Reference">Lodish, Harvey, Arnold Berk, S. Lawrence Zipursky, Paul Matsudaira, David Baltimore, and James Darnell. 2000. <em>Molecular Cell Biology</em>, 4th edition. New York: W. H. Freeman; 2000. <a href="http://www.ncbi.nlm.nih.gov/books/NBK21535/" class="LinkURL">http://www.ncbi.nlm.nih.gov/books/NBK21535/</a>.</p>
<p class="Reference">McCulloch, Warren S., and Walter Pitts. 1943. “A Logical Calculus of the Ideas Immanent in Nervous Activity.” <em>Bulletin of Mathematical Biophysics</em> 5, no. 1/2: 115–33. <a href="http://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf" class="LinkURL">http://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf</a>.</p>
<p class="Reference">Meunier, David, Renaud Lambiotte, Alex Fornito, Karen D. Ersche, and Edward T. Bullmore. 2009. “Hierarchical Modularity in Human Brain Functional Networks.” <em>Frontiers in Neuroinformatics</em> (October 30). <a href="https://www.frontiersin.org/articles/10.3389/neuro.11.037.2009/full" class="LinkURL">https://www.frontiersin.org/articles/10.3389/neuro.11.037.2009/full</a>.</p>
<p class="Reference">Minsky, Martin, and Seymour Papert. 1969. <em>Perceptrons: An Introduction to Computational Geometry</em>. Cambridge, MA: MIT Press.</p>
<p class="Reference">Oppenheim, Alan V., and S. Hamid Nawab. 1996. <em>Signals and Systems</em>, 2nd ed. Upper Saddle River, NJ: Prentice Hall.</p>
<p class="Reference">Purves, Dale, George J. Augustine, David Fitzpatrick, Lawrence C. Katz, Anthony-Samuel LaMantia, Jomes O. McNamara, and S. Mark Williams. 2001<em>. Neuroscience</em>, 2nd edition, Sunderland, MA: Sinauer Associates. <a href="http://www.ncbi.nlm.nih.gov/books/NBK11117/" class="LinkURL">http://www.ncbi.nlm.nih.gov/books/NBK11117/</a>.</p>
<p class="Reference">Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. 2017. “Swish: A Self-Gated Activation Function.” Cornell University, Computer Science, arXiv:1710.05941, October 27, 2017. <a href="https://arxiv.org/abs/1710.05941" class="LinkURL">https://arxiv.org/abs/1710.05941</a>.</p>
<p class="Reference">Rosenblatt, Frank. 1962. <em>Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms</em>. Washington, DC: Spartan.</p>
<p class="Reference">Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams, 1986. “Learning Representations by Back-Propagating Errors.” <em>Nature</em> 323, no. 9: 533–36. <a href="https://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf" class="LinkURL">https://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf</a>.</p>
<p class="Reference">Serre, Thomas. 2014. “Hierarchical Models of the Visual System.” (Research notes), Cognitive Linguistic and Psychological Sciences Department, Brain Institute for Brain Sciences, Brown University. <a href="https://serre-lab.clps.brown.edu/wp-content/uploads/2014/10/Serre-encyclopedia_revised.pdf" class="LinkURL">https://serre-lab.clps.brown.edu/wp-content/uploads/2014/10/Serre-encyclopedia_revised.pdf</a>.</p>
<p class="Reference">Seung, Sebastian. 2013. <em>Connectome: How the Brain’s Wiring Makes Us Who We Are</em>. Boston: Mariner Books.</p>
<p class="Reference"><span epub:type="pagebreak" title="700" id="Page_700"/>Sitzmann, Vincent, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. 2020. “Implicit Neural Representations with Periodic Activation Functions.” Cornell University, Computer Science, arXiv:2006.09661, June 17, 2020. <a href="https://arxiv.org/abs/2006.09661" class="LinkURL">https://arxiv.org/abs/2006.09661</a>.</p>
<p class="Reference">Sporns, Olaf, Giulio Tononi, and Rolf Kötter. 2005. “The Human Connectome: A Structural Description of the Human Brain.” <em>PLoS Computational Biology</em> 1 no. 4 (September 2005). <a href="http://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.0010042&amp;type=printable" class="LinkURL">http://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.0010042&amp;type=printable</a>.</p>
<p class="Reference">Timmer, John. 2014. “IBM Researchers Make a Chip Full of Artificial Neurons.” <em>Ars Technica</em>. August 7, 2014. <a href="https://arstechnica.com/science/2014/08/ibm-researchers-make-a-chip-full-of-artificial-neurons/" class="LinkURL">https://arstechnica.com/science/2014/08/ibm-researchers-make-a-chip-full-of-artificial-neurons/</a>.</p>
<p class="Reference">Trudeau, Richard J. 1994. <em>Introduction to Graph Theory</em>, 2nd ed. Garden City, NY: Dover Books on Mathematics.</p>
<p class="Reference">Wikipedia. 2020a. “History of Artificial Intelligence.” Last modified September 4, 2020. <a href="https://en.wikipedia.org/wiki/History_of_artificial_intelligence" class="LinkURL">https://en.wikipedia.org/wiki/History_of_artificial_intelligence</a>.</p>
<p class="Reference">Wikipedia. 2020b. “Neuron.” Last modified September 11, 2020. <a href="https://en.wikipedia.org/wiki/Neuron" class="LinkURL">https://en.wikipedia.org/wiki/Neuron</a>.</p>
<p class="Reference">Wikipedia. 2020c. “Perceptron.” Last modified August 28, 2020. <a href="https://en.wikipedia.org/wiki/Perceptron" class="LinkURL">https://en.wikipedia.org/wiki/Perceptron</a>.</p>
<h2 id="h1-500723b01-0014">Chapter 14</h2>
<p class="Reference">Dauphin, Yann, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. 2014. “Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization.” Cornell University, Computer Science, arXiv:1406.2572, June 10, 2014. <a href="http://arxiv.org/abs/1406.2572" class="LinkURL">http://arxiv.org/abs/1406.2572</a>.</p>
<p class="Reference">Fullér, Robert. 2010. “The Delta Learning Rule Tutorial.” Institute for Advanced Management Systems Research, Department of Information Technologies, Åbo Adademi University, November 4, 2010. <a href="http://uni-obuda.hu/users/fuller.robert/delta.pdf" class="LinkURL">http://uni-obuda.hu/users/fuller.robert/delta.pdf</a>.</p>
<p class="Reference">NASA. 2012. “Astronomers Predict Titanic Collision: Milky Way vs. Andromeda.” <em>NASA Science Blog</em>, Production editor Dr. Tony Phillips, May 31, 2012. <a href="https://science.nasa.gov/science-news/science-at-nasa/2012/31may_andromeda" class="LinkURL">https://science.nasa.gov/science-news/science-at-nasa/2012/31may_andromeda</a>.</p>
<p class="Reference">Nielsen, Michael A. 2015. “Using Neural Networks to Recognize Handwritten Digits.” In <em>Neural Networks and Deep Learning</em>. Determination Press. Available at <a href="http://neuralnetworksanddeeplearning.com/chap1.html" class="LinkURL">http://neuralnetworksanddeeplearning.com/chap1.html</a>.</p>
<p class="Reference">Pyle, Katherine. 1918. <em>Mother’s Nursery Tales</em>. New York: E. F. Dutton &amp; Company. Available at <a href="https://www.gutenberg.org/files/49001/49001-h/49001-h.htm#Page_207" class="LinkURL">https://www.gutenberg.org/files/49001/49001-h/49001-h.htm#Page_207</a>.</p>
<p class="Reference">Quote Investigator, 2020. “You Just Chip Away Everything That Doesn’t Look Like David.” Quote Investigator: Tracing Quotations. Accessed October 26, 2020. <a href="https://quoteinvestigator.com/tag/michelangelo/" class="LinkURL">https://quoteinvestigator.com/tag/michelangelo/</a>.</p>
<p class="Reference">Seung, Sebastian. 2005. “Introduction to Neural Networks.” 9.641J course notes, Spring 2005. MITOpenCourseware, Massachusetts Institute of Technology. <a href="https://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-641j-introduction-to-neural-networks-spring-2005/" class="LinkURL">https://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-641j-introduction-to-neural-networks-spring-2005/</a>.<em> </em></p>
<h2 id="h1-500723b01-0015">Chapter 15</h2>
<p class="Reference">Bengio, Yoshua. 2012. “Practical Recommendations for Gradient-Based Training of Deep Architectures.” Cornell University, Computer Science, arXiv:1206.5533. <a href="https://arxiv.org/abs/1206.5533v2" class="LinkURL">https://arxiv.org/abs/1206.5533v2</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="701" id="Page_701"/>Darken, C., J. Chang, and J. Moody. 1992. “Learning Rate Schedules for Faster Stochastic Gradient Search.” In <em>Neural Networks for Signal Processing II, Proceedings of the 1992 IEEE Workshop</em> (September): 1–11.</p>
<p class="Reference">Dauphin, Y., R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. 2014. “Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-convex Optimization.” Cornell University, Computer Science, arXiv:1406.2572, June 10, 2014. <a href="http://arxiv.org/abs/1406.2572" class="LinkURL">http://arxiv.org/abs/1406.2572</a>.</p>
<p class="Reference">Duchi, John, Elad Hazan, and Yoram Singer. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” <em>Journal of Machine Learning Research</em> 12, no. 61: 2121–59. <a href="http://jmlr.org/papers/v12/duchi11a.html" class="LinkURL">http://jmlr.org/papers/v12/duchi11a.html</a>.</p>
<p class="Reference">Hinton, Geoffrey, Nitish Srivastava, and Kevin Swersky. 2015. “Neural Networks for Machine Learning: Lecture 6a, Overview of Mini-Batch Gradient Descent.” (Lecture slides). University of Toronto, Computer Science. https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf.</p>
<p class="Reference">Ioffe, Sergey, and Christian Szegedy. 2015. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” Cornell University, Computer Science, arXiv:1502.03167, March 2, 2015. <a href="https://arxiv.org/abs/1502.03167" class="LinkURL">https://arxiv.org/abs/1502.03167</a>.</p>
<p class="Reference">Karpathy, Andrej. 2016. “Neural Networks Part 3: Learning and Evaluation.” (Course notes for Stanford CS231n.) Stanford CA: Stanford University. <a href="http://cs231n.github.io/neural-networks-2/" class="LinkURL">http://cs231n.github.io/neural-networks-2/</a>.</p>
<p class="Reference">Kingma, Diederik. P., and Jimmy L. Ba. 2015. “Adam: A Method for Stochastic Optimization.” Conference paper for the 3rd International Conference on Learning Representations (San Diego, CA, May 7–9): 1–13.</p>
<p class="Reference">Nesterov, Y. 1983. “A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence o(1/k2).” <em>Doklady ANSSSR</em> (trans. as <em>Soviet Mathematics: Doclady</em>) 269: 543–47.</p>
<p class="Reference">Orr, Genevieve. 1999a. “Learning Rate Adaptation.” In <em>CS-449: Neural Networks</em>. (Course notes.) Salem, OR: Willamette University. <a href="https://www.willamette.edu/~gorr/classes/cs449/intro.html" class="LinkURL">https://www.willamette.edu/~gorr/classes/cs449/intro.html</a>.</p>
<p class="Reference">Orr, Genevieve. 1999b. “Momentum.” In <em>CS-449: Neural Networks</em>. (Course notes.) Salem, OR: Willamette University. <a href="https://www.willamette.edu/~gorr/classes/cs449/intro.html" class="LinkURL">https://www.willamette.edu/~gorr/classes/cs449/intro.html</a>.</p>
<p class="Reference">Qian, N. 1999. “On the Momentum Term in Gradient Descent Learning Algorithms.” <em>Neural Networks</em> 12(1): 145–51. <a href="http://www.columbia.edu/~nq6/publications/momentum.pdf" class="LinkURL">http://www.columbia.edu/~nq6/publications/momentum.pdf</a>.</p>
<p class="Reference">Ruder, Sebastian. 2017. “An Overview of Gradient Descent Optimization Algorithms.” Cornell University, Computer Sciences, arXiv:1609.04747. June 15, 2017. <a href="https://arxiv.org/abs/1609.04747" class="LinkURL">https://arxiv.org/abs/1609.04747</a>.</p>
<p class="Reference">Srivasta, Nitish, Geoffrey Hinton, Alex Krizhevsky, and Ilya Sutskever. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” <em>Journal of Machine Learning Research</em> 15 (2014): 1929–58. <a href="https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" class="LinkURL">https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf</a>.</p>
<p class="Reference">Wolpert, David H. 1996. “The Lack of A Priori Distinctions Between Learning Theorems.” <em>Neural Computation</em> 8, 1341–90. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.9734&amp;rep=rep1&amp;type=pdf" class="LinkURL">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.9734&amp;rep=rep1&amp;type=pdf</a>.</p>
<p class="Reference">Wolpert, D. H., and W. G. Macready. 1997. “No Free Lunch Theorems for Optimization.” <em>IEEE Transactions on Evolutionary Computation</em> 1, no. 1: 67–82. <a href="https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf" class="LinkURL">https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="702" id="Page_702"/>Zeiler, Matthew D. 2012. “ADADELTA: An Adaptive Learning Rate Method.” Cornell University, Computer Science, arXiv:1212.5701. <a href="http://arxiv.org/abs/1212.5701" class="LinkURL">http://arxiv.org/abs/1212.5701</a>.</p>
<h2 id="h1-500723b01-0016">Chapter 16</h2>
<p class="Reference">Aitken, Andrew, Christian Ledig, Lucas Theis, Jose Caballero, Zehan Wang, and Wenzhe Shi. 2017. “Checkerboard Artifact Free Sub-pixel Convolution: A Note on Sub-pixel Convolution, Resize Convolution and Convolution.” Cornell University, Computer Science, arXiv:1707.02937, July 10, 2017. <a href="https://arxiv.org/abs/1707.02937" class="LinkURL">https://arxiv.org/abs/1707.02937</a>.</p>
<p class="Reference">Britz, Denny. 2015. “Understanding Convolutional Neural Networks for NLP,” <em>WildML</em> (blog), November 7, 2015. <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" class="LinkURL">http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/</a>.</p>
<p class="Reference">Canziani, Alfredo, Adam Paszke, and Eugenio Culurciello. 2017. “An Analysis of Deep Neural Network Models for Practical Applications.” Cornell University, Computer Science, ArXiv:1605.07678, April 4, 2016. <a href="https://arxiv.org/abs/1605.07678" class="LinkURL">https://arxiv.org/abs/1605.07678</a>.</p>
<p class="Reference">Culurciello, Eugenio. 2017. “Neural Network Architectures.” <em>Medium: Towards Data Science</em>, March 23, 2017. <a href="https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba" class="LinkURL">https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba</a>. </p>
<p class="Reference">Dumoulin, Vincent, and Francesco Visin. 2018. “A Guide to Convolution Arithmetic for Deep Learning.” Cornell University, Computer Science, arXiv:1603.07285, January 11, 2018. <a href="https://arxiv.org/abs/1603.07285" class="LinkURL">https://arxiv.org/abs/1603.07285</a>.</p>
<p class="Reference">Esteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, and Sebastian Thrun. 2017. “Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks.” <em>Nature</em> 542 (February 2): 115–18. <a href="http://cs.stanford.edu/people/esteva/nature/" class="LinkURL">http://cs.stanford.edu/people/esteva/nature/</a>.</p>
<p class="Reference">Ewert, J. P. 1985. “Concepts in Vertebrate Neuroethology.” <em>Animal Behaviour</em> 33, no. 1 (February): 1–29.</p>
<p class="Reference">Glorot, Xavier, and Yoshua Bengio. 2010. “Understanding the Difficulty of Training Deep Feedforward Neural Networks.” In <em>Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</em> (Sardinia, Italy, May 13–15): 249–56. <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf" class="LinkURL">http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf</a>.</p>
<p class="Reference">He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.” Cornell University, Computer Science, arXiv:1502.01852, February 6, 2015. <a href="https://arxiv.org/abs/1502.01852v1" class="LinkURL">https://arxiv.org/abs/1502.01852v1</a>.</p>
<p class="Reference">Kalchbrenner, Nal, Edward Grefenstette, and Phil Blunsom. 2014. “A Convolutional Neural Network for Modelling Sentences.” Cornell University, Computer Science, arXiv:1404.2188v1, April 8, 2014. <a href="https://arxiv.org/abs/1404.2188" class="LinkURL">https://arxiv.org/abs/1404.2188</a>.</p>
<p class="Reference">Karpathy, Andrej. 2016. “Optimization: Stochastic Gradient Descent.” Course notes for Stanford CS231n, Stanford, CA: Stanford University. <a href="http://cs231n.github.io/neural-networks-2/" class="LinkURL">http://cs231n.github.io/neural-networks-2/</a>.</p>
<p class="Reference">Kim, Yoon. 2014. “Convolutional Neural Networks for Sentence Classification.” Cornell University, Computer Science, arXiv:1408.5882, September 3, 2014. <a href="https://arxiv.org/abs/1408.5882" class="LinkURL">https://arxiv.org/abs/1408.5882</a>.</p>
<p class="Reference">Levi, Gil, and Tal Hassner. 2015. “Age and Gender Classification Using Convolutional Neural Networks.” IEEE Workshop on Analysis and Modeling of <span epub:type="pagebreak" title="703" id="Page_703"/>Faces and Gestures (AMFG), at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (Boston, June). <a href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/" class="LinkURL">http://www.openu.ac.il/home/hassner/projects/cnn_agegender/</a>.</p>
<p class="Reference">Lin, Min, Qiang Chen, and Shuicheng Yan. 2014. “Network in Network.” Cornell University, Computer Science, arXiv:1312-4400v3, March 4, 2014. <a href="https://arxiv.org/abs/1312.4400v3" class="LinkURL">https://arxiv.org/abs/1312.4400v3</a>.</p>
<p class="Reference">Mao, Xiao-Jiao, Chinhua Shen, and Yu-Bin Yang. 2016. “Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections.” Cornell University, Computer Science, arXiv:16.06.08921v3, August 30, 2016. <a href="https://arxiv.org/abs/1606.08921" class="LinkURL">https://arxiv.org/abs/1606.08921</a>.</p>
<p class="Reference">Memmott, Mark. “Do You Suffer from RAS Syndrome?” <em>‘Memmos’: Memmott’s Missives and Musings</em>, NPR, 2015. <a href="https://www.npr.org/sections/memmos/2015/01/06/605393666/do-you-suffer-from-ras-syndrome" class="LinkURL">https://www.npr.org/sections/memmos/2015/01/06/605393666/do-you-suffer-from-ras-syndrome</a>.  </p>
<p class="Reference">Odena, Augustus, Vincent Dumoulin, and Chris Olah. 2016. “Deconvolution and Checkerboard Artifacts.” <em>Distill</em>. October 17, 2016. <a href="https://distill.pub/2016/deconv-checkerboard/" class="LinkURL">https://distill.pub/2016/deconv-checkerboard/</a>.</p>
<p class="Reference">Oppenheim, Alan V., and S. Hamid Nawab. 1996. <em>Signals and Systems</em>, 2nd ed. Upper Saddle River, NJ: Prentice Hall.</p>
<p class="Reference">Quiroga, R. Quian, L. Reddy, G. Kreiman, C. Koch, and I. Fried, 2005. “Invariant Visual Representation by Single Neurons in the Human Brain.” <em>Nature</em> 435 (June 23): 1102–07. <a href="https://www.nature.com/articles/nature03687" class="LinkURL">https://www.nature.com/articles/nature03687</a>.</p>
<p class="Reference">Serre, Thomas. 2014. “Hierarchical Models of the Visual System.” In Jaeger D., Jung R. (eds), <em>Encyclopedia of Computational Neuroscience</em>. New York: Springer. <a href="https://link.springer.com/referenceworkentry/10.1007%2F978-1-4614-7320-6_345-1" class="LinkURL">https://link.springer.com/referenceworkentry/10.1007%2F978-1-4614-7320-6_345-1</a>.</p>
<p class="Reference">Snavely, Noah. “CS1114 Section 6: Convolution.” Course notes for Cornell CS1114, Introduction to Computing Using Matlab and Robotics, Ithaca, NY: Cornell University, February 27, 2013. <a href="https://www.cs.cornell.edu/courses/cs1114/2013sp/sections/S06_convolution.pdf" class="LinkURL">https://www.cs.cornell.edu/courses/cs1114/2013sp/sections/S06_convolution.pdf</a>.</p>
<p class="Reference">Springenberg, Jost Tobias, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2015. “Striving for Simplicity: The All Convolutional Net.” Cornell University, Computer Science, arXiv:1412.6806, April 13, 2015. <a href="https://arxiv.org/abs/1412.6806" class="LinkURL">https://arxiv.org/abs/1412.6806</a>.</p>
<p class="Reference">Sun, Y., X. Wang, and X. Tang. 2014. “Deep Learning Face Representation from Predicting 10,000 Classes.” Conference paper, for the <em>2014 IEEE Conference on Computer Vision and Pattern Recognition</em> (Columbus, OH, June 23–28): 1891–98.</p>
<p class="Reference">Zeiler, Matthew D., Dilip Crishnana, Graham W. Taylor, and Rob Fergus. 2010. “Deconvolutional Networks.” Conference paper for the Computer Vision and Pattern Recognition conference (June 13–18). <a href="https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf" class="LinkURL">https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf</a>.</p>
<p class="Reference">Zhang, Richard. 2019. “Making Convolutional Networks Shift-Invariant Again.” Cornell University, Computer Science, arXiv:1904.11486, June 9, 2019. <a href="https://arxiv.org/abs/1904.11486" class="LinkURL">https://arxiv.org/abs/1904.11486</a>. </p>
<h2 id="h1-500723b01-0017">Chapter 17</h2>
<p class="Reference">Chollet, François. 2017. “Keras-team/Keras.” GitHub. <a href="https://keras.io/" class="LinkURL">https://keras.io/</a>.</p>
<p class="Reference">Fei-Fei, Li, Jia Deng, Olga Russakovsky, Alex Berg, and Kai Li. 2020. “Download.” ImageNet website. Stanford Vision Lab. Stanford University/Princeton University. Accessed October 4, 2020. <a href="http://image-net.org/download" class="LinkURL">http://image-net.org/download</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="704" id="Page_704"/>ImageNet. 2020. “Results of ILSVRC2014: Classification + Localization Results.” Stanford Vision Lab. Stanford University/Princeton University. Accessed October 4, 2020. <a href="http://image-net.org/challenges/LSVRC/2014/results" class="LinkURL">http://image-net.org/challenges/LSVRC/2014/results</a>. </p>
<p class="Reference">LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. 1989. “Backpropagation Applied to Handwritten Zip Code Recognition.” <em>Neural Computing</em> 1(4): 541–51. Available at <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf" class="LinkURL">http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf</a>.</p>
<p class="Reference">Moosavi-Dezfooli, Seyed-Mohsen, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. 2017. “Universal Adversarial Perturbations.” Cornell University, Computer Science, arXiv:1610.08401, March 9, 2017. <a href="https://arxiv.org/abs/1610.08401" class="LinkURL">https://arxiv.org/abs/1610.08401</a>.</p>
<p class="Reference">Rauber, Jonas, and Wieland Brendel. 2017. “Welcome to Foolbox Native.” Foolbox. <a href="https://foolbox.readthedocs.io/en/latest" class="LinkURL">https://foolbox.readthedocs.io/en/latest</a>.</p>
<p class="Reference">Rauber, Jonas, Wieland Brendel, and Matthias Bethge. 2018. “Foolbox: A Python Toolbox to Benchmark the Robustness of Machine Learning Models.” Cornell University, Computer Science, arXiv:1707.04131, March 20, 2018. <a href="https://arxiv.org/abs/1707.04131" class="LinkURL">https://arxiv.org/abs/1707.04131</a>.</p>
<p class="Reference">Russakovsky, Olga, et al. 2015. “ImageNet Large Scale Visual Recognition Challenge.” Cornell University, Computer Science, arXiv:1409.0575, January 30, 2015. <a href="https://arxiv.org/abs/1409.0575" class="LinkURL">https://arxiv.org/abs/1409.0575</a>.</p>
<p class="Reference">Simonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” Cornell University, Computer Science, arXiv:1409.1556, April 10, 2015. <a href="https://arxiv.org/abs/1409.1556" class="LinkURL">https://arxiv.org/abs/1409.1556</a>.</p>
<p class="Reference">Zeiler, Matthew D., and Rob Fergus. 2013. “Visualizing and Understanding Convolutional Networks.” Cornell University, Computer Science, arXiv:1311.2901, November 28, 2013. <a href="https://arxiv.org/abs/1311.2901" class="LinkURL">https://arxiv.org/abs/1311.2901</a>.</p>
<h2 id="h1-500723b01-0018">Chapter 18</h2>
<p class="Reference">Altosaar, Jann. 2020. “Tutorial: What Is a Variational Autoencoder?” <em>Jaan Altosaar</em> (blog). Accessed on September 30, 2020. <a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/" class="LinkURL">https://jaan.io/what-is-variational-autoencoder-vae-tutorial/</a>.</p>
<p class="Reference">Audio Mountain. 2020. “Audio File Size Calculations.” AudioMountain.com Tech Resources. Accessed on September 30, 2020. <a href="http://www.audiomountain.com/tech/audio-file-size.html" class="LinkURL">http://www.audiomountain.com/tech/audio-file-size.html</a>.</p>
<p class="Reference">Bako, Steve, Thijs Vogels, Brian McWilliams, Mark Meyer, Jan Novák, Alex Harvill, Prdeep Sen, Tony DeRose, and Fabrice Rousselle. 2017. “Kernel-Predicting Convolutional Networks for Denoising Monte Carlo Renderings.” In <em>Proceedings of SIGGRAPH 17, ACM Transactions on Graphics</em> 36, no. 4 (Article 97). <a href="https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20170630135237/Kernel-Predicting-Convolutional-Networks-for-Denoising-Monte-Carlo-Renderings-Paper33.pdf" class="LinkURL">https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20170630135237/Kernel-Predicting-Convolutional-Networks-for-Denoising-Monte-Carlo-Renderings-Paper33.pdf</a>.</p>
<p class="Reference">Chaitanya, Chakravarty R. Alla, Anton Kaplanyan, Christoph Schied, Marco Salvi, Aaron Lefohn, Derek Nowrouzezahrai, and Timo Aila. 2017. “Interactive Reconstruction of Monte Carlo Image Sequences Using a Recurrent Denoising Autoencoder.” In <em>Proceedings of SIGGRAPH 17, ACM Transactions on Graphics</em> 36, no. 4 (July 1). <a href="http://research.nvidia.com/publication/interactive-reconstruction-monte-carlo-image-sequences-using-recurrent-denoising" class="LinkURL">http://research.nvidia.com/publication/interactive-reconstruction-monte-carlo-image-sequences-using-recurrent-denoising</a>.</p>
<p class="Reference">Chollet, François. 2017. “Building Autoencoders in Keras.” <em>The Keras Blog</em>, March 14, 2017. <a href="https://blog.keras.io/building-autoencoders-in-keras.html" class="LinkURL">https://blog.keras.io/building-autoencoders-in-keras.html</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="705" id="Page_705"/>Doersch, Carl. 2016. “Tutorial on Variational Autoencoders.” Cornell University, Statistics, arXiv:1606.05908, August 13, 2016. <a href="https://arxiv.org/abs/1606.05908" class="LinkURL">https://arxiv.org/abs/1606.05908</a>.</p>
<p class="Reference">Donahue, Jeff. 2015. “mnist_autoencoder.prototxt.” BVLC/Caffe. GitHub. February 5, 2015. <a href="https://github.com/BVLC/caffe/blob/master/examples/mnist/mnist_autoencoder.prototxt" class="LinkURL">https://github.com/BVLC/caffe/blob/master/examples/mnist/mnist_autoencoder.prototxt</a>.</p>
<p class="Reference">Dürr, Oliver. 2016. “Introduction to Variational Autoencoders.” Presentation at Datalab-Lunch Seminar Series, Winterthur, Switzerland, May 11, 2016. <a href="https://tensorchiefs.github.io/bbs/files/vae.pdf" class="LinkURL">https://tensorchiefs.github.io/bbs/files/vae.pdf</a>.</p>
<p class="Reference">Frans, Kevin. “Variational Autoencoders Explained.” (Tutorial) Kevin Frans website, August 6, 2016. <a href="http://kvfrans.com/variational-autoencoders-explained/" class="LinkURL">http://kvfrans.com/variational-autoencoders-explained/</a>.</p>
<p class="Reference">Jia, Yangqing, and Evan Shelhamer, 2020. “Caffe.” Berkeley Vision online documentation, Accessed October 1, 2020. <em>http://caffe.berkeleyvision.org/</em>.</p>
<p class="Reference">Kingma, Diederik P., and Max Welling, “Auto-Encoding Variational Bayes.” Cornell, Statistics, arXiv:1312.6114, May 1, 2014. <a href="https://arxiv.org/abs/1312.6114" class="LinkURL">https://arxiv.org/abs/1312.6114</a>.</p>
<p class="Reference">Raffel, Colin. 2019. “A Few Unusual Autoencoders.” Slideshare.net. February 24, 2019.<em> </em><a href="https://www.slideshare.net/marlessonsa/a-few-unusual-autoencoder-colin-raffel" class="LinkURL">https://www.slideshare.net/marlessonsa/a-few-unusual-autoencoder-colin-raffel</a>.</p>
<p class="Reference">Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. 2014. “Stochastic Backpropagation and Approximate Inference in Deep Generative Models.” In <em>Proceedings of the 31st International Conference on Machine Learning (ICML), JMLR: W\&amp;CP</em> 32 (May 30). <a href="https://arxiv.org/abs/1401.4082" class="LinkURL">https://arxiv.org/abs/1401.4082</a>.</p>
<p class="Reference">Wikipedia. 2020a. “JPEG.” Accessed on September 30, 2020. <a href="https://en.wikipedia.org/wiki/JPEG" class="LinkURL">https://en.wikipedia.org/wiki/JPEG</a>.</p>
<p class="Reference">Wikipedia. 2020b. “MP3.” Accessed on September 30, 2020. <a href="https://en.wikipedia.org/wiki/MP3" class="LinkURL">https://en.wikipedia.org/wiki/MP3</a>.</p>
<h2 id="h1-500723b01-0019">Chapter 19</h2>
<p class="Reference">Barrat, Robbie. 2018. “Rapping-Neural-Network.” GitHub, October 29, 2018. <a href="https://github.com/robbiebarrat/rapping-neural-network" class="LinkURL">https://github.com/robbiebarrat/rapping-neural-network</a>.</p>
<p class="Reference">Bryant, Alice. 2019. “A Simple Sentence with Seven Meanings.” <em>VOA Learning English: Everyday Grammar</em> (blog). May 16, 2019. <a href="https://learningenglish.voanews.com/a/a-simple-sentence-with-seven-meanings/4916769.html" class="LinkURL">https://learningenglish.voanews.com/a/a-simple-sentence-with-seven-meanings/4916769.html</a>.</p>
<p class="Reference">Chen, Yutian, Matthew W. Hoffman, Sergio Gómez Colmenarejo, Misha Denil, Timothy P. Lillicrap, Matt Botvinick, and Nando de Freitas. 2017. “Learning to Learn Without Gradient Descent by Gradient Descent.” Cornell University, Statistics, arXiv:1611.03824v6, June 12, 2017. <a href="https://arxiv.org/abs/1611.03824" class="LinkURL">https://arxiv.org/abs/1611.03824</a>.</p>
<p class="Reference">Chen, Qiming, and Ren Wu. 2017. “CNN Is All You Need.” Cornell University, Computer Science, arXiv:1712.09662, December 27, 2017. <a href="https://arxiv.org/abs/1712.09662" class="LinkURL">https://arxiv.org/abs/1712.09662</a>.</p>
<p class="Reference">Chollet, Francois. 2017. “A Ten-Minute Introduction to Sequence-to-Sequence Learning in Keras.” <em>The Keras Blog</em>, September 29, 2017. <a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" class="LinkURL">https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html</a>.</p>
<p class="Reference">Chu, Hang, Raquel Urtasun, and Sanja Fidler. 2016. “Song from PI: A Musically Plausible Network for Pop Music Generation.” Cornell University, Computer Science, arXiv:1611.03477, November 10, 2016. <a href="https://arxiv.org/abs/1611.03477" class="LinkURL">https://arxiv.org/abs/1611.03477</a>.</p>
<p class="Reference">Deutsch, Max. 2016a. “Harry Potter: Written by Artificial Intelligence.” <em>Deep Writing</em> (blog), Medium. July 8, 2016. <a href="https://medium.com/deep-writing/harry-potter-written-by-artificial-intelligence-8a9431803da6" class="LinkURL">https://medium.com/deep-writing/harry-potter-written-by-artificial-intelligence-8a9431803da6</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="706" id="Page_706"/>Deutsch, Max. 2016b. “Silicon Valley: A New Episode Written by AI.” <em>Deep Writing</em> (blog), Medium, July 11, 2016. <a href="https://medium.com/deep-writing/silicon-valley-a-new-episode-written-by-ai-a8f832645bc2" class="LinkURL">https://medium.com/deep-writing/silicon-valley-a-new-episode-written-by-ai-a8f832645bc2</a>.</p>
<p class="Reference">Dickens, Charles. 1859. <em>A Tale of Two Cities</em>. Project Gutenberg. <a href="https://www.gutenberg.org/ebooks/98" class="LinkURL">https://www.gutenberg.org/ebooks/98</a>.</p>
<p class="Reference">Dictionary.com. 2020. “How Many Words Are There in the English Language?” Accessed October 29, 2020. <a href="https://www.dictionary.com/e/how-many-words-in-english/" class="LinkURL">https://www.dictionary.com/e/how-many-words-in-english/</a>.</p>
<p class="Reference">Doyle, Arthur Conan. 1892. <em>The Adventures of Sherlock Holmes</em>. Project Gutenberg. <a href="https://www.gutenberg.org/files/1661/1661-0.txt" class="LinkURL">https://www.gutenberg.org/files/1661/1661-0.txt</a>.</p>
<p class="Reference">Full Fact. 2020. “Automated Fact Checking.” Accessed October 29, 2020. <a href="https://fullfact.org/automated" class="LinkURL">https://fullfact.org/automated</a>.</p>
<p class="Reference">Geitgey, Adam. 2016. “Machine Learning Is Fun Part 6: How to Do Speech Recognition with Deep Learning.” Medium. December 23, 2016. <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a" class="LinkURL">https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a</a>.</p>
<p class="Reference">Google. “Google Translate.” 2020. <a href="https://translate.google.com/" class="LinkURL">https://translate.google.com/</a>.</p>
<p class="Reference">Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton, “Speech Recognition with Deep Recurrent Neural Networks.” <em>2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, Vancouver, BC, Canada, May 26–31. <a href="https://www.cs.toronto.edu/~fritz/absps/RNN13.pdf" class="LinkURL">https://www.cs.toronto.edu/~fritz/absps/RNN13.pdf</a>.</p>
<p class="Reference">Heerman, Victor, dir. 1930. <em>Animal Crackers</em>. Written by George S. Kaufman, Morrie Ryskind, Bert Kalmar, and Harry Ruby. Paramount Studios. <a href="https://www.imdb.com/title/tt0020640/" class="LinkURL">https://www.imdb.com/title/tt0020640/</a>.</p>
<p class="Reference">Hochreiter, Sepp, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. 2001. “Gradient Flow in Recurrent Nets: The Difficulty of Learning Long-Term Dependencies.” in S. C. Kremer and J. F. Kolen, eds.<em> A Field Guide to Dynamical Recurrent Neural Networks</em>. New York: IEEE Press. <a href="https://www.bioinf.jku.at/publications/older/ch7.pdf" class="LinkURL">https://www.bioinf.jku.at/publications/older/ch7.pdf</a>.</p>
<p class="Reference">Hughes, John. 2020. “English-to-Dutch Neural Machine Translation via Seq2Seq Architecture.” GitHub. Accessed October 29, 2020. <a href="https://colab.research.google.com/github/hughes28/Seq2SeqNeuralMachineTranslator/blob/master/Seq2SeqEnglishtoDutchTranslation.ipynb#scrollTo=8q4ESVzKJgHd" class="LinkURL">https://colab.research.google.com/github/hughes28/Seq2SeqNeuralMachineTranslator/blob/master/Seq2SeqEnglishtoDutchTranslation.ipynb#scrollTo=8q4ESVzKJgHd</a>.</p>
<p class="Reference">Johnson, Daniel. 2015. “Composing Music with Recurrent Neural Networks.” <em>Daniel D. Johnson</em> (blog). August 3, 2015. <a href="https://www.danieldjohnson.com/2015/08/03/composing-music-with-recurrent-neural-networks/" class="LinkURL">https://www.danieldjohnson.com/2015/08/03/composing-music-with-recurrent-neural-networks/</a>.</p>
<p class="Reference">Jurafsky, Dan. 2020. “Language Modeling: Introducing N-grams.” Class notes, Stanford University, Winter 2020. <a href="https://web.stanford.edu/~jurafsky/slp3/slides/LM_4.pdf" class="LinkURL">https://web.stanford.edu/~jurafsky/slp3/slides/LM_4.pdf</a>.</p>
<p class="Reference">Kaggle. 2020. “Sunspots.” Dataset, Kaggle.com. Accessed October 29, 2020. <a href="https://www.kaggle.com/robervalt/sunspots" class="LinkURL">https://www.kaggle.com/robervalt/sunspots</a>.</p>
<p class="Reference">Karim, Raimi. 2019. “Attn: Illustrated Attention.” Post in <em>Towards Data Science</em> (blog), Medium, January 20, 2019. <a href="https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3" class="LinkURL">https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3</a>.</p>
<p class="Reference">Karpathy, Andrej, and Fei-Fei Li. 2013. “Automated Image Captioning with ConvNets and Recurrent Nets.” Presentation slides, Stanford Computer Science Department, Stanford University. <a href="https://cs.stanford.edu/people/karpathy/sfmltalk.pdf" class="LinkURL">https://cs.stanford.edu/people/karpathy/sfmltalk.pdf</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="707" id="Page_707"/>Karpathy, Andrej. 2015. “The Unreasonable Effectiveness of Recurrent Neural Networks.” <em>Andrej Karpathy blog</em>, GitHub, May 21, 2015. <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" class="LinkURL">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a>.</p>
<p class="Reference">Kelly, Charles. 2020. “Tab-Delimited Bilingual Sentence Pairs.” Manythings.org. Last updated August 23, 2020. <a href="http://www.manythings.org/anki/" class="LinkURL">http://www.manythings.org/anki/</a>.</p>
<p class="Reference">Krishan. 2016. “Bollywood Lyrics via Recurrent Neural Networks.” <em>From Data to Decisions</em> (blog), December 8, 2016. <a href="https://iksinc.wordpress.com/2016/12/08/bollywood-lyrics-via-recurrent-neural-networks/" class="LinkURL">https://iksinc.wordpress.com/2016/12/08/bollywood-lyrics-via-recurrent-neural-networks/</a>.</p>
<p class="Reference">LISA Lab, 2018. “Modeling and Generating Sequences of Polyphonic Music with the RNN-RBM.” Tutorial, Deeplearning.net. Last updated June 15, 2018. <a href="http://deeplearning.net/tutorial/rnnrbm.html#rnnrbm" class="LinkURL">http://deeplearning.net/tutorial/rnnrbm.html#rnnrbm</a>.</p>
<p class="Reference">Mao, Junhua, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille. 2015. “Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN).” Cornell University, Computer Science, arXiv:1412.6632, June 11, 2015. <a href="https://arxiv.org/abs/1412.6632" class="LinkURL">https://arxiv.org/abs/1412.6632</a>.</p>
<p class="Reference">McCrae, Pat. 2018. Comment on “How Many Nouns Are There in English?” Quora. November 15, 2018. <a href="https://www.quora.com/How-many-nouns-are-there-in-English" class="LinkURL">https://www.quora.com/How-many-nouns-are-there-in-English</a>.</p>
<p class="Reference">Moocarme, Matthew. 2020. “Country Lyrics Created with Recurrent Neural Networks.” <em>Matthew Moocarme</em> (blog). Accessed October 29, 2020. <a href="http://www.mattmoocar.me/blog/RNNCountryLyrics/" class="LinkURL">http://www.mattmoocar.me/blog/RNNCountryLyrics/</a>.</p>
<p class="Reference">Mooney, Raymond J. 2019. “CS 343: Artificial Intelligence: Natural Language Processing.” Course notes, PowerPoint slides, University of Texas at Austin. <a href="http://www.cs.utexas.edu/~mooney/cs343/slides/nlp.ppt" class="LinkURL">http://www.cs.utexas.edu/~mooney/cs343/slides/nlp.ppt</a>.</p>
<p class="Reference">O’Brien, Tim, and Irán Román. 2017. “A Recurrent Neural Network for Musical Structure Processing and Expectation.” Report for CS224d: Deep Learning for Natural Language Processing, Stanford University, Winter 2017. <a href="https://cs224d.stanford.edu/reports/O%27BrienRom%C2%B4an.pdf" class="LinkURL">https://cs224d.stanford.edu/reports/O%27BrienRom%C2%B4an.pdf</a>.</p>
<p class="Reference">Olah, Christopher. 2015. “Understanding LSTM Networks.” <em>Colah’s Blog</em>, GitHub, August 27, 2015. <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="LinkURL">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>.</p>
<p class="Reference">Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. 2013. “On the Difficulty of Training Recurrent Neural Networks.” Cornell University, Computer Science, arXiv:1211.5063, February 16, 2013. <a href="https://arxiv.org/abs/1211.5063" class="LinkURL">https://arxiv.org/abs/1211.5063</a>.</p>
<p class="Reference">R2RT. 2016. “Written Memories: Understanding, Deriving and Extending the LSTM.” <em>R2RT</em> (blog), July 26, 2016. <a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html" class="LinkURL">https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html</a>.</p>
<p class="Reference">Rajpurkar, Pranav, Robin Jia, and Percy Liang. 2018. “Know What You Don’t Know: Unanswerable Questions for SQuAD.” Cornell University, Computer Science, arXiv:1806.03822, June 11, 2018. <a href="https://arxiv.org/abs/1806.03822" class="LinkURL">https://arxiv.org/abs/1806.03822</a>.</p>
<p class="Reference">Roberts, Adam, Colin Raffel, and Noam Shazeer. 2020. “How Much Knowledge Can You Pack into the Parameters of a Language Model?” Cornell University, Computer Science, arXiv:2002.08910, October 5, 2020. <a href="https://arxiv.org/abs/2002.08910" class="LinkURL">https://arxiv.org/abs/2002.08910</a>.</p>
<p class="Reference">Robertson, Sean. 2017. “NLP from Scratch: Translation with a Sequence to Sequence Network and Attention.” Tutorial, PyTorch. <a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html" class="LinkURL">https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html</a>.</p>
<p class="Reference">Schuster, Mike, and Kuldip K. Paliwal. 1997. “Bidirectional Recurrent Neural Networks.” IEEE Transactions on Signal Processing, 45, no. 11 (November). <em>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.331.9441&amp;rep=rep1&amp;type=pdf</em>.</p>
<p class="Reference"><span epub:type="pagebreak" title="708" id="Page_708"/>Sturm, Bob L. 2015a. “The Infinite Irish Trad Session.” <em>High Noon GMT</em> (blog), Folk the Algorithms, August 7, 2015. <a href="https://highnoongmt.wordpress.com/2015/08/07/the-infinite-irish-trad-session/" class="LinkURL">https://highnoongmt.wordpress.com/2015/08/07/the-infinite-irish-trad-session/</a>. </p>
<p class="Reference">Sturm, Bob L. 2015b. “‘Lisl’s Stis’: Recurrent Neural Networks for Folk Music Generation.” <em>High Noon GMT</em> (blog), Folk the Algorithms, May 22, 2015. <a href="https://highnoongmt.wordpress.com/2015/05/22/lisls-stis-recurrent-neural-networks-for-folk-music-generation/" class="LinkURL">https://highnoongmt.wordpress.com/2015/05/22/lisls-stis-recurrent-neural-networks-for-folk-music-generation/</a>.</p>
<p class="Reference">Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. 2014. “Sequence to Sequence Learning with Neural Networks.” Cornell University, Computer Science, arXiv:1409.3215, December 14, 2014. <a href="https://arxiv.org/abs/1409.3215" class="LinkURL">https://arxiv.org/abs/1409.3215</a>.</p>
<p class="Reference">Unicode Consortium. 2020. Version 13.0.0, March 10, 2020. <a href="https://www.unicode.org/versions/Unicode13.0.0/" class="LinkURL">https://www.unicode.org/versions/Unicode13.0.0/</a>.</p>
<p class="Reference">van den Oord, Äaron, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016. “WaveNet: A Generative Model for Raw Audio.” Cornell University, Computer Science, arXiv:1609.03499, September 19, 2016. <a href="https://arxiv.org/abs/1609.03499" class="LinkURL">https://arxiv.org/abs/1609.03499</a>.</p>
<p class="Reference">Vicente, Agustin, and Ingrid L. Falkum. 2017. “Polysemy.” <em>Oxford Research Encyclopedias: Linguistics</em>. July 27, 2017. <a href="https://oxfordre.com/linguistics/view/10.1093/acrefore/9780199384655.001.0001/acrefore-9780199384655-e-325" class="LinkURL">https://oxfordre.com/linguistics/view/10.1093/acrefore/9780199384655.001.0001/acrefore-9780199384655-e-325</a>.</p>
<h2 id="h1-500723b01-0020">Chapter 20</h2>
<p class="Reference">Alammar, Jay. 2018. “How GPT3 Works - Visualizations and Animations.” <em>Jay Alammar: Visualizing Machine Learning One Concept at a Time </em>(blog). GitHub. Accessed November 5, 2020. <a href="http://jalammar.github.io/how-gpt3-works-visualizations-animations/" class="LinkURL">http://jalammar.github.io/how-gpt3-works-visualizations-animations/</a>.</p>
<p class="Reference">Alammar, Jay. 2019. “A Visual Guide to Using BERT for the First Time.”<em> Jay Alammar: Visualizing Machine Learning One Concept at a Time </em>(blog). GitHub. November 26, 2019. <a href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" class="LinkURL">http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/</a>.</p>
<p class="Reference">Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2016. “Neural Machine Translation by Jointly Learning to Align and Translate.” Cornell University, Computer Science, arXiv:1409.0473, May 19, 2016. <a href="https://arxiv.org/abs/1409.0473" class="LinkURL">https://arxiv.org/abs/1409.0473</a>.</p>
<p class="Reference">Brown, Tom B., et al., 2020. “Language Models Are Few-Shot Learners.” Cornell University, Computer Science, arXiv:2005.14165, July 22, 2020. <a href="https://arxiv.org/pdf/2005.14165.pdf" class="LinkURL">https://arxiv.org/pdf/2005.14165.pdf</a>.</p>
<p class="Reference">Cer, Daniel, et al. 2018. “Universal Sentence Encoder.” Cornell University, Computer Science, arXiv:1803.11175 April 12, 2018. <a href="https://arxiv.org/abs/1803.11175" class="LinkURL">https://arxiv.org/abs/1803.11175</a>. </p>
<p class="Reference">Cho, Kyunghyun, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations Using RNN Encoder–Decoder for Statistical Machine Translation.” In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em> (Doha, Qatar, October 25–29, 2014): 1724–34. <a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf" class="LinkURL">http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf</a>.</p>
<p class="Reference">Chromiak, Michał. 2017. “The Transformer—Attention Is All You Need.” <em>Michał Chromiak’s Blog</em>, GitHub, October 30, 2017.  <a href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/" class="LinkURL">https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/</a>.</p>
<p class="Reference">Common Crawl. 2020. Common Crawl home page. Accessed November 15, 2020. <a href="https://commoncrawl.org/the-data/" class="LinkURL">https://commoncrawl.org/the-data/</a>.</p>
<p class="Reference">Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” Cornell University, Computer Science, arXiv:1810.04805, May 24, 2019. <a href="https://arxiv.org/abs/1810.04805" class="LinkURL">https://arxiv.org/abs/1810.04805</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="709" id="Page_709"/>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. 2020. “Google-Research/bert.” GitHub. Accessed November 5, 2020. <a href="https://github.com/google-research/bert" class="LinkURL">https://github.com/google-research/bert</a>.</p>
<p class="Reference">El Boukkouri, Hicham. 2018. “Arithmetic Properties of Word Embeddings.” <em>Data from the Trenches</em> (blog), Medium, November 21, 2018. <a href="https://medium.com/data-from-the-trenches/arithmetic-properties-of-word-embeddings-e918e3fda2ac" class="LinkURL">https://medium.com/data-from-the-trenches/arithmetic-properties-of-word-embeddings-e918e3fda2ac</a>.</p>
<p class="Reference">Facebook Open Source. 2020. “fastText: Library for Efficient Text Classification and Representation Learning.” Open source software. Accessed November 5, 2020. <a href="https://fasttext.cc/" class="LinkURL">https://fasttext.cc/</a>.</p>
<p class="Reference">Frankenheimer, John, dir. 1962. <em>The Manchurian Candidate</em>, written by George Axelrod, based on a novel by Richard Condon., M. C. Productions. <a href="https://www.imdb.com/title/tt0056218/" class="LinkURL">https://www.imdb.com/title/tt0056218/</a>. </p>
<p class="Reference">Gluon authors. 2020. “Extracting Sentence Features with Pre-trained ELMo.” Tutorial, Gluon, Accessed November 5, 2020. <a href="https://gluon-nlp.mxnet.io/examples/sentence_embedding/elmo_sentence_representation.html" class="LinkURL">https://gluon-nlp.mxnet.io/examples/sentence_embedding/elmo_sentence_representation.html</a>.</p>
<p class="Reference">He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” Cornell University, Computer Science, arXiv:1512.03385, December 10, 2015. <a href="https://arxiv.org/abs/1512.03385" class="LinkURL">https://arxiv.org/abs/1512.03385</a>.</p>
<p class="Reference">Hendrycks, Dan, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. “Measuring Massive Multitask Language Understanding.” Cornell University, Computer Science, arXiv:2009.03300, September 21, 2020. <a href="https://arxiv.org/abs/2009.03300" class="LinkURL">https://arxiv.org/abs/2009.03300</a>.</p>
<p class="Reference">Howard, Jeremy and Sebastian Ruder. 2018. “Universal Language Model Fine-Tuning for Text Classification.” Cornell University, Computer Science, arXiv:1801.06146, May 23, 2018. <a href="https://arxiv.org/abs/1801.06146" class="LinkURL">https://arxiv.org/abs/1801.06146</a>.</p>
<p class="Reference">Huston, Scott. 2020. “GPT-3 Primer: Understanding OpenAI’s Cutting-Edge Language Model.” <em>Towards Data Science</em> (blog), August 20, 2020. <a href="https://towardsdatascience.com/gpt-3-primer-67bc2d821a00" class="LinkURL">https://towardsdatascience.com/gpt-3-primer-67bc2d821a00</a>.</p>
<p class="Reference">Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. “Scaling Laws for Neural Language Models.” Cornell University, Computer Science, arXiv:2001.08361, January 23, 2020. <a href="https://arxiv.org/abs/2001.08361" class="LinkURL">https://arxiv.org/abs/2001.08361</a>.</p>
<p class="Reference">Kazemnejad, Amirhossein. 2019. “Transformer Architecture: The Positional Encoding.” <em>Amirhossein Kazemnejad’s Blog</em>, September 20, 2019. <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" class="LinkURL">https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</a>.</p>
<p class="Reference">Kelly, Charles. 2020. “Tab-Delimited Bilingual Sentence Pairs.” Manythings.org. Accessed November 6, 2020. <a href="http://www.manythings.org/anki/" class="LinkURL">http://www.manythings.org/anki/</a>.</p>
<p class="Reference">Klein, Guillaume, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. 2017. “OpenNMT: Open-Source Toolkit for Neural Machine Translation.” Cornell University, Computer Science, arXiv:1701.02810, March 6, 2017. <a href="https://arxiv.org/abs/1701.02810" class="LinkURL">https://arxiv.org/abs/1701.02810</a>.</p>
<p class="Reference">Liu, Yang, Lixin Ji, Ruiyang Huang, Tuosiyu Ming, Chao Gao, and Jianpeng Zhang. 2018. “An Attention-Gated Convolutional Neural Network for Sentence Classification.” Cornell University, Computer Science, arXiv:2018.07325. December 28, 2018. <a href="https://arxiv.org/abs/1808.07325" class="LinkURL">https://arxiv.org/abs/1808.07325</a>.</p>
<p class="Reference">Mansimov, Elman, Alex Wang, Sean Welleck, and Kyunghyun Cho. 2020. “A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models.” Cornell University, Computer Science, arXiv:1905.12790, February 7, 2020. <a href="https://arxiv.org/abs/1905.12790" class="LinkURL">https://arxiv.org/abs/1905.12790</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="710" id="Page_710"/>McCormick Chris, and Nick Ryan, 2020. “BERT Fine-Tuning Tutorial with PyTorch.” <em>Chris McCormick</em> (blog). Last updated March 20, 2020. <a href="https://mccormickml.com/2019/07/22/BERT-fine-tuning/" class="LinkURL">https://mccormickml.com/2019/07/22/BERT-fine-tuning/</a>.</p>
<p class="Reference">Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. “Efficient Estimation of Word Representations in Vector Space.” Cornell University, Computer Science, arXiv:1301.3781, September 7, 2013. <a href="https://arxiv.org/abs/1301.3781" class="LinkURL">https://arxiv.org/abs/1301.3781</a>.</p>
<p class="Reference">Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. “Distributed Representations of Words and Phrases and Their Compositionality.” Cornell University, Computer Science, arXiv:1310.4546, October 16, 2013. <a href="https://arxiv.org/abs/1310.4546" class="LinkURL">https://arxiv.org/abs/1310.4546</a>.</p>
<p class="Reference">Mishra, Prakhar. 2020. “Natural Language Generation Using BERT Introduction.” <em>TechViz: The Data Science Guy</em> (blog). Accessed November 6, 2020. <a href="https://prakhartechviz.blogspot.com/2020/04/natural-language-generation-using-bert.html" class="LinkURL">https://prakhartechviz.blogspot.com/2020/04/natural-language-generation-using-bert.html</a>.</p>
<p class="Reference">Paulus, Romain, Caiming Xiong, and Richard Socher, 2017. “A Deep Reinforced Model for Abstractive Summarization.” Cornell University, Computer Science, arXiv:1705.04304, November 13, 2017. <a href="https://arxiv.org/abs/1705.04304" class="LinkURL">https://arxiv.org/abs/1705.04304</a>.</p>
<p class="Reference">Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. 2014. “GloVe: Global Vectors for Word Representation.” in <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em> (October): 1532–43. <a href="https://nlp.stanford.edu/pubs/glove.pdf" class="LinkURL">https://nlp.stanford.edu/pubs/glove.pdf</a>.</p>
<p class="Reference">Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. “Deep Contextualized Word Representations.” Cornell University, Computer Science, arXiv:1802.05365. March 22, 2018. <a href="https://arxiv.org/abs/1802.05365" class="LinkURL">https://arxiv.org/abs/1802.05365</a>.</p>
<p class="Reference">Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.” OpenAI, San Francisco, CA, 2019. <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" class="LinkURL">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a>.</p>
<p class="Reference">Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” Cornell University, Computer Science, arXiv:1910.10683, July 28, 2020. <a href="https://arxiv.org/abs/1910.10683" class="LinkURL">https://arxiv.org/abs/1910.10683</a>. </p>
<p class="Reference">Rajasekharan, Ajit. 2019. “A Review of BERT Based Models.” <em>Towards Data Science</em> (blog), June 17, 2019. <a href="https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58" class="LinkURL">https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58</a>.</p>
<p class="Reference">Reisner, Alex. 2020. “What’s It Like to Be an Animal?” SpeedofAnimals.com. Accessed November 6, 2020. <a href="https://www.speedofanimals.com/" class="LinkURL">https://www.speedofanimals.com/</a>.</p>
<p class="Reference">Russell, Stuart, and Peter Norvig. 2009. <em>Artificial Intelligence: A Modern Approach</em>, 3rd ed. New York: Pearson Press.</p>
<p class="Reference">Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020. “DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter.” Cornell University, Computer Science, arXiv:1910.01108, March 1, 2020. <a href="https://arxiv.org/abs/1910.01108" class="LinkURL">https://arxiv.org/abs/1910.01108</a>.</p>
<p class="Reference">Scott, Kevin. 2020. “Microsoft Teams Up with OpenAI to Exclusively License GPT-3 Language Model.” <em>Official Microsoft Blog</em>, September 22, 2020. <a href="https://blogs.microsoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/" class="LinkURL">https://blogs.microsoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="711" id="Page_711"/>Shao, Louis, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, and Ray Kurzweil. 2017. “Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models.” Cornell University, Computer Science, arXiv:1701.03185, July 31, 2017. <a href="https://arxiv.org/abs/1701.03185" class="LinkURL">https://arxiv.org/abs/1701.03185</a>.</p>
<p class="Reference">Singhal, Vivek. 2020. “Transformers for NLP.” <em>Research/Blog</em>, CellStrat, May 19, 2020. <a href="https://www.cellstrat.com/2020/05/19/transformers-for-nlp/" class="LinkURL">https://www.cellstrat.com/2020/05/19/transformers-for-nlp/</a>.</p>
<p class="Reference">Socher, Richard, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013a. “Deeply Moving: Deep Learning for Sentiment Analysis—Dataset.” Sentiment Analysis. August 2013. <a href="https://nlp.stanford.edu/sentiment/index.html" class="LinkURL">https://nlp.stanford.edu/sentiment/index.html</a>.</p>
<p class="Reference">Socher, Richard, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013b. “Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.” Oral presentation at the Conference on Empirical Methods in Natural Language Processing (EMNLP) (Seattle, WA, October 18–21). <a href="https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf" class="LinkURL">https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf</a>.</p>
<p class="Reference">spaCy authors. 2020. “Word Vectors and Semantic Similarity.” spaCy: Usage. <a href="https://spacy.io/usage/vectors-similarity" class="LinkURL">https://spacy.io/usage/vectors-similarity</a>.</p>
<p class="Reference">Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. 2014. “Sequence to Sequence Learning with Neural Networks.” Cornell University, Computer Science, arXiv:1409.3215, December 14, 2014. <a href="https://arxiv.org/abs/1409.3215" class="LinkURL">https://arxiv.org/abs/1409.3215</a>.</p>
<p class="Reference">Tay, Yi, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. “Efficient Transformers: A Survey.” Cornell University, Mathematics, arXiv:2009.0673, September 1, 2020. <a href="https://arxiv.org/abs/2009.0673" class="LinkURL">https://arxiv.org/abs/2009.0673</a>.</p>
<p class="Reference">Taylor, Wilson L. 1953. “‘Cloze Procedure’: A New Tool for Measuring Readability.” <em>Journalism Quarterly</em>, 30(4): 415–33. <a href="https://www.gwern.net/docs/psychology/writing/1953-taylor.pdf" class="LinkURL">https://www.gwern.net/docs/psychology/writing/1953-taylor.pdf</a>.</p>
<p class="Reference">TensorFlow authors. 2018. “Universal Sentence Encoder.” Tutorial, TensorFlow model archives, GitHub, 2018. <a href="https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb#scrollTo=co7MV6sX7Xto" class="LinkURL">https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb#scrollTo=co7MV6sX7Xto</a>.</p>
<p class="Reference">TensorFlow authors, 2019a. “Why Add Positional Embedding Instead of Concatenate?” <em>tensorflow/tensor2tensor</em> (blog). May 30, 2019. <a href="https://github.com/tensorflow/tensor2tensor/issues/1591" class="LinkURL">https://github.com/tensorflow/tensor2tensor/issues/1591</a>.</p>
<p class="Reference">TensorFlow authors. 2019b. “Transformer Model for Language Understanding.” Documentation, TensorFlow, GitHub. <a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb" class="LinkURL">https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb</a>.</p>
<p class="Reference">TensorFlow authors. 2020a. “Elmo.” TensorFlow Hub, November 6, 2020. <a href="https://tfhub.dev/google/elmo/3" class="LinkURL">https://tfhub.dev/google/elmo/3</a>. </p>
<p class="Reference">TensorFlow authors. 2020b. “Transformer Model for Language Understanding.” Tutorial, TensorFlow Core documentation. Last updated November 2, 2020. <a href="https://www.tensorflow.org/tutorials/text/transformer" class="LinkURL">https://www.tensorflow.org/tutorials/text/transformer</a>.</p>
<p class="Reference">Thiruvengadam, Aditya. 2018. “Transformer Architecture: Attention Is All You Need.” <em>Aditya Thiruvengadam</em> (blog), Medium. October 9, 2018. <a href="https://medium.com/@adityathiruvengadam/transformer-architecture-attention-is-all-you-need-aeccd9f50d09" class="LinkURL">https://medium.com/@adityathiruvengadam/transformer-architecture-attention-is-all-you-need-aeccd9f50d09</a>.</p>
<p class="Reference">Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jacob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhim. 2017. “Attention Is All You Need.” Cornell University, Computer Science, arXiv:1706.03762v5, December 6, 2017. <a href="https://arxiv.org/abs/1706.03762v5" class="LinkURL">https://arxiv.org/abs/1706.03762v5</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="712" id="Page_712"/>Vijayakumar, Ashwin K., Michael Cogswell, Ramprasath R. Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. 2018. “Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models.” Cornell University, Computer Science, arXiv:1610.02424. October 22, 2018. <a href="https://arxiv.org/abs/1610.02424" class="LinkURL">https://arxiv.org/abs/1610.02424</a>.</p>
<p class="Reference">von Platen, Patrick. 2020. “How to Generate Text: Using Different Decoding Methods for Language Generation with Transformers.” <em>Huggingface</em> (blog), GitHub, May 2020. <a href="https://huggingface.co/blog/how-to-generate" class="LinkURL">https://huggingface.co/blog/how-to-generate</a>. </p>
<p class="Reference">Wallace, Eric, Tony Z. Zhao, Shi Feng, and Sameer Singh. 2020. “Customizing Triggers with Concealed Data Poisoning.” Cornell University, Computer Science, arXiv:2010.12563, October 3, 2020. <a href="https://arxiv.org/abs/2010.12563" class="LinkURL">https://arxiv.org/abs/2010.12563</a>. </p>
<p class="Reference">Walton, Nick. 2020. “AI Dungeon: Dragon Model Upgrade.” <em>Nick Walton</em> (blog) July 14, 2020. <a href="https://medium.com/@aidungeon/ai-dungeon-dragon-model-upgrade-7e8ea579abfe" class="LinkURL">https://medium.com/@aidungeon/ai-dungeon-dragon-model-upgrade-7e8ea579abfe</a> and <a href="https://play.aidungeon.io/main/home" class="LinkURL">https://play.aidungeon.io/main/home</a>.</p>
<p class="Reference">Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” Cornell University, Computer Science, arXiv:1804.07461, February 22, 2019. <a href="https://arxiv.org/abs/1804.07461" class="LinkURL">https://arxiv.org/abs/1804.07461</a>.</p>
<p class="Reference">Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2020. “GLUE Leaderboards.” GLUE Benchmark. Accessed November 6, 2020. <a href="https://gluebenchmark.com/leaderboard/submission/zlssuBTm5XRs0aSKbFYGVIVdvbj1/-LhijX9VVmvJcvzKymxy" class="LinkURL">https://gluebenchmark.com/leaderboard/submission/zlssuBTm5XRs0aSKbFYGVIVdvbj1/-LhijX9VVmvJcvzKymxy</a>.</p>
<p class="Reference">Warstadt, Alex, Amanpreet Singh, and Sam Bowman. 2018. “CoLA: The Corpus of Linguistic Acceptability.” NYU-MLL. <a href="https://nyu-mll.github.io/CoLA/" class="LinkURL">https://nyu-mll.github.io/CoLA/</a>.</p>
<p class="Reference">Warstadt, Alex, Amanpreet Singh, and Sam Bowman. 2019. “Neural Network Ability Judgements.” Cornell University, Computer Science, arXiv:1805.12471, October 1, 2019. <a href="https://arxiv.org/abs/1805.12471" class="LinkURL">https://arxiv.org/abs/1805.12471</a>.</p>
<p class="Reference">Welleck, Sean, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, and Kyunghyun Cho. 2020. “Consistency of a Recurrent Language Model with Respect to Incomplete Decoding.” Cornell University, Computer Science, arXiv:2002.02492, October 2, 2020. <a href="https://arxiv.org/abs/2002.02492" class="LinkURL">https://arxiv.org/abs/2002.02492</a>.</p>
<p class="Reference">Woolf, Max. 2019. “Train a GPT-2 Text-Generating Model w/ GPU.” Google Colab Notebook, 2019. <a href="https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=-xInIZKaU104" class="LinkURL">https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=-xInIZKaU104</a>.</p>
<p class="Reference">Xu, Lei, Ivan Ramirez, and Kalyan Veeramachaneni. 2020. “Rewriting Meaningful Sentences via Conditional BERT Sampling and an Application on Fooling Text Classifiers.” Cornell University, Computer Science, arXiv:2010.11869, October 22, 2020. <a href="https://arxiv.org/abs/2010.11869" class="LinkURL">https://arxiv.org/abs/2010.11869</a>. </p>
<p class="Reference">Zhang, Aston, Zachary C. Lipton, Mu Li, and Alexander J. Smola. 2020. “10.3: Transformer.” In <em>Dive into Deep Learning</em>. <a href="https://d2l.ai/chapter_attention-mechanisms/transformer.html" class="LinkURL">https://d2l.ai/chapter_attention-mechanisms/transformer.html</a>.</p>
<p class="Reference">Zhang, Han, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. 2019. “Self-Attention Generative Adversarial Networks.” Cornell University, Statistics, arXiv:1805.08318. June 14, 2019. <a href="https://arxiv.org/abs/1805.08318" class="LinkURL">https://arxiv.org/abs/1805.08318</a>.</p>
<p class="Reference">Zhu, Yukun, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. “Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books.” Cornell University, Computer Science, arXiv:1506.06724, June 22, 2015. <a href="https://arxiv.org/abs/1506.06724" class="LinkURL">https://arxiv.org/abs/1506.06724</a>.</p>
<h2 id="h1-500723b01-0021"><span epub:type="pagebreak" title="713" id="Page_713"/>Chapter 21</h2>
<p class="Reference">Asadi, Kavosh, and Michael L. Littman. 2017. “An Alternative Softmax Operator for Reinforcement Learning.” In <em>Proceedings of the 34th International Conference on Machine Learning</em> (Sydney, Australia, August 6–11). <a href="https://arxiv.org/abs/1612.05628" class="LinkURL">https://arxiv.org/abs/1612.05628</a>.</p>
<p class="Reference">Craven, Mark, and David Page. 2018. “Reinforcement Learning with DNNs: AlphaGo to AlphaZero.” CS 760 course notes, Spring, School of Medicine and Public Health, University of Wisconsin-Madison. <a href="https://www.biostat.wisc.edu/~craven/cs760/lectures/AlphaZero.pdf" class="LinkURL">https://www.biostat.wisc.edu/~craven/cs760/lectures/AlphaZero.pdf</a>.</p>
<p class="Reference">DeepMind team. 2020. “Alpha Go.” <em>DeepMind </em>(blog). Accessed October 8, 2020. <a href="https://deepmind.com/research/alphago/" class="LinkURL">https://deepmind.com/research/alphago/</a>.</p>
<p class="Reference">Eden, Tim, Anthony Knittel, and Raphael van Uffelen. 2020. “Reinforcement Learning.” University of New South Wales. Accessed October 8, 2020. <a href="http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html" class="LinkURL">http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html</a>.</p>
<p class="Reference">François-Lavet, Vincent, Peter Henderson, Riashat Islam, Marc G. Bellemare, Joelle Pineau, “An Introduction to Deep Reinforcement Learning.” Cornell University, Machine Learning, arXiv:1811.12560, December 3, 2018. <a href="https://arxiv.org/abs/1811.12560" class="LinkURL">https://arxiv.org/abs/1811.12560</a>.</p>
<p class="Reference">Hassabis, Demis, and David Silver. 2017. “AlphaGo Zero: Learning from Scratch.” <em>DeepMind</em> (Blog), October 18, 2017. <a href="https://deepmind.com/blog/alphago-zero-learning-scratch/" class="LinkURL">https://deepmind.com/blog/alphago-zero-learning-scratch/</a>.</p>
<p class="Reference">Matiisen, Tambet. 2015. “Demystifying Deep Reinforcement Learning.” Computational Neuroscience Lab, Institute of Computer Science, University of Tartu, December 15, 2015. <a href="https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/" class="LinkURL">https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/</a>.</p>
<p class="Reference">Melo, Francisco S. 2020. “Convergence of Q-Learning: A Simple Proof.” Institute for Systems and Robotics, Instituto Superior Técnico, Portugal. Accessed October 9, 2020. <a href="http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf" class="LinkURL">http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf</a>.</p>
<p class="Reference">Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. “Playing Atari with Deep Reinforcement Learning.” NIPS Deep Learning Workshop, December 19, 2013. <a href="https://arxiv.org/abs/1312.5602v1" class="LinkURL">https://arxiv.org/abs/1312.5602v1</a>.</p>
<p class="Reference">Rummery, G. A., and M. Niranjan. 1994. “On-Line Q-Learning Using Connectionist Systems.” Engineering Department, Cambridge University, UK, September 1994. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.2539&amp;rep=rep1&amp;type=pdf" class="LinkURL">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.2539&amp;rep=rep1&amp;type=pdf</a>.</p>
<p class="Reference">Silver, David, et al. 2017. “Mastering the Game of Go Without Human Knowledge.” <em>Nature</em> 550 (October 19, 2017): 354–59. <a href="https://www.nature.com/articles/nature24270.epdf" class="LinkURL">https://www.nature.com/articles/nature24270.epdf</a>.</p>
<p class="Reference">Sutton, Richard S., and Andrew G. Baro. 2018. <em>Reinforcement Learning: An Introduction</em>, 2nd ed. Cambridge, MA: MIT Press. Available at <a href="http://www.incompleteideas.net/book/the-book-2nd.html" class="LinkURL">http://www.incompleteideas.net/book/the-book-2nd.html</a>.</p>
<p class="Reference">Villanueva, John Carl. 2009. “How Many Atoms Are There in the Universe?” <em>Universe Today</em>, July 30, 2009. <a href="http://www.universetoday.com/36302/atoms-in-the-universe/" class="LinkURL">http://www.universetoday.com/36302/atoms-in-the-universe/</a>.</p>
<p class="Reference">Watkins, Christopher. 1989. “Learning from Delayed Rewards.” PhD thesis, Cambridge University, UK. <a href="http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf" class="LinkURL">http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf</a>.</p>
<h2 id="h1-500723b01-0022"><span epub:type="pagebreak" title="714" id="Page_714"/>Chapter 22</h2>
<p class="Reference">Achlioptas, Panos, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. 2018. “Representation Learning and Adversarial Generation of 3D Point Clouds.” Cornell University, Computer Science, arXiv:1707.02392, June 12, 2018. <a href="https://arxiv.org/abs/1707.02392v1" class="LinkURL">https://arxiv.org/abs/1707.02392v1</a>.</p>
<p class="Reference">Arjovsky, Martin, and Léon Bottou. 2017. “Towards Principled Methods for Training Generative Adversarial Networks.” Cornell University, Statistics, arXiv:1701.04862, January 17, 2017. <a href="https://arxiv.org/abs/1701.04862v1" class="LinkURL">https://arxiv.org/abs/1701.04862v1</a>.</p>
<p class="Reference">Arjovsky, Martin, Soumith Chintala, and Léon Bottou. 2017. “Wasserstein GAN.” Cornell University, Statistics, arXiv:1701.07875, December 6, 2017. <a href="https://arxiv.org/abs/1701.07875v1" class="LinkURL">https://arxiv.org/abs/1701.07875v1</a>.</p>
<p class="Reference">Bojanowski, Piotr, Armand Joulin, David Lopez-Paz, and Arthur Szlam. 2019. “Optimizing the Latent Space of Generative Networks.” Cornell University, Statistics, arXiv 1717.05776, May 20, 2019. <a href="https://arxiv.org/abs/1707.05776" class="LinkURL">https://arxiv.org/abs/1707.05776</a>.</p>
<p class="Reference">Chen, Janet, Su-I Lu, and Dan Vekhter. 2020. “Strategies of Play.” In <em>Game Theory</em>, Stanford Department of Computer Science, Stanford University, Stanford, CA. Accessed October 6, 2020. <a href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/1998-99/game-theory/Minimax.html" class="LinkURL">https://cs.stanford.edu/people/eroberts/courses/soco/projects/1998-99/game-theory/Minimax.html</a>.</p>
<p class="Reference">Geitgey, Adam. 2017. “Machine Learning Is Fun Part 7: Abusing Generative Adversarial Networks to Make 8-bit Pixel Art.” Medium. February 12, 2017. <a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7#.v1o6o0dyi" class="LinkURL">https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7#.v1o6o0dyi</a>.</p>
<p class="Reference">Gildenblat, Jacob. 2020. “KERAS-DCGAN.” GitHub. Accessed October 6, 2020. <a href="https://github.com/jacobgil/keras-dcgan" class="LinkURL">https://github.com/jacobgil/keras-dcgan</a>.</p>
<p class="Reference">Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Networks.” Cornell University, Statistics, arXiv:1406.2661, June 10, 2014. <a href="https://arxiv.org/abs/1406.2661" class="LinkURL">https://arxiv.org/abs/1406.2661</a>.</p>
<p class="Reference">Goodfellow, Ian. 2016. “NIPS 2016 Tutorial: Generative Adversarial Networks.” Cornell University, Computer Science, arXiv:1701.00160, December 31, 2016. <a href="https://arxiv.org/abs/1701.00160" class="LinkURL">https://arxiv.org/abs/1701.00160</a>.</p>
<p class="Reference">Karras, Tero, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2018. “Progressive Growing of GANs for Improved Quality, Stability, and Variation.” Cornell University, Computer Science, arXiv:1710.10196, February 26, 2018. <a href="https://arxiv.org/abs/1710.10196" class="LinkURL">https://arxiv.org/abs/1710.10196</a>.</p>
<p class="Reference">Myers, Andrew. 2002. “CS312 Recitation 21: Minimax Search and Alpha-Beta Pruning.” Computer Science Department, Cornell University. <a href="https://www.cs.cornell.edu/courses/cs312/2002sp/lectures/rec21.htm" class="LinkURL">https://www.cs.cornell.edu/courses/cs312/2002sp/lectures/rec21.htm</a>.</p>
<p class="Reference">Radford, Alec, Luke Metz, and Soumith Chintala. 2016. “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.” Cornell University, Computer Science, arXiv:1511.06434, January 7, 2016. <a href="https://arxiv.org/abs/1511.06434" class="LinkURL">https://arxiv.org/abs/1511.06434</a>.</p>
<p class="Reference">Watson, Joel. 2013. <em>Strategy: An Introduction to Game Theory</em>, 3rd ed. New York: W.W. Norton and Company.</p>
<h2 id="h1-500723b01-0023">Chapter 23</h2>
<p class="Reference">The Art Story Foundation, 2020. “Classical, Modern, and Contemporary Movements and Styles.” Art Story site. Accessed October 7, 2020. <a href="http://www.theartstory.org/section_movements.htm" class="LinkURL">http://www.theartstory.org/section_movements.htm</a>. </p>
<p class="Reference"><span epub:type="pagebreak" title="715" id="Page_715"/>Bonaccorso, Giuseppe. 2020. “Neural_Artistic_Style_Transfer.” GitHub. Accessed October 7, 2020. <a href="https://github.com/giuseppebonaccorso/keras_deepdream" class="LinkURL">https://github.com/giuseppebonaccorso/keras_deepdream</a>.</p>
<p class="Reference">Chollet, François. 2017. <em>Deep Learning with Python</em>. Shelter Island, NY: Manning Publications. <a href="https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.3-neural-style-transfer.ipynb" class="LinkURL">https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.3-neural-style-transfer.ipynb</a>. </p>
<p class="Reference">Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. 2015. “Neural Algorithm of Artistic Style.” Cornell University, Computer Science, arXiv:1508.06576, September 2, 2015. <a href="https://arxiv.org/abs/1508.06576" class="LinkURL">https://arxiv.org/abs/1508.06576</a>. </p>
<p class="Reference">Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. 2016. “Image Style Transfer Using Convolutional Neural Networks.” in <em>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition </em>(Las Vegas, NV, June 27–30). <a href="https://pdfs.semanticscholar.org/7568/d13a82f7afa4be79f09c295940e48ec6db89.pdf" class="LinkURL">https://pdfs.semanticscholar.org/7568/d13a82f7afa4be79f09c295940e48ec6db89.pdf</a>. </p>
<p class="Reference">Jing, Yongcheng, Yezhou Yang, Zunlei Feng, Jingwen Ye, and Mingli Song. 2018. “Neural Style Transfer: A Review.” Cornell University, Computer Science, arXiv:1705.04058v1, October 30, 2018. <a href="https://arxiv.org/abs/1705.04058" class="LinkURL">https://arxiv.org/abs/1705.04058</a>. </p>
<p class="Reference">Li, Yanghao, Naiyan Wang, Jiaying Liu, and Xiaodi Hou. 2017. “Demystifying Neural Style Transfer.” Cornell University, Computer Science, arXiv:1701.01036, July 1, 2017. <a href="https://arxiv.org/abs/1701.01036" class="LinkURL">https://arxiv.org/abs/1701.01036</a>. </p>
<p class="Reference">Lowensohn, Josh. 2014. “I Let Apple’s QuickType Keyboard Take Over My iPhone.” <em>The Verge</em> (blog), September 17, 2014. <a href="https://www.theverge.com/2014/9/17/6337105/breaking-apples-quicktype-keyboard" class="LinkURL">https://www.theverge.com/2014/9/17/6337105/breaking-apples-quicktype-keyboard</a>. </p>
<p class="Reference">Majumdar, Somshubra. 2020. “Titu1994/Neural-Style-Transfer.” GitHub, Accessed October 7, 2020. <a href="https://github.com/titu1994/Neural-Style-Transfer" class="LinkURL">https://github.com/titu1994/Neural-Style-Transfer</a>. </p>
<p class="Reference">Mordvintsev, Alexander, Christopher Olah, and Mike Tyka. 2015. “Inceptionism: Going Deeper into Neural Networks.” <em>Google AI Blog</em>, June 17, 2015. <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html" class="LinkURL">https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html</a>. </p>
<p class="Reference">O’Neil, Cathy. 2016. <em>Weapons of Math Destruction</em>. New York: Broadway Books.</p>
<p class="Reference">Orlowski, Jeff. 2020. <em>The Social Dilemma.</em> Exposure Labs, Argent Pictures, and Netflix. Accessed October 7, 2020. <a href="https://www.thesocialdilemma.com/the-film/" class="LinkURL">https://www.thesocialdilemma.com/the-film/</a>.</p>
<p class="Reference">Ruder, Manuel, Alexey Dosovitskiy, and Thomas Brox. 2018. “Artistic Style Transfer for Videos and Spherical Images.” Cornell University, Computer Science, arXiv:1708.04538, August 5, 2018. <a href="https://arxiv.org/abs/1708.04538" class="LinkURL">https://arxiv.org/abs/1708.04538</a>. </p>
<p class="Reference">Simonyan, Karen, and Andrew Zisserman. 2020. “Very Deep Convolutional Networks for Large-Scale Visual Recognition.” <em>Visual Geometry Group</em> (blog), University of Oxford. Accessed October 7, 2020. <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" class="LinkURL">http://www.robots.ox.ac.uk/~vgg/research/very_deep/</a>. </p>
<p class="Reference">Tyka, Mike. 2015. “Deepdream/Inceptionism - recap.” <em>Mike Tyka</em> (blog), July 21, 2015. <a href="https://mtyka.github.io/code/2015/07/21/one-month-after-deepdream.html" class="LinkURL">https://mtyka.github.io/code/2015/07/21/one-month-after-deepdream.html</a>. </p>
<p class="Reference">Wikipedia authors. 2020. “Style (visual arts).” Wikipedia. September 2, 2020. <a href="https://en.wikipedia.org/wiki/Style_(visual_arts)" class="LinkURL">https://en.wikipedia.org/wiki/Style_(visual_arts)</a>.</p>
</section>
</div></body></html>