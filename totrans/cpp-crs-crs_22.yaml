- en: '**19'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CONCURRENCY AND PARALLELISM**
  prefs: []
  type: TYPE_NORMAL
- en: '*The Senior Watchdog had her own watchwords: “Show me a completely smooth operation
    and I’ll show you someone who’s covering mistakes. Real boats rock.”'
  prefs: []
  type: TYPE_NORMAL
- en: '—Frank Herbert,* Chapterhouse: Dune'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In programming, *concurrency* means two or more tasks running in a given time
    period. *Parallelism* means two or more tasks running at the same instant. Often,
    these terms are used interchangeably without negative consequence, because they’re
    so closely related. This chapter introduces the very basics of both concepts.
    Because concurrent and parallel programming are huge and complicated topics, thorough
    treatment requires an entire book. You’ll find such books in the “Further Reading”
    section at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn about concurrent and parallel programming with
    futures. Next, you’ll learn how to share data safely with mutexes, condition variables,
    and atomics. Then the chapter illustrates how execution policies help to speed
    up your code but also contain hidden dangers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Concurrent Programming**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Concurrent programs* have multiple *threads of execution* (or simply *threads*),
    which are sequences of instructions. In most runtime environments, the operating
    system acts as a scheduler to determine when a thread executes its next instruction.
    Each process can have one or more threads, which typically share resources, such
    as memory, with each other. Because the scheduler determines when threads execute,
    the programmer can’t generally rely on their ordering. In exchange, programs can
    execute multiple tasks in the same time period (or at the same time), which often
    results in serious speedups. To observe any speedup from the serial to the concurrent
    version, your system will need concurrent hardware, for example, a multicore processor.'
  prefs: []
  type: TYPE_NORMAL
- en: This section begins with asynchronous tasks, a high-level method for making
    your programs concurrent. Next, you’ll learn some basic methods for coordinating
    between these tasks when they’re handling shared mutable state. Then you’ll survey
    some low-level facilities available to you in the stdlib for unique situations
    in which the higher-level tools don’t have the performance characteristics you
    require.
  prefs: []
  type: TYPE_NORMAL
- en: '***Asynchronous Tasks***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One way to introduce concurrency into your program is by creating *asynchronous
    tasks*. An asynchronous task doesn’t immediately need a result. To launch an asynchronous
    task, you use the `std::async` function template in the `<future>` header.
  prefs: []
  type: TYPE_NORMAL
- en: '**async**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'When you invoke `std::async`, the first argument is the launch policy `std::launch`,
    which takes one of two values: `std::launch::async` or `std::launch::deferred`.
    If you pass `launch::async`, the runtime creates a new thread to launch your task.
    If you pass `deferred`, the runtime waits until you need the task’s result before
    executing (a pattern sometimes called *lazy evaluation*). This first argument
    is optional and defaults to `async|deferred`, meaning it’s up to the implementation
    which strategy to employ. The second argument to `std::async` is a function object
    representing the task you want to execute. There are no restrictions on the number
    or type of arguments the function object accepts, and it might return any type.
    The `std::async` function is a variadic template with a function parameter pack.
    Any additional arguments you pass beyond the function object will be used to invoke
    the function object when the asynchronous task launches. Also, `std::async` returns
    an object called a `std::future`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following simplified `async` declaration helps to summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now that you know how to invoke `async`, let’s look at how to interact with
    its return value.
  prefs: []
  type: TYPE_NORMAL
- en: '**Back to the future**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A `future` is a class template that holds the value of an asynchronous task.
    It has a single template parameter that corresponds with the type of the asynchronous
    task’s return value. For example, if you pass a function object that returns a
    `string`, `async` will return a `future<string>`. Given a `future`, you can interact
    with an asynchronous task in three ways.
  prefs: []
  type: TYPE_NORMAL
- en: First, you can query the `future` about its validity using the `valid` method.
    A valid `future` has a shared state associated with it. Asynchronous tasks have
    a shared state so they can communicate the results. Any `future` returned by `async`
    will be valid until you retrieve the asynchronous task’s return value, at which
    point the shared state’s lifetime ends, as [Listing 19-1](ch19.xhtml#ch19ex01)
    illustrates.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-1: The `async` function returns a valid `future`.*'
  prefs: []
  type: TYPE_NORMAL
- en: You launch an asynchronous task that simply returns a `string` ➊. Because `async`
    always returns a valid `future`, `valid` returns `true` ➋.
  prefs: []
  type: TYPE_NORMAL
- en: If you default construct a `future`, it’s not associated with a shared state,
    so `valid` will return `false`, as [Listing 19-2](ch19.xhtml#ch19ex02) illustrates.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-2: A default constructed `future` is invalid.*'
  prefs: []
  type: TYPE_NORMAL
- en: You default construct a `future` ➊, and `valid` returns `false` ➋.
  prefs: []
  type: TYPE_NORMAL
- en: Second, you can obtain the value from a valid `future` with its `get` method.
    If the asynchronous task hasn’t yet completed, the call to `get` will block the
    currently executed thread until the result is available. [Listing 19-3](ch19.xhtml#ch19ex03)
    illustrates how to employ `get` to obtain return values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-3: The `async` function returns a valid `future`.*'
  prefs: []
  type: TYPE_NORMAL
- en: You use `async` to launch an asynchronous task ➊ and then invoke the `get` method
    on the resulting `future`. As expected, the result is the return value of the
    function object you passed into `async` ➋.
  prefs: []
  type: TYPE_NORMAL
- en: If an asynchronous task throws an exception, the `future` will collect that
    exception and throw it when you invoke `get`, as [Listing 19-4](ch19.xhtml#ch19ex04)
    illustrates.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-4: The `get` method will throw the exception thrown by an asynchronous
    task.*'
  prefs: []
  type: TYPE_NORMAL
- en: You pass a lambda to `async` that throws a `runtime_error` ➊. When you invoke
    `get`, it throws the exception ➋.
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, you can check whether an asynchronous task has completed using either
    `std::wait_for` or `std::wait_until`. Which you choose depends on the sort of
    `chrono` object you want to pass. If you have a `duration` object, you’ll use
    `wait_for`. If you have a `time_point` object, you’ll use `wait_until`. Both return
    a `std::future_status`, which takes one of three values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`future_status::deferred` signals that the asynchronous task will be evaluated
    lazily, so the task will execute once you call `get`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`future_status::ready` indicates that the task has completed and the result
    is ready.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`future_status::timeout` indicates that the task isn’t ready.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the task completes before the specified waiting period, `async` will return
    early.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 19-5](ch19.xhtml#ch19ex05) illustrates how to use `wait_for` to check
    an asynchronous task’s status.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-5: Checking an asynchronous task’s status using `wait_for`*'
  prefs: []
  type: TYPE_NORMAL
- en: You first launch an asynchronous task with `async`, which simply waits for up
    to 100 milliseconds before returning ➊. Next, you call `wait_for` with 25 milliseconds
    ➋. Because the task is still sleeping (25 < 100), `wait_for` returns `future_status::timeout`
    ➌. You call `wait_for` again and wait for up to another 100 milliseconds ➍. Because
    the second `wait_for` will finish after the `async` task finishes, the final `wait_for`
    will return a `future_status::ready` ➎.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Technically, the assertions in [Listing 19-5](ch19.xhtml#ch19ex05) aren’t
    guaranteed to pass. “Waiting” on [page 389](ch12.xhtml#page_389) introduced this_thread::sleep_for,
    which isn’t exact. The operating environment is responsible for scheduling threads,
    and it might schedule the sleeping thread later than the specified duration.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**An Example with Asynchronous Tasks**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 19-6](ch19.xhtml#ch19ex06) contains the `factorize` function, which
    finds all of an integer’s factors.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The factorization algorithm in [Listing 19-6](ch19.xhtml#ch19ex06) is woefully
    inefficient but is good enough for this example. For efficient integer factorization
    algorithms, refer to Dixon’s algorithm, the continued fraction factorization algorithm,
    or the quadratic sieve.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-6: A very simple integer factorization algorithm*'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm accepts a single argument `x` and begins by initializing a `set`
    containing 1 ➊. Next, it iterates from 2 to `x` ➋, checking whether modulo division
    with the `candidate` results in 0 ➌. If it does, `candidate` is a factor, and
    you add it to the factor `set` ➍. You divide `x` by the factor you just discovered
    ➎ and then restart your search by resetting the `candidate` to 1 ➏.
  prefs: []
  type: TYPE_NORMAL
- en: Because integer factorization is a hard problem (and because [Listing 19-6](ch19.xhtml#ch19ex06)
    is so inefficient), calls to `factorize` can take a long time relative to most
    of the functions you’ve encountered so far in the book. This makes it a prime
    candidate for asynchronous tasking. The `factor_task` function in [Listing 19-7](ch19.xhtml#ch19ex07)
    uses the trusty `Stopwatch` from [Listing 12-25](ch12.xhtml#ch12ex25) in [Chapter
    12](ch12.xhtml#ch12) to wrap `factorize` and returns a nicely formatted message.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-7: A `factor_task` function that wraps a call to `factorize` and
    returns a nicely formatted message*'
  prefs: []
  type: TYPE_NORMAL
- en: Like `factorize`, `factor_task` accepts a single argument `x` to factorize ➊.
    (For simplicity, `factor_task` takes an `unsigned long` rather than a templated
    argument). Next, you initialize a `Stopwatch` within a nested scope ➋ and then
    invoke `factorize` with `x` ➌. The result is that `elapsed_ns` contains the number
    of nanoseconds elapsed while `factorize` executed, and `factors` contains all
    the factors of `x`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you construct a nicely formatted string by first converting `elapsed_ns`
    to a count in milliseconds ➍. You write this information into a `stringstream`
    object called `ss` ➎ followed by the factors of `x` ➏. Then you return the resulting
    `string` ➐.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 19-8](ch19.xhtml#ch19ex08) employs `factor_task` to factor six different
    numbers and record the total elapsed program time.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-8: A program using `factor_task` to factorize six different numbers*'
  prefs: []
  type: TYPE_NORMAL
- en: You construct an array containing six `numbers` of varied size and primality
    ➊. Next, you initialize a `Stopwatch` ➋, iterate over each element in `numbers`
    ➌, and invoke `factor_task` with them ➍. You then determine the program’s runtime
    in milliseconds ➎ and print it ➏.
  prefs: []
  type: TYPE_NORMAL
- en: The output shows that some numbers, such as 9,699,690, 4,294,967,296, and 1,307,674,368,000,
    factor almost immediately because they contain small factors. However, the prime
    numbers take quite a while. Note that because the program is single threaded,
    the runtime for the entire program roughly equals the sum of the times taken to
    factorize each number.
  prefs: []
  type: TYPE_NORMAL
- en: What if you treat each `factor_task` as an asynchronous task? [Listing 19-9](ch19.xhtml#ch19ex09)
    illustrates how to do this with `async`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-9: A program using `factor_task` to factorize six different numbers
    asynchronously*'
  prefs: []
  type: TYPE_NORMAL
- en: As in [Listing 19-8](ch19.xhtml#ch19ex08), you initialize a `Stopwatch` to keep
    track of how long the program executes ➊. Next, you initialize a `vector` called
    `factor_tasks` that contains objects of type `future<string>` ➋. You iterate over
    `numbers` ➌, invoking `async` with the `launch::async` strategy, specifying `factor_task`
    as the function object, and passing a `number` as the task’s argument. You invoke
    `emplace_back` on each resulting `future` into `factor_tasks` ➍. Now that `async`
    has launched each task, you iterate over each element of `factor_tasks` ➎, invoke
    `get` on each `task`, and write it to `cout` ➏. Once you’ve received values from
    all the futures, you determine the number of milliseconds it took to run all tasks
    ➐ and write it to `cout` ➑.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to concurrency, the total program time of [Listing 19-9](ch19.xhtml#ch19ex09)
    roughly equals the maximum task execution time (28,988 ms) rather than the sum
    of task execution times, as in [Listing 19-8](ch19.xhtml#ch19ex08) (37,115 ms).
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The times in [Listing 19-8](ch19.xhtml#ch19ex08) and [Listing 19-9](ch19.xhtml#ch19ex09)
    will vary from run to run.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Sharing and Coordinating***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Concurrent programming with asynchronous tasks is simple as long as the tasks
    don’t require synchronization and don’t involve sharing mutable data. For example,
    consider a simple situation in which two threads access the same integer. One
    thread will increment the integer while the other decrements it. To modify a variable,
    each thread must read the variable’s current value, perform an addition or subtraction
    operation, and then write the variable to memory. Without synchronization, the
    two threads will perform these operations in an undefined, interleaved order.
    Such situations are sometimes called *race conditions* because the result depends
    on which thread executes first. [Listing 19-10](ch19.xhtml#ch19ex10) illustrates
    just how disastrous this situation is.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-10: An illustration of how disastrous unsynchronized, mutable,
    shared data access can be*'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*You’ll get different results on each run of the program in [Listing 19-10](ch19.xhtml#ch19ex10)
    because the program has undefined behavior.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 19-10](ch19.xhtml#ch19ex10) involves defining a function called `goat_rodeo`,
    which involves a catastrophic race condition, and a `main` that invokes `goat_rodeo`
    three times. Within `goat_rodeo`, you initialize the shared data `tin_cans_available`
    ➊. Next, you launch an asynchronous task called `eat_cans` ➋ in which a trip of
    goats decrements the shared variable `tin_cans_available` one million times ➌.
    Next, you launch another asynchronous task called `deposit_cans` ➍ in which you
    increment `tin_cans_available` ➎. After launching the two tasks, you wait for
    them to complete by calling `get` (the order doesn’t matter) ➏➐. Once the tasks
    complete, you print the `tin_cans_available` variable ➑.'
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, you might expect `tin_cans_available` to equal zero after each
    task completes. After all, no matter how you order increments and decrements,
    if you perform them in equal number, they’ll cancel. You invoke `goat_rodeo` three
    times, and each invocation produces a wildly different result.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 19-1](ch19.xhtml#ch19tab01) illustrates one of the many ways the unsynchronized
    access in [Listing 19-10](ch19.xhtml#ch19ex10) goes awry.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 19-1:** One Possible Schedule for `eat_cans` and `deposit_cans`'
  prefs: []
  type: TYPE_NORMAL
- en: '| **eat_cans** | **deposit_cans** | **cans_available** |'
  prefs: []
  type: TYPE_TB
- en: '| Read `cans_available` (0) |  | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Read `cans_available` (0) ➊ | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Compute `cans_available+1` (1) |  | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compute `cans_available-1` (-1) ➌ | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Write `cans_available+1` (1) ➋ |  | 1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Write `cans_available-1` (-1) ➍ | -1 |'
  prefs: []
  type: TYPE_TB
- en: '[Table 19-1](ch19.xhtml#ch19tab01) shows how interleaving reads and writes
    invites disaster. In this particular incarnation, the read by `deposit_cans` ➊
    precedes the write from `eat_cans` ➋, so `deposit_cans` computes a stale result
    ➌. If this weren’t bad enough, it clobbers the write from `eat_cans` when it writes
    ➍.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The fundamental problem with this data race is *unsynchronized access to mutable
    shared data*. You might wonder why `cans_available` doesn’t update immediately
    whenever a thread computes `cans_available+1` or `cans_available-1`. The answer
    lies in the fact that each of the rows in [Table 19-1](ch19.xhtml#ch19tab01) represents
    a moment in time when some instruction completes execution, and the instructions
    for adding, subtracting, reading, and writing memory are all separate. Because
    the `cans_available` variable is shared and both threads write to it without synchronizing
    their actions, the instructions get interleaved in an undefined way at runtime
    (with catastrophic results). In the following subsections, you’ll learn three
    tools for dealing with such situations: *mutexes*, *condition variables*, and
    atomics.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutexes**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A *mutual exclusion algorithm* (*mutex*) is a mechanism for preventing multiple
    threads from accessing resources simultaneously. Mutexes are *synchronization
    primitives* that support two operations: lock and unlock. When a thread needs
    to access shared data, it locks the mutex. This operation can block depending
    on the nature of the mutex and whether another thread has the lock. When a thread
    no longer needs access, it unlocks the mutex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `<mutex>` header exposes several mutex options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::mutex` provides basic mutual exclusion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::timed_mutex` provides mutual exclusion with a timeout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::recursive_mutex` provides mutual exclusion that allows recursive locking
    by the same thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::recursive_timed_mutex` provides mutual exclusion that allows recursive
    locking by the same thread and a timeout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `<shared_mutex>` header provides two additional options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::shared_mutex` provides shared mutual exclusion facility, which means
    that several threads can own the mutex at once. This option is typically used
    in scenarios when multiple readers can access shared data but a writer needs exclusive
    access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::shared_timed_mutex` provides shared mutual exclusion facility and implements
    locking with a timeout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*For simplicity, this chapter only covers mutex. See [thread.mutex] for more
    information about the other options.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mutex` class defines only a single, default constructor. When you want
    to obtain mutual exclusion, you call one of two methods on a `mutex` object: `lock`
    or `try_lock`. If you call `lock`, which accepts no arguments and returns `void`,
    the calling thread blocks until the `mutex` becomes available. If you call `try_lock`,
    which accepts no arguments and returns a `bool`, it returns immediately. If the
    `try_lock` successfully obtained mutual exclusion, it returns `true` and the calling
    thread now owns the lock. If `try_lock` was unsuccessful, it returns `false` and
    the calling thread doesn’t own the lock. To release a mutual exclusion lock, you
    simply call the method `unlock`, which accepts no arguments and returns `void`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 19-11](ch19.xhtml#ch19ex11) shows a lock-based way to solve the race
    condition in [Listing 19-10](ch19.xhtml#ch19ex10).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-11: Using a `mutex` to resolve the race condition in [Listing 19-10](ch19.xhtml#ch19ex10)*'
  prefs: []
  type: TYPE_NORMAL
- en: You add a `mutex` into `goat_rodeo` ➊ called `tin_can_mutex`, which provides
    mutual exclusion on the `tin_cans_available`. Inside each asynchronous task, a
    thread acquires a lock ➋➍ before modifying `tin_cans_available`. Once the thread
    is done modifying, it unlocks ➌➎. Notice that the resulting number of available
    tin cans at the end of each run is zero ➏➐➑, reflecting that you’ve fixed your
    race condition.
  prefs: []
  type: TYPE_NORMAL
- en: '**MUTEX IMPLEMENTATIONS**'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, mutexes are implemented in a number of ways. Perhaps the simplest
    mutex is a *spin lock* in which a thread will execute a loop until the lock is
    released. This kind of lock usually minimizes the amount of time between a lock
    getting released by one thread and acquired by another. But it’s computationally
    expensive because a CPU is spending all of its time checking for lock availability
    when some other thread could be doing productive work. Typically, mutexes require
    atomic instructions, such as `compare-and-swap`, `fetch-and-add`, or `test-and-set`,
    so they can check for and acquire a lock in one operation.
  prefs: []
  type: TYPE_NORMAL
- en: Modern operating systems, like Windows, offer more efficient alternatives to
    spin locks. For example, mutexes based on *asynchronous procedure calls* allow
    threads to wait on a mutex and go into a *wait state*. Once the mutex becomes
    available, the operating system awakens the waiting thread and hands off ownership
    of the mutex. This allows other threads to do productive work on a CPU that would
    otherwise be occupied in a spin lock.
  prefs: []
  type: TYPE_NORMAL
- en: In general, you won’t need to worry about the details of how mutexes are implemented
    by your operating system unless they become a bottleneck in your program.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re thinking that handling `mutex` locking is a perfect job for an RAII
    object, you’re right. Suppose you forgot to invoke `unlock` on a mutex, say because
    it threw an exception. When the next thread comes along and attempts to acquire
    the mutex with `lock`, your program will come to a screeching halt. For this reason,
    the stdlib provides RAII classes for handling mutexes in the `<mutex>` header.
    There you’ll find several class templates, all of which accept mutexes as constructor
    parameters and a template parameter corresponding to the class of the mutexes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::lock_guard` is a non-copyable, non-moveable RAII wrapper that accepts
    a mutex object in its constructor, where it calls `lock`. It then calls `unlock`
    in the destructor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::scoped_lock` is a deadlock avoiding RAII wrapper for multiple mutexes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::unique_lock` implements a movable mutex ownership wrapper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::shared_lock` implements a movable shared mutex ownership wrapper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For brevity, this section focuses on `lock_guard`. [Listing 19-12](ch19.xhtml#ch19ex12)
    shows how to refactor [Listing 19-11](ch19.xhtml#ch19ex11) to use `lock_guard`
    instead of manual `mutex` manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-12: Refactoring [Listing 19-11](ch19.xhtml#ch19ex11) to use `lock_guard`*'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than using `lock` and `unlock` to manage mutual exclusion, you construct
    a `lock_guard` at the beginning of each scope where you need synchronization ➊➋.
    Because your mutual exclusion mechanism is a `mutex`, you specify it as your `lock_guard`
    template parameter. [Listing 19-11](ch19.xhtml#ch19ex11) and [Listing 19-12](ch19.xhtml#ch19ex12)
    have equivalent runtime behavior, including how long it takes the programs to
    execute. RAII objects don’t involve any additional runtime costs over programming
    releases and acquisitions by hand.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, mutual exclusion locks involve runtime costs. You might also
    have noticed that executing [Listings 19-11](ch19.xhtml#ch19ex11) and [19-12](ch19.xhtml#ch19ex12)
    took substantially longer than executing [Listing 19-10](ch19.xhtml#ch19ex10).
    The reason is that acquiring and releasing locks is a relatively expensive operation.
    In [Listings 19-11](ch19.xhtml#ch19ex11) and [19-12](ch19.xhtml#ch19ex12), the
    `tin_can_mutex` gets acquired and then released two million times. Relative to
    incrementing or decrementing an integer, acquiring or releasing a lock takes substantially
    more time, so using a mutex to synchronize the asynchronous tasks is suboptimal.
    In certain situations, you can take a potentially more efficient approach by using
    atomics.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*For more information about asynchronous tasks and futures, refer to [futures.async].*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Atomics**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The word *atomic* comes from the Greek *átomos*, meaning “indivisible.” An operation
    is atomic if it occurs in an indivisible unit. Another thread cannot observe the
    operation halfway through. When you introduced locks into [Listing 19-10](ch19.xhtml#ch19ex10)
    to produce [Listing 19-11](ch19.xhtml#ch19ex11), you made the increment and decrement
    operations atomic because the asynchronous tasks could no longer interleave read
    and write operations on `tin_cans_available`. As you experienced running this
    lock-based solution, this approach is very slow because acquiring locks is expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is to use the `std::atomic` class template in the `<atomic>`
    header, which provides primitives often used in *lock-free concurrent programming*.
    Lock-free concurrent programming solves data race issues without involving locks.
    On many modern architectures, CPUs support atomic instructions. Using atomics,
    you might be able to avoid locks by leaning on atomic hardware instructions.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter doesn’t discuss `std::atomic` or how to devise your own lock-free
    solutions in detail, because it’s incredibly difficult to do correctly and is
    best left to experts. However, in simple situations, such as in [Listing 19-10](ch19.xhtml#ch19ex10),
    you can employ a `std::atomic` to make sure that the increment or decrement operations
    cannot be divided. This neatly solves your data race problem.
  prefs: []
  type: TYPE_NORMAL
- en: The `std::atomic` template offers specializations for all fundamental types,
    as shown in [Table 19-2](ch19.xhtml#ch19tab02).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 19-2:** `std::atomic` Template Specializations for the Fundamental
    Types'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Template specialization** | **Alias** |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<bool>` | `std::atomic_bool` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<char>` | `std::atomic_char` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<unsigned char>` | `std::atomic_uchar` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<short>` | `std::atomic_short` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<unsigned short>` | `std::atomic_ushort` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<int>` | `std::atomic_int` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<unsigned int>` | `std::atomic_uint` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<long>` | `std::atomic_long` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<unsigned long>` | `std::atomic_ulong` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<long long>` | `std::atomic_llong` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<unsigned long long>` | `std::atomic_ullong` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<char16_t>` | `std::atomic_char16_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<char32_t>` | `std::atomic_char32_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::atomic<wchar_t>` | `std::atomic_wchar_t` |'
  prefs: []
  type: TYPE_TB
- en: '[Table 19-3](ch19.xhtml#ch19tab03) lists some of the supported operations for
    `std::atomic`. The `std::atomic` template has no copy constructor.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 19-3:** Supported Operations for `std::atomic`'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operation** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| a`{}`a`{ 123 }` | Default constructor.Initializes value to 123. |'
  prefs: []
  type: TYPE_TB
- en: '| a`.is_lock_free()` | Returns true if a is lock-free. (Depends on the CPU.)
    |'
  prefs: []
  type: TYPE_TB
- en: '| a`.store(123)` | Stores the value 123 into a. |'
  prefs: []
  type: TYPE_TB
- en: '| a`.load()`a`()` | Returns the stored value. |'
  prefs: []
  type: TYPE_TB
- en: '| a`.exchange(123)` | Replaces the current value with 123 and returns the old
    value. This is a “read-modify-write” operation. |'
  prefs: []
  type: TYPE_TB
- en: '| a`.compare_exchange_weak(10, 20)`a`.compare_exchange_strong(10, 20)` | If
    the current value is 10, replaces with 20\. Returns true if the value was replaced.
    See `[atomic]` for details on weak versus strong. |'
  prefs: []
  type: TYPE_TB
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Specializations for the types in <cstdint> are also available. See [atomics.syn]
    for details.*'
  prefs: []
  type: TYPE_NORMAL
- en: For the numeric types, the specializations offer additional operations, as listed
    in [Table 19-4](ch19.xhtml#ch19tab04).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 19-4:** Supported Operations for Numeric Specializations of a `std::atomic`
    a'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operation** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| a`.fetch_add(123)`a`+=123` | Replaces the current value with the result of
    adding the argument to the current value. Returns the value before modification.
    This is a “read-modify-write” operation. |'
  prefs: []
  type: TYPE_TB
- en: '| a`.fetch_sub(123)`a`-=123` | Replaces the current value with the result of
    subtracting the argument from the current value. Returns the value before modification.
    This is a “read-modify-write” operation. |'
  prefs: []
  type: TYPE_TB
- en: '| a`.fetch_and(123)`a`&=123` | Replaces the current value with the result of
    bitwise ANDing the argument with the current value. Returns the value before modification.
    This is a “read-modify-write” operation. |'
  prefs: []
  type: TYPE_TB
- en: '| a`.fetch_or(123)`a`&#124;=123` | Replaces the current value with the result
    of bitwise ORing the argument with the current value. Returns the value before
    modification. This is a “read-modify-write” operation. |'
  prefs: []
  type: TYPE_TB
- en: '| a`.fetch_xor(123)`a`^=123` | Replaces the current value with the result of
    bitwise XORing the argument with the current value. Returns the value before modification.
    This is a “read-modify-write” operation. |'
  prefs: []
  type: TYPE_TB
- en: '| a`++`a`--` | Increments or decrements a. |'
  prefs: []
  type: TYPE_TB
- en: Because [Listing 19-12](ch19.xhtml#ch19ex12) is a prime candidate for a lock-free
    solution, you can replace the type of `tin_cans_available` with `atomic_int` and
    remove the `mutex`. This prevents race conditions like the one illustrated in
    [Table 19-1](ch19.xhtml#ch19tab01). [Listing 19-13](ch19.xhtml#ch19ex13) implements
    this refactor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-13: Resolving the race condition using `atomic_int` rather than
    `mutex`*'
  prefs: []
  type: TYPE_NORMAL
- en: You replace `int` with `atomic_int` ➊ and remove the `mutex`. Because the decrement
    ➋ and increment ➌ operators are atomic, the race condition remains solved.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*For more information about atomics, refer to [atomics].*'
  prefs: []
  type: TYPE_NORMAL
- en: You also probably noticed a considerable performance boost from [Listing 19-12](ch19.xhtml#ch19ex12)
    to [19-13](ch19.xhtml#ch19ex12). In general, using atomic operations will be much
    faster than acquiring a mutex.
  prefs: []
  type: TYPE_NORMAL
- en: '**WARNING**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless you have a very simple concurrent access problem, such as the one in
    this section, you really shouldn’t try to implement lock-free solutions on your
    own. Refer to the Boost Lockfree library for high-quality, thoroughly tested lock-free
    containers. As always, you must decide whether a lock-based or lock-free implementation
    is optimal.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Condition Variables**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A *condition variable* is a synchronization primitive that blocks one or more
    threads until notified. Another thread can notify the condition variable. After
    notification, the condition variable can unblock one or more threads so they can
    make progress. A very popular condition variable pattern involves a thread performing
    the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: Acquire some mutex shared with awaiting threads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the shared state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Notify the condition variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Release the mutex.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Any threads waiting on the condition variable then perform the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: Acquire the mutex.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait on the condition variable (this releases the mutex).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When another thread notifies the condition variable, this thread wakes up and
    can perform some work (this reacquires the mutex automatically).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Release the mutex.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Due to complications arising from the complexity of modern operating systems,
    sometimes threads can wake up spuriously. Therefore, it’s important to verify
    that a condition variable was in fact signaled once a waiting thread awakens.
  prefs: []
  type: TYPE_NORMAL
- en: The stdlib provides `std::condition_variable` in the `<condition_variable>`
    header, which supports several operations, including those in [Table 19-5](ch19.xhtml#ch19tab05).
    The `condition_variable` supports only default construction, and the copy constructor
    is deleted.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 19-5:** Supported Operations of a `std::condition_variable` cv'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operation** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| cv`.notify_one()` | If any threads are waiting on cv, this operation notifies
    one of them. |'
  prefs: []
  type: TYPE_TB
- en: '| cv`.notify_all()` | If any threads are waiting on cv, this operation notifies
    all of them. |'
  prefs: []
  type: TYPE_TB
- en: '| cv`.wait(`lock`, [`pred`])` | Given a lock on the mutex owned by the notifier,
    returns when awakened. If supplied, pred determines whether the notification is
    spurious (returns `false`) or real (returns `true`). |'
  prefs: []
  type: TYPE_TB
- en: '| cv`.wait_for(`lock`, [`durn`], [`pred`])` | Same as cv`.wait` except `wait_for`
    only waits for durn. If timeout occurs and no pred is supplied, returns `std::cv_status::timeout`;
    otherwise, returns `std::cv_status::no_timeout`. |'
  prefs: []
  type: TYPE_TB
- en: '| cv`.wait_until(`lock`, [`time`], [`pred`])` | Same as `wait_for` except uses
    a `std::chrono::time_point` instead of a `std::chrono::duration`. |'
  prefs: []
  type: TYPE_TB
- en: For example, you can refactor [Listing 19-12](ch19.xhtml#ch19ex12) so the *deposit
    cans* task completes before the *eat cans* task using a condition variable, as
    [Listing 19-14](ch19.xhtml#ch19ex14) illustrates.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-14: Using condition variables to ensure all cans are deposited
    before they’re eaten*'
  prefs: []
  type: TYPE_NORMAL
- en: 'You declare a `mutex` ➊ and a `condition_variable` ➋ that you’ll use to coordinate
    the asynchronous tasks. Within the *eat cans* task, you acquire a `unique_lock`
    to the `mutex`, which you pass into `wait` along with a predicate that returns
    `true` if there are cans available ➌. This method will release the mutex and then
    block until two conditions are met: the `condition_variable` awakens this thread
    and one million tin cans are available ➍ (recall that you must check that all
    the cans are available because of spurious wakeups). Within the *deposit cans*
    task, you acquire a lock on the `mutex` ➎, deposit the cans, and then notify all
    threads blocked on the `condition_variable` ➏.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, unlike with all the previous approaches, it’s impossible for `tin_cans_available`
    to be negative because the ordering of deposit cans and eat cans is guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*For more information about condition variables, refer to *[thread.condition]*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Low-Level Concurrency Facilities***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The stdlib’s `<thread>` library contains low-level facilities for concurrent
    programming. The `std::thread` class, for example, models an operating system
    thread. However, it’s best not to use `thread` directly and instead design concurrency
    into your programs with higher-level abstractions, like tasks. Should you require
    low-level thread access, [thread] offers more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the `<thread>` library does include several useful functions for manipulating
    the current thread:'
  prefs: []
  type: TYPE_NORMAL
- en: The `std::this_thread::yield` function accepts no arguments and returns `void`.
    The exact behavior of `yield` depends on the environment, but in general it provides
    a hint that the operating system should give other threads a chance to run. This
    is useful when, for example, there’s high lock contention over a particular resource
    and you want to help all threads get a chance at access.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `std::this_thread::get_id` function accepts no arguments and returns an
    object of type `std::thread::id`, which is a lightweight thread that supports
    comparison operators and `operator<<`. Typically, it’s used as a key in associative
    containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `std::this_thread::sleep_for` function accepts a `std::chrono::duration`
    argument, blocks execution on the current thread until at least the specified
    duration passes, and returns `void`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `std::this_thread::sleep_until` accepts a `std::chrono::time_point` and
    returns void. It is entirely analogous to `sleep_for` except it blocks the thread
    until at least the specified `time_point`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you need these functions, they’re indispensable. Otherwise, you really
    shouldn’t need to interact with the `<thread>` header.
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallel Algorithms**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Chapter 18](ch18.xhtml) introduced the stdlib’s algorithms, many of which
    take an optional first argument called its execution policy encoded by a `std::execution`
    value. In supported environments, there are three possible values: `seq`, `par`,
    and `par_unseq`. The latter two options indicate that you want to execute the
    algorithm in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: '***An Example: Parallel sort***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Listing 19-15](ch19.xhtml#ch19ex15) illustrates how changing a single argument
    from `seq` to `par` can have a massive impact on a program’s runtime by sorting
    a billion numbers both ways.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-15: Sorting a billion numbers using `std::sort` with `std::execution::seq`
    versus `std::execution::par`. (Results are from a Windows 10 x64 machine with
    two Intel Xeon E5-2620 v3 processors.)*'
  prefs: []
  type: TYPE_NORMAL
- en: The `make_random_vector` function ➊ produces a `vector` containing a billion
    unique numbers. You build two copies, `numbers_a` ➋ and `numbers_b` ➌. You sort
    each `vector` separately. In the first case, you sort with a sequential execution
    policy ➍, and `Stopwatch` indicates that the operation took about two and a half
    minutes (about 150 seconds). In the second case, you sort with a parallel execution
    policy ➎. In contrast, `Stopwatch` indicates that the operation took about 18
    seconds. The sequential execution took roughly 8.5 times as long.
  prefs: []
  type: TYPE_NORMAL
- en: '***Parallel Algorithms Are Not Magic***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unfortunately, parallel algorithms aren’t magic. Although they work brilliantly
    in simple situations, such as with `sort` in [Listing 19-15](ch19.xhtml#ch19ex15),
    you must be careful when using them. Any time an algorithm produces side effects
    beyond the target sequence, you have to think hard about race conditions. A red
    flag is any algorithm that passes a function object to the algorithm. If the function
    object has shared mutable state, the executing threads will have shared access
    and you might have a race condition. For example, consider the parallel `transform`
    invocation in [Listing 19-16](ch19.xhtml#ch19ex16).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 19-16: A program containing a race condition due to non-atomic access
    to `n_transformed`*'
  prefs: []
  type: TYPE_NORMAL
- en: You begin by initializing two `vector` objects, `numbers` and `squares`, which
    contain a million elements ➊. Next, you fill one of them with numbers using `iota`
    ➋ and initialize the variable `n_transformed` to `0` ➌. You then invoke `transform`
    with a parallel execution policy, `numbers` as your target sequence, `squares`
    as your result sequence, and a simple lambda ➍. The lambda increments `n_transformed`
    ➎ and returns the square of the argument `x` ➏. Because multiple threads execute
    this lambda, access to `n_transformed` must be synchronized ➐.
  prefs: []
  type: TYPE_NORMAL
- en: The previous section introduced two ways to solve this problem, locks and atomics.
    In this scenario, it’s probably best to just use a `std::atomic_size_t` as a drop-in
    replacement for `size_t`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This chapter surveyed concurrency and parallelism at a very high level. In
    addition, you learned how to launch asynchronous tasks, which allow you to easily
    introduce multithreaded programming concepts into your code. Although introducing
    parallel and concurrent concepts into your programs can provide a significant
    performance boost, you must carefully avoid introducing race conditions that invite
    undefined behavior. You also learned several mechanisms for synchronizing access
    to mutable shared state: mutexes, condition variables, and atomics.'
  prefs: []
  type: TYPE_NORMAL
- en: '**EXERCISES**'
  prefs: []
  type: TYPE_NORMAL
- en: '**19-1.** Write your own spin lock-based mutex called `SpinLock`. Expose a
    `lock`, a `try_lock`, and an `unlock` method. Your class should delete the copy
    constructor. Try using a `std::lock_guard<SpinLock>` with an instance of your
    class.'
  prefs: []
  type: TYPE_NORMAL
- en: '**19-2.** Read about the infamous double-checked locking pattern (DCLP) and
    why you shouldn’t use it. (See the article by Scott Meyers and Andrei Alexandrescu
    mentioned in the following “Further Reading” section.) Then read about the appropriate
    way to ensure that a callable gets invoked exactly once using `std::call_once`
    in [thread.once.callonce].'
  prefs: []
  type: TYPE_NORMAL
- en: '**19-3.** Create a thread-safe queue class. This class must expose an interface
    like `std::queue` (see [queue.defn]). Use a `std::queue` internally to store elements.
    Use a `std::mutex` to synchronize access to this internal `std::queue`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**19-4.** Add a `wait_and_pop` method and a `std::condition_variable` member
    to your thread-safe queue. When a user invokes `wait_and_pop` and the queue contains
    an element, it should pop the element off the queue and return it. If the queue
    is empty, the thread should block until an element becomes available and then
    proceed to pop an element.'
  prefs: []
  type: TYPE_NORMAL
- en: '**19-5.** (Optional) Read the Boost Coroutine2 documentation, especially the
    “Overview,” “Introduction,” and “Motivation” sections.'
  prefs: []
  type: TYPE_NORMAL
- en: '**FURTHER READING**'
  prefs: []
  type: TYPE_NORMAL
- en: '“C++ and The Perils of Double-Checked Locking: [Part I](part01.xhtml#part01)”
    by Scott Meyers and Andrei Alexandrescu ([*http://www.drdobbs.com/cpp/c-and-the-perils-of-double-checked-locki/184405726/*](http://www.drdobbs.com/cpp/c-and-the-perils-of-double-checked-locki/184405726/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ISO International Standard ISO/IEC (2017) — Programming Language C++* (International
    Organization for Standardization; Geneva, Switzerland; *[https://isocpp.org/std/the-standard/](https://isocpp.org/std/the-standard/)*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*C++ Concurrency in Action*, 2nd Edition, by Anthony Williams (Manning, 2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“Effective Concurrency: Know When to Use an Active Object Instead of a Mutex”
    by Herb Sutter ([*https://herbsutter.com/2010/09/24/effective-concurrency-know-when-to-use-an-active-object-instead-of-a-mutex/*](https://herbsutter.com/2010/09/24/effective-concurrency-know-when-to-use-an-active-object-instead-of-a-mutex/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Effective Modern C++: 42 Specific Ways to Improve Your Use of C++ 11 and C++
    14* by Scott Meyers (O’Reilly Media, 2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“A Survey of Modern Integer Factorization Algorithms” by Peter L. Montgomery.
    *CWI Quarterly* 7.4 (1994): 337–365.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
