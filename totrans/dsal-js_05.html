<html><head></head><body>
<section epub:type="chapter" role="doc-chapter" aria-labelledby="ch4">&#13;
<span role="doc-pagebreak" epub:type="pagebreak" id="pg_49" aria-label="49"/>&#13;
<hgroup>&#13;
&#13;
<h2 class="CHAPTER" id="ch4">&#13;
<span class="CN"><span class="SANS_Futura_Std_Bold_Condensed_B_11">4</span></span>&#13;
<span class="CT"><span class="SANS_Dogma_OT_Bold_B_11">ANALYZING ALGORITHMS</span></span>&#13;
</h2>&#13;
</hgroup>&#13;
<figure class="opener"><img class="opener" src="../images/opener.jpg" alt=""/>&#13;
</figure>&#13;
<p class="INTRO">In the previous chapter, we discussed abstract data types, and later in this book we’ll consider many more with alternative implementations and algorithms. When facing several possible ways of implementing the same abstract data types, consider the efficiency of each concrete implementation, which requires an analysis of the involved algorithms. We’ll study the basics of such analysis in this chapter to help us make better decisions. What data structure should you pick? What algorithm should you implement? Knowing objectively how to analyze their performance will produce the right answers.</p>&#13;
<section epub:type="division" aria-labelledby="sec1">&#13;
<span role="doc-pagebreak" epub:type="pagebreak" id="pg_50" aria-label="50"/>&#13;
<h3 class="H1" id="sec1"><span id="h1-15"/><span class="SANS_Futura_Std_Bold_B_11">Performance</span></h3>&#13;
<p class="TNI1">When measuring the efficiency of a given algorithm, the key is to consider the resources (such as time or random access memory [RAM]) the algorithm needs, and then you can compare different algorithms based on the needed amount. (This method doesn’t really apply to small problems. For instance, if you have a dictionary with just a dozen keys, no matter how it’s structured or what algorithm you apply for searching, the results will be fast.)</p>&#13;
<p class="TX">We always want to minimize resource usage (faster processing time, less needed RAM), but we cannot really directly compare time complexity (speed) to space complexity (memory). Often, faster-performing algorithms require larger amounts of memory, and vice versa; smaller, simpler structures may imply slower algorithms. (You’ll see an example later in this chapter.) All of these considerations are moot, however, if an algorithm takes way too much time or requires more RAM than available.</p>&#13;
<p class="TX">In all the cases in this book, we’ll see that the space complexity of algorithms is fairly stable. It grows in direct proportion to the number of input elements, so there really may be no grounds to select one algorithm over another. On the other hand, we’ll see that time complexity results in many variations, providing a solid basis for choosing which data structure to use and which algorithms to implement.</p>&#13;
<p class="TX">Accordingly, whenever the book refers to the complexity of any given algorithm, it’s always referring to time complexity, or how long the algorithm takes to perform its function in relation to the size of its input data.</p>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec2">&#13;
&#13;
<h3 class="H1" id="sec2"><span id="h1-16"/><span class="SANS_Futura_Std_Bold_B_11">Complexity</span></h3>&#13;
<p class="TNI1">All data structures always have some basic parameter upon which the efficiency of all algorithms depends. For instance, if you are searching in a dictionary, the number of keys in the dictionary will probably impact the searching speed; more keys equal more time. If sorting a set of values, having more values means a slower sort; for example, ordering the 5 cards in a poker hand can be done really quickly, but ordering a whole deck of 52 cards takes longer. In all cases, we’ll call that input parameter <i>n</i>, and you’ll express the algorithm’s time complexity as a function of that input; this is <i>analysis of algorithms</i>. An algorithm will be more efficient when that function’s values are small, or at least, it will grow slowly in comparison to the growth of the input size.</p>&#13;
<p class="TX">In some cases, an algorithm’s performance may be directly linked to the data itself; for example, sorting an almost-in-order sequence is likely faster than sorting a completely disordered, random sequence of values. This means we’ll be considering best- or worst-case performance, as well as average performance. If nothing is specified, we’ll aim for an upper bound on the algorithm’s complexity, so in this book, we’ll be looking at worst-case complexity unless otherwise noted.</p>&#13;
<p class="TX">In general, we won’t try (or won’t be able) to get a precise expression for the complexity function. We’ll look at how it compares with common <span role="doc-pagebreak" epub:type="pagebreak" id="pg_51" aria-label="51"/>mathematical functions, such as <i>n</i> or <i>n</i><sup>2</sup> or <i>n</i> log <i>n</i>, and consider in which class an algorithm is in to compare it with others on an equal basis. Algorithms in the same class don’t perform at the same speed, but roughly speaking, all algorithms in the same class will perform in the same way for larger inputs, growing at the same rate and keeping the same relationship among them. In other words, an algorithm that’s 10 times speedier will most likely keep being thus; it won’t become 100 times faster or half as much slower than others in its class.</p>&#13;
<section epub:type="division" aria-labelledby="sec3">&#13;
&#13;
<h4 class="H2" id="sec3"><span id="h2-25"/><span class="SANS_Futura_Std_Bold_Condensed_Oblique_11">Notations for Complexity</span></h4>&#13;
<p class="TNI1">To express a given function’s behavior when its argument grows, we use a family of notations called <i>asymptotic notations.</i> This family includes five different notations, including the most often used: <i>big O</i> notation. The <i>O</i> stands for “order”—or, more accurately, the German word <i>Ordnung</i>. (You’ll see the other four notations soon.)</p>&#13;
<p class="TX">Big <i>O</i> notation groups functions according to how they behave for growing values of their <i>n</i> parameter. Depending on what algorithm or data structure we’re studying, <i>n</i> could be the number of values to sort, the size of a set to be searched, or how many keys are added to a tree. This is made clear on a case-by-case basis when discussing performance.</p>&#13;
<p class="TX">Describing a function in terms of its big <i>O</i> behavior implies an upper bound on how the function grows. Without diving in to mathematical functions too deeply, if the behavior of a function <i>f(n)</i> is <i>O(g(n))</i>, that means that when <i>n</i> grows, both functions grow in the same proportion. (A complete definition also specifies that this relationship need not occur for all values of <i>n</i>, but only for large enough ones. For small values of <i>n</i>, the relationship may not apply.) In other words, saying that the behavior of a given algorithm is <i>O(some function)</i> already implies how the needed time will grow for larger values of <i>n</i>.</p>&#13;
<p class="TX">Let’s get back to the five notations (<a href="chapter4.xhtml#tab4-1">Table 4-1</a>).</p>&#13;
&#13;
<table class="table">&#13;
<caption><p class="TT" id="tab4-1"><span class="Heavy"><span class="SANS_Futura_Std_Heavy_11">Table 4-1:</span></span> <span class="SANS_Futura_Std_Book_11">The Five Asymptotic Notations</span></p></caption>&#13;
<thead>&#13;
<tr>&#13;
<th class="TCH" scope="col"><p class="TCH1"><span class="SANS_Futura_Std_Heavy_11">Notation</span></p></th>&#13;
<th class="TCH" scope="col"><p class="TCH1"><span class="SANS_Futura_Std_Heavy_11">Name</span></p></th>&#13;
<th class="TCH" scope="col"><p class="TCH1"><span class="SANS_Futura_Std_Heavy_11">Description</span></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td class="TBF"><p class="TBGREY"><span class="SANS_Futura_Std_Book_Oblique_11">f(n)</span> <span class="SANS_Futura_Std_Book_Oblique_11">=</span> <span class="SANS_Futura_Std_Book_Oblique_11">o</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">))</span></p></td>&#13;
<td class="TBF"><p class="TBGREY"><span class="SANS_Futura_Std_Book_11">Small</span> <span class="SANS_Futura_Std_Book_Oblique_11">o</span></p></td>&#13;
<td class="TBF"><p class="TBGREY"><span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">) grows much faster than</span> <span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">); the growth rate of</span> <span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">) is strictly greater than that of</span> <span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">)</span><span class="SANS_Futura_Std_Book_Oblique_11">.</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">)</span> <span class="SANS_Futura_Std_Book_Oblique_11">=</span> <span class="SANS_Futura_Std_Book_Oblique_11">O</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">))</span></p></td>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_11">Big</span> <span class="SANS_Futura_Std_Book_Oblique_11">O</span></p></td>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">) is an upper bound for</span> <span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">); the growth rate of</span> <span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">) is greater than or equal to that of</span> <span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">).</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="TB"><p class="TBGREY"><span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">)</span> <span class="SANS_Futura_Std_Book_Oblique_11">=</span> <span class="SANS_Myriad_Pro_Regular_11"><span xml:lang="el" lang="el">Θ</span></span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">))</span></p></td>&#13;
<td class="TB"><p class="TBGREY"><span class="SANS_Futura_Std_Book_11">Big Theta</span></p></td>&#13;
<td class="TB"><p class="TBGREY"><span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">) is a bound from above and below for</span> <span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">); both</span> <span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">) and</span> <span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">) grow at the same rate.</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">)</span> <span class="SANS_Futura_Std_Book_Oblique_11">=</span> <span class="SANS_Futura_Std_Book_11"><span xml:lang="el" lang="el">Ω</span>(</span><span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">))</span></p></td>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_11">Big Omega</span></p></td>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">) is a lower bound for</span> <span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">); the growth rate of</span> <span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">) is less than or equal to that of</span> <span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">).</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="TBL"><p class="TBGREY"><span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">)</span> <span class="SANS_Futura_Std_Book_Oblique_11">=</span> <span class="SANS_Myriad_Pro_Regular_11"><span xml:lang="el" lang="el">ω</span></span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">))</span></p></td>&#13;
<td class="TBL"><p class="TBGREY"><span class="SANS_Futura_Std_Book_11">Small omega</span></p></td>&#13;
<td class="TBL"><p class="TBGREY"><span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">) grows much slower than</span> <span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">); the growth rate of</span> <span class="SANS_Futura_Std_Book_Oblique_11">g</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">) is strictly less than that of</span> <span class="SANS_Futura_Std_Book_Oblique_11">f</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">).</span></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="TX">We’ll mainly be using the big <i>O</i> notation; the others are included for completeness. Big theta is more accurate than big <i>O</i>, which is really a <span role="doc-pagebreak" epub:type="pagebreak" id="pg_52" aria-label="52"/>bound, but you are aiming for a good, close one that doesn’t behave too differently from the original function. Getting a precise, exact expression for the behavior of any algorithm is quite complex (and there still are many algorithms for which the precise order isn’t yet known), so working with orders is appropriate. For example, if your personal debt is a few dollars or a few millions, actual numbers aren’t really needed to know that in the former case you’re doing very well and in the latter you’re in serious trouble.</p>&#13;
<blockquote>&#13;
<p class="Note"><span class="SANS_Dogma_OT_Bold_B_15">NOTE</span></p>&#13;
</blockquote>&#13;
<p class="NOTE-TXT"><i>Donald Knuth, renowned computer scientist, author of The Art of Computer Programming books, and expert on analysis of algorithms, once suggested that the big O should be a big omicron, another Greek character that looks exactly like an uppercase O, but it didn’t pan out. See</i> <span class="note_LinkURL_Italic"><a href="https://danluu.com/knuth-big-o.pdf">https://danluu.com/knuth-big-o.pdf</a></span> <i>for the full story.</i></p>&#13;
<p class="TX">Another (rough) interpretation is that the big <i>O</i> bound represents a worst case, while the big omega bound represents the best case, or the smallest amount of time some algorithm could take. In that sense, the big theta case implies an algorithm with a stable performance, because both the worst and best cases grow at the same rate. With this interpretation, the small <i>o</i> notation means an even worse upper limit, and the small omega would be a worse lower limit in the sense that actual behavior is greatly separated from these two bounds, with quite different growth rates.</p>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec4">&#13;
&#13;
<h4 class="H2" id="sec4"><span id="h2-26"/><span class="SANS_Futura_Std_Bold_Condensed_Oblique_11">Complexity Classes</span></h4>&#13;
<p class="TNI1">Most often we find that algorithms involve only a few common orders. <a href="chapter4.xhtml#tab4-2">Table 4-2</a> shows the orders you’ll see in this chapter.</p>&#13;
&#13;
<table class="table">&#13;
<caption><p class="TT" id="tab4-2"><span class="Heavy"><span class="SANS_Futura_Std_Heavy_11">Table 4-2:</span></span> <span class="SANS_Futura_Std_Book_11">Common Orders</span></p></caption>&#13;
<thead>&#13;
<tr>&#13;
<th class="TCH" scope="col"><p class="TCH1"><span class="SANS_Futura_Std_Heavy_11">Order</span></p></th>&#13;
<th class="TCH" scope="col"><p class="TCH1"><span class="SANS_Futura_Std_Heavy_11">Name</span></p></th>&#13;
<th class="TCH" scope="col"><p class="TCH1"><span class="SANS_Futura_Std_Heavy_11">Example</span></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td class="TBF"><p class="TBGREY"><span class="SANS_Futura_Std_Book_Oblique_11">O</span><span class="SANS_Futura_Std_Book_11">(1)</span></p></td>&#13;
<td class="TBF"><p class="TBGREY"><span class="SANS_Futura_Std_Book_11">Constant</span></p></td>&#13;
<td class="TBF"><p class="TBGREY"><span class="SANS_Futura_Std_Book_11">Accessing the first element of a list and popping the top of a stack (<a href="chapter10.xhtml">Chapter 10</a>)</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_Oblique_11">O</span><span class="SANS_Futura_Std_Book_11">(log</span> <span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">)</span></p></td>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_11">Logarithmic</span></p></td>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_11">Searching an ordered array with binary search (<a href="chapter9.xhtml">Chapter 9</a>) and average height of a binary tree (<a href="chapter12.xhtml">Chapter 12</a>)</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="TB"><p class="TBGREY"><span class="SANS_Futura_Std_Book_Oblique_11">O</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">)</span></p></td>&#13;
<td class="TB"><p class="TBGREY"><span class="SANS_Futura_Std_Book_11">Linear</span></p></td>&#13;
<td class="TB"><p class="TBGREY"><span class="SANS_Futura_Std_Book_11">Searching an unordered array (<a href="chapter9.xhtml">Chapter 9</a>) and inorder traversal of a tree (<a href="chapter12.xhtml">Chapter 12</a>)</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_Oblique_11">O</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span> <span class="SANS_Futura_Std_Book_11">log</span> <span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">)</span></p></td>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_11">Log-linear</span></p></td>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_11">Sorting an array with heapsort and average behavior of quicksort (<a href="chapter6.xhtml">Chapter 6</a>)</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="TB"><p class="TBGREY"><span class="SANS_Futura_Std_Book_Oblique_11">O</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_SUP_11">2</span><span class="SANS_Futura_Std_Book_11">)</span></p></td>&#13;
<td class="TB"><p class="TBGREY"><span class="SANS_Futura_Std_Book_11">Quadratic</span></p></td>&#13;
<td class="TB"><p class="TBGREY"><span class="SANS_Futura_Std_Book_11">Sorting an array with bubble sort and</span> <span class="SANS_Futura_Std_Book_11">worst case for quicksort (<a href="chapter6.xhtml">Chapter 6</a>)</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_Oblique_11">O</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">k</span><span class="SANS_Futura_Std_Book_Oblique_SUP_11">n</span><span class="SANS_Futura_Std_Book_11">)</span></p></td>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_11">Exponential</span></p></td>&#13;
<td class="TB"><p class="TB1"><span class="SANS_Futura_Std_Book_11">Testing whether a binary formula is a tautology (</span><span class="SANS_Futura_Std_Book_Oblique_11">k</span> <span class="SANS_Futura_Std_Book_11">= 2) and a naive implementation of the Fibonacci series (</span><span class="SANS_Futura_Std_Book_Oblique_11">k</span> <span class="SANS_Futura_Std_Book_11">= 1.618)</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="TBL"><p class="TBGREY"><span class="SANS_Futura_Std_Book_Oblique_11">O</span><span class="SANS_Futura_Std_Book_11">(</span><span class="SANS_Futura_Std_Book_Oblique_11">n</span><span class="SANS_Futura_Std_Book_11">!)</span></p></td>&#13;
<td class="TBL"><p class="TBGREY"><span class="SANS_Futura_Std_Book_11">Factorial</span></p></td>&#13;
<td class="TBL"><p class="TBGREY"><span class="SANS_Futura_Std_Book_11">Finding the optimum traveling salesman solution and sorting by random permutations (in <a href="chapter6.xhtml">Chapter 6</a>)</span></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="TX">The last two orders are algorithms that are so slow, you won’t use them in real life; their time complexity grows way too fast to be usable.</p>&#13;
<p class="TX"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_53" aria-label="53"/><a href="chapter4.xhtml#fig4-1">Figure 4-1</a> is a simple chart showing how the seven functions from <a href="chapter4.xhtml#tab4-2">Table 4-2</a> behave. Clearly an <i>O</i>(log <i>n</i>) algorithm would be preferred instead of an <i>O</i>(<i>n</i><sup>2</sup>) algorithm.</p>&#13;
<figure class="IMG"><img class="img7" id="fig4-1" src="../images/Figure4-1.jpg" alt=""/>&#13;
<figcaption><p class="CAP"><span class="SANS_Futura_Std_Book_Oblique_11">Figure 4-1: This chart (drawn using Desmos,</span> <span class="SANS_Futura_Std_Book_11"><a href="https://www.desmos.com/calculator">https://www.desmos.com/calculator</a></span><span class="SANS_Futura_Std_Book_Oblique_11">) shows the seven functions from <a href="chapter4.xhtml#tab4-2">Table 4-2</a>.</span></p></figcaption>&#13;
</figure>&#13;
<p class="TX">The two first orders at the bottom of the chart (constant and logarithmic) are excellently well behaved. When considering linear (the diagonal line from bottom left to upper right) and log-linear orders (the closest curve to the diagonal), growth starts to be important. The next order, quadratic, goes off the chart for <i>x</i> = 10 with the value <i>x</i><sup>2</sup> = 100. Finally, the exponential and factorial orders are even worse behaved; their growth makes them impossible to use.</p>&#13;
<p class="TX">You can look at this behavior another way by answering a simple question: What happens with a given algorithm if the input size is 10 times bigger? If the algorithm is <i>O</i>(1), the amount of time will stay the same, with no growth. With an <i>O</i>(log <i>n</i>) algorithm, the required time would grow, but by a fixed amount. An <i>O</i>(<i>n</i>) algorithm would (nearly) multiply its time by 10, and an <i>O</i>(<i>n</i><sup>2</sup>) algorithm would be around 100 times longer. An <i>O</i>(<i>n</i> log <i>n</i>) algorithm would be in between those two. The difference is clear, but note for future reference that it’s much closer to <i>O</i>(<i>n</i>).</p>&#13;
<p class="TX">The results from the previous paragraph are also the reason why <i>O</i>(<i>n</i>) is used rather than <i>O</i>(9<i>n</i>) or <i>O</i>(22<i>n</i>). The ratio between these three algorithms is constant, so if <i>n</i> grows, they will grow at the same rate. On the <span role="doc-pagebreak" epub:type="pagebreak" id="pg_54" aria-label="54"/>other hand, an <i>O</i>(<i>n</i><sup>2</sup>) algorithm will grow so much faster, it’s really is a class by itself. Constant values are meaningless when comparing classes: <i>O</i>(<i>n</i><sup>2</sup>) will always grow faster (and also become larger) than an <i>O</i>(<i>n</i>) algorithm, even throwing in some constant factor, if <i>n</i> is large enough.</p>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec5">&#13;
&#13;
<h4 class="H2" id="sec5"><span id="h2-27"/><span class="SANS_Futura_Std_Bold_Condensed_Oblique_11">Performance Measurements</span></h4>&#13;
<p class="TNI1">When measuring an algorithm’s performance, the best-case performance is an algorithm’s behavior in ideal conditions; for instance, in the previous section we mentioned doing a search and finding the desired element at the first position of an array. You can’t ever assume you’ll always get this optimum performance, but it’s a baseline to compare other performances.</p>&#13;
<p class="TX">The complementary case is <i>worst-case</i> performance, which means you try to measure how an algorithm will perform in the slowest possible way. For instance, later in the book we’ll see algorithms that usually have <i>O</i>(<i>n</i> log <i>n</i>) performance that may degenerate to <i>O</i>(<i>n</i><sup>2</sup>) performance for specific input data ordering. The worst-case analysis is important, because you should always assume that possibility will happen; it’s the safest (but most pessimistic) analysis.</p>&#13;
<p class="TX">A third possibility is <i>average-case</i> performance, which means determining how an algorithm will behave with typical or random input. In <span class="Xref"><a href="chapter6.xhtml">Chapter 6</a></span> you’ll see that quicksort’s average performance is <i>O</i>(<i>n</i> log <i>n</i>) despite cases when performance is much worse.</p>&#13;
<p class="TX">The fourth possibility is amortized time. Some algorithms often take a short time to perform, but periodically require more time. If you look at one individual operation, the result may be poor, but if you consider the average performance over a long series of operations, you may find that, overall, the amortized time is much better than the worst case, letting you predict the result of sequences of operations.</p>&#13;
<p class="TX">Let’s consider a simple example: adding elements to a fixed-size array. If every time you want to add a new element you need to copy the current array to a new (and longer) array, the cost of each addition would be <i>O</i>(<i>n</i>). However, if the array is full, an alternative strategy is to copy it to a new double-sized array, leaving empty space to wait for future insertions.</p>&#13;
<p class="TX">Let’s look at how this strategy works. Consider a situation with an array that is almost fully occupied (cells in gray), with just one empty space (cell in white) at the end, as shown in <a href="chapter4.xhtml#fig4-2">Figure 4-2</a>.</p>&#13;
<figure class="IMG"><img class="img4" id="fig4-2" src="../images/Figure4-2.jpg" alt=""/>&#13;
<figcaption><p class="CAP"><span class="SANS_Futura_Std_Book_Oblique_11">Figure 4-2: An array with only one empty space</span></p></figcaption>&#13;
</figure>&#13;
<p class="TX">When adding a new element (an <i>O</i>(1) operation with constant time), the array is full but you don’t need to worry yet (<a href="chapter4.xhtml#fig4-3">Figure 4-3</a>).</p>&#13;
<span role="doc-pagebreak" epub:type="pagebreak" id="pg_55" aria-label="55"/>&#13;
<figure class="IMG"><img class="img4" id="fig4-3" src="../images/Figure4-3.jpg" alt=""/>&#13;
<figcaption><p class="CAP"><span class="SANS_Futura_Std_Book_Oblique_11">Figure 4-3: Now the array is full.</span></p></figcaption>&#13;
</figure>&#13;
<p class="TX">However, if you need to add another element, there’s no place for it, so you copy the array to a new double-sized one and then add the new value, as shown in <a href="chapter4.xhtml#fig4-4">Figure 4-4</a>.</p>&#13;
<figure class="IMG"><img class="img5" id="fig4-4" src="../images/Figure4-4.jpg" alt=""/>&#13;
<figcaption><p class="CAP"><span class="SANS_Futura_Std_Book_Oblique_11">Figure 4-4: A new double-sized array provides space for the new value and more.</span></p></figcaption>&#13;
</figure>&#13;
<p class="TX">After this process, which is an <i>O</i>(<i>n</i>) operation, you now have <i>n</i> free cells and may rest easily, knowing that upcoming insertions won’t need any copying and will be <i>O</i>(1). The next time the array becomes full, the process will be repeated: a lengthy single duplication followed by many fast additions. Averaging the cost of many insertions, the costlier (infrequent) doubling will be compensated for by the inexpensive (frequent) simple additions, and the amortized performance will be <i>O</i>(1).</p>&#13;
<blockquote>&#13;
<p class="Note"><span class="SANS_Dogma_OT_Bold_B_15">NOTE</span></p>&#13;
</blockquote>&#13;
<p class="NOTE-TXT"><i>The following section is much more mathematically minded than the rest of the book. If you wish, you can skip the demonstrations and study only the results. The rest of the book won’t delve into so much math. This section is simply to give you a taste of what complete, formal proofs look like.</i></p>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec6">&#13;
&#13;
<h4 class="H2" id="sec6"><span id="h2-28"/><span class="SANS_Futura_Std_Bold_Condensed_Oblique_11">Analysis of Algorithms in Practice</span></h4>&#13;
<p class="TNI1">Let’s consider examples of actual orders. Suppose you want to search an ordered array of length <i>n</i> for a given key. The worst case, linear search, is going sequentially through the whole array (because you haven’t yet learned the better algorithms described later in the book) without finding the key. In this case, the linear search performance is <i>O</i>(<i>n</i>) because you have to go through the whole array: <i>n</i> steps and <i>n</i> (failed) tests. The best-case performance is finding the key you wanted on the first attempt: <span class="greek"><span xml:lang="el" lang="el">Ω</span></span>(1).</p>&#13;
<p class="TX">The average requires a bit of algebra. You need to consider all cases: you could find the given element at the first place, the second place, and so on, all the way up the <i>n</i>th, which means <i>n</i> possibilities in all. On average, you have to test (1 + 2 + ... + <i>n</i>)/<i>n</i> elements. The sum of numbers from 1 to <i>n</i> equals <i>n</i>(<i>n</i> + 1)/2, so the average needed (dividing by <i>n</i>) ends up (<i>n</i> + 1)/2. This expression is clearly proportional to <i>n</i>, so the average behavior of the algorithm is indeed <i>O</i>(<i>n</i>). If you had to consider using this <span role="doc-pagebreak" epub:type="pagebreak" id="pg_56" aria-label="56"/>algorithm, you’d think in terms of <i>O</i>(<i>n</i>), assuming the worst; hoping for the best case isn’t realistic.</p>&#13;
<blockquote>&#13;
<p class="Note"><span class="SANS_Dogma_OT_Bold_B_15">NOTE</span></p>&#13;
</blockquote>&#13;
<p class="NOTE-TXT"><i>There’s another way to look at this calculation. The search could succeed at the first element or could take up to the nth; on average, (n + 1)/2. Or, it could succeed either at the second element or the (n – 1)th; on average again, (n + 1)/2. The same reasoning applies for the third, fourth, and subsequent elements. For each case in which the search finishes in a few steps, a complementary case drives the average number of steps up to (n + 1)/2. Since in every case the average is the same, you can conclude that’s the result. You arrive at the same result with a bit more “hand waving” but less algebra.</i></p>&#13;
<p class="TX">Let’s discuss another way of searching an ordered array, a binary search, which you’ll see in <span class="Xref"><a href="chapter9.xhtml">Chapter 9</a></span>. Instead of starting at the beginning of an array and going through all its elements, you start at the <i>middle</i> of the array. If you find the key you want, you’re done. If not, you can discard half the array (if the key you want is less than the middle element, you know it can’t be in the higher part of the array) and recursively search in the other part. You search in that new part by picking its middle element, comparing, and so on.</p>&#13;
<p class="TX">Consider an array with the numbers 4, 9, 12, 22, 34, 56, and 60. If you wanted to check whether 12 was in it, first you’d look at the middle element: 22. That’s not what you want, so you can discard the second half of the array (34, 56, and 60), because you know that 12, if present, must be in the first half. Now look for 12 in the array that is now 4, 9, and 12. Start by looking at its middle element (9) and then discard it and the first half of the array (4). The last step of the search looks at an array with a single element (12). Its middle (and only) element is what you were looking for, so the search succeeded. If you were looking for 13 instead, the search would fail at this point, since no more pieces of the array exist.</p>&#13;
<p class="TX">To see how this algorithm performs, count how many times you need to test an element; assume that the array’s length <i>n</i> is 2<i><sup>k–</sup></i><sup>1</sup> for some <i>k</i> &gt; 0, so all halves of the array always have an odd number. (This is just to simplify calculations; see question 4.9.) In one case the element is found on the first attempt. In two cases the key is found on the second try—namely, the middle elements of the chosen halves. In four cases, the third try is successful, and in eight cases, the fourth try succeeds. <a href="chapter4.xhtml#fig4-5">Figure 4-5</a> shows this for an array with 15 elements.</p>&#13;
<figure class="IMG"><img class="img5" id="fig4-5" src="../images/Figure4-5.jpg" alt=""/>&#13;
<figcaption><p class="CAP"><span class="SANS_Futura_Std_Book_Oblique_11">Figure 4-5: Starting in the middle of an array with 15 elements</span></p></figcaption>&#13;
</figure>&#13;
<p class="TX">For a general array, the total number of comparisons is <i>S</i> = (1 × 1 + 2 × 2 + 3 × 4 + 4 × 8 + ...+ <i>k</i> × 2<i><sup>k</sup></i> <sup>− 1</sup>), which you must divide by the number of elements in the array to get the average. To calculate <i>S</i> do a math trick and first write a more general formula. Write <i>S</i> = 1 × 2<sup>0</sup> + 2 × 2<sup>1</sup> + 3 × 2<sup>2</sup> + ... + <i>k</i> × 2<i><sup>k</sup></i><sup>−1</sup> and then define <i>f</i>(<i>x</i>) = 1<i>x</i><sup>0</sup> + 2<i>x</i><sup>1</sup> + 3<i>x</i><sup>2</sup> + ... + <i>kx</i><i><sup>k</sup></i><sup>−1</sup>; note that S = <i>f</i>(2). It <span role="doc-pagebreak" epub:type="pagebreak" id="pg_57" aria-label="57"/>follows from calculus that <i>f</i>(<i>x</i>) is the derivative of <i>g</i>(<i>x</i>) = 1 + <i>x + x</i><sup>2</sup><i>+ x</i><sup>3</sup> <i>+ ... + x</i><i><sup>k</sup></i>. Since a well-known result says that <i>g</i>(<i>x</i>) = (<i>x</i><i><sup>k</sup></i><sup>+1</sup> – 1)/(<i>x</i> – 1), by deriving you find the following:</p>&#13;
<p class="Equation"><span class="epub"><math display="inline" alttext="Equation"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><msup><mi>x</mi><mi>k</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>−</mo><mfenced><mrow><msup><mi>x</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>−</mo><mn>1</mn></mrow></mfenced></mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mfrac></mrow></math></span><span class="mobi"><img class="img1" src="../images/pg57.jpg" alt=""/></span></p>&#13;
&#13;
&#13;
<p class="TX">Now undo the generalization you just made by setting <i>x</i> = 2 and remembering that <i>n</i> = 2<i><sup>k</sup></i><sup>–1</sup>. You can write <i>S</i> = (<i>k</i> + 1)(<i>n</i> + 1) – (2<i>n</i> + 1). Dividing by <i>n</i> you find that the average number of comparisons is (<i>k</i> – 1) + <i>k</i>/<i>n</i>. You can write <i>k</i> = log <i>n</i> (taking logarithms in base 2 and rounding upward) so the average performance of the algorithm is <span class="greek_alt"><span xml:lang="el" lang="el">Θ</span></span>(log <i>n</i>). Whew! The worst case (a failed search) requires <i>k</i> tests, so again you are justified in saying that binary search is an <i>O</i>(log <i>n</i>) algorithm.</p>&#13;
<p class="TX">Using big <i>O</i> notation is “safer” and provides better “cover.” Of course, you could also say that the binary search is <i>o</i>(<i>n</i>) or, even worse, <i>o</i>(<i>n</i><sup>2</sup>) because those functions behave in a worse way, growing faster. The small <i>o</i> and small omega bounds are good for a rough estimate, but you want to be more precise and aim for closer bounds whenever possible.</p>&#13;
<p class="TX">Most analysis of algorithms involves recurrences as shown here, and some studies are even more complex mathematically than what you just saw. Recurrences usually take a few well-known forms, such as <i>P</i>(<i>n</i>) = <i>aP</i>(<i>n</i> – 1) + <i>f</i>(<i>n</i>) or <i>Q</i>(<i>n</i>) = <i>aQ</i>(<i>n</i>/<i>b</i>) + <i>f</i>(<i>n</i>), among practically infinite possibilities. There are several tricks to help find expressions for <i>M</i>(<i>n</i>) in each case (in particular, the “master theorem” quickly provides a solution to the <i>Q</i>(<i>n</i>) recurrence style, but a whole book could be written on that).</p>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec7">&#13;
&#13;
<h4 class="H2" id="sec7"><span id="h2-29"/><span class="SANS_Futura_Std_Bold_Condensed_Oblique_11">Time and Space Complexity Trade-offs</span></h4>&#13;
<p class="TNI1">Earlier in the chapter, we mentioned we’d look at time performance, because from the point of view of the storage requirements, algorithms are usually well behaved. Let’s explore a simple problem and see how time and space trade-offs apply.</p>&#13;
<p class="TX">Say we have a (long) array of numbers and frequently require finding the sum of the values in a range of positions, from <i>i</i> to <i>j</i>, both inclusive, with <i>i</i> &lt; <i>j</i>. (This problem has to do with breaking a long string of text into justified lines.) A first solution just needs a couple of auxiliary variables, so extra memory requirements are <i>O</i>(1), but finding the sums themselves requires <i>O</i>(<i>n</i>) time:</p>&#13;
<pre id="pre-55"><code>const sumRange = (arr, from, to) =&gt; {&#13;
  let sum = 0;&#13;
  for (let i = from; i &lt;= to; i++) {&#13;
    sum += arr[i];&#13;
  }&#13;
  return sum;&#13;
};</code></pre>&#13;
<p class="TX"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_58" aria-label="58"/>This function is clear and correct—it consists of just a loop summing all the values between <span class="SANS_TheSansMonoCd_W5Regular_11">from</span> and <span class="SANS_TheSansMonoCd_W5Regular_11">to</span> inclusive in <span class="SANS_TheSansMonoCd_W5Regular_11">sum</span>—but its performance will impact the process negatively, because you are calling it frequently. For a function that will be called many times, a better performance is preferred.</p>&#13;
<p class="TX">You also can apply a concept of dynamic programming (which we’ll study in more detail in <span class="Xref"><a href="chapter5.xhtml">Chapter 5</a></span>) and work by tabulation, precomputing the sums from position 0 to all other positions:</p>&#13;
<pre id="pre-56"><code>const precomputeSums = (arr) =&gt; {&#13;
  let partial = Array(arr.length);&#13;
  partial[0] = arr[0];&#13;
  for (i=1; i&lt;arr.length; i++) {&#13;
    partial[i] = partial[i-1] + arr[i];&#13;
  }   &#13;
}</code></pre>&#13;
<p class="TX">With this array of partial sums, if you need the sum of elements 0 to <i>q</i>, you already have those, and for the sum of elements <i>p</i> &gt; 0 to <i>q</i>, just calculate the sum of elements from 0 to <i>q</i> minus the sum of elements from 0 to <i>p</i> – 1:</p>&#13;
<pre id="pre-57"><code>const sumRange2 = (partial, from, to) =&gt;&#13;
  from === 0 ? partial[to] : partial[to] – partial[from-1];</code></pre>&#13;
<p class="Continued">This solution implies further <i>O</i>(<i>n</i>) processing to compute the partial sums, which is only done once, and <i>O</i>(<i>n</i>) extra memory, but it provides <i>O</i>(1) sums for ranges, so you can see the trade-off: use more memory to apply faster algorithms or save memory by accepting a slower performance.</p>&#13;
<p class="TX">Which version should you choose? That depends on the problem and whether the current performance is acceptable, and even possibly whether enough memory is available!</p>&#13;
</section>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec8">&#13;
&#13;
<h3 class="H1" id="sec8"><span id="h1-17"/><span class="SANS_Futura_Std_Bold_B_11">Summary</span></h3>&#13;
<p class="TNI1">In this chapter, we’ve discussed the definitions that are relevant to studying how algorithms perform in terms of either operations or memory requirements. We’ve seen several classes of algorithms that help decide how to implement a given solution to a problem by comparing efficiency in response to larger inputs. In the next chapter, we’ll switch gears and study ways to create algorithms in preparation for the rest of the book, where we’ll consider many varied data structures and the algorithms that perform them.</p>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec9">&#13;
&#13;
<h3 class="H1" id="sec9"><span id="h1-18"/><span class="SANS_Futura_Std_Bold_B_11">Questions</span></h3>&#13;
<p class="TNI1">The questions in this chapter are visibly different from all other questions in the book because they are more mathematically oriented. Feel free to skip ahead.</p>&#13;
<p class="ListHead"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_59" aria-label="59"/><b>4.1  How Fast Did You Say?</b></p>&#13;
<p class="ListPlainFirst">An analyst has just completed a study of a new algorithm and concludes that its running time, depending on its input size <i>n</i>, is exactly 17<i>n</i> log <i>n</i> – 2<i>n</i><sup>2</sup> + 48. What do you say about that result?</p>&#13;
<p class="ListHead"><b>4.2  Weird Bound?</b></p>&#13;
<p class="ListPlainFirst">Is it valid to say that <i>n</i> is <i>O</i>(<i>n</i><sup>2</sup>)? What about <i>o</i>(<i>n</i><sup>2</sup>)? Other orders?</p>&#13;
<p class="ListHead"><b>4.3  Of Big</b> <b><i>O</i></b><b>s and Omegas</b></p>&#13;
<p class="ListPlainFirst">What can you deduce if a certain function is both <i>f</i>(<i>n</i>) = <i>O</i>(<i>g</i>(<i>n</i>)) and <i>f</i>(<i>n</i>) = <span class="listplain_greek"><span xml:lang="el" lang="el">Ω</span></span>(<i>g</i>(<i>n</i>))?</p>&#13;
<p class="ListHead"><b>4.4  Transitivity?</b></p>&#13;
<p class="ListPlainFirst">If <i>f</i>(<i>n</i>) = <i>O</i>(<i>g</i>(<i>n</i>)) and <i>g</i>(<i>n</i>) = <i>O</i>(<i>h</i>(<i>n</i>)), how are <i>f</i>(<i>n</i>) and <i>h</i>(<i>n</i>) related? What if instead of big <i>O</i>, you were looking at other orders: small <i>o</i>, big theta, and so on?</p>&#13;
<p class="ListHead"><b>4.5  A Bit of Reflection</b></p>&#13;
<p class="ListPlainFirst">It seems clear that for any function <i>f</i>(<i>n</i>), you have <i>f</i>(<i>n</i>) = <span class="greek_alt"><span xml:lang="el" lang="el">Θ</span></span>(<i>f</i>(<i>n</i>)). What would you say if working with other orders instead of big theta?</p>&#13;
<p class="ListHead"><b>4.6  Going at It Backward</b></p>&#13;
<p class="ListPlainFirst">If <i>f</i>(<i>n</i>) = <i>O</i>(<i>g</i>(<i>n</i>)), what is the order of <i>g</i>(<i>n</i>) relative to <i>f</i>(<i>n</i>)? What if <i>f</i>(<i>n</i>) = <i>o</i>(<i>g</i>(<i>n</i>))?</p>&#13;
<p class="ListHead"><b>4.7  One After the Other</b></p>&#13;
<p class="ListPlainFirst">Suppose you have a process that consists of two steps: the first is an <i>O</i>(<i>n</i> log <i>n</i>) algorithm and the second is an <i>O</i>(<i>n</i><sup>2</sup>) algorithm. What’s the order of the whole process? Can you give a general rule?</p>&#13;
<p class="ListHead"><b>4.8  Loop the Loop</b></p>&#13;
<p class="ListPlainFirst">A different but related question: suppose your process consists of an <i>O</i>(<i>n</i>) loop that does an <i>O</i>(<i>n</i><sup>2</sup>) process at each step. What’s the order of the whole? Again, can you provide a general rule?</p>&#13;
<p class="ListHead"><b>4.9  Almost a Power ...</b></p>&#13;
<p class="ListPlainFirst">When analyzing binary search, you learned that if the array’s length is 2<i><sup>k</sup></i> <sup>–1</sup> for some <i>k</i> &gt; 0, the initial array and all subsequent arrays would have an odd length. Can you prove this?</p>&#13;
<p class="ListHead"><b>4.10  It Was the Best of Times; It Was the Worst of Times</b></p>&#13;
<p class="ListPlainFirst">What happens if the best-case running time of an algorithm is <span class="listplain_greek"><span xml:lang="el" lang="el">Ω</span></span>(<i>f</i>(<i>n</i>)) and the worst case is <i>O</i>(<i>f</i>(<i>n</i>))?</p>&#13;
</section>&#13;
</section>&#13;
</body></html>