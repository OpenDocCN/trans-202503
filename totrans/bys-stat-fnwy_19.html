<html><head></head><body>
<h2 class="h2" id="ch15"><span epub:type="pagebreak" id="page_149"/><strong><span class="big">15</span><br/>FROM PARAMETER ESTIMATION TO HYPOTHESIS TESTING: BUILDING A BAYESIAN A/B TEST</strong></h2>&#13;
<div class="image1"><img alt="Image" src="../images/common.jpg"/></div>&#13;
<p class="noindent">In this chapter, we’re going to build our first hypothesis test, an <em>A/B test</em>. Companies often use A/B tests to try out product web pages, emails, and other marketing materials to determine which will work best for customers. In this chapter, we’ll test our belief that removing an image from an email will increase the <em>click-through rate</em> against the belief that removing it will hurt the click-through rate.</p>&#13;
<p class="indent">Since we already know how to estimate a single unknown parameter, all we need to do for our test is estimate both parameters—that is, the conversion rates of each email. Then we’ll use R to run a Monte Carlo simulation and determine which hypothesis is likely to perform better—in other words, <span epub:type="pagebreak" id="page_150"/>which variant, A or B, is superior. A/B tests can be performed using classical statistical techniques such as <em>t</em>-tests, but building our test the Bayesian way will help us understand each part of it intuitively and give us more useful results as well.</p>&#13;
<p class="indent">We’ve covered the basics of parameter estimation pretty well at this point. We’ve seen how to use the PDF, CDF, and quantile functions to learn the likelihood of certain values, and we’ve seen how to add a Bayesian prior to our estimate. Now we want to use our estimates to compare <em>two</em> unknown parameters.</p>&#13;
<h3 class="h3" id="ch15lev1sec1"><strong>Setting Up a Bayesian A/B Test</strong></h3>&#13;
<p class="noindent">Keeping with our email example from the previous chapter, imagine we want to see whether adding an image helps or hurts the conversion rate for our blog. Previously, the weekly email has included some image. For our test we’re going to send one variant with images like usual, and another without images. The test is called an <em>A/B test</em> because we are comparing variant A (with image) and variant B (without) to determine which one performs better.</p>&#13;
<p class="indent">Let’s assume at this point we have 600 blog subscribers. Because we want to exploit the knowledge gained during this experiment, we’re only going to be running our test on 300 of them; that way, we can send the remaining 300 subscribers what we believe to be the most effective variant of the email.</p>&#13;
<p class="indent">The 300 people we’re going to test will be split up into two groups, A and B. Group A will receive the usual email with a big picture at the top, and group B will receive an email with no picture. The hope is that a simpler email will feel less “spammy” and encourage users to click through to the content.</p>&#13;
<h4 class="h4" id="ch15lev2sec1"><strong><em>Finding Our Prior Probability</em></strong></h4>&#13;
<p class="noindent">Next, we need to figure out what prior probability we’re going to use. We’ve run an email campaign every week, so from that data we have a reasonable expectation that the probability of clicking the link to the blog on any given email should be around 30 percent. To make things simple, we’ll use the same prior for both variants. We’ll also choose a pretty weak version of our prior distribution, meaning that it considers a wider range of conversion rates to be probable. We’re using a weak prior because we don’t really know how well we expect B to do, and this is a new email campaign, so other factors could cause a better or worse conversion. We’ll settle on Beta(3,7) for our prior probability distribution. This distribution allows us to represent a beta distribution where 0.3 is the mean, but a wide range of possible alternative rates are considered. We can see this distribution in <a href="ch15.xhtml#ch15fig01">Figure 15-1</a>.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_151"/><a id="ch15fig01"/><img alt="Image" src="../images/15fig01.jpg"/></div>&#13;
<p class="figcap"><em>Figure 15-1: Visualizing our prior probability distribution</em></p>&#13;
<p class="indent">All we need now is our likelihood, which means we need to collect data.</p>&#13;
<h4 class="h4" id="ch15lev2sec2"><strong><em>Collecting Data</em></strong></h4>&#13;
<p class="noindent">We send out our emails and get the results in <a href="ch15.xhtml#ch15tab01">Table 15-1</a>.</p>&#13;
<p class="tabcap" id="ch15tab01"><strong>Table 15-1:</strong> Email Click-through Rates</p>&#13;
<table class="topbot-d">&#13;
<colgroup>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"> </p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Clicked</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Not clicked</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Observed conversion rate</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><strong>Variant A</strong></p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">36</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">114</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">0.24</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><strong>Variant B</strong></p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">50</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">100</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">0.33</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">We can treat each of these variants as a separate parameter we’re trying to estimate. In order to arrive at a posterior distribution for each, we need to combine both their likelihood distribution and prior distribution. We’ve already decided that the prior for these distributions should be Beta(3,7), representing a relatively weak belief in what possible values we expect the conversion rate to be, given no additional information. We say this is a weak belief because we don’t believe very strongly in a particular range of values, and consider all possible rates with a reasonably high probability. For the likelihood of each, we’ll again use the beta distribution, making α the number of times the link was clicked through and β the number of times it was not.</p>&#13;
<p class="indent">Recall that:</p>&#13;
<p class="equ">Beta(α<sub>posterior</sub>, β<sub>posterior</sub>) = Beta(α<sub>prior</sub> + α<sub>likelihood</sub>, β<sub>prior</sub> + β<sub>likelihood</sub>)</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_152"/>Variant A will be represented by Beta(36+3,114+7) and variant B by Beta(50+3,100+7). <a href="ch15.xhtml#ch15fig02">Figure 15-2</a> shows the estimates for each parameter side by side.</p>&#13;
<div class="image"><a id="ch15fig02"/><img alt="Image" src="../images/15fig02.jpg"/></div>&#13;
<p class="figcap"><em>Figure 15-2: Beta distributions for our estimates for both variants of our email</em></p>&#13;
<p class="indent">Clearly, our data suggests that variant B is superior, in that it garners a higher conversion rate. However, from our earlier discussion on parameter estimation, we know that the true conversion rate is one of a range of possible values. We can also see here that there’s an overlap between the possible true conversion rates for A and B. What if we were just unlucky in our A responses, and A’s true conversion rate is in fact much higher? What if we were also just lucky with B, and its conversion rate is in fact much lower? It’s easy to see a possible world in which A is actually the better variant, even though it did worse on our test. So the real question is: how sure can we be that B is the better variant? This is where the Monte Carlo simulation comes in.</p>&#13;
<h3 class="h3" id="ch15lev1sec2"><strong>Monte Carlo Simulations</strong></h3>&#13;
<p class="noindent">The accurate answer to which email variant generates a higher click-through rate lies somewhere in the intersection of the distributions of A and B. Fortunately, we have a way to figure it out: a Monte Carlo simulation. A <em>Monte Carlo simulation</em> is any technique that makes use of random sampling to solve a problem. In this case, we’re going to randomly sample from the two distributions, where each sample is chosen based on its probability in the distribution so that samples in a high-probability region will appear more frequently. For example, as we can see in <a href="ch15.xhtml#ch15fig02">Figure 15-2</a>, a value <em>greater</em> than 0.2 is far more likely to be sampled from A than a value less than 0.2. However, a random sample from distribution B is nearly certain to be above 0.2. In our random sampling, we might pick out a value of 0.2 for variant A <span epub:type="pagebreak" id="page_153"/>and 0.35 for variant B. Each sample is random, and based on the relative probability of values in the A and B distributions. The values 0.2 for A and 0.35 for B both could be the true conversion rate for each variant based on the evidence we’ve observed. This individual sampling from the two distributions confirms the belief that variant B is, in fact, superior to A, since 0.35 is larger than 0.2.</p>&#13;
<p class="indent">However, we could also sample 0.3 for variant A and 0.27 for variant B, both of which are reasonably likely to be sampled from their respective distributions. These are also both realistic possible values for the true conversion rate of each variant, but in this case, they indicate that variant B is actually worse than variant A.</p>&#13;
<p class="indent">We can imagine that the posterior distribution represents all the worlds that could exist based on our current state of beliefs regarding each conversion rate. Every time we sample from each distribution, we’re seeing what one possible world could look like. We can tell visually in <a href="ch15.xhtml#ch15fig01">Figure 15-1</a> that we should expect more worlds where B is truly the better variant. The more frequently we sample, the more precisely we can tell in exactly how many worlds, of all the worlds we’ve sampled from, B is the better variant. Once we have our samples, we can look at the ratio of worlds where B is the best to the total number of worlds we’ve looked at and get an exact probability that B is in fact greater than A.</p>&#13;
<h4 class="h4" id="ch15lev2sec3"><strong><em>In How Many Worlds Is B the Better Variant?</em></strong></h4>&#13;
<p class="noindent">Now we just have to write the code that will perform this sampling. R’s <span class="literal">rbeta()</span> function allows us to automatically sample from a beta distribution. We can consider each comparison of two samples a single trial. The more trials we run, the more precise our result will be, so we’ll start with 100,000 trials by assigning this value to the variable <span class="literal">n.trials</span>:</p>&#13;
<p class="programs">n.trials &lt;- 100000</p>&#13;
<p class="indent">Next we’ll put our prior alpha and beta values into variables:</p>&#13;
<p class="programs">prior.alpha &lt;- 3</p>&#13;
<p class="programs">prior.beta &lt;- 7</p>&#13;
<p class="indent">Then we need to collect samples from each variant. We’ll use <span class="literal">rbeta()</span> for this:</p>&#13;
<p class="programs">a.samples &lt;- rbeta(n.trials,36+prior.alpha,114+prior.beta)<br/>&#13;
b.samples &lt;- rbeta(n.trials,50+prior.alpha,100+prior.beta)</p>&#13;
<p class="indent">We’re saving the results of the <span class="literal">rbeta()</span> samples into variables, too, so we can access them more easily. For each variant, we input the number of people who clicked through to the blog and the number of people who didn’t.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_154"/>Finally, we compare how many times the <span class="literal">b.samples</span> are greater than the <span class="literal">a.samples</span> and divide that number by <span class="literal">n.trials</span>, which will give us the percentage of the total trials where variant B was greater than variant A:</p>&#13;
<p class="programs">p.b_superior &lt;- sum(b.samples &gt; a.samples)/n.trials</p>&#13;
<p class="indent">The result we end up with is:</p>&#13;
<p class="programs">p.b_superior<br/>&#13;
&gt; 0.96</p>&#13;
<p class="indent">What we see here is that in 96 percent of the 100,000 trials, variant B was superior. We can imagine this as looking at 100,000 possible worlds. Based on the distribution of possible conversion rates for each variant, in 96 percent of the worlds variant B was the better of the two. This result shows that, even with a relatively small number of observed samples, we have a pretty strong belief that B is the better variant. If you’ve ever done <em>t</em>-tests in classical statistics, this is roughly equivalent—if we used a Beta(1,1) prior—to getting a <em>p</em>-value of 0.04 from a single-tailed <em>t</em>-test (often considered “statistically significant”). However, the beauty of our approach is that we were able to build this test from scratch using just our knowledge of probability and a straightforward simulation.</p>&#13;
<h4 class="h4" id="ch15lev2sec4"><strong><em>How Much Better Is Each Variant B Than Each Variant A?</em></strong></h4>&#13;
<p class="noindent">Now we can say precisely how certain we are that B is the superior variant. However, if this email campaign were for a real business, simply saying “B is better” wouldn’t be a very satisfactory answer. Don’t you really want to know <em>how much better</em>?</p>&#13;
<p class="indent">This is the real power of our Monte Carlo simulation. We can take the exact results from our last simulation and test how much better variant B is likely to be by looking at how many times greater the B samples are than the A samples. In other words, we can look at this ratio:</p>&#13;
<div class="equ-image"><img alt="Image" src="../images/f0154-01.jpg"/></div>&#13;
<p class="indent">In R, if we take the <span class="literal">a.samples</span> and <span class="literal">b.samples</span> from before, we can compute <span class="literal">b.samples</span>/<span class="literal">a.samples</span>. This will give us a distribution of the relative improvements from variant A to variant B. When we plot out this distribution as a histogram, as shown in <a href="ch15.xhtml#ch15fig03">Figure 15-3</a>, we can see how much we expect variant B to improve our click-through rate.</p>&#13;
<p class="indent">From this histogram we can see that variant B will most likely be about a 40 percent improvement (ratio of 1.4) over A, although there is an entire range of possible values. As we discussed in <a href="ch13.xhtml#ch13">Chapter 13</a>, the cumulative distribution function (CDF) is much more useful than a histogram for reasoning about our results. Since we’re working with data rather than a mathematical function, we’ll compute the <em>empirical</em> cumulative distribution function with R’s <span class="literal">ecdf()</span> function. The eCDF is illustrated in <a href="ch15.xhtml#ch15fig04">Figure 15-4</a>.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_155"/><a id="ch15fig03"/><img alt="Image" src="../images/15fig03.jpg"/></div>&#13;
<p class="figcap"><em>Figure 15-3: A histogram of possible improvements we might see</em></p>&#13;
<div class="image"><a id="ch15fig04"/><img alt="Image" src="../images/15fig04.jpg"/></div>&#13;
<p class="figcap"><em>Figure 15-4: A distribution of possible improvements we might see</em></p>&#13;
<p class="indent">Now we can see our results more clearly. There is really just a small, small chance that A is better, and even if it is better, it’s not going to be by much. We can also see that there’s about a 25 percent chance that variant B is a 50 percent or more improvement over A, and even a reasonable chance it could be more than double the conversion rate! Now, in choosing B over A, we can actually reason about our risk by saying, “The chance that B is 20 percent worse is roughly the same that it’s 100 percent better.” Sounds like a good bet to me, and a much better statement of our knowledge than, “There is a statistically significant difference between B and A.”</p>&#13;
<h3 class="h3" id="ch15lev1sec3"><span epub:type="pagebreak" id="page_156"/><strong>Wrapping Up</strong></h3>&#13;
<p class="noindent">In this chapter we saw how parameter estimation naturally extends to a form of hypothesis testing. If the hypothesis we want to test is “variant B has a better conversion rate than variant A,” we can start by first doing parameter estimation for the possible conversion rates of each variant. Once we know those estimates, we can use the Monte Carlo simulation in order to sample from them. By comparing these samples, we can come up with a probability that our hypothesis is true. Finally, we can take our test one step further by seeing how well our new variant performs in these possible worlds, estimating not only whether the hypothesis is true, but also how much improvement we are likely to see.</p>&#13;
<h3 class="h3" id="ch15lev1sec4"><strong>Exercises</strong></h3>&#13;
<p class="noindent">Try answering the following questions to see how well you understand running A/B tests. The solutions can be found at <em><a href="https://nostarch.com/learnbayes/">https://nostarch.com/learnbayes/</a></em>.</p>&#13;
<ol>&#13;
<li class="noindent">Suppose a director of marketing with many years of experience tells you he believes very strongly that the variant without images (B) won’t perform any differently than the original variant. How could you account for this in our model? Implement this change and see how your final conclusions change as well.</li>&#13;
<li class="noindent">The lead designer sees your results and insists that there’s no way that variant B should perform better with no images. She feels that you should assume the conversion rate for variant B is closer to 20 percent than 30 percent. Implement a solution for this and again review the results of our analysis.</li>&#13;
<li class="noindent">Assume that being 95 percent certain means that you’re more or less “convinced” of a hypothesis. Also assume that there’s no longer any limit to the number of emails you can send in your test. If the true conversion for A is 0.25 and for B is 0.3, explore how many samples it would take to convince the director of marketing that B was in fact superior. Explore the same for the lead designer. You can generate samples of conversions with the following snippet of R:&#13;
<p class="programs">true.rate &lt;- 0.25<br/>&#13;
number.of.samples &lt;- 100<br/>&#13;
results &lt;- runif(number.of.samples) &lt;= true.rate</p></li>&#13;
</ol>&#13;
</body></html>