- en: '**6'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TWEAKING THE TREES**
  prefs: []
  type: TYPE_NORMAL
- en: '*AdaBoost is the best off-the-shelf classifier in the world.*'
  prefs: []
  type: TYPE_NORMAL
- en: —CART co-inventor Leo Breiman, 1996
  prefs: []
  type: TYPE_NORMAL
- en: '*XGBoost is the algorithm of choice for many winning teams of machine learning
    competitions.*'
  prefs: []
  type: TYPE_NORMAL
- en: —Wikipedia entry, 2022
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here we talk about two general techniques in ML, *bagging* and *boosting*, and
    apply them to form extensions of decision tree analysis. The extensions, *random
    forests* and *tree-based gradient boosting*, are widely used—in fact, even more
    so than individual tree methods.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Bias vs. Variance, Bagging, and Boosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*For want of a nail the shoe was lost;*'
  prefs: []
  type: TYPE_NORMAL
- en: '*for want of a shoe the horse was lost;*'
  prefs: []
  type: TYPE_NORMAL
- en: '*and for want of a horse the man was lost.*'
  prefs: []
  type: TYPE_NORMAL
- en: —Old proverb
  prefs: []
  type: TYPE_NORMAL
- en: We must always bear in mind that we are dealing with sample data. Sometimes
    the “population” being sampled is largely conceptual; for example, in the taxi
    data in [Section 5.3](ch05.xhtml#ch05lev3), we are considering the data a sample
    from the ridership in all days, past, present, and future. But in any case, there
    is sampling variation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the bike rental data, say, what if the data collection period had continued
    one more day? Even this slight change might affect the exact split at the top
    of the tree, Node 1\. And that effect could then change the splits (or possibly
    non-splits) at Nodes 2 and 3 and so on, with the those changes cascading down
    to the lowest levels of the resulting tree. Note that not only might the split
    points in the nodes change, but the membership of the nodes could also change.
    A training set data point that had been in Node 2 may now be in Node 3\. In other
    words:'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees can be very sensitive to slight changes in the inputs. That means
    they are very sensitive to sampling variation—that is, **decision trees have a
    high variance.**
  prefs: []
  type: TYPE_NORMAL
- en: Recall that splitting a node reduces bias and that, typically, reducing bias
    also increases variance. But for the reason given above, variance may be especially
    problematic in DT settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we treat two major methods for handling this problem, *bagging/random
    forests* and *boosting*. Both take this point of view: “Variance too high? Well,
    that means the sample size is too small, so let’s generate more trees!” But how?'
  prefs: []
  type: TYPE_NORMAL
- en: '6.2 Bagging: Generating New Trees by Resampling'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The term *bagging* refers to an ML version of a handy tool from modern statistics
    known as the *bootstrap*. This consists of drawing many random subsamples from
    our data, applying our given estimator to each subsample, and then averaging (or
    otherwise combining) the results. Here we apply the bootstrap to decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with our original data, once again considered a sample from some population,
    we’ll generate *s* new samples from the original dataset. We generate a new sample
    by randomly sampling *m* of our *n* data points—*with* replacement. (We may get
    a few duplicates.) We’ll fit a tree to each new sample, thus achieving the above
    goal of generating more trees, and combine the results in a manner to be presented
    shortly. The quantities *s* and *m* here are—you guessed it—hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.2.1 Random Forests***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Say we have a new case to be predicted. We will then *aggregate* the *s* trees
    by forming a prediction for each tree and then combining all those predicted values
    to form our final prediction as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In a numeric-*Y* setting, the combining would take the form of averaging all
    the predictions. In the taxi data, for example (see [Section 5.3](ch05.xhtml#ch05lev3)),
    each tree would give us a predicted trip time, and our final predicted trip time
    would be the average of all those individual predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a classification setting, such as the vertebrae example we covered in [Section
    2.3](ch02.xhtml#ch02lev3), we could combine by using a *voting* process. For each
    tree, we would find the predicted class, NO, DH, or SL, and then see what class
    received the most “votes” among the various trees. That would be our predicted
    class. Or, we could find the estimated class probabilities for this new case,
    for each tree, and then average the probabilities. Our predicted class would be
    whichever one has the largest average.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we do a bootstrap and then aggregation, hence the short name *bagging*.
    It is also commonly known as *random forests*, a specific implementation by Leo
    Breiman. (The earliest proposal along these lines seems to be that of Tin Kam
    Ho. She called the method *random decision forests*.) That approach places a limit
    on the number of features under consideration for splitting at any given node,
    with a different candidate set at each step.
  prefs: []
  type: TYPE_NORMAL
- en: Why might this strategy, which is using a different candidate set of features
    each time, work? Ordinary bagging can result in substantially correlated trees
    because it tends to choose the same features every time. It can be shown that
    the average of positively correlated numbers has a higher variance than the average
    of independent numbers. Thus the approach in which we limit the candidate feature
    set at each step hopefully reduces variance.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.2.2 The qeRF() Function***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `qe*-` series of functions actually includes several for random forests.
    For a given application, one may be more accurate or faster than others, but they
    all use the general random forest paradigm described previously. We’ll use `qeRF()`
    here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the `qe*` functions all have the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: data   A data frame containing our training data.
  prefs: []
  type: TYPE_NORMAL
- en: yName   The name of the column in `data` containing *Y*, the outcome variable
    to be predicted. The user distinguishes between numeric-*Y* and classification
    settings by having this column be numeric or an R factor, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: holdout   The size of the optional holdout set.
  prefs: []
  type: TYPE_NORMAL
- en: '**Application-specific arguments** For example, as the number *k* of nearest
    neighbors in the case of `qeKNN()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each `qe*` function is a wrapper interface to a function in a standard R ML
    package. In the case of random forests, `qeRF()` is a wrapper for `randomForest`
    in the package of the same name. The call form is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The application-specific arguments are `nTree`, which is the number of bootstrapped
    trees to generate, and `minNodeSize`, which is similar to `minsplit` in `ctree()`.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.2.3 Example: Vertebrae Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s look again at the vertebrae dataset from [Section 2.3](ch02.xhtml#ch02lev3),
    now applying random forests instead of k-NN. We’ll predict the same hypothetical
    new case as in that earlier example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With k-NN, we had predicted the same class, DH, but with slightly different
    class probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The difference between the two sets of probabilities is due both to the fact
    that we used two different ML algorithms and to the small *n* in this dataset
    (310), which caused large sample variability.
  prefs: []
  type: TYPE_NORMAL
- en: We used the default values here for `nTree` and `minNodeSize`. We could explore
    a few other pairs of these hyperparameters and then compare the performance of
    random forests and k-NN on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.2.4 Example: Remote-Sensing Soil Analysis***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here we will analyze the African Soil Property dataset from Kaggle.^([1](footnote.xhtml#ch6fn1))
    From the data site:'
  prefs: []
  type: TYPE_NORMAL
- en: Advances in rapid, low cost analysis of soil samples using infrared spectroscopy,
    georeferencing of soil samples, and greater availability of earth remote sensing
    data provide new opportunities for predicting soil functional properties at unsampled
    locations. . . . Digital mapping of soil functional properties, especially in
    data sparse regions such as Africa, is important for planning sustainable agricultural
    intensification and natural resources management.
  prefs: []
  type: TYPE_NORMAL
- en: We wish to predict various soil properties without directly testing the soil.
  prefs: []
  type: TYPE_NORMAL
- en: One important property of this dataset that we have not encountered before is
    that it has *p* > *n* (that is, more columns than rows). The original first column,
    an ID variable, has been removed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Traditionally, the statistics field has been wary of this kind of setting, as
    linear models ([Chapter 8](ch08.xhtml)) do not work there. One must first do dimension
    reduction. Tree-based methods do this as an integral aspect of their operation,
    so let’s give it a try using `qeRF()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the names of the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Columns 1 through 3594 are the *X* variables, with cryptic code names. The remaining
    columns are *Y*, some with more easily guessable names. We’ll predict pH, the
    soil acidity.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of setting is considered tough. There is a major potential for overfitting
    since, with so many features, one or more of them may accidentally look to be
    a strong predictor due to p-hacking ([Section 1.13](ch01.xhtml#ch01lev13)). Let’s
    see how well `qeRF()` does here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Use of the features has cut MAPE almost in half. Note the range under the pH
    scale used here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to predict, say, on a hypothetical new case like that of row
    88 in the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We would predict a pH level of about 0.61.
  prefs: []
  type: TYPE_NORMAL
- en: '6.3 Boosting: Repeatedly Tweaking a Tree'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine a classification problem with just two classes, so *Y* = 1 or 0, and
    just one feature, *X*, say, age. We fit a tree with just one level. Suppose our
    rule is to guess *Y* = 1 if *X* > 12.5 and guess *Y* = 0 if *X* ≤ 12.5\. *Boosting*
    would involve exploring the effect of small changes to the 12.5 threshold on our
    overall rate of correct classification.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a data point for which *X* = 5.2\. In the original analysis, we’d guess
    *Y* to be 0\. And, here is the point, if we were to move the threshold to, say,
    11.9, we would *still* guess *Y* = 0\. But the move may turn some misclassified
    data points near 12.5 to correctly classified ones. If more formerly misclassified
    points become correctly classified than vice versa, it’s a win.
  prefs: []
  type: TYPE_NORMAL
- en: So the idea of boosting is to tweak the original tree, thus forming a new tree,
    then in turn tweaking that new tree, forming a second new tree, and so on. After
    generating *s* trees (*s* is a hyperparameter), we predict a new case by plugging
    it into all those trees and somehow combining the resulting predicted values.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.3.1 Implementation: AdaBoost***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first proposal made for boosting was *AdaBoost*. The tweaking involves assigning
    weights to the points in our training set, which change with each tree. Each time
    we form a new tree, we fit a tree according to the latest set of weights, updating
    them with each new tree.
  prefs: []
  type: TYPE_NORMAL
- en: In a numeric-*Y* situation, to predict a new case with a certain *X* value,
    we plug that value into all the trees, yielding *s* predicted values. Our final
    predicted value in a numeric- *Y* setting is a weighted average of the individual
    predictions. In a classification setting, we would take a weighted average of
    the estimated probabilities of *Y* = 1 to get the final probability estimate,
    or use weighted voting.
  prefs: []
  type: TYPE_NORMAL
- en: To make this idea concrete, below is an outline of how the process could be
    implemented with `ctree()`. It relies on the fact that one of the arguments in
    `ctree()`, named `weights`, is a vector of nonnegative numbers, one for each data
    point. Say our response is named `y`, with features *x*. Denote the portion of
    the data frame `d` for *x* by `dx`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the pseudocode below, we will maintain two vectors of weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '`wts` will store the current weightings of the various rows in the training
    data. Recall that as the boosting process evolves, we will weight some rows more
    heavily than others according to their current impact on misclassification.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`alpha` will store the current weights of our various trees. Recall that in
    the end, when we do prediction, we will place more weight on some trees than others.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is an outline of the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'And to predict a new case, `newx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Since this book is aimed to be nonmathematical, we omit the formulas for `wts`
    and `alpha`. It should be noted, though, that `alpha` is an increasing sequence,
    so when we predict new cases, the later trees play a larger role.
  prefs: []
  type: TYPE_NORMAL
- en: The `qeML` package has a function for AdaBoost, `qeAdaBoost()`. But it is applicable
    to classification settings only, so let’s go right to the next form of boosting.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.3.2 Gradient Boosting***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In statistics/ML, there is the notion of a *residual*—that is, the difference
    between a predicted value and an actual value. *Gradient boosting* works by fitting
    trees to residuals. Given our dataset, a rough description of the process is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with some initial tree. Set *CurrentTree* to it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each of our data points, calculate the residuals for *CurrentTree*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a tree *to the residuals*—that is, take our residuals as the “data” and
    fit a tree T on it. Set *CurrentTree* = *T*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to Step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These steps are iterated for the number of trees specified by the user. Then,
    to predict a new case, we plug it into all the trees. The predicted value is simply
    the sum of the predicted values from the individual trees.
  prefs: []
  type: TYPE_NORMAL
- en: At any given step, we are saying, “Good, we’ve got a certain predictive ability
    so far, so let’s work on what is left over—that is, our current errors.” Hence
    our predicted value for any new case is the sum of what each tree predicts for
    that case.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2.1 The qeGBoost() Function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The `qe*` function for gradient boosting is `qeGBoost()`, a wrapper for `gbm()`
    in the package of the same name. Its call form is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is similar to `qeRF()` but with a new argument, the *learning rate*. That
    rate is a common notion in ML and will be explained shortly.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*A number of gradient boosting packages are available for R. We chose the*gbm
    *package for its simplicity. Just as was the case above for random forests, other
    packages may be faster or more accurate on some datasets, notably*qeXGBoost*.
    Here,*qeGBoost() *sticks to the “quick and easy” philosophy of the*qe**-series,
    but the reader is encouraged to explore other packages as an advanced topic.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***6.3.3 Example: Call Network Monitoring***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s first apply boosting to a dataset titled Call Test Measurements for Mobile
    Network Monitoring and Optimization,^([2](footnote.xhtml#ch6fn2)) which rates
    quality of service on mobile calls. The aim is to predict the quality rating.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3.1 The Data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Here is an introduction to the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here *Y* is `MOS`, the quality of service.
  prefs: []
  type: TYPE_NORMAL
- en: How big is it?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s fit the model.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3.2 Fitting the Model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: With over 100,000 data points and just 8 features, overfitting should not be
    an issue in this dataset. It easily satisfies our rough rule of thumb, *p* < ![Images](../images/unch08equ08.jpg)
    ([Section 3.1.3](ch03.xhtml#ch03lev1sec3)). So, let’s not bother with a holdout
    set. There is still some randomness in the algorithm, though, so for consistency,
    let’s set the random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The default value for `nTree` is only 100, but we tried a much larger number,
    750, for reasons that will become clear below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do a prediction. Say we have a case like `ds[3,]` but with distance being
    1,500 and duration 62:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '6.3.3.3 Hyperparameter: Number of Trees'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: But should we have used so many trees? After all, 750 may be overfitting. Maybe
    the later trees were doing “noise fitting.” The package has a couple of ways of
    addressing that issue, one of which is to use the auxiliary function `gbm.perf()`.
    Applied to the output of `gbm()`, it estimates the optimal number of trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'As noted, `qeGBoost()` calls `gbm()` and places the output of the latter in
    the `gbmOuts` component of its own output. So, we are able to call `gbm.perf()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: See the output graph in [Figure 6-1](ch06.xhtml#ch06fig01). The dashed vertical
    line shows the estimated “sweet spot”—that is, the best number of trees, 382 in
    this case. (This value is also printed to the R console.)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch06fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-1: Output from* gbm.perf'
  prefs: []
  type: TYPE_NORMAL
- en: 'But we need not refit the model. We can change the number of trees in the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we did not form a holdout set, we’ll need to calculate MAPE manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Details on other features of the `gbm` package are in its documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.3.4 Example: Vertebrae Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Boosting can be used in classification settings as well as numeric-*Y* cases.
    (And its usage is probably much more common on the classification side.) Here
    is `qeGBoost()` applied to the the vertebrae data (see [Section 2.3](ch02.xhtml#ch02lev3)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And, say we were to predict a new case like that of row 12 in the training
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We predict DH, with an estimated probability of about 0.63\. (Unfortunately,
    `gbm.perf()` is not available for the multiclass case.)
  prefs: []
  type: TYPE_NORMAL
- en: '***6.3.5 Bias vs. Variance in Boosting***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Boosting is “tweaking” a tree, potentially making it more stable, especially
    since we are averaging many trees, thus smoothing out “For want of a nail . .
    .” problems. So, it may reduce variance. By making small adjustments to a tree,
    we are potentially developing a more detailed analysis, thus reducing bias.
  prefs: []
  type: TYPE_NORMAL
- en: But all of that is true only “potentially.” Though the tweaking process has
    some theoretical basis, it still can lead us astray, actually *increasing* bias
    and possibly increasing variance too. If the hyperparameter *s* is set too large,
    producing too many trees, we may overfit.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.3.6 Computational Speed***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Boosting can take up tons of CPU cycles, so we may need something to speed things
    up. The `n.cores` argument in `gbm()` tries to offload computation to different
    cores in your machine. If you have a quad core system, you may try setting this
    argument to 4 or even 8 (and then call `gbm()` directly rather than through `qeGBoost()`).
  prefs: []
  type: TYPE_NORMAL
- en: '***6.3.7 Further Hyperparameters***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Boosting algorithms typically have a number of hyperparameters. We have already
    mentioned `nTree` (`n.trees` in `gbm()`), which is the number of trees to be generated.
  prefs: []
  type: TYPE_NORMAL
- en: Another hyperparameter is `minNodeSize` (`n.minobsinnode` in `gbm()`), which
    is the minimum number of data points we are willing to have in one tree node.
    As we saw in [Chapter 5](ch05.xhtml), reducing this value will reduce bias but
    increase variance.
  prefs: []
  type: TYPE_NORMAL
- en: The `shrinkage` hyperparameter is so important in the general ML context that
    we’ll cover it in a separate subsection, next.
  prefs: []
  type: TYPE_NORMAL
- en: '***6.3.8 The Learning Rate***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The notion of a learning rate comes up often in ML. We’ll describe it here in
    general and then explain how it works for gradient boosting. We’ll see it again
    in our material on support vector machines ([Chapter 10](ch10.xhtml)) and neural
    networks ([Chapter 11](ch11.xhtml)).
  prefs: []
  type: TYPE_NORMAL
- en: This section has a bit of math in it, in the form of curves and lines tangent
    to them, which is an exception to the avowedly nonmathematical nature of this
    book. But there are still no equations, and even math-averse readers should be
    able to follow the discussion.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.8.1 General Concepts
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Recall that in ML methods we are usually trying to minimize some loss function,
    such as MAPE, or the overall misclassification error OME. Computationally, this
    minimization can be a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the function graphed in [Figure 6-2](ch06.xhtml#ch06fig02). It is a
    function of a one-dimensional variable *x*, whereas typically our *x* is high-dimensional,
    but it will make our point.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch06fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-2: A function to be minimized*'
  prefs: []
  type: TYPE_NORMAL
- en: There is an overall minimum at approximately *x* = 2.2\. This is termed the
    *global minimum*. But there is also a *local minimum*, at about *x* = 0.4; that
    term means that this is the minimum value of the function only for points near—“local
    to”—0.4\. Let’s give the name *x*[0] to the value of *x* at the global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: To us humans looking at the graph, it’s clear where *x*[0] is, but we need our
    software to be able to find it. That may be problematic. Here’s why.
  prefs: []
  type: TYPE_NORMAL
- en: Most ML algorithms use an *iterative* approach to finding the desired minimum
    point *x*[0]. This involves a series of guesses for *x*[0] . The code starts with
    an initial guess, *g*[0], say, randomly chosen, then evaluates *f*(*g*[0]). Based
    on the result, the algorithm then somehow (see below) updates the guess to *g*[1]
    . It then evaluates *f*(*g*[1]), producing the next guess, *g*[2], and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm keeps generating guesses until they don’t change much, say, until
    | *g**[i]*[+1] − *g**[i]* | < 0.00000001 for Step *i*. We say that the algorithm
    has *converged* to this point. Let’s give the name *c* to that value of *i*. It
    then reports *x*[0], the global minimum point, to be the latest guess, *g**[c]*.
  prefs: []
  type: TYPE_NORMAL
- en: So, what about that “somehow” alluded to above? How does the algorithm generate
    the next guess from the present one? The answer lies in the *gradient*. In our
    simple example here with *x* being one-dimensional, the gradient is the slope
    of the function at the given point—that is, the slope of the tangent line to the
    curve.
  prefs: []
  type: TYPE_NORMAL
- en: Say our initial guess *g*[0] = 1.1\. The tangent line is shown in [Figure 6-3](ch06.xhtml#ch06fig03).
    The line is pointing upward to the right—that is, it has a positive slope—so it
    tells us that by going to the left we will go to smaller values of the function.
    We want to find the point at which *f*() is smallest, and the tangent line is
    saying, “Oh, you want a smaller value than *f* (1.1)? Move to the left!” But actually
    we should be moving to the right, toward 2.2, where the global minimum is.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch06fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-3: A function to be minimized, plus the tangent*'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the reader can already see that iterative algorithms are fraught with danger.
    Worse, it also adds yet another hyperparameter: we must decide not only *in which
    direction* to move for our next guess but also *how far* to move in that direction.
    The learning rate addresses the latter point.'
  prefs: []
  type: TYPE_NORMAL
- en: As noted, we should be moving to the right from 1.1, not to the left. The function
    *f*(*x*) is fooling the algorithm here. Actually, in this scenario, our algorithm
    may converge to the wrong point. Or it may not even converge at all and just wander
    aimlessly.
  prefs: []
  type: TYPE_NORMAL
- en: This is why typical ML packages allow the user to set the learning rate. Small
    values may be preferable, as large ones may result in our guesses lurching back
    and forth, always missing the target. On the other hand, if it is too small, we
    will just inch along, taking a long time to get there. Or worse, we converge to
    a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we have a hyperparameter that we need to be at a “Goldilocks” level—not
    too large and not too small—and may have to experiment with various values.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.8.2 The Learning Rate in gbm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This is the `shrinkage` argument in `gbm()`, called `learnRate` in `qeGBoost()`.
    Say we set it to 0.2\. Recall the pseudocode describing gradient boosting in [Section
    6.3.2](ch06.xhtml#ch06lev3sec2). The revised version is this:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with an initial tree. Set *CurrentTree* to it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each of our data points, calculate the residuals for *CurrentTree*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a tree *to the residuals*—that is, take our residuals as the “data” and
    fit a tree T on it. Set *CurrentTree* to the old *CurrentTree*, plus `shrinkage
    * T`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to Step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, `shrinkage * T` means multiplying all the values in the terminal nodes
    of the tree by the factor `shrinkage`. In the end, we still add up all our trees
    to produce the “supertree” used in the prediction of new cases.
  prefs: []
  type: TYPE_NORMAL
- en: Again, a small value of `shrinkage` is more cautious and slower, and it may
    cause us to need more trees in order to get good predictive power. But it may
    help prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '6.4 Pitfall: No Free Lunch'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*There is no such thing as a free lunch.*'
  prefs: []
  type: TYPE_NORMAL
- en: —Old economics saying
  prefs: []
  type: TYPE_NORMAL
- en: Though Leo Breiman had a point on the considerable value of AdaBoost (especially
    in saying “off the shelf,” meaning usable with just default values of hyperparameters),
    that old saying about no free lunch applies as well. As always, applying cross-validation
    and so on is indispensable to developing good models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar advice concerns another famous Breiman statement: that it is impossible
    to overfit using random forests. The reader who has come this far in this book
    will immediately realize that Breiman did not mean his statement in the way some
    have interpreted it. Any ML method may overfit. What Breiman meant was that it
    is impossible to set the value of *s*, the number of trees, too high. But the
    trees themselves still can overfit, for example, by having too small a minimum
    value for the number of data points in a node or, for that matter, by including
    too many features.'
  prefs: []
  type: TYPE_NORMAL
