- en: '**6'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**6'
- en: TWEAKING THE TREES**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 微调决策树**
- en: '*AdaBoost is the best off-the-shelf classifier in the world.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*AdaBoost 是世界上最好的现成分类器。*'
- en: —CART co-inventor Leo Breiman, 1996
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: —CART联合发明人 Leo Breiman，1996年
- en: '*XGBoost is the algorithm of choice for many winning teams of machine learning
    competitions.*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*XGBoost 是许多机器学习竞赛获胜团队首选的算法。*'
- en: —Wikipedia entry, 2022
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: —维基百科条目，2022年
- en: '![Image](../images/common.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: Here we talk about two general techniques in ML, *bagging* and *boosting*, and
    apply them to form extensions of decision tree analysis. The extensions, *random
    forests* and *tree-based gradient boosting*, are widely used—in fact, even more
    so than individual tree methods.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论机器学习中的两种通用技术，*bagging* 和 *boosting*，并将它们应用于扩展决策树分析。这些扩展，*随机森林* 和 *基于树的梯度提升*，被广泛使用——事实上，比起单一的树方法，它们的使用更加普遍。
- en: 6.1 Bias vs. Variance, Bagging, and Boosting
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 偏差与方差，Bagging 和 Boosting
- en: '*For want of a nail the shoe was lost;*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*失去一颗钉子，鞋子也丢了；*'
- en: '*for want of a shoe the horse was lost;*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*失去一只鞋子，马也丢了；*'
- en: '*and for want of a horse the man was lost.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*失去一匹马，人也丢了。*'
- en: —Old proverb
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: —古老的谚语
- en: We must always bear in mind that we are dealing with sample data. Sometimes
    the “population” being sampled is largely conceptual; for example, in the taxi
    data in [Section 5.3](ch05.xhtml#ch05lev3), we are considering the data a sample
    from the ridership in all days, past, present, and future. But in any case, there
    is sampling variation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须始终记住，我们处理的是样本数据。有时，“被采样的总体”大多是概念性的；例如，在[第5.3节](ch05.xhtml#ch05lev3)的出租车数据中，我们将数据视为来自所有天数的乘客数据样本，包括过去、现在和未来。但无论如何，都存在采样变异。
- en: 'In the bike rental data, say, what if the data collection period had continued
    one more day? Even this slight change might affect the exact split at the top
    of the tree, Node 1\. And that effect could then change the splits (or possibly
    non-splits) at Nodes 2 and 3 and so on, with the those changes cascading down
    to the lowest levels of the resulting tree. Note that not only might the split
    points in the nodes change, but the membership of the nodes could also change.
    A training set data point that had been in Node 2 may now be in Node 3\. In other
    words:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在自行车租赁数据中，假设数据收集周期多延续一天会怎么样？即使是这个微小的变化，也可能会影响树的顶部的精确分裂，即节点1\. 这个影响可能会继续影响节点2和3的分裂（或可能的非分裂），以此类推，直到影响到结果树的最底层。需要注意的是，不仅节点中的分裂点可能发生变化，节点的成员资格也可能发生变化。曾经在节点2的数据点现在可能会被归到节点3\.
    换句话说：
- en: Decision trees can be very sensitive to slight changes in the inputs. That means
    they are very sensitive to sampling variation—that is, **decision trees have a
    high variance.**
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树对输入的微小变化非常敏感。这意味着它们对采样变异非常敏感——也就是说，**决策树具有较高的方差**。
- en: Recall that splitting a node reduces bias and that, typically, reducing bias
    also increases variance. But for the reason given above, variance may be especially
    problematic in DT settings.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，分裂一个节点可以减少偏差，通常，减少偏差也会增加方差。但正如上面所说，方差在决策树设置中可能特别成问题。
- en: 'In this chapter, we treat two major methods for handling this problem, *bagging/random
    forests* and *boosting*. Both take this point of view: “Variance too high? Well,
    that means the sample size is too small, so let’s generate more trees!” But how?'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们讨论处理这一问题的两种主要方法，*bagging/随机森林*和*boosting*。这两种方法的观点是：“方差太大？好吧，那意味着样本量太小，那么我们就生成更多的树！”但怎么做呢？
- en: '6.2 Bagging: Generating New Trees by Resampling'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 Bagging：通过重采样生成新树
- en: The term *bagging* refers to an ML version of a handy tool from modern statistics
    known as the *bootstrap*. This consists of drawing many random subsamples from
    our data, applying our given estimator to each subsample, and then averaging (or
    otherwise combining) the results. Here we apply the bootstrap to decision trees.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*bagging* 这个术语指的是机器学习中一种现代统计学工具——*自助法*（bootstrap）的应用。这包括从我们的数据中随机抽取多个子样本，应用我们给定的估计器到每个子样本，然后对结果进行平均（或以其他方式组合）。在这里，我们将自助法应用于决策树。'
- en: Starting with our original data, once again considered a sample from some population,
    we’ll generate *s* new samples from the original dataset. We generate a new sample
    by randomly sampling *m* of our *n* data points—*with* replacement. (We may get
    a few duplicates.) We’ll fit a tree to each new sample, thus achieving the above
    goal of generating more trees, and combine the results in a manner to be presented
    shortly. The quantities *s* and *m* here are—you guessed it—hyperparameters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们原始数据开始，再次考虑它是某一总体的样本，我们将从原始数据集中生成 *s* 个新样本。我们通过从 *n* 个数据点中随机抽取 *m* 个样本（*有*
    放回）来生成新样本。（我们可能会得到一些重复的样本。）我们将为每个新样本拟合一棵树，从而实现上述目标，即生成更多的树，并以稍后展示的方式合并结果。这里的 *s*
    和 *m* 是——你猜对了——超参数。
- en: '***6.2.1 Random Forests***'
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***6.2.1 随机森林***'
- en: 'Say we have a new case to be predicted. We will then *aggregate* the *s* trees
    by forming a prediction for each tree and then combining all those predicted values
    to form our final prediction as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个新的案例需要预测。然后，我们将通过对每棵树形成一个预测值，并将所有这些预测值结合起来，按照如下方式*聚合* *s* 棵树，以形成我们的最终预测：
- en: In a numeric-*Y* setting, the combining would take the form of averaging all
    the predictions. In the taxi data, for example (see [Section 5.3](ch05.xhtml#ch05lev3)),
    each tree would give us a predicted trip time, and our final predicted trip time
    would be the average of all those individual predictions.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数值型 *Y* 设置中，合并的方式是对所有预测值进行平均。例如，在出租车数据中（见[第5.3节](ch05.xhtml#ch05lev3)），每棵树会给出一个预测的行程时间，而我们的最终预测行程时间将是所有这些单独预测值的平均值。
- en: In a classification setting, such as the vertebrae example we covered in [Section
    2.3](ch02.xhtml#ch02lev3), we could combine by using a *voting* process. For each
    tree, we would find the predicted class, NO, DH, or SL, and then see what class
    received the most “votes” among the various trees. That would be our predicted
    class. Or, we could find the estimated class probabilities for this new case,
    for each tree, and then average the probabilities. Our predicted class would be
    whichever one has the largest average.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分类问题中，例如我们在[第2.3节](ch02.xhtml#ch02lev3)中讨论的脊椎骨示例，我们可以通过使用*投票*过程来进行合并。对于每棵树，我们将找到预测的类别：NO、DH
    或 SL，然后查看哪些类别在不同的树之间获得了最多的“投票”。这将是我们的预测类别。或者，我们可以找到每棵树对于这个新案例的估计类别概率，然后取平均概率。我们的预测类别将是概率最大的那个类别。
- en: So, we do a bootstrap and then aggregation, hence the short name *bagging*.
    It is also commonly known as *random forests*, a specific implementation by Leo
    Breiman. (The earliest proposal along these lines seems to be that of Tin Kam
    Ho. She called the method *random decision forests*.) That approach places a limit
    on the number of features under consideration for splitting at any given node,
    with a different candidate set at each step.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们进行自助抽样然后聚合，简称*bagging*。它也通常被称为*随机森林*，这是Leo Breiman的具体实现。（这一思路最早的提出者似乎是Tin
    Kam Ho，她称这种方法为*随机决策森林*。）该方法限制了在任何给定节点进行分裂时考虑的特征数量，每一步都有一个不同的候选特征集。
- en: Why might this strategy, which is using a different candidate set of features
    each time, work? Ordinary bagging can result in substantially correlated trees
    because it tends to choose the same features every time. It can be shown that
    the average of positively correlated numbers has a higher variance than the average
    of independent numbers. Thus the approach in which we limit the candidate feature
    set at each step hopefully reduces variance.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这种策略，即每次使用不同的候选特征集，会奏效呢？普通的bagging可能导致树之间有较高的相关性，因为它每次都倾向于选择相同的特征。可以证明，正相关的数值的平均值比独立数值的平均值具有更高的方差。因此，在每一步限制候选特征集的方法有助于降低方差。
- en: '***6.2.2 The qeRF() Function***'
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***6.2.2 qeRF()函数***'
- en: The `qe*-` series of functions actually includes several for random forests.
    For a given application, one may be more accurate or faster than others, but they
    all use the general random forest paradigm described previously. We’ll use `qeRF()`
    here.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`qe*-` 系列函数实际上包括了多个用于随机森林的函数。对于给定的应用，某些函数可能比其他函数更准确或更快速，但它们都使用前面描述的通用随机森林范式。我们将在这里使用
    `qeRF()`。'
- en: 'Recall that the `qe*` functions all have the following arguments:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，`qe*` 函数都有以下参数：
- en: data   A data frame containing our training data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: data   一个包含我们训练数据的数据框。
- en: yName   The name of the column in `data` containing *Y*, the outcome variable
    to be predicted. The user distinguishes between numeric-*Y* and classification
    settings by having this column be numeric or an R factor, respectively.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: yName   `data`中包含*Y*（即需要预测的结果变量）的列名。用户通过将该列设置为数字型或R因子，分别区分数值型*Y*和分类设置。
- en: holdout   The size of the optional holdout set.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: holdout   可选的holdout集的大小。
- en: '**Application-specific arguments** For example, as the number *k* of nearest
    neighbors in the case of `qeKNN()`.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**特定应用的参数** 例如，在`qeKNN()`的情况下，*k*是最近邻的数量。'
- en: 'Each `qe*` function is a wrapper interface to a function in a standard R ML
    package. In the case of random forests, `qeRF()` is a wrapper for `randomForest`
    in the package of the same name. The call form is:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`qe*`函数都是标准R机器学习包中函数的封装接口。在随机森林的情况下，`qeRF()`是`randomForest`包的一个封装。调用形式如下：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The application-specific arguments are `nTree`, which is the number of bootstrapped
    trees to generate, and `minNodeSize`, which is similar to `minsplit` in `ctree()`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 特定应用的参数包括`nTree`，即生成的自举树的数量，以及`minNodeSize`，它类似于`ctree()`中的`minsplit`。
- en: '***6.2.3 Example: Vertebrae Data***'
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***6.2.3 示例：脊椎数据***'
- en: 'Let’s look again at the vertebrae dataset from [Section 2.3](ch02.xhtml#ch02lev3),
    now applying random forests instead of k-NN. We’ll predict the same hypothetical
    new case as in that earlier example:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次查看[第2.3节](ch02.xhtml#ch02lev3)中的脊椎数据集，现在应用随机森林而不是k-NN。我们将预测与之前示例中相同的假设新案例：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With k-NN, we had predicted the same class, DH, but with slightly different
    class probabilities:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用k-NN时，我们预测了相同的类别DH，但具有略微不同的类别概率：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The difference between the two sets of probabilities is due both to the fact
    that we used two different ML algorithms and to the small *n* in this dataset
    (310), which caused large sample variability.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 两组概率之间的差异既来自于我们使用了两种不同的机器学习算法，也来自于该数据集中的小*n*（310），这导致了较大的样本变异性。
- en: We used the default values here for `nTree` and `minNodeSize`. We could explore
    a few other pairs of these hyperparameters and then compare the performance of
    random forests and k-NN on this dataset.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`nTree`和`minNodeSize`的默认值。我们可以探索这些超参数的其他组合，然后比较随机森林和k-NN在该数据集上的表现。
- en: '***6.2.4 Example: Remote-Sensing Soil Analysis***'
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***6.2.4 示例：遥感土壤分析***'
- en: 'Here we will analyze the African Soil Property dataset from Kaggle.^([1](footnote.xhtml#ch6fn1))
    From the data site:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将分析Kaggle上的非洲土壤属性数据集。^([1](footnote.xhtml#ch6fn1)) 来自数据网站：
- en: Advances in rapid, low cost analysis of soil samples using infrared spectroscopy,
    georeferencing of soil samples, and greater availability of earth remote sensing
    data provide new opportunities for predicting soil functional properties at unsampled
    locations. . . . Digital mapping of soil functional properties, especially in
    data sparse regions such as Africa, is important for planning sustainable agricultural
    intensification and natural resources management.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 利用红外光谱法对土壤样本进行快速、低成本分析，土壤样本的地理参考，以及地球遥感数据的更大可用性，为预测未采样地点的土壤功能特性提供了新机遇……土壤功能特性的数字化绘图，特别是在数据稀缺的地区，如非洲，对于规划可持续农业集约化和自然资源管理非常重要。
- en: We wish to predict various soil properties without directly testing the soil.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望预测不同的土壤属性，而不是直接测试土壤。
- en: One important property of this dataset that we have not encountered before is
    that it has *p* > *n* (that is, more columns than rows). The original first column,
    an ID variable, has been removed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集的一个重要特性是我们以前未曾遇到过的，即它满足*p* > *n*（即列数大于行数）。原始的第一列，一个ID变量，已经被移除。
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Traditionally, the statistics field has been wary of this kind of setting, as
    linear models ([Chapter 8](ch08.xhtml)) do not work there. One must first do dimension
    reduction. Tree-based methods do this as an integral aspect of their operation,
    so let’s give it a try using `qeRF()`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，统计学领域对这种设置持谨慎态度，因为线性模型（[第8章](ch08.xhtml)）在此类设置下无法工作。必须首先进行降维。基于树的方法作为其操作的一个整体部分已实现此功能，因此我们可以尝试使用`qeRF()`。
- en: 'Here are the names of the columns:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是各列的名称：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Columns 1 through 3594 are the *X* variables, with cryptic code names. The remaining
    columns are *Y*, some with more easily guessable names. We’ll predict pH, the
    soil acidity.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第1列到第3594列是*X*变量，具有难以理解的代码名称。其余的列是*Y*，其中一些具有更容易猜测的名称。我们将预测pH，即土壤酸度。
- en: This kind of setting is considered tough. There is a major potential for overfitting
    since, with so many features, one or more of them may accidentally look to be
    a strong predictor due to p-hacking ([Section 1.13](ch01.xhtml#ch01lev13)). Let’s
    see how well `qeRF()` does here.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置被认为很难处理。由于有这么多特征，存在严重的过拟合潜力，因为其中一个或多个特征可能由于p-hacking（[第1.13节](ch01.xhtml#ch01lev13)）而偶然被认为是强预测因子。让我们看看`qeRF()`在这里的表现如何。
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Use of the features has cut MAPE almost in half. Note the range under the pH
    scale used here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 特征的使用将MAPE减少了近一半。请注意这里使用的pH尺度的范围：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We are now ready to predict, say, on a hypothetical new case like that of row
    88 in the training data:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备进行预测，比如在训练数据中的第88行的假设新案例：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We would predict a pH level of about 0.61.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预测的pH值大约为0.61。
- en: '6.3 Boosting: Repeatedly Tweaking a Tree'
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 Boosting：反复调整一棵树
- en: Imagine a classification problem with just two classes, so *Y* = 1 or 0, and
    just one feature, *X*, say, age. We fit a tree with just one level. Suppose our
    rule is to guess *Y* = 1 if *X* > 12.5 and guess *Y* = 0 if *X* ≤ 12.5\. *Boosting*
    would involve exploring the effect of small changes to the 12.5 threshold on our
    overall rate of correct classification.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个分类问题，只有两个类别，即*Y* = 1或0，且只有一个特征，*X*，比如年龄。我们拟合一个只有一层的树。假设我们的规则是，如果*X* > 12.5，则猜测*Y*
    = 1；如果*X* ≤ 12.5，则猜测*Y* = 0。*Boosting*将涉及探索将12.5阈值的小变化对我们整体正确分类率的影响。
- en: Consider a data point for which *X* = 5.2\. In the original analysis, we’d guess
    *Y* to be 0\. And, here is the point, if we were to move the threshold to, say,
    11.9, we would *still* guess *Y* = 0\. But the move may turn some misclassified
    data points near 12.5 to correctly classified ones. If more formerly misclassified
    points become correctly classified than vice versa, it’s a win.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个数据点，其中*X* = 5.2。在原始分析中，我们会猜测*Y*为0。而且，关键在于，如果我们将阈值调整到11.9，我们*仍然*会猜测*Y* =
    0。但这个调整可能会将一些原本分类错误的接近12.5的数据点修正过来。如果更多曾经被错误分类的点被正确分类，那么这就是一个成功。
- en: So the idea of boosting is to tweak the original tree, thus forming a new tree,
    then in turn tweaking that new tree, forming a second new tree, and so on. After
    generating *s* trees (*s* is a hyperparameter), we predict a new case by plugging
    it into all those trees and somehow combining the resulting predicted values.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，Boosting的思想是调整原始树，从而形成一棵新树，然后再调整这棵新树，形成第二棵新树，依此类推。生成*s*棵树（*s*是一个超参数）后，我们通过将新案例输入到所有这些树中并以某种方式结合生成的预测值来进行预测。
- en: '***6.3.1 Implementation: AdaBoost***'
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***6.3.1 实现：AdaBoost***'
- en: The first proposal made for boosting was *AdaBoost*. The tweaking involves assigning
    weights to the points in our training set, which change with each tree. Each time
    we form a new tree, we fit a tree according to the latest set of weights, updating
    them with each new tree.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting的第一个提议是*AdaBoost*。调整过程涉及为训练集中的每个数据点分配权重，每棵树都会更新这些权重。每次形成一棵新树时，我们都会根据最新的权重集拟合一棵树，并用每棵新树更新这些权重。
- en: In a numeric-*Y* situation, to predict a new case with a certain *X* value,
    we plug that value into all the trees, yielding *s* predicted values. Our final
    predicted value in a numeric- *Y* setting is a weighted average of the individual
    predictions. In a classification setting, we would take a weighted average of
    the estimated probabilities of *Y* = 1 to get the final probability estimate,
    or use weighted voting.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在数值型*Y*的情况下，为了预测具有某个*X*值的新案例，我们将该值输入到所有树中，得到*s*个预测值。在数值型*Y*设置中，我们的最终预测值是各个预测值的加权平均。在分类设置中，我们将对*Y*
    = 1的估计概率进行加权平均，以获得最终的概率估计，或者使用加权投票法。
- en: To make this idea concrete, below is an outline of how the process could be
    implemented with `ctree()`. It relies on the fact that one of the arguments in
    `ctree()`, named `weights`, is a vector of nonnegative numbers, one for each data
    point. Say our response is named `y`, with features *x*. Denote the portion of
    the data frame `d` for *x* by `dx`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个概念更具体，下面是如何使用`ctree()`实现该过程的概述。它依赖于`ctree()`中的一个参数，名为`weights`，它是一个非负数向量，为每个数据点分配一个权重。假设我们的响应变量名为`y`，特征为*x*。用`dx`表示数据框`d`中关于*x*的部分。
- en: 'In the pseudocode below, we will maintain two vectors of weights:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的伪代码中，我们将维护两个权重向量：
- en: '`wts` will store the current weightings of the various rows in the training
    data. Recall that as the boosting process evolves, we will weight some rows more
    heavily than others according to their current impact on misclassification.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`wts`将存储训练数据中各行的当前权重。回想一下，随着提升过程的进行，我们会根据某些行对分类错误的影响程度，对这些行赋予更高的权重。'
- en: '`alpha` will store the current weights of our various trees. Recall that in
    the end, when we do prediction, we will place more weight on some trees than others.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`alpha`将存储我们各种树的当前权重。回想一下，当我们进行预测时，我们会对某些树赋予比其他树更多的权重。'
- en: 'Here is an outline of the algorithm:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是算法的大纲：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And to predict a new case, `newx`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测新的案例，`newx`：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Since this book is aimed to be nonmathematical, we omit the formulas for `wts`
    and `alpha`. It should be noted, though, that `alpha` is an increasing sequence,
    so when we predict new cases, the later trees play a larger role.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书旨在避免过于数学化，我们省略了`wts`和`alpha`的公式。然而需要指出的是，`alpha`是一个递增序列，因此在预测新案例时，后面的树会发挥更大的作用。
- en: The `qeML` package has a function for AdaBoost, `qeAdaBoost()`. But it is applicable
    to classification settings only, so let’s go right to the next form of boosting.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`qeML`包有一个用于AdaBoost的函数，`qeAdaBoost()`。但它仅适用于分类任务，因此我们直接进入下一个梯度提升的形式。'
- en: '***6.3.2 Gradient Boosting***'
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***6.3.2 梯度提升***'
- en: 'In statistics/ML, there is the notion of a *residual*—that is, the difference
    between a predicted value and an actual value. *Gradient boosting* works by fitting
    trees to residuals. Given our dataset, a rough description of the process is as
    follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学/机器学习中，有一个概念叫做*残差*——即预测值与实际值之间的差异。*梯度提升*通过将树拟合到残差上来工作。给定我们的数据集，过程的大致描述如下：
- en: Start with some initial tree. Set *CurrentTree* to it.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一棵初始树开始。将*CurrentTree*设置为它。
- en: For each of our data points, calculate the residuals for *CurrentTree*.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于我们的每一个数据点，计算*CurrentTree*的残差。
- en: Fit a tree *to the residuals*—that is, take our residuals as the “data” and
    fit a tree T on it. Set *CurrentTree* = *T*.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一棵树拟合到*残差*上——即将我们的残差作为“数据”并拟合一棵树T。将*CurrentTree* = *T*。
- en: Go to Step 2.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳到第2步。
- en: These steps are iterated for the number of trees specified by the user. Then,
    to predict a new case, we plug it into all the trees. The predicted value is simply
    the sum of the predicted values from the individual trees.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤会根据用户指定的树的数量进行迭代。然后，为了预测新的案例，我们将其输入到所有树中。预测值只是各个树预测值的总和。
- en: At any given step, we are saying, “Good, we’ve got a certain predictive ability
    so far, so let’s work on what is left over—that is, our current errors.” Hence
    our predicted value for any new case is the sum of what each tree predicts for
    that case.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何给定的步骤中，我们都在说：“很好，我们已经有了一定的预测能力，那么让我们处理剩下的部分——也就是我们当前的误差。”因此，我们对任何新案例的预测值是每棵树为该案例预测值的总和。
- en: 6.3.2.1 The qeGBoost() Function
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.2.1 `qeGBoost()`函数
- en: 'The `qe*` function for gradient boosting is `qeGBoost()`, a wrapper for `gbm()`
    in the package of the same name. Its call form is:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`qe*`函数用于梯度提升是`qeGBoost()`，这是对同名包中`gbm()`的封装。它的调用形式是：'
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is similar to `qeRF()` but with a new argument, the *learning rate*. That
    rate is a common notion in ML and will be explained shortly.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于`qeRF()`，但多了一个新的参数，即*学习率*。这个学习率是机器学习中常见的概念，稍后会进行解释。
- en: '**NOTE**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*A number of gradient boosting packages are available for R. We chose the*gbm
    *package for its simplicity. Just as was the case above for random forests, other
    packages may be faster or more accurate on some datasets, notably*qeXGBoost*.
    Here,*qeGBoost() *sticks to the “quick and easy” philosophy of the*qe**-series,
    but the reader is encouraged to explore other packages as an advanced topic.*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*有很多梯度提升的包可用于R语言。我们选择了*gbm* *包，因为它简单易用。就像之前在随机森林中一样，其他包在某些数据集上可能更快或更准确，特别是*qeXGBoost*。在这里，*qeGBoost()*
    坚持了*qe**系列的“快速简便”理念，但鼓励读者将其他包作为高级话题进行探索。*'
- en: '***6.3.3 Example: Call Network Monitoring***'
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***6.3.3 示例：呼叫网络监测***'
- en: Let’s first apply boosting to a dataset titled Call Test Measurements for Mobile
    Network Monitoring and Optimization,^([2](footnote.xhtml#ch6fn2)) which rates
    quality of service on mobile calls. The aim is to predict the quality rating.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将提升算法应用于一个名为“移动网络监测与优化的呼叫测试测量”的数据集，^([2](footnote.xhtml#ch6fn2))，该数据集用于评估移动呼叫的服务质量。目标是预测质量评分。
- en: 6.3.3.1 The Data
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.3.1 数据
- en: 'Here is an introduction to the data:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据的介绍：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here *Y* is `MOS`, the quality of service.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*Y*是`MOS`，即服务质量。
- en: How big is it?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 它有多大？
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now, let’s fit the model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来拟合模型。
- en: 6.3.3.2 Fitting the Model
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.3.2 拟合模型
- en: With over 100,000 data points and just 8 features, overfitting should not be
    an issue in this dataset. It easily satisfies our rough rule of thumb, *p* < ![Images](../images/unch08equ08.jpg)
    ([Section 3.1.3](ch03.xhtml#ch03lev1sec3)). So, let’s not bother with a holdout
    set. There is still some randomness in the algorithm, though, so for consistency,
    let’s set the random seed.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在超过100,000个数据点和仅8个特征的情况下，过拟合应该不是该数据集的问题。它轻松满足我们粗略的经验法则，*p* < ![Images](../images/unch08equ08.jpg)
    （[第3.1.3节](ch03.xhtml#ch03lev1sec3)）。因此，我们不需要使用持出集。不过，算法中仍然存在一些随机性，因此为了保证一致性，我们设置随机种子。
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The default value for `nTree` is only 100, but we tried a much larger number,
    750, for reasons that will become clear below.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`nTree`的默认值仅为100，但我们尝试了一个更大的数字，750，这一点将在下面说明。'
- en: 'Let’s do a prediction. Say we have a case like `ds[3,]` but with distance being
    1,500 and duration 62:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做个预测。假设我们有一个像`ds[3,]`这样的案例，但距离为1500，持续时间为62：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '6.3.3.3 Hyperparameter: Number of Trees'
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.3.3 超参数：树木数量
- en: But should we have used so many trees? After all, 750 may be overfitting. Maybe
    the later trees were doing “noise fitting.” The package has a couple of ways of
    addressing that issue, one of which is to use the auxiliary function `gbm.perf()`.
    Applied to the output of `gbm()`, it estimates the optimal number of trees.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们是否应该使用这么多的树呢？毕竟，750棵可能会导致过拟合。也许后面的树是在进行“噪音拟合”。该包有几种方式来解决这个问题，其中一种是使用辅助函数`gbm.perf()`。将其应用于`gbm()`的输出，它可以估算出最佳的树木数量。
- en: 'As noted, `qeGBoost()` calls `gbm()` and places the output of the latter in
    the `gbmOuts` component of its own output. So, we are able to call `gbm.perf()`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`qeGBoost()`调用`gbm()`并将后者的输出放入其输出的`gbmOuts`组件中。因此，我们能够调用`gbm.perf()`：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: See the output graph in [Figure 6-1](ch06.xhtml#ch06fig01). The dashed vertical
    line shows the estimated “sweet spot”—that is, the best number of trees, 382 in
    this case. (This value is also printed to the R console.)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见[图6-1](ch06.xhtml#ch06fig01)中的输出图。虚线垂直线表示估算的“甜点”——即最佳树木数量，在本例中为382。（该值也会打印到R控制台。）
- en: '![Image](../images/ch06fig01.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch06fig01.jpg)'
- en: '*Figure 6-1: Output from* gbm.perf'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-1：来自* gbm.perf的输出'
- en: 'But we need not refit the model. We can change the number of trees in the prediction:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们无需重新拟合模型。我们可以在预测中改变树木的数量：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Since we did not form a holdout set, we’ll need to calculate MAPE manually:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有形成持出集，我们需要手动计算MAPE：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Details on other features of the `gbm` package are in its documentation.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`gbm`包的其他特性可以参考其文档。'
- en: '***6.3.4 Example: Vertebrae Data***'
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***6.3.4 示例：椎骨数据***'
- en: Boosting can be used in classification settings as well as numeric-*Y* cases.
    (And its usage is probably much more common on the classification side.) Here
    is `qeGBoost()` applied to the the vertebrae data (see [Section 2.3](ch02.xhtml#ch02lev3)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting可以用于分类问题以及数值-*Y*类型的情况。（而且它在分类方面的使用可能更为普遍。）这里展示了`qeGBoost()`应用于椎骨数据（请参见[第2.3节](ch02.xhtml#ch02lev3)）。
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And, say we were to predict a new case like that of row 12 in the training
    set:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们要预测一个像训练集第12行这样的新案例：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We predict DH, with an estimated probability of about 0.63\. (Unfortunately,
    `gbm.perf()` is not available for the multiclass case.)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预测DH，估算概率大约为0.63（不幸的是，`gbm.perf()`在多分类情况下不可用）。
- en: '***6.3.5 Bias vs. Variance in Boosting***'
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***6.3.5 Boosting中的偏差与方差***'
- en: Boosting is “tweaking” a tree, potentially making it more stable, especially
    since we are averaging many trees, thus smoothing out “For want of a nail . .
    .” problems. So, it may reduce variance. By making small adjustments to a tree,
    we are potentially developing a more detailed analysis, thus reducing bias.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting是对树进行“调整”，有可能使其更稳定，特别是因为我们在平均多个树，从而平滑掉“缺一根钉子……问题”。因此，它可能会降低方差。通过对树进行小的调整，我们有可能开发出更详细的分析，从而减少偏差。
- en: But all of that is true only “potentially.” Though the tweaking process has
    some theoretical basis, it still can lead us astray, actually *increasing* bias
    and possibly increasing variance too. If the hyperparameter *s* is set too large,
    producing too many trees, we may overfit.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 但这一切只是“潜在的”真实。虽然调整过程有一定的理论基础，但它仍然可能会把我们引入误区，实际上*增加*偏差，并且可能也会增加方差。如果超参数*s*设置得过大，导致树木过多，我们可能会发生过拟合。
- en: '***6.3.6 Computational Speed***'
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***6.3.6 计算速度***'
- en: Boosting can take up tons of CPU cycles, so we may need something to speed things
    up. The `n.cores` argument in `gbm()` tries to offload computation to different
    cores in your machine. If you have a quad core system, you may try setting this
    argument to 4 or even 8 (and then call `gbm()` directly rather than through `qeGBoost()`).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法可能会占用大量的CPU周期，因此我们可能需要一些方法来加速。`n.cores`参数在`gbm()`中尝试将计算任务分配到机器的不同核心。如果你有一台四核系统，可以尝试将这个参数设置为4甚至8（然后直接调用`gbm()`，而不是通过`qeGBoost()`）。
- en: '***6.3.7 Further Hyperparameters***'
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***6.3.7 进一步的超参数***'
- en: Boosting algorithms typically have a number of hyperparameters. We have already
    mentioned `nTree` (`n.trees` in `gbm()`), which is the number of trees to be generated.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法通常有许多超参数。我们之前提到过`nTree`（在`gbm()`中为`n.trees`），它是生成的树的数量。
- en: Another hyperparameter is `minNodeSize` (`n.minobsinnode` in `gbm()`), which
    is the minimum number of data points we are willing to have in one tree node.
    As we saw in [Chapter 5](ch05.xhtml), reducing this value will reduce bias but
    increase variance.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个超参数是`minNodeSize`（在`gbm()`中为`n.minobsinnode`），它是我们愿意在一个树节点中拥有的最小数据点数量。如我们在[第5章](ch05.xhtml)中看到的，减少此值会降低偏差但增加方差。
- en: The `shrinkage` hyperparameter is so important in the general ML context that
    we’ll cover it in a separate subsection, next.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`shrinkage`超参数在一般机器学习环境中非常重要，因此我们将在下一小节中单独讨论它。'
- en: '***6.3.8 The Learning Rate***'
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***6.3.8 学习率***'
- en: The notion of a learning rate comes up often in ML. We’ll describe it here in
    general and then explain how it works for gradient boosting. We’ll see it again
    in our material on support vector machines ([Chapter 10](ch10.xhtml)) and neural
    networks ([Chapter 11](ch11.xhtml)).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率的概念在机器学习中经常出现。我们将在此处一般性地描述它，然后解释它在梯度提升中的工作原理。我们将在后续关于支持向量机（[第10章](ch10.xhtml)）和神经网络（[第11章](ch11.xhtml)）的材料中再次提到它。
- en: This section has a bit of math in it, in the form of curves and lines tangent
    to them, which is an exception to the avowedly nonmathematical nature of this
    book. But there are still no equations, and even math-averse readers should be
    able to follow the discussion.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含了一些数学内容，涉及曲线及其切线，这是本书明确不以数学为主的例外。但仍然没有方程式，甚至对于数学有排斥的读者也应该能跟上讨论。
- en: 6.3.8.1 General Concepts
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.8.1 一般概念
- en: Recall that in ML methods we are usually trying to minimize some loss function,
    such as MAPE, or the overall misclassification error OME. Computationally, this
    minimization can be a challenge.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在机器学习方法中，我们通常试图最小化某个损失函数，比如MAPE或整体误分类错误OME。从计算角度来看，这种最小化可能是一个挑战。
- en: Consider the function graphed in [Figure 6-2](ch06.xhtml#ch06fig02). It is a
    function of a one-dimensional variable *x*, whereas typically our *x* is high-dimensional,
    but it will make our point.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在[图 6-2](ch06.xhtml#ch06fig02)中绘制的函数。它是一个一维变量* x *的函数，而通常我们的* x *是高维的，但这个例子足以说明我们的观点。
- en: '![Image](../images/ch06fig02.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch06fig02.jpg)'
- en: '*Figure 6-2: A function to be minimized*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-2：一个需要最小化的函数*'
- en: There is an overall minimum at approximately *x* = 2.2\. This is termed the
    *global minimum*. But there is also a *local minimum*, at about *x* = 0.4; that
    term means that this is the minimum value of the function only for points near—“local
    to”—0.4\. Let’s give the name *x*[0] to the value of *x* at the global minimum.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在* x * = 2.2处有一个全局最小值，这被称为*全局最小值*。但在* x * = 0.4处也有一个*局部最小值*；该术语意味着这个值仅适用于0.4附近的点——即“局部”的最小值。我们将全局最小值处的*
    x *称为* x *[0]。
- en: To us humans looking at the graph, it’s clear where *x*[0] is, but we need our
    software to be able to find it. That may be problematic. Here’s why.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们这些看图的人来说，* x *[0]的位置是显而易见的，但我们需要我们的软件能够找到它。这可能会成为一个问题。原因如下。
- en: Most ML algorithms use an *iterative* approach to finding the desired minimum
    point *x*[0]. This involves a series of guesses for *x*[0] . The code starts with
    an initial guess, *g*[0], say, randomly chosen, then evaluates *f*(*g*[0]). Based
    on the result, the algorithm then somehow (see below) updates the guess to *g*[1]
    . It then evaluates *f*(*g*[1]), producing the next guess, *g*[2], and so on.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法采用*迭代*方法来寻找所需的最小值点* x *[0]。这涉及到一系列对* x *[0]的猜测。代码从一个初始猜测* g *[0]开始，比如随机选择，然后评估*f*(*
    g *[0])。根据结果，算法然后以某种方式（见下文）更新猜测到* g *[1]。接着它评估*f*(* g *[1])，生成下一个猜测* g *[2]，依此类推。
- en: The algorithm keeps generating guesses until they don’t change much, say, until
    | *g**[i]*[+1] − *g**[i]* | < 0.00000001 for Step *i*. We say that the algorithm
    has *converged* to this point. Let’s give the name *c* to that value of *i*. It
    then reports *x*[0], the global minimum point, to be the latest guess, *g**[c]*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 算法不断生成猜测，直到它们变化不大，比如说，直到|*g**[i]*[+1] − *g**[i]*| < 0.00000001为止，步骤*i*。我们说算法已经*收敛*到这个点。我们将*i*的值称为*c*。然后它报告*
    x* [0]，即全局最小点为最新的猜测，*g**[c]*。
- en: So, what about that “somehow” alluded to above? How does the algorithm generate
    the next guess from the present one? The answer lies in the *gradient*. In our
    simple example here with *x* being one-dimensional, the gradient is the slope
    of the function at the given point—that is, the slope of the tangent line to the
    curve.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，之前提到的“某种方式”又是怎么回事呢？算法如何从当前猜测生成下一个猜测？答案就在于*梯度*。在我们这里的简单示例中，*x*是一维的，梯度是给定点上函数的斜率——也就是曲线的切线的斜率。
- en: Say our initial guess *g*[0] = 1.1\. The tangent line is shown in [Figure 6-3](ch06.xhtml#ch06fig03).
    The line is pointing upward to the right—that is, it has a positive slope—so it
    tells us that by going to the left we will go to smaller values of the function.
    We want to find the point at which *f*() is smallest, and the tangent line is
    saying, “Oh, you want a smaller value than *f* (1.1)? Move to the left!” But actually
    we should be moving to the right, toward 2.2, where the global minimum is.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的初始猜测是*g*[0] = 1.1。切线如[图6-3](ch06.xhtml#ch06fig03)所示。该线向右上方指，意味着它的斜率是正的，因此它告诉我们，向左走会让我们得到更小的函数值。我们想找到*f*()最小的点，而切线则说：“哦，你想要比*f*(1.1)更小的值吗？往左走！”但实际上我们应该向右走，朝着2.2走，那是全局最小值所在。
- en: '![Image](../images/ch06fig03.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch06fig03.jpg)'
- en: '*Figure 6-3: A function to be minimized, plus the tangent*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-3：需要最小化的函数，以及切线*'
- en: 'So, the reader can already see that iterative algorithms are fraught with danger.
    Worse, it also adds yet another hyperparameter: we must decide not only *in which
    direction* to move for our next guess but also *how far* to move in that direction.
    The learning rate addresses the latter point.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，读者可以看到，迭代算法充满了危险。更糟糕的是，它还增加了另一个超参数：我们不仅需要决定下一步猜测应该*朝哪个方向*移动，还需要决定*在那个方向上移动多远*。学习率就是为了处理后者的问题。
- en: As noted, we should be moving to the right from 1.1, not to the left. The function
    *f*(*x*) is fooling the algorithm here. Actually, in this scenario, our algorithm
    may converge to the wrong point. Or it may not even converge at all and just wander
    aimlessly.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们应该从1.1向右移动，而不是向左。函数*f*(*x*)在这里欺骗了算法。实际上，在这种情况下，我们的算法可能会收敛到错误的点。或者它甚至可能根本不收敛，反而漫无目的地游荡。
- en: This is why typical ML packages allow the user to set the learning rate. Small
    values may be preferable, as large ones may result in our guesses lurching back
    and forth, always missing the target. On the other hand, if it is too small, we
    will just inch along, taking a long time to get there. Or worse, we converge to
    a local minimum.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么典型的机器学习包允许用户设置学习率。较小的值可能更可取，因为较大的值可能导致我们的猜测来回摆动，总是错失目标。另一方面，如果学习率太小，我们将只会缓慢前进，花费很长时间才能到达目标。或者更糟的是，我们会收敛到局部最小值。
- en: Once again, we have a hyperparameter that we need to be at a “Goldilocks” level—not
    too large and not too small—and may have to experiment with various values.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们有一个超参数，它需要处于“恰到好处”的水平——既不能太大，也不能太小——可能需要通过尝试不同的值来进行实验。
- en: 6.3.8.2 The Learning Rate in gbm
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.8.2 gbm中的学习率
- en: 'This is the `shrinkage` argument in `gbm()`, called `learnRate` in `qeGBoost()`.
    Say we set it to 0.2\. Recall the pseudocode describing gradient boosting in [Section
    6.3.2](ch06.xhtml#ch06lev3sec2). The revised version is this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`gbm()`中的`shrinkage`参数，在`qeGBoost()`中叫做`learnRate`。假设我们将其设置为0.2。回想一下[6.3.2节](ch06.xhtml#ch06lev3sec2)中描述的梯度提升伪代码。修订后的版本是：
- en: Start with an initial tree. Set *CurrentTree* to it.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个初始树开始。将*CurrentTree*设为它。
- en: For each of our data points, calculate the residuals for *CurrentTree*.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于我们的每个数据点，计算*CurrentTree*的残差。
- en: Fit a tree *to the residuals*—that is, take our residuals as the “data” and
    fit a tree T on it. Set *CurrentTree* to the old *CurrentTree*, plus `shrinkage
    * T`.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一棵树*拟合到残差*上——也就是说，把我们的残差当作“数据”，并在其上拟合一棵树T。将*CurrentTree*设为旧的*CurrentTree*，加上`shrinkage
    * T`。
- en: Go to Step 2.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到第2步。
- en: Here, `shrinkage * T` means multiplying all the values in the terminal nodes
    of the tree by the factor `shrinkage`. In the end, we still add up all our trees
    to produce the “supertree” used in the prediction of new cases.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`shrinkage * T`意味着将树的终端节点中的所有值乘以`shrinkage`因子。最终，我们仍然将所有树加起来，生成用于预测新案例的“超树”。
- en: Again, a small value of `shrinkage` is more cautious and slower, and it may
    cause us to need more trees in order to get good predictive power. But it may
    help prevent overfitting.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，较小的`shrinkage`值更为谨慎且处理速度较慢，可能会导致我们需要更多的树木来获得良好的预测能力。但它可能有助于防止过拟合。
- en: '6.4 Pitfall: No Free Lunch'
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 陷阱：没有免费的午餐
- en: '*There is no such thing as a free lunch.*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*没有免费的午餐。*'
- en: —Old economics saying
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: —古老的经济学说法
- en: Though Leo Breiman had a point on the considerable value of AdaBoost (especially
    in saying “off the shelf,” meaning usable with just default values of hyperparameters),
    that old saying about no free lunch applies as well. As always, applying cross-validation
    and so on is indispensable to developing good models.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管利奥·布雷曼（Leo Breiman）在谈到AdaBoost的巨大价值时有其道理（特别是提到“现成的”，即可以直接使用默认的超参数值），但“没有免费的午餐”这句老话同样适用。像往常一样，应用交叉验证等方法对于开发良好的模型是不可或缺的。
- en: 'Similar advice concerns another famous Breiman statement: that it is impossible
    to overfit using random forests. The reader who has come this far in this book
    will immediately realize that Breiman did not mean his statement in the way some
    have interpreted it. Any ML method may overfit. What Breiman meant was that it
    is impossible to set the value of *s*, the number of trees, too high. But the
    trees themselves still can overfit, for example, by having too small a minimum
    value for the number of data points in a node or, for that matter, by including
    too many features.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的建议涉及另一个著名的布雷曼（Breiman）声明：即使用随机森林是不可能过拟合的。读到这本书此处的读者会立即意识到，布雷曼并不是以某些人所理解的方式表达他的意思。任何机器学习方法都有可能发生过拟合。布雷曼的意思是，不可能将*s*（树木的数量）设置得太高。但树木本身仍然可能发生过拟合，例如，节点中数据点的最小数量设置得太小，或者包含了过多的特征。
