- en: '**12**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ARITHMETIC AND LOGICAL EXPRESSIONS**'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'One of the major advantages that high-level languages provide over low-level
    languages is the use of algebraic arithmetic and logical expressions (hereafter,
    “arithmetic expressions”). HLL arithmetic expressions are an order of magnitude
    more readable than the sequence of machine instructions the compiler produces.
    However, the conversion process from arithmetic expressions into machine code
    is also one of the more difficult transformations to do efficiently, and a fair
    percentage of a typical compiler’s optimization phase is dedicated to handling
    it. Because of the difficulty with translation, this is one area where you can
    help the compiler. This chapter will describe:'
  prefs: []
  type: TYPE_NORMAL
- en: How computer architecture affects the computation of arithmetic expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The optimization of arithmetic expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Side effects of arithmetic expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence points in arithmetic expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Order of evaluation in arithmetic expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Short-circuit and complete evaluation of arithmetic expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The computational cost of arithmetic expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Armed with this information, you’ll be able to write more efficient and more
    robust applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.1 Arithmetic Expressions and Computer Architecture**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With respect to arithmetic expressions, we can classify traditional computer
    architectures into three basic types: stack-based machines, register-based machines,
    and accumulator-based machines. The major difference between these architectural
    types has to do with where the CPUs keep the operands for the arithmetic operations.
    Once the CPU fetches the data from these operands, the data is passed along to
    the arithmetic and logical unit, where the actual arithmetic or logical calculation
    occurs.^([1](footnotes.xhtml#ch12fn1)) We’ll explore each of these architectures
    in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: '**12.1.1 Stack-Based Machines**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Stack-based machines use memory for most calculations, employing a data structure
    called the *stack* in memory to hold all operands and results. Computer systems
    with a stack architecture offer some important advantages over other architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: The instructions are often smaller in stack architectures because the instructions
    generally don’t have to specify any operands.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is usually easier to write compilers for stack architectures than for other
    machines because converting arithmetic expressions to a sequence of stack operations
    is very easy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temporary variables are rarely needed in a stack architecture, because the stack
    itself serves that purpose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unfortunately, stack machines also suffer from some serious disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Almost every instruction references memory (which is slow on modern machines).
    Though caches can help mitigate this problem, memory performance is still a major
    problem on stack machines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though conversion from HLLs to a stack machine is very easy, there’s less
    opportunity for optimization than there is with other architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because stack machines are constantly accessing the same data elements (that
    is, data on the *top of the stack*), pipelining and instruction parallelism is
    difficult to achieve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*See* WGC1 *for details on pipelining and instruction parallelism.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'With a stack you generally do one of three things: push new data onto it, pop
    data from it, or operate on the data that is currently sitting on the *top of
    stack* (and possibly the data immediately below that, or *next on stack*).'
  prefs: []
  type: TYPE_NORMAL
- en: '**12.1.1.1 Basic Stack Machine Organization**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A typical stack machine maintains a couple of registers inside the CPU (see
    [Figure 12-1](ch12.xhtml#ch12fig1)). In particular, you can expect to find a *program
    counter register* (like the 80x86’s RIP register) and a *stack pointer register*
    (like the 80x86 RSP register).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-1: Typical stack machine architecture*'
  prefs: []
  type: TYPE_NORMAL
- en: The stack pointer register contains the memory address of the current top of
    stack (TOS) element in memory. The CPU increments or decrements the stack pointer
    register whenever a program places data onto the stack or removes data from the
    stack. On some architectures the stack expands from higher memory locations to
    lower memory locations; on other architectures, the stack grows from lower memory
    locations toward higher memory locations. Fundamentally, the direction of stack
    growth is irrelevant; all it really determines is whether the machine decrements
    the stack pointer register when placing data on the stack (if the stack grows
    toward lower memory addresses) or increments the stack pointer register (when
    the stack grows toward higher memory addresses).
  prefs: []
  type: TYPE_NORMAL
- en: '**12.1.1.2 The push Instruction**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To place data on the stack, you typically use the machine instruction `push`.
    This instruction generally takes a single operand that specifies the value to
    push onto the stack, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are a couple of concrete examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A `push` operation typically increases the value of the stack pointer register
    by the size of its operand in bytes and then copies that operand to the memory
    location the stack pointer now specifies. For example, [Figures 12-2](ch12.xhtml#ch12fig2)
    and [12-3](ch12.xhtml#ch12fig3) illustrate what the stack looks like before and
    after a `push 10` operation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-2: Before a push 10 operation*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-3: After a push 10 operation*'
  prefs: []
  type: TYPE_NORMAL
- en: '**12.1.1.3 The pop Instruction**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To remove a data item from the top of a stack, you use a `pop` or `pull` instruction.
    (This book will use `pop`; just be aware that some architectures use `pull` instead.)
    A typical `pop` instruction might look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*You cannot pop data into a constant. The *pop* operand must be a memory location.*'
  prefs: []
  type: TYPE_NORMAL
- en: The `pop` instruction makes a copy of the data pointed at by the stack pointer
    and stores it into the destination memory location. Then it decrements (or increments)
    the stack pointer register to point at the next lower item on the stack, or next
    on stack (NOS); see [Figures 12-4](ch12.xhtml#ch12fig4) and [12-5](ch12.xhtml#ch12fig5).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-4: Before a pop mem operation*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-5: After a pop mem operation*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the value in stack memory that the `pop` instruction removes from
    the stack is still physically present in memory above the new TOS. However, the
    next time the program pushes data onto the stack, it will overwrite this value
    with the new value.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.1.1.4 Arithmetic Operations on a Stack Machine**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The arithmetic and logical instructions found on a stack machine generally do
    not allow any operands. This is why stack machines are often called *zero-address
    machines*; the arithmetic instructions themselves do not encode any operand addresses.
    For example, consider an `add` instruction on a typical stack machine. This instruction
    will pop two values from the stack (TOS and NOS), compute their sum, and push
    the result back onto the stack (see [Figures 12-6](ch12.xhtml#ch12fig6) and [12-7](ch12.xhtml#ch12fig7)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-6: Before an add operation*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-7: After an add operation*'
  prefs: []
  type: TYPE_NORMAL
- en: Because arithmetic expressions are recursive in nature, and recursion requires
    a stack for proper implementation, it’s no surprise that converting arithmetic
    expressions to a sequence of stack machine instructions is relatively simple.
    Arithmetic expressions found in common programming languages use an *infix notation*,
    where the operator appears between two operands. For example, `a + b` and `c -
    d` are examples of infix notation because the operators (`+` and `-`) appear between
    the operands ([`a`, `b`] and [`c`, `d`]). Before you can do the conversion to
    stack machine instructions, you must convert these infix expressions into *postfix
    notation* (also known as *reverse polish notation*), where the operator immediately
    follows the operands to which it applies. For example, the infix expressions `a
    + b` and `c – d` would have the corresponding postfix forms `a b +` and `c d –`,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have an expression in postfix form, converting it to a sequence of
    stack machine instructions is very easy. You simply emit a `push` instruction
    for each operand and the corresponding arithmetic instruction for the operators.
    For example, `a b +` becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'and `c d -` becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: assuming, of course, that `add` adds the top two items on the stack and `sub`
    subtracts the TOS from the value immediately below it on the stack.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.1.1.5 Real-World Stack Machines**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A big advantage of the stack architecture is that it’s easy to write a compiler
    for such a machine. It’s also very easy to write an emulator for a stack-based
    machine. For these reasons, stack architectures are popular in *virtual machines
    (VMs)* such as the Java Virtual Machine, the UCSD Pascal p-machine, and the Microsoft
    Visual Basic, C#, and F# CIL. Although a few real-world stack-based CPUs do exist,
    such as a hardware implementation of the Java VM, they’re not very popular because
    of the performance limitations of memory access. Nonetheless, understanding the
    basics of a stack architecture is important, because many compilers translate
    HLL source code into a stack-based form prior to emitting actual machine code.
    Indeed, in the worst (though rare) case, compilers are forced to emit code that
    emulates a stack-based machine when compiling complex arithmetic expressions.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.1.2 Accumulator-Based Machines**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The simplicity of a stack machine instruction sequence hides an enormous amount
    of complexity. Consider the following stack-based instruction from the previous
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction looks simple, but it actually specifies a large number of
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Fetch an operand from the memory location pointed to by the stack pointer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Send the stack pointer’s value to the *ALU (arithmetic/logical unit)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instruct the ALU to decrement the stack pointer’s value just sent to it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Route the ALU’s value back to the stack pointer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fetch the operand from the memory location pointed to by the stack pointer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Send the values from the previous step and the first step to the ALU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instruct the ALU to add those values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store the sum away in the memory location pointed to by the stack pointer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The organization of a typical stack machine prevents many parallel operations
    that are possible with pipelining (see *WGC1* for more details on pipelining).
    So stack architectures are hit twice: typical instructions require many steps
    to complete, and those steps are difficult to execute in parallel with other operations.'
  prefs: []
  type: TYPE_NORMAL
- en: One big problem with the stack architecture is that it goes to memory for just
    about everything. For example, if you simply want to compute the sum of two variables
    and store this result in a third variable, you have to fetch the two variables
    and write them to the stack (four memory operations); then you have to fetch the
    two values from the stack, add them, and write their sum back to the stack (three
    memory operations); and finally, you have to pop the item from the stack and store
    the result into the destination memory location (two memory operations). That’s
    a total of nine memory operations. When memory access is slow, this is an expensive
    way to compute the sum of two numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to avoid this large number of memory accesses is to provide a general-purpose
    arithmetic register within the CPU. This is the idea behind an accumulator-based
    machine: you provide a single *accumulator* register, where the CPU computes temporary
    results rather than computing temporary values in memory (on the stack). Accumulator-based
    machines are also known as *one-address* or *single-address machines*, because
    most instructions that operate on two operands use the accumulator as the default
    destination operand for the computation and require a single memory or constant
    operand to use as the source operand. A typical example of an accumulator machine
    is the 6502, which includes the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Because one-address instructions require an operand that isn’t present in many
    of the zero-address instructions, individual instructions found on an accumulator-based
    machine tend to be larger than those found on a typical stack-based machine (because
    you have to encode the operand address as part of the instruction; see *WGC1*
    for details). However, programs are often smaller because fewer instructions are
    needed to do the same thing. Suppose, for example, you want to compute `x = y
    + z`. On a stack machine, you might use an instruction sequence like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'On an accumulator machine, you’d probably use a sequence like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Assuming that the `push` and `pop` instructions are roughly the same size as
    the accumulator machine’s `lda`, `add`, and `sta` instructions (a safe assumption),
    it’s clear that the stack machine’s instruction sequence is actually longer, because
    it requires more instructions. Even ignoring the extra instruction on the stack
    machine, the accumulator machine will probably execute the code faster, because
    it requires only three memory accesses (to fetch `y` and `z` and to store `x`),
    compared with the nine memory accesses the stack machine will require. Furthermore,
    the accumulator machine doesn’t waste any time manipulating the stack pointer
    register during computation.
  prefs: []
  type: TYPE_NORMAL
- en: Even though accumulator-based machines generally have higher performance than
    stack-based machines (for reasons you’ve just seen), they’re not without their
    own problems. Having only one general-purpose register available for arithmetic
    operations creates a bottleneck in the system, resulting in *data hazards*. Many
    calculations produce temporary results that the application must write to memory
    in order to compute other components of the expression. This leads to extra memory
    accesses that could be avoided if the CPU provided additional accumulator registers.
    Thus, most modern general-purpose CPUs do not use an accumulator-based architecture,
    but instead provide a large number of general-purpose registers.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*See* WGC1 *for a discussion of data hazards.*'
  prefs: []
  type: TYPE_NORMAL
- en: Accumulator-based architectures were popular in early computer systems when
    the manufacturing process limited the number of features within the CPU, but today
    you rarely see them outside of low-cost embedded microcontrollers.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.1.3 Register-Based Machines**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Of the three architectures discussed in this chapter, register-based machines
    are the most prevalent today because they offer the highest performance. By providing
    a fair number of on-CPU registers, this architecture spares the CPU from expensive
    memory accesses during the computation of complex expressions.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, a register-based machine could have as few as two general-purpose
    (arithmetic-capable) registers. In practice, about the only machines that fall
    into this category are the Motorola 680x processors, which most people consider
    to be a special case of the accumulator architecture with two separate accumulators.
    Register machines generally contain at least eight “general-purpose” registers
    (this number isn’t arbitrary; it’s the number of general-purpose registers found
    on the 80x86 CPU, the 8080 CPU, and the Z80 CPU, which are probably the minimalist
    examples of what a computer architect would call a “register-based” machine).
  prefs: []
  type: TYPE_NORMAL
- en: Although some register-based machines (such as the 32-bit 80x86) have a small
    number of registers available, a general principle is “the more, the better.”
    Typical RISC machines, such as the PowerPC and ARM, have at least 16 general-purpose
    registers and often at least 32 registers. Intel’s Itanium processor, for example,
    provides 128 general-purpose integer registers. IBM’s CELL processor provides
    128 registers in each of the processing units found on the device (each processing
    unit is a mini-CPU capable of certain operations); a typical CELL processor contains
    eight such processing units along with a PowerPC CPU core.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main reason for having as many general-purpose registers as possible is
    to avoid memory access. In an accumulator-based machine, the accumulator is a
    transient register used for calculations, but you can’t keep a variable’s value
    there for long periods of time, because you’ll need the accumulator for other
    purposes. In a register machine with a large number of registers, it’s possible
    to keep certain (often-used) variables in registers so you don’t have to access
    memory at all when using those variables. Consider the assignment statement `x
    := y+z`;. On a register-based machine (such as the 80x86), we could compute this
    result using the following HLA code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Only two instructions and no memory accesses (for the variables) are required
    here. This is quite a bit more efficient than the accumulator- or stack-based
    architectures. From this example, you can see why the register-based architecture
    has become prevalent in modern computer systems.
  prefs: []
  type: TYPE_NORMAL
- en: As you’ll see in the following sections, register machines are often described
    as either two-address machines or three-address machines, depending on the particular
    CPU’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.1.4 Typical Forms of Arithmetic Expressions**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Computer architects have studied typical source files extensively, and one
    thing they’ve discovered is that a large percentage of assignment statements take
    one of the following forms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Although other assignments do exist, the set of statements in a program that
    takes one of these forms is generally larger than any other group of assignment
    statements. Therefore, computer architects usually optimize their CPUs to efficiently
    handle these forms.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.1.5 Three-Address Architectures**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many machines use a *three-address architecture*. This means that an arithmetic
    statement supports three operands: two source operands and a destination operand.
    For example, most RISC CPUs offer an `add` instruction that will add together
    the values of two operands and store the result into a third operand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'On such architectures, the operands are usually machine registers (or small
    constants), so typically you’d write this instruction as follows (assuming you
    use the names *R*0, *R*1, . . . , *Rn* to denote registers):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Because RISC compilers attempt to keep variables in registers, this single
    instruction handles the last assignment statement given in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Handling an assignment of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'is also relatively easy—just use the destination register as one of the source
    operands, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The drawback to a three-address architecture is that you must encode all three
    operands into each instruction that supports three operands. This is why three-operand
    instructions generally operate only upon register operands. Encoding three separate
    memory addresses can be quite expensive—just ask any VAX programmer. The DEC VAX
    computer system is a good example of a three-address CISC machine.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.1.6 Two-Address Architectures**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The 80x86 architecture is known as a *two-address machine*. In this architecture,
    one of the source operands is also the destination operand. Consider the following
    80x86/HLA `add` instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Two-address machines, such as the 80x86, can handle the first four forms of
    the assignment statement given earlier with a single instruction. The last form,
    however, requires two or more instructions and a temporary register. For example,
    to compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'you’d need to use the following code (assuming *var2* and *var3* are memory
    variables and the compiler is keeping *var1* in the EAX register):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**12.1.7 Architectural Differences and Your Code**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One-address, two-address, and three-address architectures have the following
    hierarchy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1Address** ⊂ **2Address** ⊂ **3Address**'
  prefs: []
  type: TYPE_NORMAL
- en: That is, two-address machines are capable of doing anything a one-address machine
    can do, and three-address machines are capable of anything one-address or two-address
    machines can do. The proof is very simple:^([2](footnotes.xhtml#ch12fn2))
  prefs: []
  type: TYPE_NORMAL
- en: To show that a two-address machine is capable of doing anything a one-address
    machine can do, simply choose one register on the two-address machine and use
    it as the “accumulator” when simulating a one-address architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To show that a three-address machine is capable of anything a two-address machine
    can do, simply use the same register for one of the source operands and the destination
    operand, thereby limiting yourself to two registers (operands/addresses) for all
    operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given this hierarchy, you might think that if you limit the code you write so
    that it runs well on a one-address machine, you’ll get good results on all machines.
    In reality, most general-purpose CPUs available today are two- or three-address
    machines, so writing your code to favor a one-address machine may limit the optimizations
    that are possible on a two- or three-address machine. Furthermore, optimization
    quality varies so widely among compilers that backing up an assertion like this
    would be very difficult. You should probably try to create expressions that take
    one of the five forms given earlier (in “Typical Forms of Arithmetic Expressions”
    on [page 394](ch12.xhtml#page_394)) if you want your compiler to produce the best
    possible code. Because most modern programs run on two- or three-address machines,
    the remainder of this chapter assumes that environment.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.1.8 Complex Expressions**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once your expressions get more complex than the five forms given earlier, the
    compiler will have to generate a sequence of two or more instructions to evaluate
    them. When compiling the code, most compilers internally translate a complex expression
    into a sequence of “three-address statements” that are semantically equivalent
    to it, as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, these five statements are semantically equivalent to the complex
    expression appearing in the comment. The major difference in the computation is
    the introduction of two temporary values (`temp1` and `temp2`). Most compilers
    will attempt to use machine registers to maintain these temporary values.
  prefs: []
  type: TYPE_NORMAL
- en: Because the compiler internally translates a complex instruction into a sequence
    of three-address statements, you may wonder if you can help it by converting complex
    expressions into three-address statements yourself. Well, it depends on your compiler.
    For many (good) compilers, breaking a complex calculation into smaller pieces
    may, in fact, thwart the compiler’s ability to optimize certain sequences. So,
    when it comes to arithmetic expressions, most of the time you should do your job
    (write the code as clearly as possible) and let the compiler do its job (optimize
    the result). However, if you can specify a calculation using a form that naturally
    converts to a two-address or three-address form, by all means do so. At the very
    least, it will have no effect on the code the compiler generates. At best, under
    some special circumstances, it could help the compiler produce better code. Either
    way, the resulting code will probably be easier to read and maintain if it is
    less complex.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.2 Optimization of Arithmetic Statements**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because HLL compilers were originally designed to let programmers use algebraic-like
    expressions in their source code, this is one area in computer science that has
    been well researched. Most modern compilers that provide a reasonable optimizer
    do a decent job of translating arithmetic expressions into machine code. You can
    usually assume that the compiler you’re using doesn’t need a whole lot of help
    with optimizing arithmetic expressions (and if it does, you might consider switching
    to a better compiler instead of trying to manually optimize the code).
  prefs: []
  type: TYPE_NORMAL
- en: To help you appreciate the job the compiler is doing for you, this section discusses
    some of the typical optimizations you can expect from modern optimizing compilers.
    By understanding what a (decent) compiler does, you can avoid hand-optimizing
    those things that it is capable of handling.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.2.1 Constant Folding**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Constant folding is an optimization that computes the value of constant expressions
    or subexpressions at compile time rather than emitting code to compute their result
    at runtime. For example, a Pascal compiler that supports this optimization would
    translate a statement of the form `i := 5 + 6;` to `i := 11;` prior to generating
    machine code for the statement. This saves it from emitting an `add` instruction
    that would have to execute at runtime. As another example, suppose you want to
    allocate an array containing 16MB of storage. One way to do this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The only problem with this approach is that 16,777,216 is a magic number. It
    represents the value 2^(24) and not some other arbitrary value. Now consider the
    following C/C++ declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Most programmers realize that 1,024 times 1,024 is a binary million, and 16
    times this value corresponds to 16 mega-somethings. Yes, you need to recognize
    that the subexpression `16*1024*1024` is equivalent to 16,777,216\. But this pattern
    is easier to recognize as 16MB (at least, when used within a character array)
    than `16777216` (or was it `16777214`?). In both cases the amount of storage the
    compiler allocates is exactly the same, but the second case is, arguably, more
    readable. Hence, it is better code.^([3](footnotes.xhtml#ch12fn3))
  prefs: []
  type: TYPE_NORMAL
- en: Variable declarations aren’t the only place a compiler can use this optimization.
    Any arithmetic expression (or subexpression) containing constant operands is a
    candidate for constant folding. Therefore, if you can write an arithmetic expression
    more clearly by using constant expressions rather than computing the results by
    hand, you should definitely go for the more readable version and leave it up to
    the compiler to handle the constant calculation at compile time. If your compiler
    doesn’t support constant folding, you can certainly simulate it by performing
    all constant calculations manually. However, you should do this only as a last
    resort. Finding a better compiler is almost always a better choice.
  prefs: []
  type: TYPE_NORMAL
- en: Some good optimizing compilers may take extreme steps when folding constants.
    For example, some compilers with a sufficiently high optimization level enabled
    will replace certain function calls, with constant parameters, to the corresponding
    constant value. For example, a compiler might translate a C/C++ statement of the
    form `sineR = sin(0);` to `sineR = 0;` during compilation (as the sine of zero
    radians is `0`). This type of constant folding, however, is not all that common,
    and you usually have to enable a special compiler mode to get it.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you ever have any questions about whether your particular compiler supports
    constant folding, have the compiler generate an assembly listing and look at its
    output (or view the disassembled output with a debugger). Here’s a trivial case
    written in C/C++ (compiled with Visual C++):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s a comparable program written in Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `ldc #2` instruction pushes a constant from a constant pool onto
    the stack. The comment attached to this bytecode instruction explains that the
    Java compiler converted `16*1024*1024` into a single constant `16777216`. Java
    performs the constant folding at compile time rather than computing this product
    at runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the comparable program in Swift, along with the assembly code emitted
    for the relevant portion^([4](footnotes.xhtml#ch12fn4)) of the main program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, Swift also supports the constant folding optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.2.2 Constant Propagation**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Constant propagation is an optimization a compiler uses to replace a variable
    access by a constant value if the compiler determines that it’s possible. For
    example, a compiler that supports constant propagation will make the following
    optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In object code, manipulating immediate constants is often more efficient than
    manipulating variables; therefore, constant propagation often produces much better
    code. In some cases, constant propagation also allows the compiler to eliminate
    certain variables and statements altogether (in this example, the compiler could
    remove `variable = 1234;` if there are no later references to the variable object
    in the source code).
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, well-written compilers can do some outrageous optimizations
    involving constant folding. Consider the following C code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the 80x86 output that GCC produces with the `-O3` (maximum) optimization
    option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: A quick glance shows that the `rtn3()` function is nowhere to be found. With
    the `-O3` command-line option enabled, GCC figured out that `rtn3()` simply returns
    a constant, so it propagates that constant return result everywhere you call `rtn3()`.
    In the case of the `printf()` function call, the combination of constant propagation
    and constant folding yielded a single constant, `5`, that the code passes on to
    the `printf()` function.
  prefs: []
  type: TYPE_NORMAL
- en: As with constant folding, if your compiler doesn’t support constant propagation
    you can simulate it manually, but only as a last resort. Again, finding a better
    compiler is almost always a better choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can turn on the compiler’s assembly language output to determine if your
    compiler support constant propagation. For example, here is Visual C++’s output
    (with the `/O2` optimization level turned on):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, Visual C++ also eliminated the `f()` function as well as the
    `i` and `j` variables. It computed the function result (`i+1`) at compile time
    and substituted the constant `16777217` (`16*1024*1024 + 1`) for all the computations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example using Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: A quick review of this Java bytecode shows that the Java compiler (`java version
    "1.6.0_65"`) does not support the constant propagation optimization. Not only
    did it not eliminate the `f()` function, but it also doesn’t eliminate variables
    `i` and `j`, and it passes the value of `i` to function `f()` rather than passing
    the appropriate constant. One could argue that Java’s bytecode interpretation
    dramatically affects performance, so a simple optimization such as constant propagation
    won’t impact performance that much.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the comparable program written in Swift, with the compiler’s assembly
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The Swift compiler generates a tremendous amount of code in support of its runtime
    system, so you can hardly call Swift an *optimizing* compiler. That being said,
    the assembly code that it does generate demonstrates that Swift supports the constant
    propagation optimization. It eliminates the function `f()` and propagates the
    constants resulting from the calculations into the calls that print the values
    of `i` and `j`. It doesn’t eliminate `i` and `j` (probably because of some consistency
    issues regarding the runtime system), but it does propagate the constants through
    the compiled code.
  prefs: []
  type: TYPE_NORMAL
- en: Given the excessive amount of code that the Swift compiler generates, it’s questionable
    whether this optimization is worthwhile. However, even with all the extra code
    (too much to print here, so feel free to look at it yourself), the output still
    runs faster than interpreted Java code.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.2.3 Dead Code Elimination**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Dead code elimination is the removal of the object code associated with a particular
    source code statement if the program never again uses the result of that statement.
    Often, this is a result of a programming error. (After all, why would someone
    compute a value and not use it?) If a compiler encounters dead code in the source
    file, it may warn you to check the logic of your code. In some cases, however,
    earlier optimizations can produce dead code. For example, the constant propagation
    for the value variable in the earlier example could result in the statement `variable
    = 1234;` being dead. Compilers that support dead code elimination will quietly
    remove the object code for this statement from the object file.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example of dead code elimination, consider the following C program and
    its corresponding assembly code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the 32-bit 80x86 code GCC emits when supplied the `-O3` command-line
    option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now consider the 80x86 output from GCC when optimization is not enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In fact, one of the main reasons that program examples throughout this book
    call a function like `printf()` to display various values is to explicitly use
    those values to prevent dead code elimination from erasing the code we’re examining
    from the assembly output file. If you remove the final `printf()` from the C program
    in many of these examples, most of the assembly code will disappear because of
    dead code elimination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the output from the previous C++ code from Visual C++:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Unlike GCC, Visual C++ did not eliminate the `rtn3()` function. However, it
    did remove the assignment to `i`—and the call to `rtn3()`—in the main program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the equivalent Java program and the JBC output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'At first blush, it looks like Java does not support dead code elimination.
    However, the problem might be that our example code doesn’t trigger this optimization
    in the compiler. Let’s try something more obvious to the compiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Now we’ve given the Java compiler something it can chew on. The main program
    eliminates the call to `rtn3()` and the assignment to `i`. The optimization isn’t
    quite as smart as GCC’s or Visual C++’s optimization, but (at least) for some
    cases, it works. Unfortunately, without constant propagation, Java misses many
    opportunities for dead code elimination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the equivalent Swift code for the earlier example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that Swift (at least for this example) does not support dead code elimination.
    However, let’s try the same thing we did with Java. Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Compiling this code produces a list of warnings about the dead code, but the
    output demonstrates that Swift does support dead code elimination. Furthermore,
    because Swift supports constant propagation as well, it won’t miss as many opportunities
    for dead code elimination as Java (though Swift will need to mature a bit more
    before it catches up to GCC or Visual C++).
  prefs: []
  type: TYPE_NORMAL
- en: '**12.2.4 Common Subexpression Elimination**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Often, a portion of some expressions—a *subexpression*—may appear elsewhere
    in the current function. If there are no changes to the values of the variables
    appearing in the subexpression, the program doesn’t need to compute its value
    twice. Instead, it can save the subexpression’s value on the first evaluation
    and then use that value everywhere the subexpression appears again. For example,
    consider the following Pascal code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'A decent compiler might translate these to the following sequence of three-address
    statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Although the former statements use the subexpression `(a + b)` twice and the
    subexpression `(e div f)` three times, the three-address code sequence computes
    these subexpressions only once and uses their values when the common subexpressions
    appear later.
  prefs: []
  type: TYPE_NORMAL
- en: 'As another example, consider the following C/C++ code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the 32-bit 80x86 assembly file that GCC generates (with the `-O3` option)
    for the preceding C code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Note how the compiler maintains the results of the common subexpressions in
    various registers (see the comments in the assembly output for details).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the (64-bit) output from Visual C++:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Because of the extra registers available on the x86-64, Visual C++ was able
    to keep all the temporaries in registers and did an even better job of reusing
    precomputed values for common subexpressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the compiler you’re using doesn’t support common subexpression optimizations
    (you can determine this by examining the assembly output), chances are pretty
    good that its optimizer is subpar, and you should consider using a different compiler.
    However, in the meantime, you can always explicitly code this optimization yourself.
    Consider this version of the former C code, which manually computes common subexpressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Of course, there was no reason to create the `ijExpr` and `kmnExpr` variables,
    as we could have simply used the *expr2* and *expr3* variables for this purpose.
    However, this code was written to make the changes to the original program as
    obvious as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the similar Java code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Notice that Java does not optimize common subexpressions; instead, it recomputes
    the subexpressions each time it encounters them. Therefore, you should manually
    compute the values of common subexpressions when writing Java code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a variant of the current example in Swift (along with the assembly output):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: If you carefully read through this code, you can see the Swift compiler properly
    optimizes away the common subexpressions and computes each subexpression only
    once.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.2.5 Strength Reduction**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Often, the CPU can directly compute some value using a different operator than
    the source code specifies, thereby replacing a more complex (or stronger) instruction
    with a simpler instruction. For example, a `shift` operation can implement multiplication
    or division by a constant that is a power of 2, and certain modulo (remainder)
    operations are possible using a bitwise `and` instruction (the `shift` and `and`
    instructions generally execute much faster than `multiply` and `divide` instructions).
    Most compiler optimizers are good at recognizing such operations and replacing
    the more expensive computation with a less expensive sequence of machine instructions.
    To see strength reduction in action, consider this C code and the 80x86 GCC output
    that follows it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the resulting 80x86 code generated by GCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: In this 80x86 code, note that GCC never emitted a multiplication or division
    instruction, even though the C code used these two operators extensively. GCC
    replaced each of these (expensive) operations with less expensive address calculations,
    shifts, and logical AND operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'This C example declared its variables as `unsigned` rather than as `int`. There’s
    a very good reason for this modification: strength reduction produces more efficient
    code for certain unsigned operands than it does for signed operands. This is a
    very important point: if you have a choice between using either signed or unsigned
    integer operands, always try to use unsigned values, because compilers can often
    generate better code when processing unsigned operands. To see the difference,
    here’s the previous C code rewritten using signed integers, followed by GCC’s
    80x86 output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is GCC’s (32-bit) 80x86 assembly output for this C code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The difference in these two coding examples demonstrates why you should opt
    for unsigned integers (over signed integers) whenever you don’t absolutely need
    to deal with negative numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Attempting strength reduction manually is risky. While certain operations (like
    division) are almost always slower than others (like shifting to the right) on
    most CPUs, many strength reduction optimizations are not portable across CPUs.
    That is, substituting a left shift operation for multiplication may not always
    produce faster code when you compile for different CPUs. Some older C programs
    contain manual strength reductions that were originally added to improve performance.
    Today, those strength reductions can actually cause the programs to run slower
    than they should. Be very careful about incorporating strength reductions directly
    into your HLL code—this is one area where you should let the compiler do its job.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.2.6 Induction**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In many expressions, particularly those appearing within a loop, the value
    of one variable in the expression is completely dependent on some other variable.
    As an example, consider the following `for` loop in Pascal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'A compiler’s optimizer may recognize that `j` is completely dependent on the
    value of `i` and rewrite this code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This optimization saves some work in the loop (specifically, the computation
    of `j := i * 2`).
  prefs: []
  type: TYPE_NORMAL
- en: 'As another example, consider the following C code and the MASM output that
    Microsoft’s Visual C++ compiler produces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the MASM (32-bit 80x86) output from Visual C++:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in this MASM output, the Visual C++ compiler recognizes that
    `i` is not used in this loop. There are no calculations involving `i`, and it’s
    completely optimized away. Furthermore, there’s no `j = i * 2` computation. Instead,
    the compiler uses induction to determine that `j` increases by 2 on each iteration,
    and emits the code to do this rather than computing the value of `j` value from
    `i`. Finally, note that the compiler doesn’t index into the vector array. Instead,
    it marches a pointer through the array on each iteration of the loop—once again
    using induction to produce a faster and shorter code sequence than you’d get without
    this optimization.
  prefs: []
  type: TYPE_NORMAL
- en: As for common subexpressions, you can manually incorporate induction optimization
    into your programs. The result is almost always harder to read and understand,
    but if your compiler’s optimizer fails to produce good machine code in a section
    of your program, manual optimization is always an option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the Java variation of this example and the JBC output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s probably obvious that Java doesn’t optimize this code at all. If you want
    better code, you’ll have to manually optimize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, Java isn’t the best language choice if you’re interested in
    producing optimized runtime code. Perhaps Java’s authors felt that as a result
    of the interpreted bytecode execution, there was no real reason to try to optimize
    the compiler’s output, or perhaps they felt that optimization was the JIT compiler’s
    responsibility.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.2.7 Loop Invariants**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The optimizations shown so far have all been techniques a compiler can use
    to improve code that is already well written. Handling loop invariants, by contrast,
    is a compiler optimization for fixing bad code. A *loop invariant* is an expression
    that does not change on each iteration of some loop. The following Visual Basic
    code demonstrates a trivial loop-invariant calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The value of `k` does not change during the loop’s execution. Once the loop
    completes execution, `k`’s value is exactly the same as if the calculation of
    `k` had been moved before or after the loop. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The difference between these two code fragments, of course, is that the second
    example computes the value `k = i * 2` only once rather than on each iteration
    of the loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many compilers’ optimizers will spot a loop-invariant calculation and use *code
    motion* to move it outside the loop. As an example of this operation, consider
    the following C program and its corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the 80x86 MASM code emitted by Visual C++:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: As you can see by reading the comments in the assembly code, the loop-invariant
    expression `j = k + 2` was moved out of the loop and executed prior to the start
    of the loop’s code, saving some execution time on each iteration of the loop.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike most optimizations, which you should leave up to the compiler if possible,
    you should move all loop-invariant calculations out of a loop unless there’s a
    justifiable reason for leaving them there. Loop-invariant calculations raise questions
    for someone reading your code (“Isn’t this supposed to change in the loop?”),
    because their presence actually makes the code harder to read and understand.
    If you want to leave the invariant code in the loop for some reason, be sure to
    comment your justification for anyone looking at your code later.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.2.8 Optimizers and Programmers**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'HLL programmers fall into three groups based on their understanding of these
    compiler optimizations:'
  prefs: []
  type: TYPE_NORMAL
- en: The first group is unaware of how compiler optimizations work, and they write
    their code without considering the effect that their code organization will have
    on the optimizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second group understands how compiler optimizations work, so they write
    their code to be more readable. They assume that the optimizer will handle issues
    such as converting multiplication and division to shifts (where appropriate) and
    preprocessing constant expressions. This second group places a fair amount of
    faith in the compiler’s ability to correctly optimize their code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third group is also aware of the general types of optimizations that compilers
    can do, but they don’t trust the compilers to do the optimization for them. Instead,
    they manually incorporate those optimizations into their code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interestingly enough, compiler optimizers are actually designed for the first
    group of programmers, those who are ignorant of how the compiler operates. Therefore,
    a good compiler will usually produce roughly the same quality of code for all
    three types of programmers (at least with respect to arithmetic expressions).
    This is particularly true when you compile the same program across different compilers.
    However, keep in mind that this assertion is valid only for compilers that have
    decent optimization capabilities. If you have to compile your code on a large
    number of compilers and you can’t be confident that all of them have good optimizers,
    manual optimization may be one way to achieve consistently good performance across
    all compilers.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the real question is, “Which compilers are good, and which are not?”
    It would be nice to provide a table or chart in this book that describes the optimization
    capabilities of all the different compilers you might encounter, but unfortunately,
    the rankings change as compiler vendors improve their products, so anything printed
    here would rapidly become obsolete.^([5](footnotes.xhtml#ch12fn5)) Fortunately,
    there are several websites that try to keep up-to-date comparisons of compilers.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.3 Side Effects in Arithmetic Expressions**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’ll definitely want to give a compiler some guidance with respect to side
    effects that may occur in an expression. If you don’t understand how compilers
    deal with side effects in arithmetic expressions, you might write code that doesn’t
    always produce correct results, particularly when moving source code between different
    compilers. Wanting to write the fastest or the smallest possible code is all well
    and good, but if it doesn’t produce the correct answer any optimizations you make
    on the code are all for naught.
  prefs: []
  type: TYPE_NORMAL
- en: 'A *side effect* is any modification to the global state of a program outside
    the immediate result a piece of code is producing. The primary purpose of an arithmetic
    expression is to produce the expression’s result. Any other change to the system’s
    state in an expression is a side effect. The C, C++, C#, Java, Swift, and other
    C-based languages are especially guilty of allowing side effects in an arithmetic
    expression. For example, consider the following C code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This expression exhibits four separate side effects:'
  prefs: []
  type: TYPE_NORMAL
- en: The decrement of `k` at the end of the expression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The assignment to `j` prior to using `j`’s value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The increment of the pointer `pi` after dereferencing `pi`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The assignment to `i`^([6](footnotes.xhtml#ch12fn6))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although few non–C-based languages provide as many ways to create side effects
    in arithmetic expressions as C does, most languages do allow you to create side
    effects within an expression via a function call. Side effects in functions are
    useful, for example, when you need to return more than a single value as a function
    result in languages that don’t directly support this capability. Consider the
    following Pascal code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the call to the `hasSideEffect()` function produces two different
    side effects:'
  prefs: []
  type: TYPE_NORMAL
- en: The modification of the global variable `k`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The modification of the pass-by-reference parameter `j` (the actual parameter
    is `n` in this code fragment).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The real purpose of the function is to compute its return result. Any modification
    of global values or reference parameters constitutes a side effect of that function;
    hence, invoking that function within an expression produces side effects. Any
    language that allows you to modify global values (either directly or through parameters)
    from a function is capable of producing side effects within an expression; this
    concept is not limited to Pascal programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with side effects in an expression is that most languages do not
    guarantee the order of evaluation of the components that make up an expression.
    Many novice programmers incorrectly assume that when they write an expression
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'the compiler will emit code that first calls function `f()` and then calls
    function `g()`. Very few programming languages, however, require this order of
    execution. That is, some compilers will indeed call `f()`, then `g()`, and add
    their return results together. Other compilers, however, will call `g()` first,
    then `f()`, and compute the sum of the function return results. That is, the compiler
    could translate this expression into either of the following simplified code sequences
    before actually generating native machine code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: These two different function call sequences could produce completely different
    results if `f()` or `g()` produces a side effect. For example, if function `f()`
    modifies the value of the `x` parameter you pass to it, the preceding sequence
    could produce different results.
  prefs: []
  type: TYPE_NORMAL
- en: Note that issues such as precedence, associativity, and commutativity have no
    bearing on whether the compiler evaluates one subcomponent of an expression before
    another.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the following arithmetic expression and several possible
    intermediate forms for the expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Other combinations are also possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The specifications for most programming languages explicitly leave the order
    of evaluation undefined. This may seem somewhat bizarre, but there’s a good reason
    for it: sometimes the compiler can produce better machine code by rearranging
    the order in which it evaluates certain subexpressions within an expression. Any
    attempt by the language designer to force a particular order of evaluation on
    a compiler’s implementer, therefore, could limit the range of optimizations possible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are, of course, certain rules that most languages do enforce. Probably
    the most common rule is that all side effects within an expression will occur
    prior to the completion of that statement’s execution. For example, if the function
    `f()` modifies the global variable `x`, then the following statements will always
    print the value of `x` after `f()` modifies it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Another rule you can count on is that the assignment to a variable on the left-hand
    side of an assignment statement does not occur prior to the use of that same variable
    on the right-hand side of the expression. That is, the following code won’t store
    the result of the expression into variable `n` until it uses the previous value
    of `n` within the expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Because the order of the production of side effects within an expression is
    undefined in most languages, the result of the following code is generally undefined
    (in Pascal):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The compiler is free to call the `incN()` function first (so `n` will contain
    `3` prior to executing the subexpression `n * 2`), or it can compute `n * 2` first
    and then call the `incN()` function. As a result, one compilation of this statement
    could produce the output `8`, while a different compilation might produce `6`.
    In both cases, `n` would contain `3` after the `writeln` statement is executed,
    but the order of computation of the expression in the `writeln` statement could
    vary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Don’t make the mistake of thinking you can run some experiments to determine
    the order of evaluation. At the very best, such experiments will tell you only
    the order a particular compiler uses. A different compiler may well compute subexpressions
    in a different order. In fact, the same compiler might also compute the components
    of a subexpression differently based on the context of that subexpression. This
    means that a compiler might compute the result using one ordering at one point
    in the program and using a different ordering somewhere else in the same program.
    This is why it’s dangerous to “determine” the ordering your particular compiler
    uses and rely on that ordering. Even if the compiler is consistent in the order
    it uses to compute side effects, the compiler vendor could change the ordering
    in a later version. If you must depend upon the order of evaluation, first break
    the expression down into a sequence of simpler statements whose computational
    order you can control. For example, if you really need to have your program call
    `f()` before `g()` in this statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'then you should write the code this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: If you must control the order of evaluation within an expression, take special
    care to ensure that all side effects are computed at the appropriate time. To
    do this, you need to learn about sequence points.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.4 Containing Side Effects: Sequence Points**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted earlier, most languages guarantee that the computation of side effects
    completes before certain points, known as *sequence points*, in your program’s
    execution. For example, almost every language guarantees that all side effects
    will be computed by the time the statement containing the expression completes
    execution. The end of a statement is an example of a sequence point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The C programming language provides several important sequence points within
    expressions, in addition to the semicolon at the end of a statement. C defines
    sequence points between each of the following operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: In these examples, C^([7](footnotes.xhtml#ch12fn7)) guarantees that all side
    effects in expression1 are completed before the computation of expression2 or
    expression3. Note that for the conditional expression, C evaluates only one of
    expression2 or expression3 so the side effects of only one of these subexpressions
    ever occurs on a given execution of the conditional expression. Similarly, short-circuit
    evaluation may cause only expression1 to evaluate in the `&&` and `||` operations.
    So, take care when using the last three forms.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how side effects and sequence points can affect the operation
    of your program, consider the following example in C:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that C does not define a sequence point across the assignment operator.
    Therefore, the language makes no guarantees about the value of the expression
    `i` it uses as an index. The compiler can choose to use the value of `i` before
    or after indexing into array. That the `++` operator is a post-increment operation
    implies only that `i++` returns the value of `i` prior to the increment; it doesn’t
    guarantee that the compiler will use the pre-increment value of `i` anywhere else
    in the expression. The bottom line is that the last statement in this example
    could be semantically equivalent to either of the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: The C language definition allows either form; it doesn’t require the first form
    simply because the array index appears in the expression before the post-increment
    operator.
  prefs: []
  type: TYPE_NORMAL
- en: To control the assignment to `array` in this example, you have to ensure that
    no part of the expression depends upon the side effects of some other part of
    the expression. That is, you cannot both use the value of `i` at one point in
    the expression and apply the post-increment operator to `i` in another part of
    the expression, unless there is a sequence point between the two uses. Because
    there’s no such sequence point in this statement, the result is undefined by the
    C language standard.
  prefs: []
  type: TYPE_NORMAL
- en: 'To guarantee that a side effect occurs at an appropriate point, you must have
    a sequence point between two subexpressions. For example, if you’d like to use
    the value of `i` prior to the increment as the index into the array, you could
    write the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the value of `i` after the increment operation as the array index, you
    could use code such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Note, by the way, that a decent compiler won’t increment `i` and then compute
    `i - 1`. It will recognize the symmetry here, grab the value of `i` prior to the
    increment, and use that value as the index into array. This is an example of where
    someone who is familiar with typical compiler optimizations could take advantage
    of this knowledge to write code that is more readable. A programmer who inherently
    mistrusts compilers and their ability to optimize well might write code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: An important distinction is that a sequence point does not specify exactly when
    a computation will take place, only that it will happen before crossing the sequence
    point. The side effect could have been computed much earlier in the code, at any
    point between the previous sequence point and the current one. Another takeaway
    is that sequence points do not force the compiler to complete some computations
    between a pair of sequence points if that computation does not produce any side
    effects. Eliminating common subexpressions, for example, would be a far less useful
    optimization if the compiler could only use the result of common subexpression
    computations between sequence points. The compiler is free to compute the result
    of a subexpression as far ahead as necessary as long as that subexpression produces
    no side effects. Similarly, a compiler can compute the result of a subexpression
    as late as it cares to, as long as that result doesn’t become part of a side effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because statement endings (that is, semicolons) are a sequence point in most
    languages, one way to control the computation of side effects is to manually break
    a complex expression down into a sequence of three-address-like statements. For
    example, rather than relying on the Pascal compiler to translate an earlier example
    into three-address code using its own rules, you can explicitly write the code
    using whichever set of semantics you prefer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Again, operator precedence and associativity do not control when a computation
    takes place in an expression. Even though addition is left associative, the compiler
    may compute the value of the addition operator’s right operand before it computes
    the value of the addition operator’s left operand. Precedence and associativity
    control how the compiler arranges the computation to produce the result. They
    do not control when the program computes the subcomponents of the expression.
    As long as the final computation produces the results expected based on precedence
    and associativity, the compiler is free to compute the subcomponents in any order
    and at any time it pleases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus far, this section has implied that a compiler always computes the value
    of an assignment statement and completes that assignment (and any other side effects)
    upon encountering the semicolon at the end of the statement. Strictly speaking,
    this isn’t true. What many compilers do is ensure that all side effects occur
    between a sequence point and the next reference to the object changed by the side
    effect. For example, consider the following two statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Although the first statement in this code fragment has a side effect, some compilers
    might compute the value (or portions thereof) of the second statement before completing
    the execution of the first statement. Many compilers will rearrange various machine
    instructions to avoid data hazards and other execution dependencies in the code
    that might hamper performance (for details on data hazards, see *WGC1*). The semicolon
    sitting between these two statements does not guarantee that all computations
    for the first statement are complete before the CPU begins any new computation;
    it guarantees only that the program computes any side effects that precede the
    semicolon before executing any code that depends on them. Because the second statement
    does not depend upon the values of `j` or `i`, the compiler is free to start computing
    the second assignment prior to completing the first statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequence points act as barriers. A code sequence must complete its execution
    before any subsequent code affected by the side effect can execute. A compiler
    cannot compute the value of a side effect before executing all the code up to
    the previous sequence point in the program. Consider the following two code fragments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: In code fragment 1, the compiler must not rearrange the code so that it produces
    the side effect `++k` prior to using `k` in the previous statement. The end-of-statement
    sequence point guarantees that the first statement in this example uses the value
    of `k` prior to any side effects produced in subsequent statements. In code fragment
    2, however, the result of the side effect that `++n` produces does not affect
    anything in the `i = j + k;` statement, so the compiler is free to move the `++n`
    operation into the code that computes `i`’s value if doing so is more convenient
    or efficient.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.5 Avoiding Problems Caused by Side Effects**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Because it’s often difficult to see the impact of side effects in your code,
    it’s a good idea to try to limit your program’s exposure to problems with side
    effects. Of course, the best way to do this is to eliminate side effects altogether
    in your programs. Unfortunately, that isn’t a realistic option. Many algorithms
    depend upon side effects for proper operation (functions returning multiple results
    via reference parameters or even global variables are good examples). You can,
    however, reduce unintended consequences of side effects by observing a few simple
    rules:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid placing side effects in Boolean expressions within program flow control
    statements such as `if`, `while`, and `do..until`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a side effect exists on the right side of an assignment operator, try moving
    the side effect into its own statement before or after the assignment (depending
    on whether the assignment statement uses the value of the object before or after
    it applies the side effect).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid multiple assignments in the same statement; break them into separate statements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid calling more than one function (that might produce a side effect) in the
    same expression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid modifications to global objects (such as side effects) when writing functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always document side effects thoroughly. For functions, you should note the
    side effect in the function’s documentation, as well as on every call to that
    function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**12.6 Forcing a Particular Order of Evaluation**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted earlier, operator precedence and associativity do not control when
    a compiler may compute subexpressions. For example, if `X`, `Y`, and `Z` are each
    subexpressions (which could be anything from a single constant or variable reference
    to a complex expression in and of themselves), then an expression of the form
    `X / Y * Z` does not imply that the compiler computes the value for `X` before
    it computes the value for `Y` and `Z`. In fact, the compiler is free to compute
    the value for `Z` first, then `Y`, and finally `X`. Operator precedence and associativity
    require only that the compiler must compute the value of `X` and `Y` (in any order)
    before computing `X/Y`, and must compute the value of the subexpression `X/Y`
    before computing `(X / Y) * Z`. Of course, compilers can transform expressions
    via applicable algebraic transformations, but they’re generally careful about
    doing so, because not all standard algebraic transformations apply in limited-precision
    arithmetic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although compilers can compute subexpressions in any order they choose (which
    is why side effects can create obscure problems), they usually avoid rearranging
    the order of actual computations. For example, mathematically, the following two
    expressions are equivalent following the standard rules of algebra (versus limited-precision
    computer arithmetic):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'In standard mathematics, this identity exists because the multiplication operator
    is *commutative*; that is, *A* × *B* is equal to *B* × *A*. Indeed, these two
    expressions will generally produce the same result as long as they are computed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'The parentheses are used here not to show precedence, but to group calculations
    that the CPU must perform as a unit. That is, the statements are equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'In most algebraic systems, `C` and `D` should have the same value. To understand
    why `C` and `D` may not be equivalent, consider what happens when `X`, `Y`, and
    `Z` are all integer objects with the values `5`, `2`, and `3`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Again, this is why compilers are careful about algebraically rearranging expressions.
    Most programmers realize that `X * (Y / Z)` is not the same thing as `(X * Y)
    / Z`. Most compilers realize this too. In theory, a compiler should translate
    an expression of the form `X * Y / Z` as though it were `(X * Y) / Z`, because
    the multiplication and division operators have the same precedence and are left
    associative. However, good programmers never rely on the rules of associativity
    to guarantee this. Although most compilers will correctly translate this expression
    as intended, the next engineer who comes along might not realize what’s going
    on. Therefore, explicitly including the parentheses to clarify the intended evaluation
    is a good idea. Better still, treat integer truncation as a side effect and break
    the expression down into its constituent computations (using three-address-like
    expressions) to ensure the proper order of evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Integer arithmetic obviously obeys its own rules, and those of real algebra
    don’t always apply. However, don’t assume that floating-point arithmetic doesn’t
    suffer from the same set of problems. Any time you’re doing limited-precision
    arithmetic involving the possibility of rounding, truncation, overflow, or underflow—as
    is the case with floating-point arithmetic—standard real-arithmetic algebraic
    transformations may not be legal. In other words, applying arbitrary real-arithmetic
    transformations to a floating-point expression can introduce inaccuracies in the
    computation. Therefore, a good compiler won’t perform these types of transformations
    on real expressions. Unfortunately, some compilers do apply the rules of real
    arithmetic to floating-point operations. Most of the time, the results they produce
    are reasonably correct (within the limitations of the floating-point representation);
    in some special cases, however, they’re particularly bad.
  prefs: []
  type: TYPE_NORMAL
- en: In general, if you must control the order of evaluation and when the program
    computes subcomponents of an expression, your only choice is to use assembly language.
    Subject to minor issues, such as out-of-order instruction execution, you can specify
    exactly when your software will compute various components of an expression when
    implementing the expression in assembly code. For very accurate computations,
    when the order of evaluation can affect the results you obtain, assembly language
    may be the safest approach. Although fewer programmers are capable of reading
    and understanding it, there’s no question that it allows you to exactly specify
    the semantics of an arithmetic expression—what you read is what you get without
    any modification by the assembler. This simply isn’t true for most HLL systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7 Short-Circuit Evaluation**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For certain arithmetic and logical operators, if one component of the expression
    has a certain value, the value for the whole expression is automatically known
    regardless of the values of the expression’s remaining components. A classic example
    is the multiplication operator. If you have an expression `A * B` and you know
    that either `A` or `B` is `0`, there’s no need to compute the other component,
    because the result is already `0`. If the cost of computing the subexpressions
    is expensive relative to the cost of a comparison, then a program can save some
    time by testing the first component to determine if it needs to bother computing
    the second component. This optimization is known as *short-circuit evaluation*
    because the program skips over (“short-circuits” in electronics terminology) computing
    the remainder of the expression.
  prefs: []
  type: TYPE_NORMAL
- en: Although a couple of arithmetic operations could employ short-circuit evaluation,
    the cost of checking for the optimization is usually more expensive than just
    completing the computation. Multiplication, for example, could use short-circuit
    evaluation to avoid multiplication by zero, as just described. However, in real
    programs, multiplication by zero occurs so infrequently that the cost of the comparison
    against zero in all the other cases generally overwhelms any savings achieved
    by avoiding multiplication by zero. For this reason, you’ll rarely see a language
    system that supports short-circuit evaluation for arithmetic operations.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.1 Using Short-Circuit Evaluation with Boolean Expressions**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One type of expression that *can* benefit from short-circuit evaluation is
    a Boolean/logical expression. Boolean expressions are good candidates for short-circuit
    evaluation for three reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Boolean expressions produce only two results, `true` and `false`; therefore,
    it’s highly likely (50/50 chance, assuming random distribution) that one of the
    short-circuit “trigger” values will appear.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boolean expressions tend to be complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boolean expressions occur frequently in programs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of these characteristics, you’ll find that many compilers use short-circuit
    evaluation when processing Boolean expressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following two C statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that if `B` is `false`, then `A` will be `false` regardless of `C`’s value.
    Similarly, if `E` is `true`, then `D` will be `true` regardless of `F`’s value.
    We can, therefore, compute the values for `A` and `D` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Now this might seem like a whole lot of extra work (it’s certainly more typing!),
    but if `C` and `F` represent complex Boolean expressions, then this code sequence
    could possibly run much faster if `B` is usually `false` and `E` is usually `true`.
    Of course, if your compiler fully supports short-circuit evaluation, you’d never
    type this code; the compiler would generate the equivalent code for you.
  prefs: []
  type: TYPE_NORMAL
- en: By the way, the converse of short-circuit evaluation is *complete Boolean evaluation*.
    In complete Boolean evaluation, the compiler emits code that always computes each
    subcomponent of a Boolean expression. Some languages (such as C, C++, C#, Swift,
    and Java) specify the use of short-circuit evaluation. A few languages (such as
    Ada) let the programmer specify whether to use short-circuit or complete Boolean
    evaluation. Most languages (such as Pascal) don’t define whether expressions will
    use short-circuit or complete Boolean evaluation—the language leaves the choice
    up to the implementer. Indeed, the same compiler could use complete Boolean evaluation
    for one instance of an expression and use short-circuit evaluation for another
    occurrence of that same expression in the same program. Unless you’re using a
    language that strictly defines the type of Boolean evaluation, you’ll have to
    check with your specific compiler’s documentation to determine how it processes
    Boolean expressions. (Remember to avoid compiler-specific mechanisms if there’s
    a chance you’ll have to compile your code with a different compiler in the future.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Look again at the expansions of the earlier Boolean expressions. It should
    be clear that the program won’t evaluate `C` and `F` if `A` is `false` and `D`
    is `true`. Therefore, the left-hand side of a conjunction (`&&`) or disjunction
    (`||`) operator can act as a gate, preventing the execution of the right-hand
    side of the expression. This is an important point and, indeed, many algorithms
    depend on this property for correct operation. Consider the following (very common)
    C statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: This example could fail if it used complete Boolean evaluation. Consider the
    case where the `ptr` variable contains `NULL`. With short-circuit evaluation the
    program will not compute the subexpression `*ptr !=` '`\0`'`;` because it realizes
    the result is always `false`. As a result, control immediately transfers to the
    first statement beyond the ending brace in this `if` statement. Consider, however,
    what would happen if this compiler utilized complete Boolean evaluation instead.
    After determining that `ptr` contains `NULL`, the program would still attempt
    to dereference `ptr`. Unfortunately, this attempt would probably produce a runtime
    error. Therefore, complete Boolean evaluation would cause this program to fail,
    even though it dutifully checks to make sure that access via pointer is legal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another semantic difference between complete and short-circuit Boolean evaluation
    has to do with side effects. In particular, if a subexpression does not execute
    because of short-circuit evaluation, then that subexpression doesn’t produce any
    side effects. This behavior is incredibly useful but inherently dangerous. It
    is useful insofar as some algorithms absolutely depend upon short-circuit evaluation.
    It is dangerous because some algorithms also expect all the side effects to occur,
    even if the expression evaluates to `false` at some point. As an example, consider
    the following bizarre (but absolutely legal) C statement, which advances a “cursor”
    pointer to the next 8-byte boundary in a string or the end of the string (whichever
    comes first):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: This statement begins by incrementing a pointer and then fetching a byte from
    memory (pointed to by `ptr`). If the byte fetched was `0`, execution of this expression/statement
    immediately stops, as the entire expression evaluates to `false` at that point.
    If the character fetched is not `0`, the process repeats up to seven more times.
    At the end of this sequence, either `ptr` points at a `0` byte or it points 8
    bytes beyond the original position. The trick here is that the expression immediately
    terminates upon reaching the end of the string rather than mindlessly skipping
    beyond that point.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are complementary examples that demonstrate desirable behavior
    when side effects occur in Boolean expressions involving complete Boolean evaluation.
    The important thing to note is that no one scheme is correct or incorrect; it
    all depends on context. In different situations, a given algorithm may require
    the use of short-circuit Boolean evaluation or complete Boolean evaluation to
    produce correct results. If the definition of the language you’re using doesn’t
    explicitly specify which scheme to use, or you want to use the other one (such
    as complete Boolean evaluation in C), then you have to write your code such that
    it forces the evaluation scheme you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.2 Forcing Short-Circuit or Complete Boolean Evaluation**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Forcing complete Boolean evaluation in a language where short-circuit evaluation
    is used (or may be used) is relatively easy. All you have to do is break the expression
    into individual statements, place the result of each subexpression into a variable,
    and then apply the conjunction and disjunction operators to these temporary variables.
    For example, consider the following conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: The Boolean expression within the `if` statement still uses short-circuit evaluation.
    However, because this code evaluates the subexpressions prior to the `if` statement,
    this code ensures that all of the side effects produced by the `f()`, `g()`, and
    `predicate()` functions will occur.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you want to go the other way. That is, what if your language supports
    only complete Boolean evaluation (or doesn’t specify the evaluation type), and
    you want to force short-circuit evaluation? This direction is a little more work
    than the converse, but it’s still not difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the following Pascal code:^([8](footnotes.xhtml#ch12fn8))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'To force short-circuit Boolean evaluation, you need to test the value of the
    first subexpression, and, only if it evaluates to `true`, evaluate the second
    subexpression (and the conjunction of the two expressions). You can do this with
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: This code simulates short-circuit evaluation by using `if` statements to block
    (or force) execution of the `g()` and `predicate()` functions based on the current
    state of the Boolean expression (kept in the `boolResult` variable).
  prefs: []
  type: TYPE_NORMAL
- en: Converting an expression to force short-circuit evaluation or complete Boolean
    evaluation looks as though it requires far more code than the original forms.
    If you’re concerned about the efficiency of this translation, relax. Internally,
    the compiler translates those Boolean expressions to three-address code that is
    similar to the translation that you did manually.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.7.3 Comparing Short-Circuit and Complete Boolean Evaluation Efficiency**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While you might have inferred from the preceding discussion that complete Boolean
    evaluation and short-circuit evaluation have equivalent efficiencies, that’s not
    the case. If you’re processing complex Boolean expressions or the cost of some
    of your subexpressions is rather high, then short-circuit evaluation is generally
    faster than complete Boolean evaluation. As to which form produces less object
    code, they’re roughly equivalent, and the exact difference will depend entirely
    upon the expression you’re evaluating.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the efficiency issues surrounding complete versus short-circuit
    Boolean evaluation, look at the following HLA code, which implements this Boolean
    expression using both forms:^([9](footnotes.xhtml#ch12fn9))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the same expression using short-circuit Boolean evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: As you can see by simply counting statements, the version using short-circuit
    evaluation is slightly shorter (11 instructions versus 12). However, the short-circuit
    version will probably run much faster because half the time the code will evaluate
    only two of the three expressions. This code evaluates all three subexpressions
    only when the first subexpression, `a < f(x)`, evaluates to `true` and the second
    expression, `b != g(y)`, evaluates to `false`. If the outcomes of these Boolean
    expressions are equally probable, this code will test all three subexpressions
    25 percent of the time. The remainder of the time it has to test only two subexpressions
    (50 percent of the time it will test `a < f(x)` and `predicate(a + b)`, 25 percent
    of the time it will test `a < f(x)` and `b != g(y)`, and the remaining 25 percent
    of the time it will need to test all three conditions).
  prefs: []
  type: TYPE_NORMAL
- en: The interesting thing to note about these two assembly language sequences is
    that complete Boolean evaluation tends to maintain the state of the expression
    (`true` or `false`) in an actual variable, whereas short-circuit evaluation maintains
    the current state of the expression by the program’s position in the code. Take
    another look at the short-circuit example. Note that it does not maintain the
    Boolean results from each of the subexpressions anywhere other than the position
    in the code. For example, if you get to the `TryOR` label in this code, you know
    that the subexpression involving conjunction (logical AND) is `false`. Likewise,
    if the program executes the call to `g(y)`, you know that the first subexpression
    in the example, `a < f(x)`, has evaluated to `true`. When you make it to the `DoStmts`
    label, you know that the entire expression has evaluated to `true`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the execution time for the `f()`, `g()`, and `predicate()` functions is
    roughly the same in the current example, you can greatly improve the code’s performance
    with a nearly trivial modification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Again, if you assume that the outcome of each subexpression is random and evenly
    distributed (that is, there is a 50/50 chance that each subexpression produces
    `true`), then this code will, on average, run about 50 percent faster than the
    previous version. Why? Moving the test for `predicate()` to the beginning of the
    code fragment means the code can now determine with one test whether it needs
    to execute the body. Because 50 percent of the time `predicate()` returns `true`,
    you can determine if you’re going to execute the loop body with a single test
    about half the time. In the earlier example, it always took at least two tests
    to determine if we were going to execute the loop body.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two assumptions here (that the Boolean expressions are equally likely to
    produce `true` or `false` and that the costs of computing each subexpression are
    equal) rarely hold in practice. However, this means that you have an even greater
    opportunity to optimize your code, not less. For example, if the cost of calling
    the `predicate()` function is high (relative to the computation of the remainder
    of the expression), then you’ll want to arrange the expression so that it calls
    `predicate()` only when it absolutely must. Conversely, if the cost of calling
    `predicate()` is low compared to the cost of computing the other subexpressions,
    then you’ll want to call it first. The situation for the `f()` and `g()` functions
    is similar. Because the logical AND operation is commutative, the following two
    expressions are semantically equivalent (in the absence of side effects):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: When the compiler uses short-circuit evaluation, the first expression executes
    faster than the second if the cost of calling function `f()` is less than the
    cost of calling function `g()`. Conversely, if calling `f()` is more expensive
    than calling `g()`, then the second expression usually executes faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another factor that affects the performance of short-circuit Boolean expression
    evaluation is the likelihood that a given Boolean expression will return the same
    value on each call. Consider the following two templates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: When working with conjunctions, try to place the expression that is more likely
    to return `true` on the right-hand side of the conjunction operator (`&&`). Remember,
    for the logical AND operation, if the first operand is `false`, a Boolean system
    employing short-circuit evaluation will not bother to evaluate the second operand.
    For performance reasons, you want to place the operand that is most likely to
    return `false` on the left-hand side of the expression. This will avoid the computation
    of the second operand more often than had you reversed the operands.
  prefs: []
  type: TYPE_NORMAL
- en: The situation is reversed for disjunction (`||`). In this case, you’d arrange
    your operands so that *expr3* is more likely to return `true` than *expr4*. By
    organizing your disjunction operations this way, you’ll skip the execution of
    the right-hand expression more often than if you had swapped the operands.
  prefs: []
  type: TYPE_NORMAL
- en: You cannot arbitrarily reorder Boolean expression operands if those expressions
    produce side effects, because the proper computation of those side effects may
    depend upon the exact order of the subexpressions. Rearranging the subexpressions
    may cause a side effect to happen that wouldn’t otherwise occur. Keep this in
    mind when you’re trying to improve performance by rearranging operands in a Boolean
    expression.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.8 The Relative Cost of Arithmetic Operations**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most algorithm analysis methodologies use a simplifying assumption that all
    operations take the same amount of time.^([10](footnotes.xhtml#ch12fn10)) This
    assumption is rarely correct, because some arithmetic operations are two orders
    of magnitude slower than other computations. For example, a simple integer addition
    operation is often much faster than an integer multiplication. Similarly, integer
    operations are usually much faster than the corresponding floating-point operations.
    For algorithm analysis purposes, it may be okay to ignore the fact that one operation
    may be *n* times faster than some other operation. For someone interested in writing
    great code, however, knowing which operators are the most efficient is important,
    especially when you have the option of choosing among them.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, we can’t create a table of operators that lists their relative
    speeds. The performance of a given arithmetic operator will vary by CPU. Even
    within the same CPU family, you see a wide variance in performance for the same
    arithmetic operation. For example, shift and rotate operations are relatively
    fast on a Pentium III (relative, say, to an addition operation). On a Pentium
    4, however, they’re considerably slower. These operations were faster on later
    Intel CPUs. So an operator such as the C/C++ `<<` or `>>` can be fast or slow,
    relative to an addition operation, depending upon which CPU it executes.
  prefs: []
  type: TYPE_NORMAL
- en: That said, I can provide some general guidelines. For example, on most CPUs
    the addition operation is one of the most efficient arithmetic and logical operations
    around; few CPUs support faster arithmetic or logical operations than addition.
    Therefore, it’s useful to group various operations into classes based on their
    performance relative to an operation like addition (see [Table 12-1](ch12.xhtml#ch12tab1)
    for an example).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 12-1:** Relative Performances of Arithmetic Operations (Guidelines)'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Relative performance** | **Operations** |'
  prefs: []
  type: TYPE_TB
- en: '| Fastest | Integer addition, subtraction, negation, logical AND, logical OR,
    logical XOR, logical NOT, and comparisons |'
  prefs: []
  type: TYPE_TB
- en: '|  | Logical shifts |'
  prefs: []
  type: TYPE_TB
- en: '|  | Logical rotates |'
  prefs: []
  type: TYPE_TB
- en: '|  | Multiplication |'
  prefs: []
  type: TYPE_TB
- en: '|  | Division |'
  prefs: []
  type: TYPE_TB
- en: '|  | Floating-point comparisons and negation |'
  prefs: []
  type: TYPE_TB
- en: '|  | Floating-point addition and subtraction |'
  prefs: []
  type: TYPE_TB
- en: '|  | Floating-point multiplication |'
  prefs: []
  type: TYPE_TB
- en: '| Slowest | Floating-point division |'
  prefs: []
  type: TYPE_TB
- en: The estimates in [Table 12-1](ch12.xhtml#ch12tab1) are not accurate for all
    CPUs, but they provide a “first approximation” from which you can work until you
    gain more experience with a particular processor. On many processors you’ll find
    anywhere between two and three orders of magnitude difference in the performances
    between the fastest and slowest operations. In particular, division tends to be
    quite slow on most processors (floating-point division is even slower). Multiplication
    is usually slower than addition, but again, the exact variance differs greatly
    between processors.
  prefs: []
  type: TYPE_NORMAL
- en: If you absolutely need to do floating-point division, there’s little you can
    do to improve your application’s performance by using a different operation (although,
    in some cases, it is faster to multiply by the reciprocal). However, note that
    you can compute many integer arithmetic calculations using different algorithms.
    For example, a left shift is often less expensive than multiplication by 2\. While
    most compilers automatically handle such “operator conversions” for you, compilers
    aren’t omniscient and can’t always figure out the best way to calculate some result.
    However, if you manually do the “operator conversion” yourself, you don’t have
    to rely on the compiler to get this right for you.
  prefs: []
  type: TYPE_NORMAL
- en: '**12.9 For More Information**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Aho, Alfred V., Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman. *Compilers:
    Principles, Techniques, and Tools*. 2nd ed. Essex, UK: Pearson Education Limited,
    1986.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Barrett, William, and John Couch. *Compiler Construction: Theory and Practice*.
    Chicago: SRA, 1986.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fraser, Christopher, and David Hansen. *A Retargetable C Compiler: Design and
    Implementation*. Boston: Addison-Wesley Professional, 1995.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Duntemann, Jeff. *Assembly Language Step-by-Step*. 3rd ed. Indianapolis: Wiley,
    2009.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyde, Randall. *The Art of Assembly Language*. 2nd ed. San Francisco: No Starch
    Press, 2010.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Louden, Kenneth C. *Compiler Construction: Principles and Practice*. Boston:
    Cengage, 1997.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parsons, Thomas W. *Introduction to Compiler Construction*. New York: W. H.
    Freeman, 1992.'
  prefs: []
  type: TYPE_NORMAL
- en: Willus.com. “Willus.com’s 2011 Win32/64 C Compiler Benchmarks.” Last updated
    April 8, 2012\. *[https://www.willus.com/ccomp_benchmark2.shtml](https://www.willus.com/ccomp_benchmark2.shtml)*.
  prefs: []
  type: TYPE_NORMAL
