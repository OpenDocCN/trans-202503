<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="167" id="Page_167"/>10</span><br/>
<span class="ChapterTitle">Concurrency (and Parallelism)</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">With this chapter I hope to provide you with all the information and tools you’ll need to take effective advantage of concurrency in your Rust programs, to implement support for concurrent use in your libraries, and to use Rust’s concurrency primitives correctly. I won’t directly teach you how to implement a concurrent data structure or write a high-performance concurrent application. Instead, my goal is to give you sufficient understanding of the underlying mechanisms that you’re equipped to wield them yourself for whatever you may need them for.</p>
<p>Concurrency comes in three flavors: single-thread concurrency (like with <code>async</code>/<code>await</code>, as we discussed in <span class="xref" itemid="xref_target_Chapter 8">Chapter 8</span>), single-core multithreaded concurrency, and multicore concurrency, which yields true parallelism. <span epub:type="pagebreak" title="168" id="Page_168"/>Each flavor allows the execution of concurrent tasks in your program to be interleaved in different ways. There are even more subflavors if you take the details of operating system scheduling and preemption into account, but we won’t get too deep into that.</p>
<p>At the type level, Rust represents only one aspect of concurrency: multithreading. Either a type is safe for use by more than one thread, or it is not. Even if your program has multiple threads (and so is concurrent) but only one core (and so is not parallel), Rust must assume that if there are multiple threads, there may be parallelism. Most of the types and techniques we’ll be talking about apply equally whether two threads actually execute in parallel or not, so to keep the language simple, I’ll be using the word <em>concurrency</em> in the informal sense of “things running more or less at the same time” throughout this chapter. When the distinction is important, I’ll call that out.</p>
<p>What’s particularly neat about Rust’s approach to type-based safe multithreading is that it is not a feature of the compiler, but rather a library feature that developers can extend to develop sophisticated concurrency contracts. Since thread safety is expressed in the type system through <code>Send</code> and <code>Sync</code> implementations and bounds, which propagate all the way out to application code, the thread safety of the entire program is checked through type checking alone.</p>
<p><em>The Rust Programming Language</em> already covers most of the basics when it comes to concurrency, including the <code>Send</code> and <code>Sync</code> traits, <code>Arc</code> and <code>Mutex</code>, and channels. I therefore won’t reiterate much of that here, except where it’s worth repeating something specifically in the context of some other topic. Instead, we’ll look at what makes concurrency difficult and some common concurrency patterns intended to deal with those difficulties. We’ll also explore how concurrency and asynchrony interact (and how they don’t) before diving into how to use atomic operations to implement lower-level concurrent operations. Finally, I’ll close out the chapter with some advice for how to retain your sanity when working with concurrent code.</p>
<h2 id="h1--0001">The Trouble with Concurrency</h2>
<p class="BodyFirst">Before we dive into good patterns for concurrent programming and the details of Rust’s concurrency mechanisms, it’s worth taking some time to understand why concurrency is challenging in the first place. That is, why do we need special patterns and mechanisms for concurrent code?</p>
<h3 id="h2--0001">Correctness</h3>
<p class="BodyFirst">The primary difficulty in concurrency is coordinating access—in particular, write access—to a resource that is shared among multiple threads. If lots of threads want to share a resource solely for the purposes of reading it, then that’s usually easy: you stick it in an <code>Arc</code> or place it in something you can get a <code>&amp;'static</code> to, and you’re all done. But once any thread wants to write, all sorts of problems arise, usually in the form of <em>data races</em>. Briefly, a data race occurs when one thread updates shared state while a second thread is also accessing that state, either to read it or to update it. Without additional <span epub:type="pagebreak" title="169" id="Page_169"/>safeguards in place, the second thread may read partially overwritten state, clobber parts of what the first thread wrote, or fail to see the first thread’s write at all! In general, all data races are considered undefined behavior.</p>
<p>Data races are a part of a broader class of problems that primarily, though not exclusively, occur in a concurrent setting: <em>race conditions</em>. A race condition occurs whenever multiple outcomes are possible from a sequence of instructions, depending on the relative timing of other events in the system. These events can be threads executing a particular piece of code, a timer going off, a network packet coming in, or any other time-variable occurrence. Race conditions, unlike data races, are not inherently bad, and are not considered undefined behavior. However, they are a breeding ground for bugs when particularly peculiar races occur, as you’ll see throughout this chapter.</p>
<h3 id="h2--0002">Performance</h3>
<p class="BodyFirst">Often, developers introduce concurrency into their programs in the hope of increasing performance. Or, to be more precise, they hope that concurrency will enable them to perform more operations per second in aggregate by taking advantage of more hardware resources. This can be done on a single core by having one thread run while another is waiting, or across multiple cores by having threads do work simultaneously, one on each core, that would otherwise happen serially on one core. Most developers are referring to the latter kind of performance gain when they talk about concurrency, which is often framed in terms of scalability. Scalability in this context means “the performance of this program scales with the number of cores,” implying that if you give your program more cores, its performance improves.</p>
<p>While achieving such a speedup is possible, it’s harder than it seems. The ultimate goal in scalability is linear scalability, where doubling the number of cores doubles the amount of work your program completes per unit of time. Linear scalability is also often called perfect scalability. However, in reality, few concurrent programs achieve such speedups. Sublinear scaling is more common, where the throughput increases nearly linearly as you go from one core to two, but adding more cores yields diminishing returns. Some programs even experience negative scaling, where giving the program access to more cores <em>reduces</em> throughput, usually because the many threads are all contending for some shared resource.</p>
<p>It might help to think of a group of people trying to pop all the bubbles on a piece of bubble wrap—adding more people helps initially, but at some point you get diminishing returns as the crowding makes any one person’s job harder. If the humans involved are particularly ineffective, your group may end up standing around discussing who should pop next and pop no bubbles at all! This kind of interference among tasks that are supposed to execute in parallel is called <em>contention</em> and is the archnemesis of scaling well. Contention can arise in a number of ways, but the primary offenders are mutual exclusion, shared resource exhaustion, and false sharing.</p>
<h4 id="h3--0001"><span epub:type="pagebreak" title="170" id="Page_170"/>Mutual Exclusion</h4>
<p class="BodyFirst">When only a single concurrent task is allowed to execute a particular piece of code at any one time, we say that execution of that segment of code is mutually exclusive—if one thread executes it, no other thread can do so at the same time. The archetypal example of this is a mutual exclusion lock, or <em>mutex</em>, which explicitly enforces that only one thread gets to enter a particular critical section of your program code at any one time. Mutual exclusion can also happen implicitly, however. For example, if you spin up a thread to manage a shared resource and send jobs to it over an <code>mpsc</code> channel, that thread effectively implements mutual exclusion, since only one such job gets to execute at a time.</p>
<p>Mutual exclusion can also occur when invoking operating system or library calls that internally enforce single-threaded access to a critical section. For example, for many years, the standard memory allocator required mutual exclusion for some allocations, which made memory allocation an operation that incurred significant contention in otherwise highly parallel programs. Similarly, many operating system operations that may seem like they should be independent, such as creating two files with different names in the same directory, may end up having to happen sequentially inside the kernel.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	Scalable concurrent allocations is the raison d’être for the <code>jemalloc</code> memory allocator!</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Mutual exclusion is the most obvious barrier to parallel speedup since, by definition, it forces serial execution of some portion of your program. Even if you make the remainder of your program scale with the number of cores perfectly, the total speedup you can achieve is limited by the length of the mutually exclusive, serial section. Be mindful of your mutually exclusive sections, and seek to restrict them to only where strictly necessary.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	For the theoretically minded, the limits on the achievable speedup as a result of mutually exclusive sections of code can be computed using Amdahl’s law.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h4 id="h3--0002">Shared Resource Exhaustion</h4>
<p class="BodyFirst">Unfortunately, even if you achieve perfect concurrency within your tasks, the environment those tasks need to interact with may itself not be perfectly scalable. The kernel can handle only so many sends on a given TCP socket per second, the memory bus can do only so many reads at once, and your GPU has a limited capacity for concurrency. There’s no cure for this. The environment is usually where perfect scalability falls apart in practice, and fixes for such cases tend to require substantial re-engineering (or even new hardware!), so we won’t talk much more about this topic in this chapter. Just remember that scalability is rarely something you can “achieve,” and more something you just strive for.</p>
<h4 id="h3--0003"><span epub:type="pagebreak" title="171" id="Page_171"/>False Sharing</h4>
<p class="BodyFirst">False sharing occurs when two operations that shouldn’t contend with one another contend anyway, preventing efficient simultaneous execution. This usually happens because the two operations happen to intersect on some shared resource even though they use unrelated parts of that resource.</p>
<p>The simplest example of this is lock oversharing, where a lock guards some composite state, and two operations that are otherwise independent both need to take the lock to update their particular parts of the state. This in turn means the operations must execute serially instead of in parallel. In some cases it’s possible to split the single lock into two, one for each of the disjoint parts, which enables the operations to proceed in parallel. However, it’s not always straightforward to split a lock like this—the state may share a single lock because some third operation needs to lock over all the parts of the state. Usually you can still split the lock, but you have to be careful about the order in which different threads take the split locks to avoid deadlocks that can occur when two operations attempt to take them in different orders (look up the “dining philosophers problem,” if you’re curious). Alternatively, for some problems, you may be able to avoid the critical section entirely by using a lock-free version of the underlying algorithm, though those are also tricky to get right. Ultimately, false sharing is a hard problem to solve, and there isn’t a single catchall solution—but identifying the problem is a good start.</p>
<p>A more subtle example of false sharing occurs on the CPU level, as we discussed briefly in <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span>. The CPU internally operates on memory in terms of cache lines—longer sequences of consecutive bytes in memory—rather than individual bytes, to amortize the cost of memory accesses. For example, on most Intel processors, the cache line size is 64 bytes. This means that every memory operation really ends up reading or writing some multiple of 64 bytes. The false sharing comes into play when two cores want to update the value of two different bytes that happen to fall on the same cache line; those updates must execute sequentially even though the updates are logically disjoint.</p>
<p>This might seem too low-level to matter, but in practice this kind of false sharing can decimate the parallel speedup of an application. Imagine that you allocate an array of integer values to indicate how many operations each thread has completed, but the integers all fall within the same cache line—now, all your otherwise parallel threads will contend on that one cache line for every operation they perform. If the operations are relatively quick, <em>most</em> of your execution time may end up being spent contending on those counters!</p>
<p>The trick to avoiding false cache line sharing is to pad your values so that they are the size of a cache line. That way, two adjacent values always fall on different cache lines. But of course, this also inflates the size of your data structures, so use this approach only when benchmarks indicate a problem.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2><span epub:type="pagebreak" title="172" id="Page_172"/>The Cost of Scalability</h2>
<p class="BoxBodyFirst">A somewhat orthogonal aspect of concurrency that you should be mindful of is the cost of introducing concurrency in the first place. Compilers are really good at optimizing single-threaded code—they’ve been doing it for a long time, after all—and single-threaded code tends to get away with fewer expensive safeguards (like locks, channels, or atomic instructions) than concurrent code can. In aggregate, the various costs of concurrency can make a parallel program slower than its single-threaded counterpart, given <em>any number</em> of cores! This is why it’s important to measure both before and after you optimize and parallelize: the results may surprise you.</p>
<p>If you’re curious about this topic, I highly recommend you read Frank McSherry’s 2015 paper “Scalability! But at what COST?” (<a href="https://www.frankmcsherry.org/assets/COST.pdf" class="LinkURL">https://www.frankmcsherry.org/assets/COST.pdf</a>), which uncovers some particularly egregious examples of “costly scaling.”</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1--0002">Concurrency Models</h2>
<p class="BodyFirst">Rust has three patterns for adding concurrency to your programs that you’ll come across fairly often: shared memory concurrency, worker pools, and actors. Going through every way you could add concurrency in detail would take a book of its own, so here I’ll focus on just these three patterns.</p>
<h3 id="h2--0003">Shared Memory</h3>
<p class="BodyFirst">Shared memory concurrency is, conceptually, very straightforward: the threads cooperate by operating on regions of memory shared between them. This might take the form of state guarded by a mutex or stored in a hash map with support for concurrent access from many threads. The many threads may be doing the same task on disjoint pieces of data, such as if many threads perform some function over disjoint subranges of a <code>Vec</code>, or they may be performing different tasks that require some shared state, such as in a database where one thread handles user queries to a table while another optimizes the data structures used to store that table in the background.</p>
<p>When you use shared memory concurrency, your choice of data structures is significant, especially if the threads involved need to cooperate very closely. A regular mutex might prevent scaling beyond a very small number of cores, a reader/writer lock might allow many more concurrent reads at the cost of slower writes, and a sharded reader/writer lock might allow perfectly scalable reads at the cost of making writes highly disruptive. Similarly, some concurrent hash maps aim for good all-round performance while others specifically target, say, concurrent reads where writes are rare. In general, in shared memory concurrency, you want to use data structures <span epub:type="pagebreak" title="173" id="Page_173"/>that are specifically designed for something as close to your target use case as possible, so that you can take advantage of optimizations that trade off performance aspects your application does not care about for those it does.</p>
<p>Shared memory concurrency is a good fit for use cases where threads need to jointly update some shared state in a way that does not commute. That is, if one thread has to update the state <code>s</code> with some function <code>f</code>, and another has to update the state with some function <code>g</code>, and <code>f(g(s)) != g(f(s))</code>, then shared memory concurrency is likely necessary. If that is not the case, the other two patterns are likely better fits, as they tend to lead to simpler and more performant designs.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	Some problems have known algorithms that can provide concurrent shared memory operations without the use of locks. As the number of cores grows, these <em>lock-free</em> algorithms may scale better than lock-based algorithms, though they also often have slower per-core performance due to their complexity. As always with performance matters, benchmark first, then look for alternative solutions.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2--0004">Worker Pools</h3>
<p class="BodyFirst">In the worker pool model, many identical threads receive jobs from a shared job queue, which they then execute entirely independently. Web servers, for example, often have a worker pool handling incoming connections, and multithreaded runtimes for asynchronous code tend to use a worker pool to collectively execute all of an application’s futures (or, more accurately, its top-level tasks).</p>
<p>The lines between shared memory concurrency and worker pools are often blurry, as worker pools tend to use shared memory concurrency to coordinate how they take jobs from the queue and how they return incomplete jobs back to the queue. For example, say you’re using the data parallelism library <code>rayon</code> to perform some function over every element of a vector in parallel. Behind the scenes <code>rayon</code> spins up a worker pool, splits the vector into subranges, and then hands out subranges to the threads in the pool. When a thread in the pool finishes a range, <code>rayon</code> arranges for it to start working on the next unprocessed subrange. The vector is shared among all the worker threads, and the threads coordinate through a shared memory queue–like data structure that supports work stealing.</p>
<p>Work stealing is a key feature of most worker pools. The basic premise is that if one thread finishes its work early, and there’s no more unassigned work available, that thread can steal jobs that have already been assigned to a different worker thread but haven’t been started yet. Not all jobs take the same amount of time to complete, so even if every worker is given the same <em>number</em> of jobs, some workers may end up finishing their jobs more quickly than others. Rather than sit around and wait for the threads that drew longer-running jobs to complete, those threads that finish early should help the stragglers so the overall operation is completed sooner.</p>
<p>It’s quite a task to implement a data structure that supports this kind of work stealing without incurring significant overhead from threads constantly trying to steal work from one another, but this feature is vital to a <span epub:type="pagebreak" title="174" id="Page_174"/>high-performance worker pool. If you find yourself in need of a worker pool, your best bet is usually to use one that has already seen a lot of work go into it, or at least reuse data structures from an existing one, rather than to write one yourself from scratch.</p>
<p>Worker pools are a good fit when the work that each thread performs is the same, but the data it performs it <em>on</em> varies. In a <code>rayon</code> parallel map operation, every thread performs the same map computation; they just perform it on different subsets of the underlying data. In a multithreaded asynchronous runtime, each thread simply calls <code>Future::poll</code>; they just call it on different futures. If you start having to distinguish between the threads in your thread pool, a different design is probably more appropriate.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>Connection Pools</h2>
<p class="BoxBodyFirst">A connection pool is a shared memory construct that keeps a set of established connections and hands them out to threads that need a connection. It’s a common design pattern in libraries that manage connections to external services. If a thread needs a connection but one isn’t available, either a new connection is established or the thread is forced to block. When a thread is done with a connection, it returns that connection to the pool, and thus makes it available to other threads that may be waiting.</p>
<p>Usually, the hardest task for a connection pool is managing connection life cycles. A connection can be returned to the pool in whatever state it was put in by the last thread that used it. The connection pool therefore has to make sure any state associated with the connection, whether on the client or on the server, has been reset so that when the connection is subsequently used by another thread, that thread can act as though it was given a fresh, dedicated connection.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2--0005">Actors</h3>
<p class="BodyFirst">The actor concurrency model is, in many ways, the opposite of the worker pool model. Whereas the worker pool has many identical threads that share a job queue, the actor model has many separate job queues, one for each job “topic.” Each job queue feeds into a particular actor, which handles all jobs that pertain to a subset of the application’s state. That state might be a database connection, a file, a metrics collection data structure, or any other structure that you can imagine many threads may need to be able to access. Whatever it is, a single actor owns that state, and if some task wants to interact with that state, it needs to send a message to the owning actor summarizing the operation it wishes to perform. When the owning actor receives that message, it performs the indicated action and responds to the inquiring task with the result of the operation, if relevant. <span epub:type="pagebreak" title="175" id="Page_175"/>Since the actor has exclusive access to its inner resource, no locks or other synchronization mechanisms are required beyond what’s needed for the messaging.</p>
<p>A key point in the actor pattern is that actors all talk to one another. If, say, an actor that is responsible for logging needs to write to a file and a database table, it might send off messages to the actors responsible for each of those, asking them to perform the respective actions, and then proceed to the next log event. In this way, the actor model more closely resembles a web than spokes on a wheel—a user request to a web server might start as a single request to the actor responsible for that connection but might transitively spawn tens, hundreds, or even thousands of messages to actors deeper in the system before the user’s request is satisfied.</p>
<p>Nothing in the actor model requires that each actor is its own thread. To the contrary, most actor systems suggest that there should be a large number of actors, and so each actor should map to a task rather than a thread. After all, actors require exclusive access to their wrapped resources only when they execute, and do not care whether they are on a thread of their own or not. In fact, very frequently, the actor model is used in conjunction with the worker pool model—for example, an application that uses the multithreaded asynchronous runtime Tokio can spawn an asynchronous task for each actor, and Tokio will then make the execution of each actor a job in its worker pool. Thus, the execution of a given actor may move from thread to thread in the worker pool as the actor yields and resumes, but every time the actor executes it maintains exclusive access to its wrapped resource.</p>
<p>The actor concurrency model is well suited for when you have many resources that can operate relatively independently, and where there is little or no opportunity for concurrency within each resource. For example, an operating system might have an actor responsible for each hardware device, and a web server might have an actor for each backend database connection. The actor model does not work so well if you need only a few actors, if work is skewed significantly among the actors, or if some actors grow large—in all of those cases, your application may end up being bottlenecked on the execution speed of a single actor in the system. And since actors each expect to have exclusive access to their little slice of the world, you can’t easily parallelize the execution of that one bottleneck actor.</p>
<h2 id="h1--0003">Asynchrony and Parallelism</h2>
<p class="BodyFirst">As we discussed in <span class="xref" itemid="xref_target_Chapter 8">Chapter 8</span>, asynchrony in Rust enables concurrency without parallelism—we can use constructs like selects and joins to have a single thread poll multiple futures and continue when one, some, or all of them complete. Because there is no parallelism involved, concurrency with futures does not fundamentally require those futures to be <code>Send</code>. Even spawning a future to run as an additional top-level task does not fundamentally require <code>Send</code>, since a single executor thread can manage the polling of many futures at once.</p>
<p><span epub:type="pagebreak" title="176" id="Page_176"/>However, in <em>most</em> cases, applications want both concurrency and parallelism. For example, if a web application constructs a future for each incoming connection and so has many active connections at once, it probably wants the asynchronous executor to be able to take advantage of more than one core on the host computer. That won’t happen naturally: your code has to explicitly tell the executor which futures can run in parallel and which cannot.</p>
<p>In particular, two pieces of information must be given to the executor to let it know that it can spread the work in the futures across a worker pool of threads. The first is that the futures in question are <code>Send</code>—if they aren’t, the executor is not allowed to send the futures to other threads for processing, and no parallelism is possible; only the thread that constructed such futures can poll them.</p>
<p>The second piece of information is how to split the futures into tasks that can operate independently. This ties back to the discussion of tasks versus futures from <span class="xref" itemid="xref_target_Chapter 8">Chapter 8</span>: if one giant <code>Future</code> contains a number of <code>Future</code> instances that themselves correspond to tasks that can run in parallel, the executor must still call <code>poll</code> on the top-level <code>Future</code>, and it must do so from a single thread, since <code>poll</code> requires <code>&amp;mut self</code>. Thus, to achieve parallelism with futures, you have to explicitly spawn the futures you want to be able to run in parallel. Also, because of the first requirement, the executor function you use to do so will require that the passed-in <code>Future</code> is <code>Send</code>.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>Asynchronous Synchronization Primitives</h2>
<p class="BoxBodyFirst">Most of the synchronization primitives that exist for blocking code (think <code>std::sync</code>) also have asynchronous counterparts. There are asynchronous variants of channels, mutexes, reader/writer locks, barriers, and all sorts of other similar constructs. We need these because, as discussed in <span class="xref" itemid="xref_target_Chapter 8">Chapter 8</span>, blocking inside a future will hold up other work the executor may need to do, and so is inadvisable.</p>
<p>However, the asynchronous versions of these primitives are often slower than their synchronous counterparts because of the additional machinery needed to perform the necessary wake-ups. For that reason, you may want to use synchronous synchronization primitives even in asynchronous contexts whenever the use does not risk blocking the executor. For example, while it’s generally true that acquiring a <code>Mutex</code> might block for a long time, that might not be true for a particular <code>Mutex</code> that, perhaps, is acquired only rarely, and only ever for short periods of time. In that case, blocking for the short time until the <code>Mutex</code> becomes available again might not actually cause any problems. You will want to make sure that you never yield or perform other long-running operations while holding the <code>MutexGuard</code>, but barring that you shouldn’t run into problems.</p>
<p>As always with such optimizations, though, make sure you measure first, and choose only the synchronous primitive if it nets you significant performance improvements. If it does not, the additional footguns introduced by using a synchronous primitive in an asynchronous context are probably not worth it.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1--0004"><span epub:type="pagebreak" title="177" id="Page_177"/>Lower-Level Concurrency</h2>
<p class="BodyFirst">The standard library provides the <code>std::sync::atomic</code> module, which provides access to the underlying CPU primitives, higher-level constructs like channels and mutexes are built with. These primitives come in the form of atomic types with names starting with <code>Atomic</code>—<code>AtomicUsize</code>, <code>AtomicI32</code>, <code>AtomicBool</code>, <code>AtomicPtr</code>, and so on—the <code>Ordering</code> type, and two functions called <code>fence</code> and <code>compiler_fence</code>. We’ll look at each of these over the next few sections.</p>
<p>These types are the blocks used to build any code that has to communicate between threads. Mutexes, channels, barriers, concurrent hash tables, lock-free stacks, and all other synchronization constructs ultimately rely on these few primitives to do their jobs. They also come in handy on their own for lightweight cooperation between threads where heavyweight synchronization like a mutex is excessive—for example, to increment a shared counter or set a shared Boolean to <code>true</code>.</p>
<p>The atomic types are special in that they have defined semantics for what happens when multiple threads try to access them concurrently. These types all support (mostly) the same API: <code>load</code>, <code>store</code>, <code>fetch_*</code>, and <code>compare_exchange</code>. In the rest of this section, we’ll look at what those do, how to use them correctly, and what they’re useful for. But first, we have to talk about low-level memory operations and memory ordering.</p>
<h3 id="h2--0006">Memory Operations</h3>
<p class="BodyFirst">Informally, we often refer to accessing variables as “reading from” or “writing to” memory. In reality, a lot of machinery between code uses a variable and the actual CPU instructions that access your memory hardware. It’s important to understand that machinery, at least at a high level, in order to understand how concurrent memory accesses behave.</p>
<p>The compiler decides what instructions to emit when your program reads the value of a variable or assigns a new value to it. It is permitted to perform all sorts of transformations and optimizations on your code and may end up reordering your program statements, eliminating operations it deems redundant, or using CPU registers rather than actual memory to store intermediate computations. The compiler is subject to a number of restrictions on these transformations, but ultimately only a subset of your variable accesses actually end up as memory access instructions.</p>
<p>At the CPU level, memory instructions come in two main shapes: loads and stores. A load pulls bytes from a location in memory into a CPU register, and a store stores bytes from a CPU register into a location in memory. Loads and stores operate on small chunks of memory at a time: usually 8 bytes or less on modern CPUs. If a variable access spans more bytes than can be accessed with a single load or store, the compiler automatically turns it into multiple load or store instructions, as appropriate. The CPU also has some leeway in how it executes a program’s instructions to make better use of the hardware and improve program performance. For example, modern CPUs often execute instructions in parallel, or even out of order, when they don’t have dependencies on each other. There are also several layers of caches <span epub:type="pagebreak" title="178" id="Page_178"/>between each CPU and your computer’s DRAM, which means that a load of a given memory location may not necessarily see the latest store to that memory location, going by wall-clock time.</p>
<p>In most code, the compiler and CPU are permitted to transform the code only in ways that don’t affect the semantics of the resulting program, so these transformations are invisible to the programmer. However, in the context of parallel execution, these transformations can have a significant impact on application behavior. Therefore, CPUs typically provide multiple different variations of the load and store instructions, each with different guarantees about how the CPU may reorder them and how they may be interleaved with parallel operations on other CPUs. Similarly, compilers (or rather, the language the compiler compiles) provide different annotations you can use to force particular execution constraints for some subset of their memory accesses. In Rust, those annotations come in the form of the atomic types and their methods, which we’ll spend the rest of this section picking apart.</p>
<h3 id="h2--0007">Atomic Types</h3>
<p class="BodyFirst">Rust’s atomic types are so called because they can be accessed atomically—that is, the value of an atomic-type variable is written all at once and will never be written using multiple stores, guaranteeing that a load of that variable cannot observe that only some of the bytes composing the value have changed while others have not (yet). This is easiest understood by way of contrast with non-atomic types. For example, reassigning a new value to a tuple of type <code>(i64, i64)</code> typically requires two CPU store instructions, one for each 8-byte value. If one thread were to perform both of those stores, another thread could (if we ignore the borrow checker for a second) read the tuple’s value after the first store but before the second, and thus end up with an inconsistent view of the tuple’s value. It would end up reading the new value for the first element and the old value for the second element, a value that was never actually stored by any thread.</p>
<p>The CPU can atomically access values only of certain sizes, so there are only a few atomic types, all of which live in the <code>atomic</code> module. Each atomic type is of one of the sizes the CPU supports atomic access to, with multiple variations for things like whether the value is signed and to differentiate between an atomic <code>usize</code> and a pointer (which is of the same size as <code>usize</code>). Furthermore, the atomic types have explicit methods for loading and storing the values they hold, and a handful of more complex methods we’ll get back to later, so that the mapping between the code the programmer writes and the resulting CPU instructions is clearer. For example, <code>AtomicI32::load</code> performs a single load of a signed 32-bit value, and <code>AtomicPtr::store</code> performs a single store of a pointer-sized (64 bits on a 64-bit platform) value.</p>
<h3 id="h2--0008">Memory Ordering</h3>
<p class="BodyFirst">Most of the methods on the atomic types take an argument of type <code>Ordering</code>, which dictates the memory ordering restrictions the atomic operation is subject to. Across different threads, loads and stores of an atomic value <span epub:type="pagebreak" title="179" id="Page_179"/>may be sequenced by the compiler and CPU only in interleavings that are compatible with the requested memory ordering of each of the atomic operations on that atomic value. Over the next few sections, we’ll see some examples of why control over the ordering is important and necessary to get the expected semantics out of the compiler and CPU.</p>
<p>Memory ordering often comes across as counterintuitive, because we humans like to read programs from top to bottom and imagine that they execute line by line—but that’s not how the code actually executes when it hits the hardware. Memory accesses can be reordered, or even entirely elided, and writes on one thread may not immediately be visible to other threads, even if later writes in program order have already been observed.</p>
<p>Think of it like this: each memory location sees a sequence of modifications coming from different threads, and the sequences of modifications for different memory locations are independent. If two threads T1 and T2 both write to memory location M, then even if T1 executed first as measured by a user with a stopwatch, T2’s write to M may still appear to have happened first for M absent any other constraints between the two threads’ execution. Essentially, <em>the computer does not take wall-clock time into account</em> when it determines the value of a given memory location—all that matter are the execution constraints the programmer puts on what constitutes a valid execution. For example, if T1 writes to M and then spawns thread T2, which then writes to M, the computer must recognize T1’s write as having happened first because T2’s existence depends on T1.</p>
<p>If that’s hard to follow, don’t fret—memory ordering can be mind-bending, and language specifications tend to use very precise but not very intuitive wording to describe it. We can construct a mental model that’s easier to grasp, if a little simplified, by instead focusing on the underlying hardware architecture. Very basically, your computer memory is structured as a treelike hierarchy of storage where the leaves are CPU registers and the roots are the storage on your physical memory chips, often called main memory. Between the two are several layers of caches, and different layers of the hierarchy can reside on different pieces of hardware. When a thread performs a store to a memory location, what really happens is that the CPU starts a write request for the value in a given CPU register that then has to make its way up the memory hierarchy toward main memory. When a thread performs a load, the request flows up the hierarchy until it hits a layer that has the value available, and returns from there. Herein lies the problem: writes aren’t visible everywhere until all caches of the written memory location have been updated, but other CPUs can execute instructions against the same memory location at the same time, and weirdness ensues. Memory ordering, then, is a way to request precise semantics for what happens when multiple CPUs access a particular memory location for a particular operation.</p>
<p>With this in mind, let’s take a look at the <code>Ordering</code> type, which is the primary mechanism by which we, as programmers, can dictate additional constraints on what concurrent executions are valid.</p>
<p><code>Ordering</code> is defined as an <code>enum</code> with the variants shown in <a href="#listing10-1" id="listinganchor10-1">Listing 10-1</a>.</p>
<pre><code><span epub:type="pagebreak" title="180" id="Page_180"/>enum Ordering {
    Relaxed,
    Release,
    Acquire,
    AcqRel,
    SeqCst
}</code></pre>
<p class="CodeListingCaption"><a id="listing10-1">Listing 10-1</a>: The definition of <code>Ordering</code></p>
<p>Each of these places different restrictions on the mapping from source code to execution semantics, and we’ll explore each one in turn in the remainder of this section.</p>
<h4 id="h3--0004">Relaxed Ordering</h4>
<p class="BodyFirst">Relaxed ordering essentially guarantees nothing about concurrent access to the value beyond the fact that the access is atomic. In particular, relaxed ordering gives no guarantees about the relative ordering of memory accesses across different threads. This is the weakest form of memory ordering. <a href="#listing10-2" id="listinganchor10-2">Listing 10-2</a> shows a simple program in which two threads access two atomic variables using <code>Ordering::Relaxed</code>.</p>
<pre><code>static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);

let t1 = spawn(|| {
  <span class="CodeAnnotationCode" aria-label="annotation1">1</span> let r1 = Y.load(Ordering::Relaxed);
  <span class="CodeAnnotationCode" aria-label="annotation2">2</span> X.store(r1, Ordering::Relaxed);
});
let t2 = spawn(|| {
  <span class="CodeAnnotationCode" aria-label="annotation3">3</span> let r2 = X.load(Ordering::Relaxed);
  <span class="CodeAnnotationCode" aria-label="annotation4">4</span> Y.store(true, Ordering::Relaxed)
});</code></pre>
<p class="CodeListingCaption"><a id="listing10-2">Listing 10-2</a>: Two racing threads with <code>Ordering::Relaxed</code></p>
<p>Looking at the thread spawned as <code>t2</code>, you might expect that <code>r2</code> can never be <code>true</code>, since all values are <code>false</code> until the same thread assigns <code>true</code> to <code>Y</code> on the line <em>after</em> reading <code>X</code>. However, with a relaxed memory ordering, that outcome is completely possible. The reason is that the CPU is allowed to reorder the loads and stores involved. Let’s walk through exactly what happens here to make <code>r2 = true</code> possible.</p>
<p>First, the CPU notices that <span class="CodeAnnotation" aria-label="annotation4">4</span> doesn’t have to happen after <span class="CodeAnnotation" aria-label="annotation3">3</span>, since <span class="CodeAnnotation" aria-label="annotation4">4</span> doesn’t use any output or side effect of <span class="CodeAnnotation" aria-label="annotation3">3</span>. That is, <span class="CodeAnnotation" aria-label="annotation4">4</span> has no execution dependency on <span class="CodeAnnotation" aria-label="annotation3">3</span>. So, the CPU decides to reorder them for *waves hands* reasons that’ll make your program go faster. The CPU thus goes ahead and executes <span class="CodeAnnotation" aria-label="annotation4">4</span> first, setting <code>Y = true</code>, even though <span class="CodeAnnotation" aria-label="annotation3">3</span> hasn’t run yet. Then, <code>t2</code> is put to sleep by the operating system and thread <code>t1</code> executes a few instructions, or <code>t1</code> simply executes on another core. In <code>t1</code>, the compiler must indeed run <span class="CodeAnnotation" aria-label="annotation1">1</span> first and then <span class="CodeAnnotation" aria-label="annotation2">2</span>, since <span class="CodeAnnotation" aria-label="annotation2">2</span> depends on the value read in <span class="CodeAnnotation" aria-label="annotation1">1</span>. Therefore, <code>t1</code> reads <code>true</code> from <code/><span epub:type="pagebreak" title="181" id="Page_181"/>Y (written by <span class="CodeAnnotation" aria-label="annotation4">4</span>) into <code>r1</code> and then writes that back to <code>X</code>. Finally, <code>t2</code> executes <span class="CodeAnnotation" aria-label="annotation3">3</span>, which reads <code>X</code> and gets <code>true</code>, as was written by <span class="CodeAnnotation" aria-label="annotation2">2</span>.</p>
<p>The relaxed memory ordering allows this execution because it imposes no additional constraints on concurrent execution. That is, under relaxed memory ordering, the compiler must ensure only that execution dependencies on any given thread are respected (just as if atomics weren’t involved); it need not make any promises about the interleaving of concurrent operations. Reordering <span class="CodeAnnotation" aria-label="annotation3">3</span> and <span class="CodeAnnotation" aria-label="annotation4">4</span> is permitted for a single-threaded execution, so it is permitted under relaxed ordering as well.</p>
<p>In some cases, this kind of reordering is fine. For example, if you have a counter that just keeps track of metrics, it doesn’t really matter when exactly it executes relative to other instructions, and <code>Ordering::Relaxed</code> is fine. In other cases, this could be disastrous: say, if your program uses <code>r2</code> to figure out if security protections have already been set up, and thus ends up erroneously believing that they already have been.</p>
<p>You don’t generally notice this reordering when writing code that doesn’t make fancy use of atomics—the CPU has to promise that there is no observable difference between the code as written and what each thread actually executes, so everything seems like it runs in order just as you wrote it. This is referred to as respecting program order or evaluation order; the terms are synonyms.</p>
<h4 id="h3--0005">Acquire/Release Ordering</h4>
<p class="BodyFirst">At the next step up in the memory ordering hierarchy, we have <code>Ordering::Acquire</code>, <code>Ordering::Release</code>, and <code>Ordering::AcqRel</code> (acquire plus release). At a high level, these establish an execution dependency between a store in one thread and a load in another and then restrict how operations can be reordered with respect to that load and store. Crucially, these dependencies not only establish a relationship between a store and a load of a single value, but also put ordering constraints on <em>other</em> loads and stores in the threads involved. This is because every execution must respect the program order; if a load in thread B has a dependency on some store in thread A (the store in A must execute before the load in B), then any read or write in B after that load must also happen after that store in A.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	The <code>Acquire</code> memory ordering can be applied only to loads, <code>Release</code> only to stores, and <code>AcqRel</code> only to operations that both load <em>and</em> store (like <code>fetch_add</code>).</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Concretely, these memory orderings place the following restrictions on execution:</p>
<ol class="decimal">
<li value="1">Loads and stores cannot be moved forward past a store with <code>Ordering::Release</code>.</li>
<li value="2">Loads and stores cannot be moved back before a load with <code>Ordering::Acquire</code>.</li>
<li value="3">An <code>Ordering::Acquire</code> load of a variable must see all stores that happened before an <code>Ordering::Release</code> store that stored what the load loaded.</li>
</ol>
<p><span epub:type="pagebreak" title="182" id="Page_182"/>To see how these memory orderings change things, <a href="#listing10-3" id="listinganchor10-3">Listing 10-3</a> shows <a href="#listing10-2">Listing 10-2</a> again but with the memory ordering swapped out for <code>Acquire</code> and <code>Release</code>.</p>
<pre><code>static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);

let t1 = spawn(|| {
    let r1 = Y.load(Ordering::Acquire);
    X.store(r1, Ordering::Release);
});
let t2 = spawn(|| {
  <span class="CodeAnnotationCode" aria-label="annotation1">1</span> let r2 = X.load(Ordering::Acquire);
  <span class="CodeAnnotationCode" aria-label="annotation2">2</span> Y.store(true, Ordering::Release)
});</code></pre>
<p class="CodeListingCaption"><a id="listing10-3">Listing 10-3</a>: <a href="#listing10-2">Listing 10-2</a> with <code>Acquire/Release</code><code/> memory ordering</p>
<p>These additional restrictions mean that it is no longer possible for <code>t2</code> to see <code>r2 = true</code>. To see why, consider the primary cause of the weird outcome in <a href="#listing10-2">Listing 10-2</a>: the reordering of <span class="CodeAnnotation" aria-label="annotation1">1</span> and <span class="CodeAnnotation" aria-label="annotation2">2</span>. The very first restriction, on stores with <code>Ordering::Release</code>, dictates that we cannot move <span class="CodeAnnotation" aria-label="annotation1">1</span> below <span class="CodeAnnotation" aria-label="annotation2">2</span>, so we’re all good!</p>
<p>But these rules are useful beyond this simple example. For example, imagine that you implement a mutual exclusion lock. You want to make sure that any loads and stores a thread runs while it holds the lock are executed only while it’s actually holding the lock, and visible to any thread that takes the lock later. This is exactly what <code>Release</code> and <code>Acquire</code> enable you to do. By performing a <code>Release</code> store to release the lock and an <code>Acquire</code> load to acquire the lock, you can guarantee that the loads and stores in the critical section are never moved to before the lock was actually acquired or to after the lock was released!</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	On some CPU architectures, like x86, <code>Acquire/Release</code><code/> ordering is guaranteed by the hardware, and there is no additional cost to using <code>Ordering::Release</code> and <code>Ordering::Acquire</code> over <code>Ordering::Relaxed</code>. On other architectures that is not the case, and your program may see speedups if you switch to <code>Relaxed</code> for atomic operations that can tolerate the weaker memory ordering guarantees.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h4 id="h3--0006">Sequentially Consistent Ordering</h4>
<p class="BodyFirst">Sequentially consistent ordering (<code>Ordering::SeqCst</code>) is the strongest memory ordering we have access to. Its exact guarantees are somewhat hard to nail down, but very broadly, it requires not only that each thread sees results consistent with <code>Acquire/Release</code><code/>, but also that all threads see the <em>same</em> ordering as one another. This is best seen by way of contrast with the behavior of <code>Acquire</code> and <code>Release</code>. Specifically,  <code>Acquire/Release</code><code/> ordering does <em>not</em> guarantee that if two threads A and B atomically load values written by two other threads X and Y, A and B will see a consistent pattern of when X wrote relative to Y. That’s fairly abstract, so consider the example in <a href="#listing10-4" id="listinganchor10-4">Listing 10-4</a>, <span epub:type="pagebreak" title="183" id="Page_183"/>which shows a case where <code>Acquire/Release</code><code/> ordering can produce unexpected results. Afterwards, we’ll see how sequentially consistent ordering avoids that particular unexpected outcome.</p>
<pre><code>static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
static Z: AtomicI32 = AtomicI32::new(0);

let t1 = spawn(|| {
    X.store(true, Ordering::Release);
});
let t2 = spawn(|| {
    Y.store(true, Ordering::Release);
});
let t3 = spawn(|| {
    while (!X.load(Ordering::Acquire)) {}
  <span class="CodeAnnotationCode" aria-label="annotation1">1</span> if (Y.load(Ordering::Acquire)) {
        Z.fetch_add(1, Ordering::Relaxed); }
});
let t4 = spawn(|| {
    while (!Y.load(Ordering::Acquire)) {}
  <span class="CodeAnnotationCode" aria-label="annotation2">2</span> if (X.load(Ordering::Acquire)) {
        Z.fetch_add(1, Ordering::Relaxed); }
});</code></pre>
<p class="CodeListingCaption"><a id="listing10-4">Listing 10-4</a>: Weird results with <code>Acquire/Release</code><code/> ordering</p>
<p>The two threads <code>t1</code> and <code>t2</code> set <code>X</code> and <code>Y</code> to <code>true</code>, respectively. Thread <code>t3</code> waits for <code>X</code> to be <code>true</code>; once <code>X</code> is <code>true</code>, it checks if <code>Y</code> is <code>true</code> and, if so, adds 1 to <code>Z</code>. Thread <code>t4</code> instead waits for <code>Y</code> to become <code>true</code>, and then checks if <code>X</code> is <code>true</code> and, if so, adds 1 to <code>Z</code>. At this point the question is: what are the possible values for <code>Z</code> after all the threads terminate? Before I show you the answer, try to work your way through it given the definitions of <code>Release</code> and <code>Acquire</code> ordering in the previous section.</p>
<p>First, let’s recap the conditions under which <code>Z</code> is incremented. Thread <code>t3</code> increments <code>Z</code> if it sees that <code>Y</code> is <code>true</code> after it observes that <code>X</code> is <code>true</code>, which can happen only if <code>t2</code> runs before <code>t3</code> evaluates the load at <span class="CodeAnnotation" aria-label="annotation1">1</span>. Conversely, thread <code>t4</code> increments <code>Z</code> if it sees that <code>X</code> is <code>true</code> after it observes that <code>Y</code> is <code>true</code>, so only if <code>t1</code> runs before <code>t4</code> evaluates the load at <span class="CodeAnnotation" aria-label="annotation2">2</span>. To simplify the explanation, let’s assume for now that each thread runs to completion once it runs.</p>
<p>Logically, then, <code>Z</code> can be incremented twice if the threads run in the order 1, 2, 3, 4—both <code>X</code> and <code>Y</code> are set to <code>true</code>, and then <code>t3</code> and <code>t4</code> run to find that their conditions for incrementing <code>Z</code> are met. Similarly, <code>Z</code> can trivially be incremented just once if the threads run in the order 1, 3, 2, 4. This satisfies <code>t4</code>’s condition for incrementing <code>Z</code>, but not <code>t3</code>’s. Getting <code>Z</code> to be <code>0</code>, however, <em>seems</em> impossible: if we want to prevent <code>t3</code> from incrementing <code>Z</code>, <code>t2</code> has to run after <code>t3</code>. Since <code>t3</code> runs only after <code>t1</code>, that implies that <code>t2</code> runs after <code>t1</code>. However, <code>t4</code> won’t run until after <code>t2</code> has run, so <code>t1</code> must have run and set <code>X</code> to <code>true</code> by the time <code>t4</code> runs, and so <code>t4</code> will increment <code>Z</code>.</p>
<p>Our inability to get <code>Z</code> to be <code>0</code> stems mostly from our human inclination for linear explanations; this happened, then this happened, then this <span epub:type="pagebreak" title="184" id="Page_184"/>happened. Computers aren’t limited in the same way and have no need to box all events into a single global order. There’s nothing in the rules for <code>Release</code> and <code>Acquire</code> that says that <code>t3</code> must observe the same execution order for <code>t1</code> and <code>t2</code> as <code>t4 </code>observes. As far as the computer is concerned, it’s fine to let <code>t3</code> observe <code>t1</code> as having executed first, while having <code>t4</code> observe <code>t2</code> as having executed first. With that in mind, an execution in which <code>t3</code> observes that <code>Y</code> is <code>false</code> after it observes that <code>X</code> is <code>true</code> (implying that <code>t2</code> runs after <code>t1</code>), while in the same execution <code>t4</code> observes that <code>X</code> is <code>false</code> after it observes that <code>Y</code> is <code>true</code> (implying that <code>t2</code> runs before <code>t1</code>), is completely reasonable, even if that seems outrageous to us mere humans.</p>
<p>As we discussed earlier, <code>Acquire/Release</code><code/> requires only that an <code>Ordering::Acquire</code> load of a variable must see all stores that happened before an <code>Ordering::Release</code> store that stored what the load loaded. In the ordering just discussed, the computer <em>did</em> uphold that property: <code>t3</code> sees <code>X == true</code>, and indeed sees all stores by <code>t1</code> prior to it setting <code>X = true</code>—there are none. It also sees <code>Y == false</code>, which was stored by the main thread at program startup, so there aren’t any relevant stores to be concerned with. Similarly, <code>t4</code> sees <code>Y = true</code> and also sees all stores by <code>t2</code> prior to setting <code>Y = true</code>—again, there are none. It also sees <code>X == false</code>, which was stored by the main thread and has no preceding store. No rules are broken, yet it just seems wrong somehow.</p>
<p>Our intuitive expectation was that we could put the threads in some global order to make sense of what every thread saw and did, but that was not the case for <code>Acquire/Release</code><code/> ordering in this example. To achieve something closer to that intuitive expectation, we need sequential consistency. Sequential consistency requires all the threads taking part in an atomic operation to coordinate to ensure that what each thread observes corresponds to (or at least appears to correspond to) <em>some</em> single, common execution order. This makes it easier to reason about but also makes it costly.</p>
<p>Atomic loads and stores marked with <code>Ordering::SeqCst</code> instruct the compiler to take any extra precautions (such as using special CPU instructions) needed to guarantee sequential consistency for those loads and stores. The exact formalism around this is fairly convoluted, but sequential consistency essentially ensures that if you looked at all the related <code>SeqCst</code> operations from across all your threads, you could put the thread executions in <em>some</em> order so that the values that were loaded and stored would all match up.</p>
<p>If we replaced all the memory ordering arguments in <a href="#listing10-4">Listing 10-4</a> with <code>SeqCst</code>, <code>Z</code> could not possibly be <code>0</code> after all the threads have exited, just as we originally expected. Under sequential consistency, it must be possible to say either that <code>t1</code> definitely ran before <code>t2</code> or that <code>t2</code> definitely ran before <code>t1</code>, so the execution where <code>t3</code> and <code>t4</code> see different orders is not allowed, and thus <code>Z</code> cannot be <code>0</code>.</p>
<h3 id="h2--0009">Compare and Exchange</h3>
<p class="BodyFirst">In addition to <code>load</code> and <code>store</code>, all of Rust’s atomic types provide a method called <code>compare_exchange</code>. This method is used to atomically <em>and conditionally</em> replace a value. You provide <code>compare_exchange</code> with the last value you <span epub:type="pagebreak" title="185" id="Page_185"/>observed for an atomic variable and the new value you want to replace the original value with, and it will replace the value only if it is still the same as it was when you last observed it. To see why this is important, take a look at the (broken) implementation of a mutual exclusion lock in <a href="#listing10-5" id="listinganchor10-5">Listing 10-5</a>. This implementation keeps track of whether the lock is held in the static atomic variable <code>LOCK</code>. We use the Boolean value <code>true</code> to represent that the lock is held. To acquire the lock, a thread waits for <code>LOCK</code> to be <code>false</code>, then sets it to <code>true</code> again; it then enters its critical section and sets <code>LOCK</code> to <code>false</code> to release the lock when its work (<code>f</code>) is done.</p>
<pre><code>static LOCK: AtomicBool = AtomicBool::new(false);

fn mutex(f: impl FnOnce()) {
    <span class="LiteralGray">// Wait for the lock to become free (false).</span>
    while LOCK.load(Ordering::Acquire)
      { <span class="LiteralGray">/* .. TODO: avoid spinning .. */</span> }
    <span class="LiteralGray">// Store the fact that we hold the lock.</span>
    LOCK.store(true, Ordering::Release);
    <span class="LiteralGray">// Call f while holding the lock.</span>
    f();
    <span class="LiteralGray">// Release the lock.</span>
    LOCK.store(false, Ordering::Release);
}</code></pre>
<p class="CodeListingCaption"><a id="listing10-5">Listing 10-5</a>: An incorrect implementation of a mutual exclusion lock</p>
<p>This mostly works, but it has a terrible flaw—two threads might both see <code>LOCK == false</code> at the same time and both leave the <code>while</code> loop. Then they both set <code>LOCK</code> to <code>true</code> and both enter the critical section, which is exactly what the <code>mutex</code> function was supposed to prevent!</p>
<p>The issue in <a href="#listing10-5">Listing 10-5</a> is that there is a gap between when we load the current value of the atomic variable and when we subsequently update it, during which another thread might get to run and read or touch its value. It is exactly this problem that <code>compare_exchange</code> solves—it swaps out the value behind the atomic variable <em>only</em> if its value still matches the previous read, and otherwise notifies you that the value has changed. <a href="#listing10-6" id="listinganchor10-6">Listing 10-6</a> shows the corrected implementation using <code>compare_exchange</code>.</p>
<pre><code>static LOCK: AtomicBool = AtomicBool::new(false);

fn mutex(f: impl FnOnce()) {
    <span class="LiteralGray">// Wait for the lock to become free (false).</span>
    loop {
      let take = LOCK.compare_exchange(
          false,
          true,
          Ordering::AcqRel,
          Ordering::Relaxed
      );
      match take {
        Ok(false) =&gt; break,
        Ok(true) | Err(false) =&gt; unreachable!(),
<span epub:type="pagebreak" title="186" id="Page_186"/>        Err(true) =&gt; { <span class="LiteralGray">/* .. TODO: avoid spinning .. */</span> }
      }
    }
    <span class="LiteralGray">// Call f while holding the lock.</span>
    f();
    <span class="LiteralGray">// Release the lock.</span>
    LOCK.store(false, Ordering::Release);
}</code></pre>
<p class="CodeListingCaption"><a id="listing10-6">Listing 10-6</a>: A corrected implementation of a mutual exclusion lock</p>
<p>This time around, we use <code>compare_exchange</code> in the loop, and it takes care of both checking that the lock is currently not held and storing <code>true</code> to take the lock as appropriate. This happens through the first and second arguments to <code>compare_exchange</code>, respectively: in this case, <code>false</code> and then <code>true</code>. You can read the invocation as “Store <code>true</code> only if the current value is <code>false</code>.” The <code>compare_exchange</code> method returns a <code>Result</code> that indicates either that the value was successfully updated (<code>Ok</code>) or that it could not be updated (<code>Err</code>). In either case, it also returns the current value. This isn’t too useful with an <code>AtomicBool</code> since we know what the value must be if the operation failed, but for something like an <code>AtomicI32</code>, the updated current value will let you quickly recompute what to store and then try again without having to do another load.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	Note that <code>compare_exchange</code> checks only whether the value is the same as the one that was passed in as the current value. If some other thread modifies the atomic variable’s value and then resets it to the original value again, a <code>compare_exchange</code> on that variable will still succeed. This is often referred to as the A-B-A problem.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Unlike simple loads and stores, <code>compare_exchange</code> takes <em>two</em> <code>Ordering</code> arguments. The first is the “success ordering,” and it dictates what memory ordering should be used for the load and store that the <code>compare_exchange</code> represents in the case that the value was successfully updated. The second is the “failure ordering,” and it dictates the memory ordering for the load if the loaded value does not match the expected current value. These two orderings are kept separate so that the developer can give the CPU leeway to improve execution performance by reordering loads and stores on failure when appropriate, but still get the correct ordering on success. In this case, it’s okay to reorder loads and stores across failed iterations of the lock acquisition loop, but it’s <em>not</em> okay to reorder loads and stores inside the critical section in such a way that they end up outside of it.</p>
<p>Even though its interface is simple, <code>compare_exchange</code> is a very powerful synchronization primitive—so much so that it’s been theoretically proven that you can build all other distributed consensus primitives using only <code>compare_exchange</code>! For that reason, it is the workhorse of many, if not most, synchronization constructs when you really dig into the implementation details.</p>
<p>Be aware, though, that a <code>compare_exchange</code> requires that a single CPU has exclusive access to the underlying value, and it is therefore a form of mutual exclusion at the hardware level. This in turn means that <code>compare_exchange</code> <span epub:type="pagebreak" title="187" id="Page_187"/>can quickly become a scalability bottleneck: only one CPU can make progress at a time, so there’s a portion of your code that will not scale with the number of cores. In fact, it’s probably worse than that—the CPUs have to coordinate to ensure that only one CPU succeeds at a <code>compare_exchange</code> for a variable at a time (take a look at the MESI protocol if you’re curious about how that works), and that coordination grows quadratically more costly the more CPUs are involved!</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>compare_exchange_weak</h2>
<p class="BoxBodyFirst">The careful documentation reader will notice that <code>compare_exchange</code> has a suspiciously named cousin, <code>compare_exchange_weak</code>, and wonder what the difference is. The weak variant of <code>compare_exchange</code> is allowed to fail even if the atomic variable’s value does still match the expected value that the user passed in, whereas the strong variant must succeed in this case.</p>
<p>This might seem odd—how could an atomic value swap fail except if the value has changed? The answer lies in system architectures that do not have a native <code>compare_exchange</code> operation. For example, ARM processors instead have <em>locked load</em> and <em>conditional store</em> operations, where a conditional store will fail if the value read by an associated locked load has not been written to since the load. The Rust standard library implements <code>compare_exchange</code> on ARM by calling this pair of instructions in a loop and returning only once the conditional store succeeds. This makes the code in <a href="#listing10-6">Listing 10-6</a> needlessly inefficient—we end up with a nested loop, which requires more instructions and is harder to optimize. Since we already have a loop in this case, we could instead use <code>compare_exchange_weak</code>, remove the <code>unreachable!()</code> on <code>Err(false)</code>, and get better machine code on ARM and the same compiled code on x86!</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2--0010">The Fetch Methods</h3>
<p class="BodyFirst">Fetch methods (<code>fetch_add</code>, <code>fetch_sub</code>, <code>fetch_and</code>, and the like) are designed to allow more efficient execution of atomic operations that commute—that is, operations that have meaningful semantics regardless of the order they execute in. The motivation for this is that the <code>compare_exchange</code> method is powerful, but also costly—if two threads both want to update a single atomic variable, one will succeed, while the other will fail and have to retry. If many threads are involved, they all have to mediate sequential access to the underlying value, and there will be plenty of spinning while threads retry on failure.</p>
<p>For simple operations that commute, rather than fail and retry just because another thread modified the value, we can tell the CPU what operation to perform on the atomic variable. It’ll then perform that operation on whatever the current value happens to be when the CPU eventually gets exclusive access. Think of an <code>AtomicUsize</code> that counts the number of <span epub:type="pagebreak" title="188" id="Page_188"/>operations a pool of threads has completed. If two threads both complete a job at the same time, it doesn’t matter which one updates the counter first as long as both their increments are counted.</p>
<p>The fetch methods implement these kinds of commutative operations. They perform a read <em>and</em> a store operation in a single step and guarantee that the store operation was performed on the atomic variable when it held exactly the value returned by the method. As an example, <code>AtomicUsize::fetch_add(1, Ordering::Relaxed)</code> never fails—it always adds 1 to the current value of the <code>AtomicUsize</code>, no matter what it is, and returns the value of the <code>AtomicUsize</code> precisely when this thread’s 1 was added.</p>
<p>The fetch methods tend to be more efficient than <code>compare_exchange</code> because they don’t require threads to fail and retry when multiple threads contend for access to a variable. Some hardware architectures even have specialized fetch method implementations that scale much better as the number of involved CPUs grows. Nevertheless, if enough threads try to operate on the same atomic variable, those operations will begin to slow down and exhibit sublinear scaling due to the coordination required. In general, the best way to significantly improve the performance of a concurrent algorithm is to split contended variables into more atomic variables that are each less contended, rather than switching from <code>compare_exchange</code> to a fetch method.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	The <code>fetch_update</code> method is somewhat deceptively named—behind the scenes, it is really just a <code>compare_ex</code><code>change_weak</code> loop, so its performance profile will more closely match that of <code>compare_exchange</code> than the other fetch methods.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1--0005">Sane Concurrency</h2>
<p class="BodyFirst">Writing correct and performant concurrent code is harder than writing sequential code; you have to consider not only possible execution interleavings but also how your code interacts with the compiler, the CPU, and the memory subsystem. With such a wide array of footguns at your disposal, it’s easy to want to throw your hands in the air and just give up on concurrency altogether. In this section we’ll explore some techniques and tools that can help ensure that you write correct concurrent code without (as much) fear.</p>
<h3 id="h2--0011">Start Simple</h3>
<p class="BodyFirst">It is a fact of life that simple, straightforward, easy-to-follow code is more likely to be correct. This principle also applies to concurrent code—always start with the simplest concurrent design you can think of, then measure, and only if measurement reveals a performance problem should you optimize your algorithm.</p>
<p>To follow this tip in practice, start out with concurrency patterns that do not require intricate use of atomics or lots of fine-grained locks. Begin with multiple threads that run sequential code and communicate over channels, or that cooperate through locks, and then benchmark the resulting performance with the workload you care about. You’re much less likely <span epub:type="pagebreak" title="189" id="Page_189"/>to make mistakes this way than by implementing fancy lockless algorithms or by splitting your locks into a thousand pieces to avoid false sharing. For many use cases, these designs are plenty fast enough; it turns out a lot of time and effort has gone into making channels and locks perform well! And if the simple approach is fast enough for your use case, why introduce more complex and error-prone code?</p>
<p>If your benchmarks indicate a performance problem, then figure out exactly which part of your system scales poorly. Focus on fixing that bottleneck in isolation where you can, and try to do so with small adjustments where possible. Maybe it’s enough to split a lock in two rather than move to a concurrent hash table, or to introduce another thread and a channel rather than implement a lock-free work stealing queue. If so, do that.</p>
<p>Even when you do have to work directly with atomics and the like, keep things simple until there’s a proven need to optimize—use <code>Ordering::SeqCst</code> and <code>compare_exchange</code> at first, and then iterate if you find concrete evidence that those are becoming bottlenecks that must be taken care of.</p>
<h3 id="h2--0012">Write Stress Tests</h3>
<p class="BodyFirst">As the author, you have a lot of insight into where bugs in your code may hide, without necessarily knowing what those bugs are (yet, anyway). Writing stress tests is a good way to shake out some of the hidden bugs. Stress tests don’t necessarily perform a complex sequence of steps but instead have lots of threads doing relatively simple operations in parallel.</p>
<p>For example, if you were writing a concurrent hash map, one stress test might be to have <em>N</em> threads insert or update keys and <em>M</em> threads read keys in such a way that those <em>M</em>+<em>N</em> threads are likely to often choose the same keys. Such a test doesn’t test for a particular outcome or value but instead tries to trigger many possible interleavings of operations in the hopes that buggy interleavings might reveal themselves.</p>
<p>Stress tests resemble fuzz tests in many ways; whereas fuzzing generates many random inputs to a given function, the stress test instead generates many random thread and memory access schedules. Just like fuzzers, stress tests are therefore only as good as the assertions in your code; they can’t tell you about a bug that doesn’t manifest in some easy-to-spot way like an assertion failure or some other kind of panic. For that reason, it’s a good idea to litter your low-level concurrency code with assertions, or <code>debug_assert_*</code> if you’re worried about runtime cost in particularly hot loops.</p>
<h3 id="h2--0013">Use Concurrency Testing Tools</h3>
<p class="BodyFirst">The primary challenge in writing concurrent code is to handle all the possible ways the execution of different threads can interleave. As we saw in the <code>Ordering::SeqCst</code> example in <a href="#listing10-4">Listing 10-4</a>, it’s not just the thread scheduling that matters, but also which memory values are possible for a given thread to observe at any given point in time. Writing tests that execute every possible legal execution is not only tedious but also difficult—you need very low-level control over which threads execute when and what values their reads return, which the operating system likely doesn’t provide.</p>
<h4 id="h3--0007"><span epub:type="pagebreak" title="190" id="Page_190"/>Model Checking with Loom</h4>
<p class="BodyFirst">Luckily, a tool already exists that can simplify this execution exploration for you in the form of the <code>loom</code> crate. Given the relative release cycles of this book and that of a Rust crate, I won’t give any examples of how to use Loom here, as they’d likely be out of date by the time you read this book, but I will give an overview of what it does.</p>
<p>Loom expects you to write dedicated test cases in the form of closures that you pass into a Loom model. The model keeps track of all cross-thread interactions and tries to intelligently explore all possible iterations of those interactions by executing the test case closure multiple times. To detect and control thread interactions, Loom provides replacement types for all the types in the standard library that allow threads to coordinate with one another; that includes most types under <code>std::sync</code> and <code>std::thread</code> as well as <code>UnsafeCell</code> and a few others. Loom expects your application to use those replacement types whenever you run the Loom tests. The replacement types tie into the Loom executor and perform a dual function: they act as rescheduling points so that Loom can choose which operation to run next after each possible thread interaction point, and they inform Loom of new possible interleavings to consider. Essentially, Loom builds up a tree of all the possible future executions for each point at which multiple execution interleavings are possible and then tries to execute all of them, one after the other.</p>
<p>Loom attempts to fully explore all possible executions of the test cases you provide it with, which means it can find bugs that occur only in extremely rare executions that stress testing would not find in a hundred years. While that’s great for smaller test cases, it’s generally not feasible to apply that kind of rigorous testing to larger test cases that test more involved sequences of operations or require many threads to run at once. Loom would simply take too long to get decent coverage of the code. In practice, you may therefore want to tell Loom to consider only a subset of the possible executions, which Loom’s documentation has more details on.</p>
<p>Like with stress tests, Loom can catch only bugs that manifest as panics, so that’s yet another reason to spend some time placing strategic assertions in your concurrent code! In many cases, it may even be worthwhile to add additional state tracking and bookkeeping instructions to your concurrent code to give you better assertions.</p>
<h4 id="h3--0008">Runtime Checking with ThreadSanitizer</h4>
<p class="BodyFirst">For larger test cases, your best bet is to run the test through a couple of iterations under Google’s excellent <code>ThreadSanitizer</code>, also known as TSan. TSan automatically augments your code by placing extra bookkeeping instructions prior to every memory access. Then, as your code runs, those bookkeeping instructions update and check a special state machine that flags any concurrent memory operations that indicate a problematic race condition. For example, if thread B writes to some atomic value X, but has not synchronized (lots of hand waving here) with the thread that wrote the previous value of X that indicates a write/write race, which is nearly always a bug.</p>
<p><span epub:type="pagebreak" title="191" id="Page_191"/>Since TSan only observes your code running and does not execute it over and over again like Loom, it generally only adds a constant-factor overhead to the runtime of your program. While that factor can be significant (5–15 times at the time of writing), it’s still small enough that you can execute even most complex test cases in a reasonable amount of time.</p>
<p>At the time of writing, to use TSan you need to use a nightly version of the Rust compiler and pass in the <code>-Zsanitizer=thread</code> command-line argument (or set it in <code>RUSTFLAGS</code>), though hopefully in time this will be a standard supported option. Other sanitizers are also available that check things like out-of-bounds memory accesses, use-after-free, memory leaks, and reads of uninitialized memory, and you may want to run your concurrent test suite through those too!</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2>Heisenbugs</h2>
<p class="BoxBodyFirst"><em>Heisenbugs</em> are bugs that seem to disappear when you try to study them. This happens quite frequently when trying to debug highly concurrent code; the additional instrumentation to debug the problem changes the relative timing of concurrent events and might cause the execution interleaving that triggered the bug to no longer happen.</p>
<p>A particularly common cause of disappearing concurrency bugs is using print statements, which is by far one of the most common debugging techniques. There are two reasons why print statements have such an outsized effect on concurrency bugs. The first, and perhaps most obvious, is that relatively speaking, printing something to the user’s terminal (or wherever standard output points) takes quite a long time, especially if your program is producing a lot of output. Writing to the terminal requires, at the very least, a round-trip to the operating system kernel to perform the write, but the write may also have to wait for the terminal itself to read from the process’s output into its own buffers. All that extra time might so much delay the operation that previously raced with an operation in some other thread that the race condition disappears.</p>
<p>The second reason why print statements disturb concurrent execution patterns is that writing to standard output is (generally) guarded by a lock. If you look inside the <code>Stdout</code> type in the standard library, you’ll see that it holds a <code>Mutex</code> that guards access to the output stream. It does this so that the output isn’t garbled too badly if multiple threads try to write at the same time—without a lock, a given line might have characters interspersed from multiple thread writes, but with the lock the threads will take turns writing instead. Unfortunately, acquiring the output lock, is another thread synchronization point, and one that every printing thread is involved in. This means that if your code was previously broken due to missing synchronization between two threads, or just because a particular race between two threads was possible, adding print statements might fix that bug as a side effect!</p>
<p>In general, when you spot what seems like a Heisenbug, try to find other ways to narrow down the problem. That might involve using Loom or TSan, <span epub:type="pagebreak" title="192" id="Page_192"/>using gdb or lldb, or using a per-thread in-memory log that you print only at the end. Many logging frameworks also work hard to avoid synchronization points on the critical path of issuing log events, so switching to one of those might make your life easier. As an added bonus, good logging that you leave behind after fixing a particular bug might come in handy later. Personally I’m a big fan of the <code>tracing</code> crate, but there are many good options out there.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1--0006">Summary</h2>
<p class="BodyFirst">In this chapter, we first covered common correctness and performance pitfalls in concurrent Rust, and some of the high-level concurrency patterns that successful concurrent applications tend to use to work around them. We also explored how asynchronous Rust enables concurrency without parallelism, and how to explicitly introduce parallelism in asynchronous Rust code. We then dove deeper into Rust’s many different lower-level concurrency primitives, including how they work, how they differ, and what they’re all for. Finally, we explored techniques for writing better concurrent code and looked at tools like Loom and TSan that can help you vet that code. In the next chapter we’ll continue our journey through the lower levels of Rust by digging into foreign function interfaces, which allow Rust code to link directly against code written in other languages.</p>
</section>
</div></body></html>