<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch05"><span epub:type="pagebreak" id="page_161"/><strong><span class="big">5</span><br/>MACHINE LEARNING FUNDAMENTALS</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="189" height="189"/></div>&#13;
<p class="noindentsa">In the early days of the Android eco-system, defenders analyzed apps manually to determine whether they were malicious. This technique was feasible at the time because the operating system’s market share was small and, initially, few apps were developed for it. However, things have changed. Recent official reports show that more than 100,000 Android APKs are released each month on Google Play. Our own estimates suggest that the actual number is significantly higher.</p>&#13;
<p class="indent">It is no longer possible for companies to manually assess the security level of so many diverse apps. Initially, analysts solved this problem by relying on human-identified patterns present exclusively in malware. They wrote detection rules, using YARA or other tools, to flag applications containing such patterns. This approach failed to scale, however, as it quickly became infeasible for analysts to keep track of the features present in millions of apps.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_162"/>Instead, analysts began using machine learning algorithms, which have the ability to perform these tasks on a large number of applications without explicit programming by learning through examples. This approach proved vastly more efficient, and reduced the burden on human analysts. This chapter introduces the machine learning basics you’ll need to be familiar with in order to understand the material presented in the book’s remaining chapters, with a focus on the classification algorithms popular in malware detection. Readers already familiar with the topic can skip ahead.</p>&#13;
<h3 class="h3" id="ch05lev1"><strong>How Machine Learning for Malware Analysis Works</strong></h3>&#13;
<p class="noindent">In malware analysis, we most often use machine learning methods to classify apps as benign, malicious, or, in some cases, possibly malicious. At a deeper level, more sophisticated methods can provide increasingly fine-grained labels that identify apps as a specific type of malware, like spyware, banking trojans, and so on. Given the support that automated methods provide, security analysts can focus on examining <em>gray zone</em> apps, or those that aren’t accurately classified as either goodware or malware. Machine learning significantly reduces the number of apps that analysts have to manually review.</p>&#13;
<p class="indent">Machine learning algorithms can be either supervised or unsupervised. <em>Supervised</em> algorithms require labeled datasets, while <em>unsupervised</em> algorithms learn patterns inherent in the data. Classification algorithms are the most common type of supervised algorithms, while clustering and anomaly detection are common examples of unsupervised algorithms. Each has its own purpose in security-related machine learning.</p>&#13;
<p class="indent"><em>Classification</em> algorithms, also called <em>classifiers</em>, consider information about an entity, such as an app, a picture, or a user account, and place it into one or more classes. For example, in the case of an Android app, we might have two classes of interest: malware and goodware. But if we want to classify something else—for instance, Instagram accounts—we might have many more classes: <em>child</em> for those younger than 18, <em>young adult</em> for those 18–40 years old, <em>middle-aged</em> for those 41–65 years old, and <em>senior</em> for those 66 years old or older. Engineers working on the classification problem define the exact number of categories and the meaning of each. One challenge is that classification algorithms often require a large number of labeled samples (already classified samples that the algorithm can learn from) to produce accurate models, which might not always be available.</p>&#13;
<p class="indent"><em>Clustering</em> algorithms take information from multiple entities and group similar samples into clusters. For instance, malicious developers often create multiple versions of their malware over time as they look for ways to avoid detection or add new functionality. In such cases, clusters might correspond to different versions of the same malware family. Clustering algorithms need a way to measure the similarity of or distance between the entities under observation, and domain experts are responsible for defining how that similarity will be computed based on the clustering goal.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_163"/>While clustering algorithms don’t require labeled data, the clusters they produce can be hard to interpret if the algorithm isn’t aware of what the analyst is looking for. Malware and goodware often share SDKs and libraries, which might confuse the clustering system, causing it to group malware and goodware together merely because they share an SDK.</p>&#13;
<p class="indent"><em>Anomaly detection</em> or <em>outlier detection</em> algorithms try to identify entities that are substantially different from almost all others in a given dataset. For instance, efforts have been made to find malicious apps by checking whether their behavior differs substantially from the norm. However, a challenge for Android malware detection in particular is that most malware operates within the bounds of the Android security model, asking unwitting victims for permission to execute malicious actions. Is an app that sends all of your texts to a remote server some kind of spyware, or is it an SMS backup app? Anomaly detection algorithms may have a hard time distinguishing between these two cases.</p>&#13;
<p class="indent">As it turns out, the vast majority of successful efforts to use machine learning in malware detection have relied on classification algorithms. However, some techniques use a mix of clustering and classification in an attempt to identify the family to which a given malware sample belongs, such as the system proposed in “EC2: Ensemble Clustering and Classification for Predicting Android Malware Families” by Tanmoy Chakraborty et al.</p>&#13;
<h4 class="h4" id="ch05lev1sec1"><strong><em>Identifying App Features</em></strong></h4>&#13;
<p class="noindent">Most machine learning algorithms assume that each entity of interest has an associated <em>feature vector</em>, which is an ordered list of values belonging to important properties of the entity being studied. In the case of malware analysis, the entities of interest are the apps themselves. The feature vector includes attributes derived from the analysis of the APK or the app in execution and can be either handcrafted or automatically generated.</p>&#13;
<p class="indent">For the purposes of malware detection and classification, features might relate to whether or not the app requests a specific permission (such as permission to read incoming texts), whether or not the code contains encrypted portions, whether or not the code tries to connect to an external server, and so forth. For each of these questions, the feature is set to 1 if the answer is yes and 0 otherwise. Other features may have <em>non-binary</em> values. For instance, we might have a feature corresponding to the number of times an app’s source code calls a given package in the Android API. Machine learning methods use these features to identify and classify entities. <a href="ch06.xhtml">Chapter 6</a> will describe various types of features that are important to the analysis of Android malware.</p>&#13;
<p class="indent">The content of an Android app alone provides a seemingly unlimited number of potential features, but we don’t have to limit the feature set to data found inside the APK. In fact, there is surprising value in connecting features from APK files to external information. For example, in the case of an app that connects to a certain domain, we could turn the Whois information for that domain into features. Similarly, if an app connects to a certain IP address, we can pull in information about who owns the IP address, the <span epub:type="pagebreak" id="page_164"/>datacenter that serves it, the country where the associated server is located, or even information from the server itself, such as the operating system it runs or the other software it hosts.</p>&#13;
<p class="indent">To give another example, if an app sends messages to a premium SMS number, we might be able to determine which mobile carrier owns that number and what commercial entity has it registered in partnership with the mobile carrier. The developers of the machine learning system can choose to include these pieces of information as features to characterize certain malware families and developers.</p>&#13;
<h4 class="h4" id="ch05lev1sec2"><strong><em>Creating Training Sets</em></strong></h4>&#13;
<p class="noindent">A <em>training sample</em> for a classification algorithm is a computational object consisting of a feature vector and a class label. In formal terms, we say that it consists of a pair (<em>f</em> , <em>c</em>) where <em>f</em> is a feature vector and <em>c</em> is the class to which we believe the sample belongs. Note that in Android malware analysis, for an app to be a training sample we must already know its class (for instance, malware or goodware).</p>&#13;
<p class="indent">A <em>training set</em> is a finite set of training samples. We can usually represent it as a table or spreadsheet. In the case of the malware versus goodware classification, the rows in the table would correspond to apps and the columns would correspond to various features. A special column would represent the label or class; that is, whether the app in a given row is malware (set to 1) or goodware (set to 0). <a href="ch05.xhtml#ch5tab1">Table 5-1</a> shows a small sample training set associated with Android apps.</p>&#13;
<p class="indent">In this training set, we show only two features, for the sake of simplicity. The <em>telephony</em> feature captures the number of calls made to the <em>android.telephony.cdma</em> package by the app, and the <em>app</em> feature does the same for the <em>android.app</em> package. For instance, the app shown in the first row of <a href="ch05.xhtml#ch5tab1">Table 5-1</a> makes 27 calls in its source code to classes in the <em>telephony</em> package and 2,655 calls to classes in the <em>app</em> API package. This app is malware: we see that its value in the Label column is 1.</p>&#13;
<p class="indent">Each app in this training set can be thought of as a point on a scatter plot. For instance, we could position the first app in the table at the coordinate (27,2655). In <a href="ch05.xhtml#ch5fig1">Figure 5-1</a>, we depict it using a cross because it is malware. We denote goodware using dots. As you can see, an app’s feature vector determines its location in this feature space.</p>&#13;
<p class="indent">Of course, in the real world, analysts might use a much larger training set (one with thousands of apps). The number of features might also be in the hundreds, thousands, or higher.</p>&#13;
<p class="indent">Creating good training sets is challenging. Training sets should ideally be vast and diverse, and their data, particularly their labels, must be as accurate as possible. For malware analysis, this poses a problem. How does one put together an accurately labeled set of thousands of malware and goodware apps without many months of careful and costly manual research and analysis?</p>&#13;
<p id="ch5tab1" class="tabcap"><span epub:type="pagebreak" id="page_165"/><strong>Table 5-1:</strong>  Simple Training Set</p>&#13;
<table class="all">&#13;
<colgroup>&#13;
<col style="width:10%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:65%"/>&#13;
<col style="width:10%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>The telephony feature</strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>The app feature</strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Sample name</strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Label</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">27</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">2655</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">14292932679d6930f521a21de4e8bffd.apk</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1764</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">04276665aaa3725ea34097c4c874873c.apk</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">3</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">870</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">e8290db04c7004ec8bb53f7cda155eb9.apk</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">2086</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">03f9eff3229e3a4eefc9224f916202b8.apk</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">3</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">329</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1c4e357a8ec5f13de4ffd57cc2711afe.apk</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1499</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">080b0ed2d9bf87e9f3d061a1ba48da33.apk</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">27</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">2652</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">08026e2b63ec51cb36bc6cff00c28909.apk</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">27</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">2637</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">094f67a3a682a0cd4305d720cc786e00.apk</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">3</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">877</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">3a895a2d19f040d7826e68c2f9596c55.apk</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">2163</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1a7409b8e0f6cc299a4ac0b9ca67856e.apk</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">2016</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">Starbucks_2020-10-22_16_06_36.apk</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1823</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">Starbucks_2017-09-29_16_05_56.apk</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">6</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">6604</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">TikTok_2020-12-03_19_11_34.apk</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">6</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">6604</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">TikTok_2020-12-03_19_17_05.apk</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">11483</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">Walgreens_2020-11-21_21_45_17.apk</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1555</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">Starbucks_2016-01-19_16_04_34.apk</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1738</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">Starbucks_2016-09-08_16_02_23.apk</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">11483</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">Walgreens_2020-11-21_21_46_22.apk</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1384</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">Starbucks_2015-12-07_16_07_02.apk</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1812</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">Starbucks_2017-09-26_16_02_39.apk</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<div class="image"><img id="ch5fig1" src="../images/ch05fig01.jpg" alt="Image" width="568" height="371"/></div>&#13;
<p class="figcap"><em>Figure 5-1: A visualization of the training set as a scatter plot</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_166"/>Fortunately, academic researchers have released a few training sets for Android malware analysis, of which three are well known. The <em>Drebin</em> dataset contains 5,560 applications from 179 different malware families collected between August 2010 and October 2012. You can find it at <a href="https://www.sec.cs.tu-bs.de/~danarp/drebin"><em>https://www.sec.cs.tubs.de/danarp/drebin</em></a>. The <em>AndroZoo</em> dataset is a growing collection of Android applications that currently contains over 17 million APKs, each of which has been analyzed by different antivirus products. You can find it at <a href="https://androzoo.uni.lu"><em>https://androzoo.uni.lu</em></a>. The <em>CCCS-CIC-AndMal-2020</em> dataset contains 200,000 benign samples and 200,000 malicious ones drawn from 191 prominent malware families. The dataset can be found at <a href="https://www.unb.ca/cic/datasets/andmal2020.html"><em>https://www.unb.ca/cic/datasets/andmal2020.html</em></a>. If one of these websites goes offline in the future, we’ll publish the samples at <a href="https://github.com/android-malware-ml-book"><em>https://github.com/android-malware-ml-book</em></a> so long as there is no legal impediment to doing so.</p>&#13;
<h4 class="h4" id="ch05lev1sec3"><strong><em>Using Classification Algorithms</em></strong></h4>&#13;
<p class="noindent">Classification algorithms take a training set as input and try to find some condition such that, when the condition is true for a given app’s feature vector, the probability that it is malicious is very high, whereas when the condition is false, the probability that the app is malicious is very low.</p>&#13;
<p class="indent">For instance, if you consider <a href="ch05.xhtml#ch5fig2">Figure 5-2</a>, you’ll see that the condition of an app calling the <em>app</em> package fewer than 3,000 times does the job. All apps that satisfy this condition are malware (crosses), while all apps that don’t satisfy it are goodware (dots). In this case, the horizontal line at the 3,000 API calls mark splits the feature space into these two parts.</p>&#13;
<div class="image"><img id="ch5fig2" src="../images/ch05fig02.jpg" alt="Image" width="586" height="381"/></div>&#13;
<p class="figcap"><em>Figure 5-2:  Two possible separators for the training set shown in <a href="ch05.xhtml#ch5fig1">Figure 5-1</a></em></p>&#13;
<p class="indent">However, this is not the only possible separator. We could just as easily have selected the dashed line shown in the plot. This line is <em>y</em> = 40<em>x</em> + 2,000, where <em>x</em> = <em>APIPackage:android.telephony.cdma</em> and <em>y</em> = <em>APIPackage:android.app</em>. All points above this line are goodware, and all points below it are malware.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_167"/>At this point, you might have a number of questions. Should separators always split the feature space into two parts, as shown in this figure? Should separators always be linear, or can they include circles, ellipses, or other, even weirder shapes? <a href="ch05.xhtml#ch5fig3">Figure 5-3</a> shows a situation in which the data is grouped into different regions, some containing goodware and some containing malware.</p>&#13;
<div class="image"><img id="ch5fig3" src="../images/ch05fig03.jpg" alt="Image" width="577" height="376"/></div>&#13;
<p class="figcap"><em>Figure 5-3: Rectangular separators for the training set shown in <a href="ch05.xhtml#ch5fig1">Figure 5-1</a></em></p>&#13;
<p class="indent">One potential problem here is that large parts of the feature space aren’t part of either a goodware or malware region, which might make sense, as no samples from those parts of the feature space have ever been seen before. In the next section, you’ll learn that classification algorithms can take various approaches to sorting their samples.</p>&#13;
<h3 class="h3" id="ch05lev2"><strong>Classification Algorithms</strong></h3>&#13;
<p class="noindent">In this section, we’ll discuss some well-known classification methods. As you will see, classifiers work in different ways.</p>&#13;
<h4 class="h4" id="ch05lev1sec4"><strong><em>Decision Trees</em></strong></h4>&#13;
<p class="noindent">A decision tree classification algorithm builds a tree, each node of which compares a single feature with a single value. Thus, each path of the tree corresponds to a complex logical “and” condition, along with a class label showing the class that best matches that condition. Suppose we are given a training set <em>T</em> whose feature vectors are drawn from an <em>n</em>-dimensional space. The samples are Android apps that we want to classify into two classes, goodware and malware. To accomplish this, we would give each app <em>a</em> an associated feature vector <em>f</em><sub><em>a</em></sub> consisting of <em>n</em> features. <a href="ch05.xhtml#ch5fig4">Figure 5-4</a> shows a sample decision tree for classifying the apps.</p>&#13;
<div class="image"><img id="ch5fig4" src="../images/ch05fig04.jpg" alt="Image" width="1250" height="680"/></div>&#13;
<p class="figcap"><em>Figure 5-4: A sample decision tree</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_168"/>Each node in a decision tree implicitly represents a subset of the training set. For instance, the root of the decision tree shown in <a href="ch05.xhtml#ch5fig4">Figure 5-4</a> represents a training set of 1,000 apps. Each node includes a Boolean condition that splits the set into two disjoint sets: one consisting of all members that satisfy the condition and one consisting of all members that do not. For example, in the root node, the condition checks whether the number of calls to the opcode <code>iget-boolean</code> is less than or equal to 1989.5. This opcode reads a Boolean instance field from registers and is expressed as bytecode in <em>Dalvik format</em>, which is the instruction set used by Android runtimes. All apps that satisfy this condition are associated with the left child of the root node, while those that do not satisfy the condition end up in the set of apps associated with the right child.</p>&#13;
<p class="indent">The nodes also contain some other information to help us classify the apps as malicious or benign. For instance, if you look at the root node, you’ll see the number of samples, 1,000, and that each of the two classes (goodware followed by malware) contains 500 apps. Because there is a 50-50 split at the root node, this node can be labeled using either class. In this case, we’ve opted to call it goodware. Look now at the left child of the root node. We see that it contains 519 apps. (That is, 519 samples from the training set satisfied the condition in the root node.) From the <em>Value</em> field, we see that 39 of these 519 apps are goodware, while the remaining 480 are malware. This node is therefore marked as malware, because that is how we classify the majority of its apps.</p>&#13;
<p class="indent">Now, two questions naturally arise. First, how does the decision tree algorithm decide what condition to choose at each node in the tree? And second, what is the <em>Gini</em> field shown in the nodes in <a href="ch05.xhtml#ch5fig4">Figure 5-4</a>? The answers to these questions are closely related. Every decision tree considers some family of constraints in order to choose the conditions with which to label the nodes. In our sample decision tree, this class consists of constraints of <span epub:type="pagebreak" id="page_169"/>the form <em>feature</em> ≤ <em>value</em>. Beyond this, the algorithm relies on the Gini value, an effort to measure the heterogeneity of the classes represented within the set of apps in the training sample for a node. For the root node in <a href="ch05.xhtml#ch5fig4">Figure 5-4</a> the heterogeneity is maximized, as both classes are equally represented. But for its left child, the apps are overwhelming malware. The Gini metric assigns a high value to nodes that are heterogeneous and a low value to nodes that are homogeneous. Consequently, the Gini value assigned to the root node is higher than that assigned to its left child. The Gini value itself is defined as:</p>&#13;
<div class="equationc"><em>Gini</em>(<em>X</em>) = 1 – <em>P</em>(<em>malware</em>|<em>X</em>)<sup>2</sup> – <em>P</em>(<em>goodware</em>|<em>X</em>)<sup>2</sup></div>&#13;
<p class="indent">Because at the root node the probability of an app being goodware is the same as the probability of it being malware, namely 50 percent, the Gini value of the root is 1 <em>–</em> (0.5)<sup>2</sup> – (0.5)<sup>2</sup> = 0.5. For the left child of the root, the probability of an app being goodware is 480/519. Hence, its Gini value is 1 – (39/519)<sup>2</sup> – (480/519)<sup>2</sup> = 1 – 0.075<sup>2</sup> – 0.925<sup>2</sup> = 1 – 0.0056 – 0.856 = 0.139.</p>&#13;
<p class="indent">We won’t go into the details of the decision tree algorithm itself. It suffices to say that when we build a decision tree, we try to find a condition from the set of all permitted splitting conditions such that the resulting Gini value of a combination of the two children is minimized. The process of splitting nodes continues until we reach nodes that are considered homogeneous enough, meaning their Gini scores fall below a given threshold.</p>&#13;
<p class="indent">There are many variants of decision trees. Some use criteria such as entropy rather than Gini scores to assess the quality of possible ways to split a node. Other variants change the types of conditions at each node and even set things up so that a decision tree makes a ternary or <em>n</em>-ary decision at each node, rather than a binary one. You can find more details about the construction of decision trees and their variants in “Top-Down Induction of Decision Trees Classifiers—A Survey” by Lior Rokach and Oded Maimon and “Optimizing Multi-Path Decision Tree by Clustering and K-Nearest Neighbor Methods” by Nasib S. Gill and Reena Hooda.</p>&#13;
<h4 class="h4" id="ch05lev1sec5"><strong><em>Bagging and Random Forest</em></strong></h4>&#13;
<p class="noindent">Bagging and random forest (RF) are quintessential examples of <em>ensemble</em> classifiers, algorithms that combine the predictions of multiple other classifiers. Ensemble classifiers typically start with a known classifier (sometimes referred to as a <em>weak learner</em>). In the case of bagging and RF classifiers, this is often a decision tree.</p>&#13;
<p class="indent">Bagging and RF algorithms use two instruments to provide a level of robustness to the classification result based on the training set. The first instrument is to randomly select some number of subsets from the training set. The second is to randomly select a subset of the features. There is typically no requirement for constructing these subsets other than that the size of each be some percentage of the size of the original training set–for example, 65 or 80 percent. A weak learner is then separately and independently trained on each subset to yield a class label. The class of a new app is declared to be the class predicted by the majority of the weak learners.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_170"/>Bagging does not require the weak learner to be a decision tree; it could be any type of classifier. On the other hand, random forest classifiers assume that the weak learner is a decision tree. For each of the subsets discussed earlier, a decision tree is constructed, and at every node in any of the decision trees involved, a given set of attributes is deemed <em>active</em>. These active attributes are those that haven’t been used in the path from the decision tree’s root to that node. A random subset of active attributes is then selected at each node, and the best splitting condition is selected from the conditions definable by the active attributes only; inactive attributes are not considered, even if they provide a better Gini result. Each decision tree then generates a label, as before, and the class that gets more “votes” ends up being the class assigned to a given app.</p>&#13;
<h4 class="h4" id="ch05lev1sec6"><strong><em>Support Vector Machines</em></strong></h4>&#13;
<p class="noindent">A support vector machine (SVM) algorithm tries to find a <em>hyperplane</em> that splits the feature space into two in such a way that the feature vectors associated with one class (in our case, malware) primarily lie on one side of the hyperplane and the feature vectors associated with the other (goodware) lie on the other side. We showed two such hyperplanes in <a href="ch05.xhtml#ch5fig2">Figure 5-2</a>. In a two-dimensional feature space, a hyperplane is just a straight line. However, a hyperplane could also be a quadratic line or even a sine curve.</p>&#13;
<p class="indent">A <em>linear SVM</em> uses only straight lines as separators. In higher dimensions, a linear hyperplane has the following form:</p>&#13;
<div class="equationc"><em>a</em><sub>1</sub><em>x</em><sub>1</sub> + <em>a</em><sub>2</sub><em>x</em><sub>2</sub> + ... + <em>a<sub>n</sub>x<sub>n</sub></em> = <em>b</em></div>&#13;
<p class="noindent">Here, <em>x</em><sub>1</sub>, <em>…</em>, <em>x</em><sub><em>n</em></sub> represent the <em>n</em> features and <em>a</em><sub>1</sub>, <em>…</em>, <em>a</em><sub><em>n</em></sub> and <em>b</em> are constants. Such a hyperplane divides the feature space into two parts: one part that satisfies <em>a</em><sub>1</sub><em>x</em><sub>1</sub> + <em>a</em><sub>2</sub><em>x</em><sub>2</sub> + <em>…</em>  + <em>a</em><sub><em>n</em></sub><em>x</em><sub><em>n</em></sub> ≥ <em>b</em> and another part that satisfies <em>a</em><sub>1</sub><em>x</em><sub>1</sub> + <em>a</em><sub>2</sub><em>x</em><sub>2</sub> + … + <em>a<sub>n</sub>x<sub>n</sub></em> ≤ <em>b</em>. The idea is that most malware will lie in one of these two parts and most goodware in the other. Implementers must decide what to do with apps whose feature vectors lie directly on the separating line.</p>&#13;
<p class="indent">To find a good separating hyperplane in a linear SVM, we usually consider two major factors: homogeneity of the feature vectors on either side of the hyperplane, and avoidance of feature vectors that lie close to the hyperplane. In terms of homogeneity, we want most of the app feature vectors on one side of the separator to be malware and most of the feature vectors on the other side to be goodware. For example, recall the horizontal separation in <a href="ch05.xhtml#ch5fig2">Figure 5-2</a>, where we classified a new app by merely counting the number of calls it made to classes in a certain package: if that number was greater than 3,000, we classified the app as goodware, and otherwise we classified it as malware. Of course, this is a highly simplified example. In the real world, the implementation would consider many more features, and the separator line’s equation would likely be far more complex.</p>&#13;
<p class="indent">The second major factor in SVM design concerns feature vectors that lie very close to the separator line, like the malware specimen represented as a cross on the right side of <a href="ch05.xhtml#ch5fig2">Figure 5-2</a>, just below both separator lines. How certain can we be that such samples are correctly classified? To increase the <span epub:type="pagebreak" id="page_171"/>distance between feature vectors and the separator line, we make use of <em>support vectors</em>, which are feature vectors in the training set that are as close to the separator line as possible. The distance between a separator line and its support vectors is called the <em>margin</em>. SVMs try to find the nearest separator line that maximizes the margin, reflecting the intuition that we do not want training points that are too close to the edge.</p>&#13;
<p class="indent">The goals of maximizing the margin and minimizing classification errors often conflict. As a consequence, we usually formulate the problem of finding the best separator line as an optimization problem. We won’t go into the mathematical details of SVMs in this chapter, but the interested reader can find more information in “Support-Vector Networks” by Corinna Cortes and Vladimir Vapnik and “Improving the Accuracy and Speed of Support Vector Machines” by Christopher J. Burges and Bernhard Schölkopf.</p>&#13;
<p class="indent">There are also many nonlinear versions of SVMs. For instance, quadratic SVMs allow the separator hyperplane to take the form of a quadratic curve and tackle a more complex distribution of feature vectors. Other kinds of SVMs use <em>kernel</em> tricks, which map the original feature vector to a new feature vector. The mapping usually involves a nonlinear method. When we apply a linear SVM to this nonlinear transformation, it yields a nonlinear separator for the original data. As a consequence, the resulting separators can have unusual shapes. For example, <a href="ch05.xhtml#ch5fig5">Figure 5-5</a> shows a modified training dataset in part (a) that is similar to the training set visualizations shown earlier in this chapter. Parts (b), (c), and (d) show the separators generated by SVMs using different kinds of kernels.</p>&#13;
<div class="image"><img id="ch5fig5" src="../images/ch05fig05.jpg" alt="Image" width="984" height="749"/></div>&#13;
<p class="figcap"><em>Figure 5-5:  Sample nonlinear SVM separators generated with kernels for a training set (a) with malware feature vectors (crosses) and goodware feature vectors (dots), (b) SVM separator using a polynomial kernel, (c) SVM separator using a quadratic kernel, and (d) SVM separator using a radial basis kernel</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_172"/>Notice that the generated regions are not as easy to describe as those using linear separators.</p>&#13;
<h4 class="h4" id="ch05lev1sec7"><strong><em>k-Nearest Neighbors</em></strong></h4>&#13;
<p class="noindent">A <em>k</em>-nearest neighbor classifier is very simple. It doesn’t really “learn” a model. It takes the feature vector of an app that it has never seen before, identifies the <em>k</em> feature vectors in the training data that are closest to the app’s feature vector using some distance metric (for example, Euclidean distance or cosine distance), and then finds the classes of those <em>k</em> apps. If more than half of the <em>k</em> apps are malware, it declares the app to be malware, too; otherwise it declares it to be goodware. For instance, consider the two apps, <em>A</em>1 and <em>A</em>2, shown in <a href="ch05.xhtml#ch5fig6">Figure 5-6</a>.</p>&#13;
<div class="image"><img id="ch5fig6" src="../images/ch05fig06.jpg" alt="Image" width="585" height="383"/></div>&#13;
<p class="figcap"><em>Figure 5-6: A sample k-nearest neighbor classifier with</em> k = 3</p>&#13;
<p class="indent">Suppose we consider the three nearest neighbors (in other words, <em>k</em> = 3). In the case of <em>A</em>1, two of the three nearest neighbors are goodware, so app <em>A</em>1 would be considered goodware. However, in the case of <em>A</em>2, two of the three nearest neighbors are classified as malware; hence, this app would also be classified as malware.</p>&#13;
<h4 class="h4" id="ch05lev1sec8"><strong><em>Naive Bayes</em></strong></h4>&#13;
<p class="noindent">Naive Bayes classifiers use a very different kind of intuition than the preceding classifier types. They learn a set of simple probabilities from the training data, then use these probabilities later to classify new feature vectors.</p>&#13;
<p class="indent">To classify apps as goodware or malware, a naive Bayes classifier may compute what we call <em>class-conditional</em> probabilities. For a given class (in our case, either goodware or malware), we could use the training set to derive the class-conditional probability that a feature vector’s <em>i</em>th feature has a <span epub:type="pagebreak" id="page_173"/>certain value given that the app belongs to a specific class. Consider the small training set shown in <a href="ch05.xhtml#ch5tab2">Table 5-2</a>.</p>&#13;
<p class="indent">Features <em>A</em> and <em>B</em> represent calls to <em>APIPackage:android.app</em> and <em>Opcode:if-eq</em>, respectively. The probabilities <em>P</em>(<em>A</em> = 10<em>|</em>0) and <em>P</em>(<em>A</em> = 10<em>|</em>1) are the class-conditional probabilities of the attribute <em>A</em> having the value 10 when the classes are 0 and 1, respectively. Using the training set, we can see that <em>P</em>(<em>A</em> = 10<em>|</em>0) is 0.2, because 2 of the 10 goodware apps in the training set have a value of 10 for <em>A</em>. <em>P</em>(<em>A</em> = 10<em>|</em>1) is also 0.2. In contrast, <em>P</em>(<em>A</em> = 3<em>|</em>0) equals 0, while <em>P</em>(<em>A</em> = 3<em>|</em>1) equals 0.3.</p>&#13;
<p id="ch5tab2" class="tabcap"><strong>Table 5-2:</strong>  Sample Training Set</p>&#13;
<table class="all">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:30%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>A</strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>B</strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>App ID</strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Class</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">3</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">app1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">app2</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">5</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">app3</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">12</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">app4</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">3</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">app5</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">app6</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">10</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1pp7</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">72</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">82</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">app8</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">72</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">24</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">app9</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">30</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">app10</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">app11</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">app12</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">app13</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">app14</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">app15</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">app16</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">10</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">app17</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">72</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">190</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">app18</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">72</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">190</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">app19</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">30</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">144</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">app20</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">A naive Bayes classifier might also calculate the <em>prior probability</em> of each class, which is simply the probability of a random app in the training set belonging to that class. In our small training set, these prior probabilities are 0.5 for each of the two classes, as the data has 10 goodware samples and 10 malware samples in it. Given a new app <em>a</em> with an associated feature vector consisting of values <em>f</em><sub><em>a</em></sub> = (<em>v</em><sub>1</sub>, <em>…</em>, <em>v</em><sub><em>n</em></sub>), naive Bayes computes the probability of this app belonging to class <em>c</em> via the Bayes rule, as follows:</p>&#13;
<div class="image1"><img src="../images/math173.jpg" alt="image" width="235" height="51"/></div>&#13;
<p class="indent">In plain English, this says that the probability of the app <em>a</em> belonging to the class <em>c</em> is the probability of <em>a</em>’s feature vector being generated by class <em>c</em> <span epub:type="pagebreak" id="page_174"/>times the prior probability of class <em>c</em> divided by the prior probability of the feature vector of app <em>a</em>.</p>&#13;
<p class="indent">To determine the class of a new app <em>a</em> with the feature vector <em>f</em><sub><em>a</em></sub>, naive Bayes would find the class <em>c</em> for which <em>P</em>(<em>c</em>|<em>f</em><sub><em>a</em></sub>) is maximal across all possible classes. The result is the same as finding the class <em>c</em> such that <em>P</em>(<em>f</em><sub><em>a</em></sub>|<em>c</em>) ×<em>P</em>(<em>c</em>) is maximal, as the denominator of the probability formula remains the same regardless of the class considered. Let us call this product a <em>pseudo-probability</em>. We want to find the class <em>c</em> that maximizes this pseudo-probability.</p>&#13;
<p class="indent">Now consider a new app <em>a</em> whose feature vector is (3, 1), meaning feature <em>A</em> equals 3 and <em>B</em> equals 1. Notice that there is no app in the training set with this feature vector. Naive Bayes computes the probability of seeing the feature vector (3, 1) by making an independence assumption; it assumes that the probability of seeing the feature vector is the product of the probability of seeing each component of the feature vector. In formal terms, we can write this as:</p>&#13;
<div class="image1"><img src="../images/math174-01.jpg" alt="image" width="251" height="27"/></div>&#13;
<p class="noindent">Here, <img class="middle" src="../images/math174-02.jpg" alt="image" width="101" height="25"/> is the conditional probability that <img class="middle" src="../images/math174-03.jpg" alt="image" width="61" height="25"/>, given that an app is in class <em>c</em>, and <img class="middle" src="../images/math174-04.jpg" alt="image" width="21" height="25"/> represents the <em>i</em>th component of app <em>a</em>’s feature vector <em>f</em><sub><em>a</em></sub>.</p>&#13;
<p class="indent">Returning to the example in <a href="ch05.xhtml#ch5tab2">Table 5-2</a>, we see that the pseudo-probabilities for the feature vector (3, 1) are given by the following:</p>&#13;
<div class="image1"><img src="../images/math174-05.jpg" alt="image" width="231" height="110"/></div>&#13;
<p class="noindent">As the latter is larger than the former, this particular app with feature vector (3, 1) is classified as malware.</p>&#13;
<p class="indent">Naive Bayes classifiers have several problems. Often, especially when the feature vector is long, the numerator in the product calculation ends up being zero, which results in a probability of zero. A number of variants of naive Bayes fix such problems by making different types of assumptions about the way the values in each feature are distributed. For example, Gaussian naive Bayes assumes they are distributed in accordance with a normal distribution whose mean and standard deviation are computed from the observed values of the feature in the training data. You can find more information about naive Bayes classifiers and their different variants in “Discrete Bayesian Network Classifiers: A Survey” by Concha Bielza and Pedro Larranaga.</p>&#13;
<h3 class="h3" id="ch05lev3"><strong>Evaluating Machine Learning Models</strong></h3>&#13;
<p class="noindent">Once we’re done training a model, we want to know how well it performs. Researchers have developed multiple metrics to evaluate machine learning models. We’ll discuss a few important ones in this section, focusing on binary classifiers.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_175"/>For the evaluation results to be useful, we should compute these metrics using samples that aren’t present in the training data. Having a large, randomly sampled evaluation set is key to understanding a classifier’s strengths and deficiencies. This evaluation set, like the training set, should contain individual samples, along with labels for each sample. Generally, our evaluation should take into account the following:</p>&#13;
<div class="bqparan">&#13;
<p class="noindentin"><strong>True positives (TPs)</strong> Apps that are predicted by the classifier to be malware and that are in fact labeled as malware</p>&#13;
<p class="noindentin"><strong>False positives (FPs)</strong> Apps that are predicted by the classifier to be malware but are in fact labeled as goodware</p>&#13;
<p class="noindentin"><strong>True negatives (TNs)</strong> Apps that are predicted to be goodware and are labeled as goodware</p>&#13;
<p class="noindentin"><strong>False negatives (FNs)</strong> Apps that are predicted to be goodware but are labeled as malware</p>&#13;
</div>&#13;
<p class="indent">Too many false positives or false negatives indicates poor performance. Other important statistical metrics to consider are shown in <a href="ch05.xhtml#ch5tab3">Table 5-3</a>, which presents the results of a random forest classifier on a “Goodware vs. Android Banking Trojans” dataset we’ve collected from a host of online websites. The following discussion describes those metrics in detail.</p>&#13;
<p id="ch5tab3" class="tabcap"><strong>Table 5-3:</strong> Example Metrics for Evaluating Machine Learning Models</p>&#13;
<table class="all">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Dataset</strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Classifier</strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Accuracy</strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Precision</strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Recall</strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>F1 score</strong></p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>AUC</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent-tab">Goodware vs. Banking Trojans</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">RF</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0.9908</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0.9909</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0.9910</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0.9910</p></td>&#13;
<td style="vertical-align: top"><p class="noindent-tab">0.9931</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent"><em>Accuracy</em> measures how many predictions a classifier got right in the evaluation set (in other words, the proportion of TPs and TNs with respect to the total number of predictions). We calculate it as follows:</p>&#13;
<div class="equationc"><em>A</em> = (<em>TP</em> + <em>TN</em>)/(<em>TP</em> + <em>FP</em> + <em>TN</em> + <em>FN</em>)</div>&#13;
<p class="indent">While accuracy is an intuitive measurement, it has several issues when applied to the detection of malicious apps. Chief among these is that malware occurs very rarely, so most of the evaluation data is likely to be labeled as goodware if it is representative of real-world conditions. This means that a classifier can obtain a very good accuracy rating simply by predicting that every app is goodware. It isn’t uncommon for less than 1 percent of samples to be malware; in that case, such a classifier would have over 99 percent accuracy.</p>&#13;
<p class="indent"><em>Precision</em> measures how accurate our classifier is when it correctly predicts an app to be malware. We calculate it as follows:</p>&#13;
<div class="equationc"><em>P</em> = <em>TP</em>/(<em>TP</em> + <em>FP</em>)</div>&#13;
<p class="indent">This metric captures the percentage of items predicted to belong to a class that were actually in the class. However, it does not capture the full <span epub:type="pagebreak" id="page_176"/>picture. Let’s consider a set that contains 100 samples, of which 50 are malware. If our classifier predicts that only 1 sample from the set is malware and that 99 are goodware, it will have 100 percent precision, but it isn’t doing a very good job at finding malware.</p>&#13;
<p class="indent"><em>Recall</em> is a complementary measurement to precision that computes how many positive samples a classifier misses. We calculate it as follows:</p>&#13;
<div class="equationc"><em>R</em> = <em>TP</em>/(<em>TP</em> + <em>FN</em>)</div>&#13;
<p class="indent">Recall by itself might not be a good performance indicator, as a classifier that predicts everything to be malware will achieve 100 percent recall. In general, we want a classifier to have both good precision <em>and</em> good recall.</p>&#13;
<p class="indent">One solution is to combine the two. For example, we sometimes calculate the <em>F1 score</em> of a classifier, or the harmonic mean of precision and recall. This value is an attempt to balance the two metrics to identify strong classifiers. Most malware classifiers produce an F1 score between 0 and 1, where 0 represents higher confidence that the prediction is goodware and 1 means that the app is malware.</p>&#13;
<p class="indent">The <em>receiver operating characteristic (ROC) curve</em> plots the performance of a classifier at various thresholds to help us pick a good threshold and compare different classifiers. A ROC graph has two axes. The <em>true positive rate</em> is the same as recall, and the <em>false positive rate</em> is calculated as follows:</p>&#13;
<div class="equationc"><em>FPR</em> = <em>FP</em>/(<em>FP</em> + <em>TN</em>)</div>&#13;
<p class="indent">The <em>area under the ROC curve (AUC)</em> gives an overall measurement of the classifier across all thresholds. To understand AUC, imagine sorting all the apps in an evaluation by the classifier’s score. AUC measures the probability of a randomly selected malware app having a higher score than a randomly selected goodware app. An ideal classifier would always provide lower scores to goodware than to malware; such a classifier would have an AUC of 1. A really bad classifier that does the opposite would have an AUC of 0, and a random classifier would have an AUC of 0.5. A sample ROC curve for the decision tree algorithm is shown in <a href="ch05.xhtml#ch5fig7">Figure 5-7</a>.</p>&#13;
<p class="indent">AUC has several advantages: notably, it is invariant to class skew (which occurs when the number of samples in one class far outnumbers that of the other class) and independent of specific thresholds. However, it treats both FPs and FNs equally. This might not be desirable if you want to make sure that no malware slips through. When you’re trying to protect a store like Google Play, for example, it’s better to err on the side of caution. That is, it’s preferable to manually review too many apps, even if they turn out to be goodware for the most part, than too few. So, you might instead want to pick a model that treats FPs as more desirable.</p>&#13;
<div class="image"><img id="ch5fig7" src="../images/ch05fig07.jpg" alt="Image" width="560" height="371"/></div>&#13;
<p class="figcap"><em>Figure 5-7: The ROC curve for the decision tree classifier</em></p>&#13;
<h3 class="h3" id="ch05lev4"><span epub:type="pagebreak" id="page_177"/><strong>Struggles of Machine Learning Classifiers</strong></h3>&#13;
<p class="noindent">In this section, we describe some common pitfalls that can adversely affect the performance of machine learning classifiers.</p>&#13;
<h4 class="h4" id="ch05lev1sec9"><strong><em>Identical Feature Vectors</em></strong></h4>&#13;
<p class="noindent">Some malware identification datasets include apps with identical feature vectors. This can happen in two broad cases. In the first case, different apps in the dataset are variants of one another. We call these <em>isomorphic</em> apps. It’s important to make sure that no app in the training data has corresponding isomorphic apps in the test data. Otherwise, they will artificially inflate the performance of the classifier. The second case occurs when the feature set is impoverished. This is also very serious, because it suggests that the selected features aren’t adequate enough to distinguish between apps that are truly different.</p>&#13;
<h4 class="h4" id="ch05lev1sec10"><strong><em>Balance vs. Imbalance</em></strong></h4>&#13;
<p class="noindent">Machine learning algorithms generally produce good models when the data is <em>balanced</em>, meaning it has reasonably comparable percentages of samples in the different classes. Conversely, some algorithms may struggle to perform when the data is massively imbalanced.</p>&#13;
<p class="indent">For instance, suppose we are trying to distinguish between Android spyware and goodware. In most general malware datasets available today, the number of spyware samples will be much smaller than the number of goodware samples. This may be due to the fact that the dataset was collected to <span epub:type="pagebreak" id="page_178"/>distinguish all forms of malware (not just spyware) from goodware. If a classifier is trained to separate spyware from goodware, the number of spyware samples would be relatively small compared to the number of goodware samples.</p>&#13;
<p class="indent">Such imbalances in the class sizes can severely affect the performance of classification algorithms.</p>&#13;
<h4 class="h4" id="ch05lev1sec11"><strong><em>Interpretability</em></strong></h4>&#13;
<p class="noindent">When detecting malware, security analysts must identify compelling evidence of an app’s malicious nature. Machine learning algorithms that return a verdict without providing information regarding why a particular app was flagged as malicious or benign may be useful for automated protection efforts but not for human-supported analysis. For confirmation, an analyst needs the algorithm to lead them toward the source of malicious behavior. Without any such guidance, verifying the algorithm’s verdict by analyzing the training set, the model, and its output becomes the equivalent of a complete app review and is like finding a needle in a haystack.</p>&#13;
<p class="indent">For that reason, many malware detection methods use handcrafted features that enable the analyst to find malicious parts of the code or demonstrate malicious behavior when the code is run. This is also one major reason why deep learning methods aren’t always the best option for malware detection in industry. It is difficult to understand how these machine learning algorithms produce their output.</p>&#13;
<h4 class="h4" id="ch05lev1sec12"><strong><em>Cross-Validation vs. Rolling Window Prediction</em></strong></h4>&#13;
<p class="noindent">Many machine learning–based malware detection algorithms have been evaluated in the literature using <em>k</em>-fold cross-validation, a technique that randomly splits the training data into <em>k</em> disjointed pieces, called <em>folds</em>, then performs <em>k</em> iterations over the folds. In each iteration, a corresponding fold (for example, the third fold on the third iteration) is removed and some classifier of a given type (such as an SVM) learns from all the remaining folds. The model then makes predictions about the removed fold, and its performance is computed on that iteration alone using a metric such as the AUC or F1 score, discussed earlier. The technique then makes a final assessment of a model type (for instance, SVM) by taking an aggregate value of the performance metric across all <em>k</em> folds.</p>&#13;
<p class="indent">However, the use of cross-validation may not always be appropriate, because malware evolves over time and <em>k</em>-fold cross validation ignores the time at which a given app in the training data was first released into the wild. As a consequence, the folds used during any iteration might include apps that were released into the wild <em>after</em> some of the apps in the removed fold. Intuitively, what this means is that we are likely predicting the status of some apps using information from the future, which can artificially boost the performance of the classifier.</p>&#13;
<p class="indent">In contrast, <em>rolling window prediction</em> sorts the apps <em>a</em><sub>1</sub>, <em>…</em>, <em>a</em><sub><em>n</em></sub> in the dataset based on the times at which each app entered the wild. We then assume that <span epub:type="pagebreak" id="page_179"/>we need at least <em>j</em> apps for decent training. For each <em>i</em>, such that <em>j</em> &lt; <em>i</em> ≤ <em>n</em>, we train on the dataset {<em>a</em><sub>1</sub>, <em>…</em>, <em>a</em><sub><em>j</em></sub>} and assess the performance <img class="middle" src="../images/math179-01.jpg" alt="image" width="22" height="23"/> of a given classifier. We consider the overall performance of the classifier to be the average of the <img class="middle" src="../images/math179-01.jpg" alt="image" width="22" height="23"/>s for <em>i</em> &lt; <em>j</em> ≤ <em>n.</em> This methodology avoids the possibility of using information from the future to predict the past.</p>&#13;
<h3 class="h3" id="ch05lev5"><strong>Up Next</strong></h3>&#13;
<p class="noindent">This chapter presented an overview of machine learning algorithms that are widely used in malware analysis and detection. In the next chapter, we explore the features we can use as input to these algorithms.</p>&#13;
<p class="indent">Though publicly available machine learning libraries are constantly evolving, you may find it worthwhile to explore the possibilities offered by the R, scikit-learn, and TensorFlow libraries. You can also find a list of app hashes, as well as the static and dynamic features described in this and the next chapter, at <a href="https://github.com/android-malware-ml-book"><em>https://github.com/android-malware-ml-book</em></a>. Use these libraries to learn the different types of predictive models capable of separating malware from goodware.<span epub:type="pagebreak" id="page_180"/></p>&#13;
</div>
</div>
<div style="float: none; margin: 10px 0px 10px 0px; text-align: center;"><p><a href="https://oceanofpdf.com"><i>OceanofPDF.com</i></a></p></div></body></html>