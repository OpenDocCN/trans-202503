<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h1" id="ch13"><span epub:type="pagebreak" id="page_211" class="calibre2"/><strong class="calibre3"><span class="big">13</span><br class="calibre18"/>HANDLING TIME SERIES AND TEXT DATA</strong></h2>
<div class="imagec"><img alt="Image" src="../images/common.jpg" class="calibre14"/></div>
<p class="noindent">A <em class="calibre13">time series</em> is a dataset indexed by time, usually at regular time intervals. Here are some familiar examples:</p>
<ul class="calibre15">
<li class="noindent3">Stock market data consisting of the price of a given equity on a daily basis, or even hourly, and so on</li>
<li class="noindent3">Weather data, daily or in even finer granularity</li>
<li class="noindent3">Demographic data, such as the number of births, say, monthly or even yearly, to plan for school capacity</li>
<li class="noindent3">Electrocardiogram data measuring electrical activity in the heart at regular time intervals</li>
</ul>
<p class="indent">A special type of time series is that of written or spoken speech. Here “time” is word positioning. If, say, we are working at the sentence level, and a sentence consists of eight words, there would be Word 1, Word 2, and so on through Word 8, with the index 1 through 8 playing the role of “time.”</p>
<p class="indent">The field of time series methodology has been highly developed by statisticians, economists, and the like. As usual, ML specialists have developed their own methods, mainly as applications of neural networks. The methods known as <em class="calibre13">recurrent neural networks (RNNs)</em> and <em class="calibre13">long short-term memories (LSTMs)</em> are especially notable.<span epub:type="pagebreak" id="page_212"/></p>
<p class="indent">Both the statistical and ML approaches use very subtle and intricate techniques whose mathematical content is well above the math level of this book. Nevertheless, one can still build some very powerful ML applications while sticking to the basics, and this chapter will have this theme. It will present methods to apply the <code>qe*</code>-series functions to general time series problems, and to a special kind of text recognition setting (that does not make use of the time series nature of the text).</p>
<h3 class="h2" id="ch13lev1">13.1 Converting Time Series Data to Rectangular Form</h3>
<p class="noindent">One often hears the terms <em class="calibre13">rectangular data</em> and <em class="calibre13">tabular data</em> in discussions of ML, referring to the usual <em class="calibre13">n</em> × <em class="calibre13">p</em> data frame or matrix of <em class="calibre13">n</em> rows, with each row representing one data point of <em class="calibre13">p</em> features. As a quick non−time series example we’ve used several times in this book, say we are trying to predict human weight from height and age, with a sample of 1,000 people. Then we would have <em class="calibre13">n</em> = 1000 and <em class="calibre13">p</em> = 2.</p>
<p class="indent">It’s clear that the words “rectangular” and “tabular” are allusions to the rectangular shape or table of the associated data frame or matrix. But this is rather misleading. Image data also has the form, such as <em class="calibre13">n</em> = 70000 and <em class="calibre13">p</em> = 28<sup class="calibre11">2</sup> = 784 for the MNIST data, yet image data is not referred to as rectangular.</p>
<p class="indent">In the case of time series, though, one in fact can convert a time series to rectangular form and then apply ML methods, which is what we’ll do here.</p>
<h4 class="h3" id="ch13lev1sec1"><em class="calibre22"><strong class="calibre3">13.1.1 Toy Example</strong></em></h4>
<p class="noindent">Say our training set time series <code>x</code> is (5,12,13,8,88,6). For concreteness, let’s say this is daily data, so we have six days of data here, which we’ll call day 1, day 2, and so on. On each day, we know the series values up to the present and wish to predict the next day.</p>
<p class="indent">We’ll use a <em class="calibre13">lag</em> of 2, which means that we predict a given day by the previous two. In <code>x</code> above, that means we:</p>
<ul class="calibre15">
<li class="noindent3">Predict day 3 from the 5 and 12</li>
<li class="noindent3">Predict day 4 from the 12 and 13</li>
<li class="noindent3">Predict day 5 from the 13 and 8</li>
<li class="noindent3">Predict day 6 from the 8 and 88</li>
</ul>
<p class="indent">Think of what the above description (“predicting the 13 . . .”) means in terms of our usual “X” (features matrix) and “Y” (outcomes vector) notation:<span epub:type="pagebreak" id="page_213"/></p>
<p class="center"><img alt="Image" src="../images/unch13equ01.jpg" class="calibre108"/></p>
<p class="noindent">Note that X has only 4 rows, not 6, and Y is of length 4, not 6. That is due to our lag of 2; we need 2 prior data points. So we cannot even start our analysis until day 3.</p>
<p class="indent">Here we will deal only with <em class="calibre13">univariate</em> time series. But we can also handle the multivariate case—for example, predicting daily temperature, humidity, and wind speed from their previous values.</p>
<h4 class="h3" id="ch13lev1sec2"><em class="calibre22"><strong class="calibre3">13.1.2 The regtools Function TStoX()</strong></em></h4>
<p class="noindent">The function <code>TStoX()</code> does what its name implies—converts a time series to an “X” matrix. “Y” is created too and returned in the final column. For the previous toy example, we have:</p>
<pre class="calibre16">&gt; <span class="codestrong">x &lt;- c(5,12,13,8,88,6)</span>
&gt; <span class="codestrong">w &lt;- TStoX(x,2)</span>
     [,1] [,2] [,3]
[1,]    5   12   13
[2,]   12   13    8
[3,]   13    8   88
[4,]    8   88    6</pre>
<p class="noindent">Our “X” data are then in the first two columns, and “Y” is the third column.</p>
<p class="indent">The function returns a matrix, which we can convert to a data frame if we wish:</p>
<pre class="calibre16">&gt; <span class="codestrong">wd &lt;- as.data.frame(w)</span>
&gt; <span class="codestrong">wd</span>
  V1 V2 V3
1  5 12 13
2 12 13  8
3 13  8 88
4  8 88  6</pre>
<p class="noindent">We could then use any of our <code>qe*</code>-series functions, such as random forests:</p>
<pre class="calibre16">qeRF(wd,'V3',holdout=NULL)</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_214"/>In other words, everything was done as before, with one exception: we cannot take our holdout set to be a random subset of the data, as the remaining data would no longer be for consecutive time periods. We will elaborate on this point shortly.</p>
<h3 class="h2" id="ch13lev2">13.2 The qeTS() Function</h3>
<p class="noindent">But, instead of calling, say, <code>qeRF()</code> “by hand,” as above, we again have a convenient wrapper, <code>qeTS()</code>, which transforms from time series format to “X, Y” form and then applies our favorite ML method to the result. The wrapper’s call form is:</p>
<pre class="calibre16">qeTS(lag,data,qeName,opts=NULL,
   holdout=floor(min(1000, 0.1 * length(data))))</pre>
<p class="noindent">Here <code>qeName</code> is the quoted name of a <code>qe*</code>-series function—for example, <code>'qeRF'</code>.</p>
<p class="indent">The argument <code>opts</code> allows us to use nondefault versions of the arguments of the quoted-name function. For instance, to use k-NN and <em class="calibre13">k</em> = 10, write:</p>
<pre class="calibre16">&gt; <span class="codestrong">eus &lt;- EuStockMarkets</span>  # built-in R dataset
&gt; <span class="codestrong">tsout &lt;- qeTS(5,eus,'qeKNN',opts=list(k=10))</span>  # use k-NN with k = 10</pre>
<p class="indent">A comment should be made regarding <code>holdout</code>. While it plays its usual role in the <code>qe*</code>-series, note that cross-validation is usually difficult in time series contexts. We cannot choose for our holdout set some randomly chosen numbers from our data, since in time series we predict one datum from its immediately preceding, time-contiguous values. But here, we conduct the holdout operation on the output of <code>TStoX()</code>, whose output <em class="calibre13">is</em> rows of sets of contiguous values, so it works.</p>
<h3 class="h2" id="ch13lev3">13.3 Example: Weather Data</h3>
<p class="noindent">Here we will use some weather time series data collected by NASA, which is included in <code>regtools</code>.</p>
<pre class="calibre16">&gt; <span class="codestrong">data(weatherTS)</span>
&gt; <span class="codestrong">head(weatherTS)</span>
     LON       LAT YEAR MM DD DOY   YYYYMMDD  RH2M   T2M PRECTOT
1 151.81 -27.47999 1985  1  1   1 1985-01-01 48.89 25.11    1.07
2 151.81 -27.47999 1985  1  2   2 1985-01-02 41.78 28.42    0.50
3 151.81 -27.47999 1985  1  3   3 1985-01-03 40.43 27.53    0.03
4 151.81 -27.47999 1985  1  4   4 1985-01-04 46.42 24.65    0.10
5 151.81 -27.47999 1985  1  5   5 1985-01-05 50.77 26.54    2.13
6 151.81 -27.47999 1985  1  6   6 1985-01-06 58.57 26.81    5.32</pre>
<p class="noindent">That last column is precipitation. Let’s fit a model for it and then predict the first day after the end of the data, day 4018, based on day 4016 and day 4017:<span epub:type="pagebreak" id="page_215"/></p>
<pre class="calibre16">&gt; <span class="codestrong">ptot &lt;- weatherTS$PRECTOT</span>
&gt; <span class="codestrong">z &lt;- qeTS(2,ptot,'qeRF',holdout=NULL)</span>
&gt; <span class="codestrong">length(ptot)</span>
[1] 4017
&gt; <span class="codestrong">predict(z,ptot[4016:4017])</span>
       2
1.087949</pre>
<p class="noindent">So, we predict a bit more than 1 inch of rain.</p>
<p class="indent">We used a lag of 2 days here. How would other lag values fare? We could use <code>qeFT()</code> here, but things are a bit complicated. For example, there is no <code>yName</code> argument for <code>qeTS()</code>, so instead we use <code>replicMeans()</code> (see <a href="ch03.xhtml#ch03lev2sec2" class="calibre12">Section 3.2.2</a>).</p>
<p class="indent">How about a lag of 1 instead of 2? We call <code>replicMeans()</code>, asking it to execute</p>
<pre class="calibre16">&gt; <span class="codestrong">qeTS(1,ptot,"qeKNN")$testAcc</span></pre>
<p class="noindent">1,000 times and then report the mean of the resulting 1,000 values of <code>testAcc</code>:</p>
<pre class="calibre16">&gt; <span class="codestrong">replicMeans(1000,'qeTS(1,ptot,"qeKNN")$testAcc')</span>
[1] 2.116511</pre>
<p class="noindent">This gives us a Mean Squared Prediction Error of 2.12. Is that good? As usual, let’s compare this to how well we can predict from the mean alone:</p>
<pre class="calibre16">&gt; <span class="codestrong">mean(abs(ptot - mean(ptot)))</span>
[1] 2.626195</pre>
<p class="noindent">Ah, we’re in business.</p>
<p class="indent">What about other lags?</p>
<pre class="calibre16">&gt; <span class="codestrong">replicMeans(1000,'qeTS(1,ptot,"qeKNN")$testAcc')</span>
[1] 2.116511
&gt; <span class="codestrong">replicMeans(1000,'qeTS(2,ptot,"qeKNN")$testAcc')</span>
[1] 2.051895
&gt; <span class="codestrong">replicMeans(1000,'qeTS(3,ptot,"qeKNN")$testAcc')</span>
[1] 2.033376
&gt; <span class="codestrong">replicMeans(1000,'qeTS(4,ptot,"qeKNN")$testAcc')</span>
[1] 2.067625
&gt; <span class="codestrong">replicMeans(1000,'qeTS(5,ptot,"qeKNN")$testAcc')</span>
[1] 2.092022
&gt; <span class="codestrong">replicMeans(1000,'qeTS(6,ptot,"qeKNN")$testAcc')</span>
[1] 2.085409
&gt; <span class="codestrong">replicMeans(1000,'qeTS(7,ptot,"qeKNN")$testAcc')</span>
[1] 2.093377
&gt; <span class="codestrong">replicMeans(1000,'qeTS(8,ptot,"qeKNN")$testAcc')</span>
[1] 2.118068
&gt; <span class="codestrong">replicMeans(1000,'qeTS(9,ptot,"qeKNN")$testAcc')</span>
[1] 2.135797
&gt; <span class="codestrong">replicMeans(1000,'qeTS(10,ptot,"qeKNN")$testAcc')</span>
[1] 2.157187</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_216"/>It does seem that the lag makes some difference. A lag of 3 days seems best, though as usual, we must keep in mind the effect of sampling variation. (The <code>replicMeans()</code> function also provides a standard error, which is not shown here.)</p>
<p class="indent">How about trying some other ML methods? Let’s consider a linear model, since most classical time series methods use linear models:</p>
<pre class="calibre16">&gt; <span class="codestrong">replicMeans(1000,'qeTS(3,ptot,"qeLin")$testAcc')</span>
[1] 2.245138
<br class="calibre1"/>
&gt; <span class="codestrong">replicMeans(1000,'qeTS(3,ptot,"qePolyLin")$testAcc')</span>
[1] 2.167949</pre>
<p class="noindent">As noted, classical time series methods, for example, the <em class="calibre13">autoregressive</em> model, are linear. We see that a linear model doesn’t work so well on this particular dataset. Fitting a polynomial improves things substantially but still doesn’t match k-NN.</p>
<p class="indent">Maybe random forests?</p>
<pre class="calibre16">&gt; <span class="codestrong">replicMeans(1000,'qeTS(3,ptot,"qeRF")$testAcc')</span>
[1] 2.138265</pre>
<p class="noindent">It’s still not as good as k-NN. However, with hyperparameter tuning in both cases, either method might end up the victor.</p>
<h3 class="h2" id="ch13lev4">13.4 Bias vs. Variance</h3>
<p class="noindent">The value of the lag impacts bias and variance, though in possibly complex ways.</p>
<p class="indent">A larger lag clearly increases bias; time periods in the more distant past are likely less relevant. It’s similar to the problem of a large <em class="calibre13">k</em> in k-NN.</p>
<p class="indent">On the other hand, the variance aspect is tricky. A larger lag smooths out the day-to-day (or other temporal) variation—that is, it reduces variance. But a larger lag also increases <em class="calibre13">p</em>, the number of features, increasing variance. The overall effect is thus complex.<span epub:type="pagebreak" id="page_217"/></p>
<h3 class="h2" id="ch13lev5">13.5 Text Applications</h3>
<p class="noindent">The field of text analysis is highly complex, similar to that of the image recognition field. As in the latter case, in this book we can only scratch the surface, in two senses:</p>
<ul class="calibre15">
<li class="noindent3">We will limit ourselves to document classification, as opposed to, say, language translation.</li>
<li class="noindent3">We will limit ourselves to the bag-of-words model (see the next section). This approach merely relies on how often various words appear in a document and not on the order in which the words appear.</li>
</ul>
<p class="noindent">So, we do not cover advanced methods such as the aforementioned <em class="calibre13">recurrent neural networks (RNNs)</em>, or even more advanced methods such as <em class="calibre13">hidden Markov models (HMMs)</em>.</p>
<h4 class="h3" id="ch13lev5sec1"><em class="calibre22"><strong class="calibre3">13.5.1 The Bag-of-Words Model</strong></em></h4>
<p class="noindent">Say we wish to do automatic classification of newspaper articles. Our software notices that the words <em class="calibre13">bond</em> and <em class="calibre13">yield</em> are contained in some document and classifies it in the Financial category.</p>
<p class="indent">This is the <em class="calibre13">bag-of-words model</em>. We decide on a set of words, the “bag,” and compute the frequency of appearance of each word in each document class. These frequencies are often stored in a <em class="calibre13">document-term matrix (DTM)</em>, <code>d</code>. The entry <code>d[i,j]</code> is equal to the number of times word <code>j</code> appears in document <code>i</code> in our training set. Or, <code>d[i,j]</code> may simply be 1 or 0, indicating whether word <code>j</code> appears in document <code>i</code> at all.</p>
<p class="indent">The matrix <code>d</code> then becomes our “X,” with “Y” being the vector of class labels, such as Financial, Sports, and so on. Each row of X represents our data on one document, with a corresponding label in Y.</p>
<p class="indent">Again, this is a simple model. Our guess that the document above is in the Financial class may be incorrect if, say, a sentence in the document reads “The bond between family members will typically yield a stable family environment.” A more sophisticated analysis would account for, say, the words in between <em class="calibre13">bond</em> and <em class="calibre13">yield</em>. The bag-of-words model may, in some cases, be less accurate than a time series−based approach. Yet it is easy to implement and performs well in many applications.<span epub:type="pagebreak" id="page_218"/></p>
<h4 class="h3" id="ch13lev5sec2"><em class="calibre22"><strong class="calibre3">13.5.2 The qeText() Function</strong></em></h4>
<p class="noindent">And, of course, there is a <code>qeML</code> function for this, <code>qeText()</code>. It has this call form:</p>
<pre class="calibre16">qeText(data, yName, kTop = 50, stopWords = tm::stopwords("english"),
    qeName, opts = NULL, holdout = floor(min(1000, 0.1 * length(data))))</pre>
<p class="noindent">In the <code>data</code> argument, there is assumed one row per document, with the column indicated by <code>yName</code> stating the class of each document, such as Financial; the other column (there must be exactly two) stores the document texts. The argument <code>qeName</code> specifies the ML method to be used, and <code>opts</code> specifies optional arguments for that method. The term <em class="calibre13">stop words</em> refers to rather insignificant words such as <em class="calibre13">the</em> and <em class="calibre13">is</em>, which are ignored.</p>
<p class="indent">The role of the <code>kTop</code> argument is as follows: the software does a census of all the words in the documents in the training data and selects the <code>kTop</code> most frequent ones to use as features.</p>
<h4 class="h3" id="ch13lev5sec3"><em class="calibre22"><strong class="calibre3">13.5.3 Example: Quiz Data</strong></em></h4>
<p class="noindent">The <code>qeML</code> package has a built-in dataset named <code>quizzes</code>, consisting of the text of quizzes I’ve given in various courses. One might ask whether one can predict the course from the text.</p>
<pre class="calibre16">&gt; <span class="codestrong">data(quizzes)</span>
&gt; <span class="codestrong">str(quizzes)</span>
'data.frame':   143 obs. of  2 variables:
 $ quiz  : chr  " Directions: Work only on this sheet (on both sides,
...
...
 $ course: Factor w/ 5 levels "ECS132","ECS145",..: 3 3 3 3 3 3 3 3 3 3 ...</pre>
<p class="noindent">There were 143 quiz documents. The eighth of these will have the quiz text stored in <code>quizzes[8,1]</code> as one very long character string:</p>
<pre class="calibre16">&gt; <span class="codestrong">quizzes[8,1]</span>
...
...
largest thread number.  The code with print out
...
...</pre>
<p class="noindent">The course number is in <code>quizzes[8,2]</code>:</p>
<pre class="calibre16">&gt; <span class="codestrong">quizzes[8,2]</span>
[1] ECS158
Levels: ECS132 ECS145 ECS158 ECS256 ECS50</pre>
<p class="noindent">This was ECS 158, Introduction to Parallel Computation.<span epub:type="pagebreak" id="page_219"/></p>
<p class="indent">As an illustration, let’s pretend we don’t know the class of this document and try to predict it using random forests:</p>
<pre class="calibre16">&gt; <span class="codestrong">z &lt;- qeText(quizzes,qeName='qeRF')</span>
holdout set has  14 rows
<br class="calibre1"/>
&gt; <span class="codestrong">predict(z,quizzes[8,1])</span>
$predClasses
[1] "ECS158"
$probs
   ECS132 ECS145 ECS158 ECS256 ECS50
11  0.062  0.066  0.812  0.002 0.058</pre>
<p class="noindent">The predicted course is ECS 158.</p>
<h4 class="h3" id="ch13lev5sec4"><em class="calibre22"><strong class="calibre3">13.5.4 Example: AG News Dataset</strong></em></h4>
<p class="noindent">This dataset consists of short news articles in four categories: World, Sports, Business, and Sci/Tech. It is obtainable from the CRAN package <code>textdata</code>, which provides interfaces for downloading various text data testbeds:</p>
<pre class="calibre16">&gt; <span class="codestrong">library(textdata)</span>
&gt; <span class="codestrong">ag &lt;- dataset_ag_news()</span>
Do you want to download:
 Name: AG News
...
&gt; <span class="codestrong">agdf &lt;- as.data.frame(ag)</span>  # qe-series functions require data frames
&gt; <span class="codestrong">agdf[,1] &lt;- as.factor(agdf[,1])</span>  # qe requires a factor Y</pre>
<p class="indent">Let’s take a look around:</p>
<pre class="calibre16">&gt; <span class="codestrong">dim(ag)</span>
[1] 120000      3
&gt; <span class="codestrong">agdf[28,]</span>  # for example
      class                           title
28 Business HP shares tumble on profit news
                                      description
28 Hewlett-Packard shares fall after disappointing third-quarter profits,
while the firm warns the final quarter will also fall short of expectations.</pre>
<p class="indent">Plenty of data here with 120,000 documents. Well, maybe <em class="calibre13">too</em> much, as the run time may be long. For a quick example, let’s just take 10,000 rows:</p>
<pre class="calibre16">&gt; <span class="codestrong">smallSet &lt;- sample(1:nrow(agdf),10000)</span>
&gt; <span class="codestrong">agdfSmall &lt;- agdf[smallSet,]</span></pre>
<p class="indent"><span epub:type="pagebreak" id="page_220"/>So, let’s try fitting a model, say, SVM:</p>
<pre class="calibre16">&gt; <span class="codestrong">w &lt;- qeText(agdfSmall[,c(1,3)],'class',qeName='qeSVM')</span>
holdout set has  1000 rows
Loading required namespace: e1071
&gt; <span class="codestrong">w$testAcc</span>
[1] 0.461
&gt; <span class="codestrong">w$baseAcc</span>
[1] 0.7403333</pre>
<p class="noindent">Not too bad. We reduced a base error of 74 percent to 46 percent. The latter is still rather high, so we would next try tweaking the SVM hyperparameters. Note that <code>kTop</code> is also a hyperparameter! We should try different values for it too.</p>
<h3 class="h2" id="ch13lev6">13.6 Summary</h3>
<p class="noindent">We see here that, even without advanced methods, one may be able to fit good prediction models for time series and text data. In both cases, <code>qe*</code>-series functions <code>qeTS()</code> and <code>qeText()</code> enable convenient use of our favorite ML methods.</p>
</div></body></html>