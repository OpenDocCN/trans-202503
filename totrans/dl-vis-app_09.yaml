- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Classification
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: An important application of machine learning involves looking at a set of inputs,
    comparing each one to a list of possible classes (or categories), and assigning
    each input to its most probable class. This process is called *classification*
    or *categorization*, and we say that it’s carried out by a *classifier.* We can
    use classes for tasks as diverse as identifying the words someone has spoken into
    their cell phone, what animals are visible in a photograph, or whether a piece
    of fruit is ripe or not.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的一个重要应用是观察一组输入，将每个输入与一组可能的类（或类别）进行比较，并将每个输入分配给其最可能的类。这个过程称为*分类*或*类别化*，我们说它是由*分类器*执行的。我们可以使用类来处理各种任务，比如识别某人通过手机说出的词语，照片中可见的动物，或者水果是否成熟。
- en: In this chapter, we look at the basic ideas behind classification. We don’t
    consider specific classification algorithms here because we will get into those
    in Chapter 11\. Our goal now is just to become familiar with the principles. We
    also look at *clustering*, which is a way to automatically group together samples
    that don’t have labels. We wrap up by looking at how spaces of four and more dimensions
    can often foil our intuition, leading to potential problems when we’re training
    a deep learning system. Things can work in unexpected and surprising ways in spaces
    of more than three dimensions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨分类背后的基本思想。我们在这里不考虑具体的分类算法，因为这些内容会在第11章详细讨论。我们现在的目标只是熟悉这些原理。我们还将讨论*聚类*，它是一种自动将没有标签的样本分组的方式。最后，我们将探讨如何在四维及更高维度的空间中，我们的直觉往往会受到干扰，导致在训练深度学习系统时出现潜在问题。在超过三维的空间中，事物可能会以出乎意料且令人惊讶的方式运作。
- en: Two-Dimensional Binary Classification
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二维二元分类
- en: Classification is a big topic. Let’s start with a high-level overview of the
    process and then dig into some specifics.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是一个庞大的话题。让我们从一个高层次的概述开始，然后再深入探讨一些具体细节。
- en: A popular way to train a classifier uses *supervised learning*. We begin by
    gathering a collection of *samples*, or pieces of data, that we want to classify.
    We call this the *training set*. We also prepare a list of *classes,* or categories,
    such as what animals might be in a photo, or what genre of music should be assigned
    to an audio sample. Finally, we manually consider each example in the training
    set, and determine which class it should be assigned to. This is called that sample’s
    *label.*
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的训练分类器的方法是使用*监督学习*。我们首先收集一组*样本*，或者说我们想要分类的数据。这些数据被称为*训练集*。我们还准备一份*类别*列表，或者说分类标签，例如照片中可能出现的动物，或者应该分配给音频样本的音乐类型。最后，我们手动考虑训练集中的每个示例，并确定应该将其分配到哪个类别。这就是该样本的*标签*。
- en: Then we provide the computer with each sample, one at a time, but we don’t give
    it the label. The computer processes the sample and comes up with its own *prediction*
    of what class it should be assigned to. Now we compare the computer’s prediction
    with our label. If the classifier’s prediction doesn’t match our label, we modify
    the classifier a little so that it’s more likely to predict the correct class
    if it sees this sample again. We call this *training*, and say that the system
    is *learning*. We do this over and over, often with thousands or even millions
    of samples, recycled over and over. Our goal is to gradually improve the algorithm
    until its predictions match our labels so often that we feel it’s ready to be
    released to the world, where we expect it will be able to correctly classify new
    samples it’s never seen before. At that point we test it with new data to see
    how well it really works, and if it’s ready to be used for real.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将每个样本逐一提供给计算机，但不告诉它标签。计算机处理样本，并得出它认为应分配的类别的*预测*。现在，我们将计算机的预测与我们的标签进行比较。如果分类器的预测与我们的标签不匹配，我们会稍微调整分类器，使其在再次看到此样本时更有可能预测出正确的类别。我们称这个过程为*训练*，并说系统正在*学习*。我们会反复进行这一过程，通常是用成千上万甚至数百万个样本，一次又一次地重复。我们的目标是逐步改进算法，直到它的预测与我们的标签匹配的频率足够高，以至于我们认为它准备好投入实际应用，我们期望它能够正确分类之前从未见过的新样本。到那时，我们会用新数据测试它，看看它的效果如何，以及它是否准备好用于实际应用。
- en: Let’s look at this process a little more closely.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这个过程。
- en: To get started, let’s assume that our input data belongs to only two different
    classes. Using only two classes simplifies our discussion of classification, without
    missing any key points. Because there are only two possible labels (or classes)
    for every input, we call this *binary classification*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，让我们假设我们的输入数据仅属于两个不同的类别。仅使用两个类别简化了我们对分类的讨论，而不会遗漏任何关键点。因为每个输入只有两种可能的标签（或类别），我们称之为*二分类*。
- en: Another way to make things easy is to use two-dimensional (2D) data, so every
    input sample is represented by exactly two numbers. This is just complicated enough
    to be interesting, but still easy to draw, because we can show each sample as
    a point on the plane. In practical terms, it means that we have a bunch of points,
    or dots, on the page. We can use color and shape coding to show each sample’s
    label and the computer’s prediction. Our goal is to develop an algorithm that
    can predict every label accurately. When it does, we can turn the algorithm loose
    on new data that doesn’t have labels and rely on it to tell us which inputs belong
    to which class.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种简化方法是使用二维（2D）数据，因此每个输入样本由恰好两个数字表示。这恰到好处地既有趣又简单，因为我们可以将每个样本显示为平面上的一个点。在实际操作中，这意味着我们有一堆点，或称为小圆点，显示在页面上。我们可以使用颜色和形状编码来显示每个样本的标签和计算机的预测。我们的目标是开发一个算法，能够准确地预测每个标签。当它做到了这一点，我们就可以将该算法应用于没有标签的新数据，并依赖它告诉我们哪些输入属于哪个类别。
- en: We call this a *2D binary classification system*, where “2D” refers to the two
    dimensions of the point data, and “binary” refers to the two classes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这个为*二维二分类系统*，其中“二维”指的是数据点的两个维度，而“二分类”指的是两个类别。
- en: The first set of techniques that we’re going to look at are collectively called
    *boundary methods*. The idea behind these methods is that we can look at the input
    samples drawn on the plane and find a line or curve that divides up the space
    so that all the samples with one label are on one side of the curve (or boundary),
    and all those with the other label are on the other side. We’ll see that some
    boundaries are better than others when it comes to predicting future data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要看的第一组技术统称为*边界方法*。这些方法背后的思想是，我们可以查看绘制在平面上的输入样本，并找到一条将空间划分开来的线或曲线，使得所有带有一种标签的样本都位于曲线（或边界）的一侧，而所有带有另一标签的样本都位于另一侧。我们将看到，一些边界在预测未来数据时比其他边界更有效。
- en: Let’s make things concrete using chicken eggs. Suppose that we’re farmers with
    a lot of egg-laying hens. Each of these eggs might be fertilized and growing a
    new chick, or unfertilized. Let’s suppose that if we carefully measure some characteristics
    of each egg (say, its weight and length), we can tell if it’s fertilized or not.
    (This is completely imaginary, because eggs don’t work that way! But let’s pretend
    they do.) We bundle the two *features* of weight and length together to make a
    *sample*. Then we hand the sample to the classifier, which assigns it either the
    class “fertilized” or “unfertilized.”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过鸡蛋来具体化这个问题。假设我们是农场主，拥有大量下蛋的母鸡。这些鸡蛋可能已经受精并正在孵化小鸡，或者没有受精。假设如果我们仔细测量每个蛋的一些特征（例如它的重量和长度），就可以判断它是否受精。（这完全是虚构的，因为蛋并不是这么工作的！但我们假装它们是。）我们将重量和长度这两个*特征*结合起来形成一个*样本*。然后我们将样本交给分类器，它会将其归类为“受精”或“未受精”。
- en: Because each egg that we use for training needs a label, or a known correct
    answer, we use a technique called *candling* to decide if an egg is fertilized
    or not (Nebraska 2017). Someone skilled at candling is called a *candler*. Candling
    involves holding up the egg in front of a bright light source. Originally candlers
    used a candle, but now they use any strong light source. By interpreting the fuzzy
    dark shadows cast by the egg’s contents onto the eggshell, a skilled candler can
    tell if that egg is fertilized or not. Our goal is to get the classifier to give
    us the same results as the labels determined by a skilled candler.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们用于训练的每个蛋都需要一个标签，或者一个已知的正确答案，我们使用一种叫做*打光法*的技术来判断蛋是否受精（内布拉斯加州 2017）。擅长打光法的人被称为*candler*。打光法涉及将蛋在强光源前举起。最初，打光员使用蜡烛，但现在他们使用任何强光源。通过解释蛋内内容物在蛋壳上投射出的模糊暗影，熟练的打光员能够判断蛋是否受精。我们的目标是让分类器给出与熟练打光员确定的标签相同的结果。
- en: To summarize, we want our *classifier* (the computer) to consider each *sample*
    (an egg) and use its *features* (weight and length) to make a *prediction* (fertilized
    or unfertilized). Let’s start with a bunch of *training data* that gives the weight
    and length of some eggs. We can plot this data on a grid with weight on one axis
    and length on the other. [Figure 7-1](#figure7-1) shows our starting data. Fertilized
    eggs are shown with a red circle and unfertilized eggs with a blue box. With this
    data, we can draw a straight line between the two groups of eggs. Everything on
    one side of the line is fertilized, and everything on the other is unfertilized.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们希望我们的*分类器*（计算机）能够考虑每个*样本*（鸡蛋），并利用其*特征*（重量和长度）做出*预测*（受精或未受精）。让我们从一批*训练数据*开始，这些数据给出了某些鸡蛋的重量和长度。我们可以在一个网格上绘制这些数据，将重量作为一个轴，长度作为另一个轴。[图
    7-1](#figure7-1)展示了我们的初始数据。受精的鸡蛋用红色圆圈表示，未受精的鸡蛋用蓝色方框表示。通过这些数据，我们可以在两个鸡蛋群体之间画一条直线。直线一侧的所有鸡蛋都是受精的，另一侧的则是未受精的。
- en: '![F07001](Images/F07001.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![F07001](Images/F07001.png)'
- en: 'Figure 7-1: Classifying eggs. The red circles are fertilized. The blue squares
    are unfertilized. Each egg is plotted as a point given by its two dimensions of
    weight and length. The orange line separates the two clusters.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-1：鸡蛋分类。红色圆圈表示受精的，蓝色方块表示未受精的。每个鸡蛋被绘制为一个点，依据其重量和长度的两个维度。橙色线将两类分开。
- en: We’re done with our classifier! When we get new eggs (without known labels),
    we can just look to see which side of the line they lie on when plotted. The eggs
    that are on the fertilized side get assigned the class “fertilized,” and those
    on the unfertilized side are assigned the class “unfertilized.” [Figure 7-2](#figure7-2)
    shows the idea.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类器完成了！当我们得到新的鸡蛋（没有已知标签时），只需要查看它们被绘制时位于哪一边。位于受精一侧的鸡蛋被归类为“受精”，位于未受精一侧的鸡蛋被归类为“未受精”。[图
    7-2](#figure7-2)展示了这个概念。
- en: '![F07002](Images/F07002.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![F07002](Images/F07002.png)'
- en: 'Figure 7-2: Classifying eggs. Left: The boundary. Second from left: Showing
    the two regions, or domains, made by the boundary. Third from left: Four new samples
    to be classified. Far right: The classes assigned to the new samples.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-2：鸡蛋分类。左：边界。第二个从左：显示由边界划分的两个区域或域。第三个从左：四个新的样本待分类。最右边：新样本被分配的类别。
- en: Let’s say that this works out great for a few seasons, and then we buy a whole
    lot of chickens of a new breed. Just in case their eggs are different than the
    ones we’ve had before, we candle a day’s worth of new eggs from both breeds manually
    to determine if they’re fertilized or not, and then we plot the results as before.
    [Figure 7-3](#figure7-3) shows our new data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这几季都进展顺利，然后我们购买了一大批新种类的鸡。为了防止它们的鸡蛋和我们之前的不同，我们手动检查了来自两种鸡的新鸡蛋，并记录是否受精，然后像之前一样绘制结果。[图
    7-3](#figure7-3)展示了我们的新数据。
- en: '![F07003](Images/F07003.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![F07003](Images/F07003.png)'
- en: 'Figure 7-3: When we add some new types of chickens to our flock, determining
    which eggs are fertilized based just on the weight and length may get trickier.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-3：当我们将一些新的鸡种加入到我们的鸡群中时，仅根据重量和长度来判断鸡蛋是否受精可能会变得更加复杂。
- en: The two groups are still distinct, which is great, but now they’re separated
    by a squiggly curve rather than a straight line. That’s no problem, because we
    can use the curve just like the straight line from before. When we have additional
    eggs to classify, each egg gets placed on this diagram. If it’s in the red zone
    it’s predicted to be fertilized, and if it’s in the blue zone it’s predicted to
    be unfertilized. When we can split things up this nicely, we call the sections
    into which we chop up the plane *decision regions*, or *domains*, and the lines
    or curves between them *decision boundaries*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个群体仍然很清晰可分，这很棒，但现在它们是通过一条弯曲的曲线而不是直线来分开的。这没问题，因为我们可以像之前一样使用这条曲线。当我们有额外的鸡蛋需要分类时，每个鸡蛋会被放置在这个图表上。如果它位于红色区域，就预测为受精；如果它位于蓝色区域，就预测为未受精。当我们能够这样清晰地将事物区分开时，我们称将平面划分成的区域为*决策区域*，它们之间的线或曲线称为*决策边界*。
- en: Let’s suppose that word gets out and people love the eggs from our farm, so
    the next year we buy yet another group of chickens of a third variety. As before,
    we manually candle a bunch of eggs and plot the data, this time getting the diagram
    shown in [Figure 7-4](#figure7-4).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设消息传开了，人们都喜欢我们农场的鸡蛋，于是第二年我们又买了一批第三种品种的鸡。像之前一样，我们手动检查了一些鸡蛋并绘制了数据，这次得到的图表如[图
    7-4](#figure7-4)所示。
- en: '![F07004](Images/F07004.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![F07004](Images/F07004.png)'
- en: 'Figure 7-4: Our new purchase of chickens has made it much harder to distinguish
    the fertilized eggs from the unfertilized eggs.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-4：我们新购买的鸡使得区分受精蛋和未受精蛋变得更加困难。
- en: We still have a mostly red region and a mostly blue region, but we don’t have
    a clear way to draw a line or curve to separate them. Let’s take a more general
    approach and, rather than predict a single class with absolute certainty, let’s
    assign each possible class its own *probability.*
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然有一个主要是红色的区域和一个主要是蓝色的区域，但我们没有明确的办法来绘制一条线或曲线将它们分开。让我们采取一种更通用的方法，而不是以绝对确定性预测单一类别，而是为每个可能的类别分配一个*概率*。
- en: '[Figure 7-5](#figure7-5) uses color to represent the probability that a point
    in our grid has a particular class. For each point, if it’s bright red, then we’re
    very sure that egg is fertilized, while diminishing intensities of red correspond
    to diminishing probabilities of fertility. The same interpretation holds for the
    unfertilized eggs, shown in blue.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-5](#figure7-5) 使用颜色来表示我们网格中一个点属于某个特定类别的概率。对于每个点，如果它是亮红色的，那么我们非常确定该蛋已被受精，而红色的强度逐渐减弱对应着受精概率的降低。对于未受精的蛋，蓝色的区域具有相同的解释。'
- en: '![F07005](Images/F07005.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![F07005](Images/F07005.png)'
- en: 'Figure 7-5: Given the overlapping results shown at the far left, we can give
    every point a probability of being fertilized, as shown in the center where brighter
    red means the egg is more likely to be fertilized. The image at the far right
    shows a probability for the egg being unfertilized.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-5：根据最左侧显示的重叠结果，我们可以为每个点分配一个受精的概率，如中央所示，亮红色表示该蛋更可能被受精。最右侧的图像显示了该蛋未受精的概率。
- en: An egg that lands solidly in the dark-red region is probably fertilized, and
    an egg in the dark-blue region is probably unfertilized. In other places, the
    right class isn’t so clear. How we proceed depends on our farm’s policy. We can
    use our ideas of accuracy, precision, and recall from Chapter 3 to shape that
    policy and tell us what kind of curve to draw to separate the classes. For example,
    let’s say that “fertilized” corresponds to “positive.” If we want to be very sure
    we catch all the fertilized eggs and don’t mind some false positives, we might
    draw a boundary as in the center of [Figure 7-6](#figure7-6).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个落在深红色区域的蛋很可能是受精的，而落在深蓝色区域的蛋很可能是未受精的。在其他地方，正确的类别并不那么明确。我们的处理方式取决于农场的政策。我们可以利用第
    3 章中的准确率、精确率和召回率的概念来制定这个政策，并告诉我们应该绘制什么样的曲线来分隔这些类别。例如，假设“受精”对应“正例”。如果我们想非常确定地捕捉到所有受精蛋并且不介意一些假阳性，我们可以像
    [图 7-6](#figure7-6) 中的中心一样绘制边界。
- en: On the other hand, if we want to find all the unfertilized eggs, and don’t mind
    false negatives, we might draw the boundary as in the right of [Figure 7-6](#figure7-6).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们想找出所有未受精的蛋，并且不介意假阴性，我们可以像 [图 7-6](#figure7-6) 右侧那样绘制边界。
- en: '![F07006](Images/F07006.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![F07006](Images/F07006.png)'
- en: 'Figure 7-6: Given the results at the far left, we may choose a policy shown
    in the center, which accepts some false positives (unfertilized eggs classified
    as fertilized). Or we may prefer to correctly classify all unfertilized eggs,
    with the policy shown at the right.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-6：根据最左侧的结果，我们可以选择中央所示的策略，接受一些假阳性（未受精的蛋被分类为受精）。或者我们可以选择正确分类所有未受精的蛋，采用最右侧的策略。
- en: 'When the decision regions have sharp boundaries and don’t overlap, as in the
    center and right images in [Figure 7-6](#figure7-6), the probabilities for each
    sample are easy: the class where the sample falls is a certainty with probability
    1, and the other classes have probability 0\. But in the more frequent case, when
    the regions are fuzzy or overlap, as in [Figure 7-5](#figure7-5), both classes
    might have nonzero probabilities.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当决策区域具有明显的边界且不重叠时，如 [图 7-6](#figure7-6) 中的中央和右侧图像，每个样本的概率很容易计算：样本所在的类别是确定的，概率为
    1，其他类别的概率为 0。但在更常见的情况下，当区域模糊或重叠时，如 [图 7-5](#figure7-5) 中所示，两个类别可能都有非零概率。
- en: 'In practice, eventually we always have to convert probabilities into a decision:
    is this egg fertilized or not? Our final decision is influenced by the computer’s
    prediction, but ultimately, we also need to account for human factors and what
    the decision means for us.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，最终我们总是需要将概率转化为一个决策：这个蛋是受精的还是未受精的？我们的最终决策受到计算机预测的影响，但最终我们还需要考虑人为因素以及这个决策对我们的意义。
- en: 2D Multiclass Classification
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2D 多类别分类
- en: Our eggs are selling great, but there’s a problem. We’ve only been distinguishing
    between fertilized and unfertilized eggs. As we learn more about eggs, we discover
    that there are two different ways an egg ends up as unfertilized. An egg that
    is unfertilized because it was never fertilized is called a *yolker*. These are
    good for eating. Fertilized eggs we can sell to other farmers are called *winners*.
    But in some fertilized eggs the developing embryo stopped growing for some reason
    and died. Such an egg is called a *quitter* (Arcuri 2016). We don’t want to sell
    quitters, because they can burst unexpectedly and spread harmful bacteria. We
    want to identify the quitters and dispose of them.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have three classes of egg: winners (viable fertilized eggs), yolkers
    (safe unfertilized eggs), and quitters (unsafe fertilized eggs). As before, let’s
    pretend that we can tell these three kinds of eggs apart just on the basis of
    their weight and length. [Figure 7-7](#figure7-7) shows a set of measured eggs,
    along with the classes we manually assigned to them by candling the eggs.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![F07007](Images/F07007.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-7: Left: Three classes of eggs. The red circles are fertilized. The
    blue squares are unfertilized but edible yolkers. The yellow triangles are quitters
    that we want to remove from our incubators. Right: Possible boundaries and regions
    for each class.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: The job of assigning one of these three classes to new inputs is called *multiclass
    classification*. When we have multiple classes, we again find the boundaries between
    the regions associated with different classes. When a trained multiclass classifier
    has been released to the world and receives a new sample, it determines which
    region the sample falls into, and then assigns that sample the class corresponding
    to that region.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we might add in more features (or dimensions) to each sample,
    such as egg color, average circumference, and the time of day the egg was laid.
    This would give us a total of five numbers per egg. Five dimensions is a weird
    place to think about, and we definitely can’t draw a useful picture of it. But
    we can reason by analogy to the situation we can picture. In 2D, our data points
    tended to clump together in their locations, allowing us to draw boundary lines
    (and curves) between them. In higher dimensions, the same thing is true for the
    most part (we discuss this idea more near the end of the chapter). Just as we
    broke up our 2D square into several smaller 2D shapes, each one for a different
    class, we can break up our 5D space into multiple, smaller 5D shapes. These 5D
    regions also each define a different class.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: The math doesn’t care how many dimensions we have, and the algorithms we build
    upon the math don’t care, either. That’s not to say that we, as people, don’t
    care, because typically, as the number of dimensions goes up, the running time
    and memory consumption of our algorithms goes up as well. We’ll come back to some
    more issues involved in working with high-dimensional data at the end of the chapter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数学原理不关心我们有多少个维度，我们基于这些数学原理构建的算法也不在乎。这并不是说我们作为人类不在乎，因为通常情况下，维度数越多，算法的运行时间和内存消耗也会相应增加。我们将在本章末尾回到处理高维数据时涉及的一些问题。
- en: Multiclass Classification
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多分类分类
- en: Binary classifiers are generally simpler and faster than multiclass classifiers.
    But in the real world, most data have multiple classes. Happily, instead of building
    a complicated multiclass classifier, we can build a whole bunch of binary classifiers
    and combine their results to produce a multiclass answer. Let’s look at two popular
    methods for this technique.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类器通常比多分类器更简单、更快速。但在实际应用中，大多数数据都有多个类别。幸运的是，我们可以通过构建多个二分类器并结合它们的结果来生成一个多分类的答案，而无需构建一个复杂的多分类器。让我们看看两种流行的方法来实现这一技术。
- en: One-Versus-Rest
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一对其余
- en: 'Our first technique goes by several names: *one-versus-rest* *(OvR)*, *one-versus-all*
    *(OvA)*, *one-against-all* *(OAA)*, or the *binary relevance* method. Let’s suppose
    that we have five classes for our data, named with the letters A through E. Instead
    of building one complicated classifier that assigns one of these five labels,
    let’s instead build five simpler, binary classifiers and name them A through E
    for the class each one focuses on. Classifier A tells us whether a given piece
    of data does or does not belong to class A. Because it’s a binary classifier,
    it has one decision boundary that splits the space into two regions: class A and
    everything else. We can now see where the name “one-versus-rest” comes from. In
    this classifier, class A is the one, and classes B through E are the rest.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一种技术有几个名字：*一对多*（*OvR*）、*一对所有*（*OvA*）、*一对其余*（*OAA*）或*二元相关性*方法。假设我们有五个类别的数据，分别用字母
    A 到 E 命名。我们不是构建一个复杂的分类器来分配这五个标签，而是构建五个更简单的二分类器，并分别命名为 A 到 E，表示每个分类器专注于的类别。分类器
    A 告诉我们一个给定数据是否属于类别 A。由于它是一个二分类器，它有一个决策边界，将空间划分为两个区域：类别 A 和其他类别。我们现在可以理解“一对其余”这个名称的来源了。在这个分类器中，类别
    A 是“一个”，类别 B 到 E 是“其余”。
- en: Our second classifier, named Classifier B, is another binary classifier. This
    time it tells us whether a sample is, or is not, in class B. In the same way,
    Classifier C tells us whether a sample is or is not in class C, and Classifiers
    D and E do the same thing for classes D and E. [Figure 7-8](#figure7-8) summarizes
    the idea. Here we used an algorithm that takes into account all of the data when
    it builds the boundary for each classifier.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个分类器，命名为分类器 B，是另一个二分类器。它告诉我们一个样本是否属于类别 B。以此类推，分类器 C 告诉我们一个样本是否属于类别 C，而分类器
    D 和 E 则分别对类别 D 和 E 执行相同的操作。[图 7-8](#figure7-8) 总结了这个概念。在这里，我们使用了一种算法，它在为每个分类器构建边界时考虑了所有数据。
- en: '![f07008](Images/f07008.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![f07008](Images/f07008.png)'
- en: 'Figure 7-8: One-versus-rest classification. Top row: Samples from five different
    classes. Bottom row: The decision regions for five different binary classifiers.
    The colors from purple to pink show increasing probability that a point belongs
    to that class.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-8：一对多分类。上排：来自五个不同类别的样本。下排：五个不同二分类器的决策区域。从紫色到粉色的颜色显示了该点属于某类别的概率逐渐增加。
- en: Note that some locations in the 2D space can belong to more than one class.
    For example, points in the upper-right corner have nonzero probabilities from
    classes A, B, and D.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，二维空间中的某些位置可能属于多个类别。例如，右上角的点来自类别 A、B 和 D，且它们的概率非零。
- en: To classify an example, we run it through each of our five binary classifiers
    in turn, getting back the probability that the point belongs to each class. We
    then find the class with the largest probability, and that’s the class the point
    is assigned to. [Figure 7-9](#figure7-9) shows this in action.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对一个样本进行分类，我们依次通过我们的五个二分类器，得到该点属于每个类别的概率。然后，我们找到概率最大的类别，这就是该点被分配到的类别。[图 7-9](#figure7-9)
    展示了这一过程的实际操作。
- en: '![F07009](Images/F07009.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![F07009](Images/F07009.png)'
- en: 'Figure 7-9: Classifying a sample using one-versus-rest. The new sample is the
    black dot.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-9：使用一对其余方法对样本进行分类。新样本是黑色的点。
- en: In [Figure 7-9](#figure7-9), the first four classifiers all return low probabilities.
    The classifier for class E assigns the point a larger probability than the others,
    so the point is predicted to be from class E.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 7-9](#figure7-9)中，前四个分类器的返回概率都较低。类别E的分类器为该点分配了比其他分类器更大的概率，因此该点被预测为来自类别E。
- en: The appeals of this approach are its conceptual simplicity and its speed. The
    downside is that we have to teach (that is, learn boundaries for) five classifiers
    instead of just one, and we have to then classify every input sample five times
    to find which class it belongs to. When we have large numbers of classes with
    complex boundaries, the time required to run the sample through lots of binary
    classifiers can add up. On the other hand, we can run all five classifiers in
    parallel, so if we have the right equipment, we can get our answers in the same
    amount of time it takes for any one of them. For any application, we need to balance
    the tradeoffs depending on our time, budget, and hardware constraints.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的吸引力在于它的概念简单且速度较快。缺点是我们需要训练五个分类器，而不是仅仅一个，并且随后我们必须对每一个输入样本进行五次分类，才能找出它属于哪个类别。当类别数较多且边界复杂时，运行大量二元分类器所需的时间会积累起来。另一方面，我们可以将所有五个分类器并行运行，因此如果我们有合适的设备，所有分类器的运行时间将与其中任何一个分类器的运行时间相同。对于任何应用，我们需要根据时间、预算和硬件限制来平衡这些权衡。
- en: One-Versus-One
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一对一
- en: Our second approach that uses binary classifiers for multiple classes is called
    *one-versus-one* *(OvO)*, and it uses even more binary classifiers than OvR. The
    general idea is to look at every pair of classes in our data and build a classifier
    for just those two classes. Because the number of possible pairings goes up quickly
    as the number of classes increases, the number of classifiers in this method also
    grows quickly with the number of classes. To keep things manageable, let’s work
    with only four classes this time, as shown in [Figure 7-10](#figure7-10).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二种方法是使用二元分类器处理多类别问题，称为*一对一*（*OvO*），它使用的二元分类器数量比一对多（OvR）还要多。一般来说，方法的核心思想是查看数据中每一对类别，并为这两类建立一个分类器。由于随着类别数的增加，可能的配对数迅速增加，因此这种方法中的分类器数量也会随着类别数的增加而迅速增长。为了让事情更容易管理，这次我们只考虑四个类别，如[图
    7-10](#figure7-10)所示。
- en: '![F07010](Images/F07010.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![F07010](Images/F07010.png)'
- en: 'Figure 7-10: Left: Four classes of points for demonstrating OvO classification.
    Right: Names for the clusters.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-10：左侧：展示OvO分类的四个类别点。右侧：各簇的名称。
- en: We start with a binary classifier that is trained with data that is onlyfrom
    classes A and B. For the purposes of training this classifier, we simply leave
    out all the samples that are not labeled A or B, as if they don’t even exist.
    This classifier finds a boundary that separates the classes A and B. Now every
    time we feed a new sample to this classifier, it tells us whether it belongs to
    class A or B. Since these are the only two options available to this classifier,
    it classifies every point in our dataset as either A or B, even when it’s neither
    one. We’ll soon see why this is okay.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个二元分类器开始，该分类器仅使用来自类别A和B的数据进行训练。为了训练这个分类器，我们仅仅排除掉所有未标记为A或B的样本，就好像它们根本不存在一样。这个分类器找到一个边界，将类别A和B分开。现在，每次我们将一个新样本输入到这个分类器时，它会告诉我们该样本属于类别A还是B。由于这是该分类器唯一可以选择的两个选项，因此它会将数据集中每个点都分类为A或B，即使它其实不是这两个中的任何一个。我们很快就会明白为什么这样做是可以的。
- en: Next, we make a classifier trained with only data from classes A and C and another
    for classes A and D. The top row of [Figure 7-11](#figure7-11) shows this graphically.
    Now we keep going for all the other pairings, building binary classifiers that
    are trained with data onlyfrom classes B and C, and B and D, as in the second
    row of [Figure 7-11](#figure7-11). Finally, we get to the last pairing of classes
    C and D, in the bottom row of [Figure 7-11](#figure7-11). The result is that we
    have six binary classifiers, each of which tells us which of two specific classes
    the data most belongs to.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们建立一个仅使用类别A和C的数据进行训练的分类器，另一个则用于类别A和D。[图 7-11](#figure7-11)的顶部行以图形方式展示了这一过程。接着，我们继续进行所有其他配对，构建仅使用类别B和C以及B和D的数据进行训练的二元分类器，正如[图
    7-11](#figure7-11)第二行所示。最后，我们到达类别C和D的配对，这一过程展示在[图 7-11](#figure7-11)的底行。最终的结果是我们有了六个二元分类器，每个分类器告诉我们数据最可能属于哪个特定的类别。
- en: To classify a new example, we run it through all six classifiers, and then we
    select the label that comes up the most often. In other words, each classifier
    votes for one of two classes, and we declare the winner to be the class with the
    most votes. [Figure 7-12](#figure7-12) shows one-versus-one in action.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对一个新示例进行分类，我们需要将其传入所有六个分类器，然后选择出现次数最多的标签。换句话说，每个分类器都会投票选出两个类中的一个，最终我们将出现最多票数的类别作为该样本的预测类。[图
    7-12](#figure7-12) 展示了 OvO 方法的实际应用。
- en: '![F07011](Images/F07011.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![F07011](Images/F07011.png)'
- en: 'Figure 7-11: Building the six binary classifiers we use for performing OvO
    classification on four classes. Top row: Left to right, binary classifiers for
    classes A and B, A and C, and A and D. Second row: Left to right, binary classifiers
    for classes B and C, and B and D. Bottom row: A binary classifier for classes
    C and D.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-11：构建用于执行 OvO 分类的六个二元分类器，分类四个类。顶部行：从左到右，分别是 A 和 B、A 和 C、A 和 D 的二元分类器。第二行：从左到右，分别是
    B 和 C、B 和 D 的二元分类器。底部行：C 和 D 类的二元分类器。
- en: In this example, class A got three votes, B got one, C got two, and D got none.
    The winner is A, so that’s the predicted class for this sample.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，A 类得到了三票，B 类得到了四票，C 类得到了两票，而 D 类没有得到任何票数。最终的赢家是 A 类，因此它是该样本的预测类别。
- en: One-versus-one usually requires far more classifiers than one-versus-rest, but
    it’s sometimes appealing because it provides a clearer understanding of how the
    sample was evaluated with respect to each possible pair of classes. This can make
    a system more transparent, or explainable, when we want to know how it came up
    with its final answer. When there’s a lot of messy overlap between multiple classes,
    it can be easier for us humans to understand the final results using one-versus-one.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: One-versus-one 方法通常需要比 one-versus-rest 方法更多的分类器，但有时它更具吸引力，因为它可以提供关于样本如何在每一对可能的类别之间进行评估的更清晰理解。这可以使得系统更具透明性，或者在我们想了解系统如何得出最终答案时具有更好的可解释性。当多个类别之间存在大量混杂的重叠时，One-versus-one
    方法能够帮助我们人类更容易理解最终结果。
- en: The cost of this clarity is significant. The number of classifiers that we need
    for one-versus-one grows very fast as the number of classes increases. We’ve seen
    that with 4 classes we’d need 6 classifiers. Beyond that, [Figure 7-13](#figure7-13)
    shows just how quickly the number of binary classifiers we need grows with the
    number of classes. With 5 classes we’d need 10, with 20 classes we’d need 190,
    and to handle 30 classes we’d need 435 classifiers! Past about 46 classes we need
    more than 1,000\.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种清晰性的代价是巨大的。One-versus-one 方法需要的分类器数量随着类别数量的增加而迅速增长。我们已经看到，当类别数量为 4 时，我们需要
    6 个分类器。更进一步，[图 7-13](#figure7-13) 展示了随着类别数量增加，所需的二元分类器数量增长的速度有多快。对于 5 类，我们需要 10
    个分类器，20 类时需要 190 个，30 类时需要 435 个分类器！当类别数量超过 46 时，我们需要超过 1,000 个分类器。
- en: '![F07012](Images/F07012.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![F07012](Images/F07012.png)'
- en: 'Figure 7-12: OvO in action, classifying a new sample shown as a black dot.
    Top row: Left to right, votes are A, A, and A. Second row: Left to right, votes
    are C and B. Bottom row: Vote is class C.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-12：OvO 方法的实际应用，分类一个显示为黑点的新样本。顶部行：从左到右，投票结果为 A、A 和 A。第二行：从左到右，投票结果为 C 和 B。底部行：投票结果为
    C 类。
- en: '![F07013](Images/F07013.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![F07013](Images/F07013.png)'
- en: 'Figure 7-13: As we increase the number of classes, the number of binary classifiers
    we need for OvO grows very quickly.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-13：随着类别数量的增加，我们需要的 OvO 二元分类器数量增长得非常快。
- en: Each of these binary classifiers has to be trained, and then we need to run
    every new sample through every classifier, which is going to consume a lot of
    computer memory and time. At some point, it becomes more efficient to use a single,
    complex, multiclass classifier.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 每个二元分类器都需要进行训练，之后我们需要将每个新样本传入每个分类器，这将消耗大量的计算机内存和时间。某些情况下，使用一个单一、复杂的多类分类器会更高效。
- en: Clustering
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: We’ve seen that one way to classify new samples is to break up the space into
    different regions and then test a point against each region. A different way to
    approach the problem is to group the training set data itself into *clusters*,
    or similar chunks. Let’s suppose that our data has associated labels. How can
    we use those to make clusters?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，一种对新样本进行分类的方法是将空间划分为不同的区域，然后测试每个点与每个区域的关系。另一种方法是将训练集数据本身分成*簇*，或者相似的块。假设我们的数据有相关的标签，我们如何利用这些标签来进行聚类？
- en: In the left image of [Figure 7-14](#figure7-14) we have data with five different
    labels, shown by color. For these nicely separated groups, we can make clusters
    just by drawing a curve around each set of points, as in the middle image. If
    we extend those curves outward until they hit one another so that each point in
    the grid is colored by the cluster that it’s closest to, we can cover the entire
    plane as in the right image.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 7-14](#figure7-14)的左图中，我们有五个不同标签的数据，通过颜色显示。对于这些分离得很好的组，我们只需画出每组点的曲线，就能形成簇，如中间的图所示。如果我们将这些曲线向外延伸，直到它们互相交汇，让网格中的每个点都根据它最接近的簇被上色，我们就可以覆盖整个平面，如右图所示。
- en: '![F07014](Images/F07014.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![F07014](Images/F07014.png)'
- en: 'Figure 7-14: Growing clusters. Left: Starting data with five classes. Middle:
    Identifying the five groups. Right: Growing the groups outward so that every point
    has been assigned to one class.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-14：聚簇增长。左图：开始时的数据，包含五个类别。中图：识别出五个组。右图：将这些组向外扩展，直到每个点都被分配到一个类别。
- en: This scheme required that our input data had labels. What if we don’t have labels?
    If we can somehow automatically group our unlabeled data into clusters, we can
    still apply the technique we just described.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法要求我们的输入数据有标签。如果我们没有标签呢？如果我们能以某种方式自动将未标记的数据分成簇，我们仍然可以应用我们刚才描述的技术。
- en: Recall that problems that involve data without labels fall into the type of
    learning we call *unsupervised learning*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，涉及无标签数据的问题属于我们称之为 *无监督学习* 的学习类型。
- en: When we use an algorithm to automatically derive clusters from unlabeled data,
    we have to tell it how many clusters we want it to find. This “number of clusters”
    value is often represented with the letter *k* (this is an arbitrary letter and
    doesn’t stand for anything in particular). We say that *k* is a *hyperparameter*,
    or a value that we choose before training our system. Our chosen value of *k*
    tells the algorithm how many regions to build (that is, how many classes to break
    up our data into). Because the algorithm uses the geometric means, or averages,
    of groups of points to develop the clusters, the algorithm is called *k-means
    clustering*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用算法自动从未标记的数据中推导出簇时，我们必须告诉它我们希望它找到多少个簇。这个“簇的数量”值通常用字母 *k* 来表示（这是一个任意的字母，并没有特别的含义）。我们说
    *k* 是一个 *超参数*，即我们在训练系统之前选择的一个值。我们选择的 *k* 值告诉算法应该构建多少个区域（即，将我们的数据分成多少个类）。由于该算法使用点群的几何均值或平均值来构建簇，因此该算法被称为
    *k-均值聚类*。
- en: The freedom to choose the value of *k* is a blessing and a curse. The upside
    of having this choice is that if we know in advance how many clusters there ought
    to be, we can say so, and the algorithm produces what we want. Keep in mind that
    the computer doesn’t know where we think the cluster boundaries ought to go, so
    although it chops things up into *k* pieces, they may not be the pieces we’re
    expecting. But if our data is well separated, so samples are clumped together
    with big spaces between the clumps, we usually get what we expect. The more the
    cluster boundaries get fuzzy, or overlap, the more things can potentially surprise
    us.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 *k* 值的自由是一个祝福也是一个诅咒。拥有这一选择的好处是，如果我们事先知道应该有多少个簇，我们可以直接告诉算法，算法就会产生我们想要的结果。请记住，计算机并不知道我们认为簇的边界应该在哪里，因此尽管它将数据分成了
    *k* 个部分，但这些部分可能不是我们预期的样子。但是，如果我们的数据已经很好地分开了，样本聚集在一起且之间有较大的空隙，我们通常会得到我们期望的结果。簇的边界越模糊或重叠，事情就越有可能让我们感到意外。
- en: The downside of specifying *k* upfront is that we may not have any idea of how
    many clusters best describe our data. If we pick too few clusters, then we don’t
    separate our data into the most useful distinct classes. But if we pick too many
    clusters, then we end up with similar pieces of data going into different classes.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 提前指定 *k* 的缺点是我们可能不知道多少个簇最能描述我们的数据。如果我们选择的簇太少，那么我们就无法将数据分成最有用的不同类别。但是，如果我们选择的簇太多，那么我们最终可能会把相似的数据分到不同的类别中。
- en: To see this in action, consider the data in [Figure 7-15](#figure7-15). There
    are 200 unlabeled points, deliberately bunched up into five groups.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看这个过程，考虑一下[图 7-15](#figure7-15)中的数据。这里有 200 个未标记的点，故意将它们聚集成五组。
- en: '![F07015](Images/F07015.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![F07015](Images/F07015.png)'
- en: 'Figure 7-15: A set of 200 unlabeled points. They seem to visually fall into
    five groups.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-15：一组 200 个未标记的点。它们似乎在视觉上被分成了五组。
- en: '[Figure 7-16](#figure7-16) shows how *k*-means clustering splits up this set
    of points for different values of *k*. Remember, we give the algorithm the value
    of *k* as an argument before it begins working.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-16](#figure7-16)展示了*k*-均值聚类如何为不同的*k*值划分这组点。记住，我们在算法开始工作之前会提供*k*值作为参数。'
- en: '![F07016](Images/F07016.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![F07016](Images/F07016.png)'
- en: 'Figure 7-16: The result of automatically clustering our data in [Figure 7-15](#figure7-15)
    for values of *k* from 2 to 7'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-16：自动对[图 7-15](#figure7-15)中的数据进行聚类，*k*值从 2 到 7
- en: It’s no surprise that *k* = 5 is doing the best job on this data, but we’re
    using a cooked example in which the boundaries are easy to see. With more complicated
    data, particularly if it has more than two or three dimensions, it can be all
    but impossible for us to easily identify the most useful number of clusters beforehand.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 不足为奇的是，*k* = 5 在这组数据上表现得最好，但我们使用的是一个简单的示例，边界容易辨识。在更复杂的数据中，尤其是当数据维度超过两三个时，我们几乎不可能轻松地在事前识别出最有用的聚类数。
- en: All is not lost, though. A popular option is to train our clustering model several
    times, each time using a different value for *k*. By measuring the quality of
    the results, this *hyperparameter tuning* lets us automatically search for a good
    value of *k*, evaluating the predictions of each choice and reporting the value
    of *k* that performed the best. The downside, of course, is that this takes computational
    resources and time. This is why it’s so useful to *preview* our data with some
    kind of visualization tool prior to clustering. If we can pick the best value
    of *k* right away, or even come up with a range of likely values, it can save
    us the time and effort of evaluating values of *k* that won’t do a good job.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 但并非一切都失去希望。一种常见的选择是多次训练我们的聚类模型，每次使用不同的*k*值。通过衡量结果的质量，这种*超参数调优*让我们能够自动搜索一个合适的*k*值，评估每个选择的预测结果，并报告表现最好的*k*值。当然，缺点是，这需要计算资源和时间。这就是为什么在聚类之前使用某种可视化工具来*预览*数据如此有用。如果我们能够立刻选出最佳的*k*值，或者甚至给出一个可能值的范围，就能节省评估那些无法提供良好结果的*k*值的时间和精力。
- en: The Curse of Dimensionality
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度灾难
- en: We’ve been using examples of data with two features, because two dimensions
    are easy to draw on the page. But in practice, our data can have any number of
    features or dimensions. It might seem that the more features we have, the better
    our classifiers will be. It makes sense that the more features the classifiers
    have to work with, the better they should be able to find the boundaries (or clusters)
    in the data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在使用具有两个特征的数据示例，因为二维数据很容易在页面上绘制。但实际上，我们的数据可以有任意数量的特征或维度。似乎特征越多，我们的分类器效果越好。因为更多的特征意味着分类器可以使用更多的信息来找到数据中的边界（或聚类），这似乎是合乎逻辑的。
- en: That’s true to a point. Past that point, adding more features to our data actually
    makes things *worse*. In our egg-classifying example, we could add in more features
    to each sample, like the temperature at the time the egg was laid, the age of
    the hen, the number of other eggs in the nest at the time, the humidity, and so
    on. But, as we’ll see, adding more features often makes it harder, not easier,
    for the system to accurately classify the inputs.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这在某种程度上是正确的。超过这个点后，向数据中添加更多特征实际上会让情况变得*更糟*。在我们的鸡蛋分类示例中，我们可以为每个样本添加更多的特征，例如蛋下的温度、母鸡的年龄、巢中同时有的其他鸡蛋数量、湿度等。但正如我们将看到的，添加更多特征通常会使得系统更难，而不是更容易，准确地分类输入。
- en: 'This counterintuitive idea shows up so frequently that it has earned a special
    name: *the curse of dimensionality* (Bellman 1957). This phrase has come to mean
    different things in different fields. We’re using it in the sense that applies
    to machine learning (Hughes 1968). Let’s see how this curse comes about, and what
    it tells us about training.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个反直觉的观点出现得如此频繁，以至于它有了一个特别的名字：*维度灾难*（Bellman 1957）。这个词在不同领域有不同的含义。在这里我们使用它指的是机器学习中的含义（Hughes
    1968）。让我们来看看这个“灾难”是如何产生的，以及它告诉我们关于训练的哪些信息。
- en: Dimensionality and Density
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度与密度
- en: One way to picture the curse of dimensionality is to think about how a classifier
    goes about finding a boundary curve or surface. If there are only a few points,
    then the classifier can invent a huge number of curves or surfaces that all divide
    the data. In order to pick the boundary that will do the best job on future data,
    we’d want more training samples. Then the classifier can choose the boundary that
    best separates that denser collection. [Figure 7-17](#figure7-17) shows the idea
    visually.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 形象地理解维度灾难的一种方法是思考分类器如何找到边界曲线或边界面。如果只有少数几个点，分类器可以发明大量的曲线或面来划分数据。为了选择一个在未来数据中表现最好的边界，我们需要更多的训练样本。这样分类器就可以选择出最能分离这些更密集样本的边界。[图
    7-17](#figure7-17)直观地展示了这一思路。
- en: '![F07017](Images/F07017.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![F07017](Images/F07017.png)'
- en: 'Figure 7-17: To find the best boundary curve, we need a good density of samples.
    Left: We have very few samples, so we can construct lots of different boundary
    curves. Right: With a higher density of samples, we can find a good boundary curve.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-17：为了找到最佳的边界曲线，我们需要样本的良好密度。左图：我们有很少的样本，因此可以构建许多不同的边界曲线。右图：通过更高密度的样本，我们可以找到一条好的边界曲线。
- en: 'As [Figure 7-17](#figure7-17) shows, finding a good curve requires a dense
    collection of points. But here’s the key insight: as we add dimensions (or features)
    to our samples, the number of samples we need in order to maintain a reasonable
    density in the sample space explodes. If we can’t keep up with the demand, a classifier
    does its best, but it doesn’t have enough information to draw a good boundary.
    It is stuck in the situation of the left diagram in [Figure 7-17](#figure7-17),
    where it’s just guessing at the best boundary, which may lead to poor results
    on future data.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 7-17](#figure7-17)所示，找到一条好的曲线需要密集的点集。但关键的洞察是：当我们为样本添加维度（或特征）时，为了在样本空间中保持合理的密度，我们所需的样本数量会爆炸性增加。如果我们跟不上这种需求，分类器会尽力而为，但它没有足够的信息来绘制出好的边界。它就像[图
    7-17](#figure7-17)左侧的情况，正在猜测最佳的边界，而这可能会导致未来数据上的结果不佳。
- en: Let’s look at the problem of loss of density using our egg example. To keep
    things simple, let’s use a convention that all the features we may measure for
    our eggs (their volume, length, etc.) lie in the range 0 to 1\. Let’s start with
    a dataset that contains 10 samples, each with one feature (the egg’s weight).
    Since we have one dimension describing each egg, we can plot it on a one-dimensional
    line segment from 0 to 1\. Because we want to see how well our samples cover every
    part of this line, let’s break it up into pieces, or bins, and see how many samples
    fall into each bin. The bins are just conceptual devices to help us estimate density.
    [Figure 7-18](#figure7-18) shows how one set of data might fall on an interval
    [0,1] with 5 bins.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过鸡蛋的例子来看一下密度丧失的问题。为了简化问题，我们假设所有可能测量的鸡蛋特征（如体积、长度等）都位于0到1的范围内。首先，我们用一个包含10个样本的数据集，每个样本有一个特征（鸡蛋的重量）。由于每个鸡蛋有一个维度描述，我们可以将它绘制在一个从0到1的单维线段上。为了查看我们的样本是否覆盖了线段的每一部分，我们将线段划分成几个部分，或者叫做区间，看看每个区间内有多少个样本。区间只是一个帮助我们估算密度的概念工具。[图
    7-18](#figure7-18)显示了一个数据集如何分布在区间[0,1]上，且该区间被划分为5个区间。
- en: '![f07018](Images/f07018.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![f07018](Images/f07018.png)'
- en: 'Figure 7-18: Our 10 pieces of data have one dimension each.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-18：我们的10个数据点每个都有一个维度。
- en: There’s nothing special about the choices of 10 samples and 5 bins. We just
    chose them because it makes the pictures easy to draw. The core of our discussion
    is unchanged if we pick 300 eggs or 1,700 bins.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 选择10个样本和5个区间并没有什么特别之处。我们之所以选择它们，是因为这样画图更方便。如果我们选择300个鸡蛋或1700个区间，我们讨论的核心内容不会发生变化。
- en: The *density* of our space is the number of samples divided by the number of
    bins. It gives us a rough way to measure how well our data is filling up the space
    of possible values. In other words, do we have examples to learn from for most
    values of the input? We can see problems start to creep in if we have too many
    empty bins. In this case, the density is 10 / 5 = 2, telling us each bin (on average)
    has 2 samples. Looking at [Figure 7-18](#figure7-18), we see that this is a pretty
    good estimate for the average number of samples per bin. In one dimension, for
    this data, a density of 2 lets us find a good boundary.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们空间的*密度*是样本数除以箱子数。这为我们提供了一种粗略的方式来衡量我们的数据在多大程度上填充了可能值的空间。换句话说，我们是否有足够的示例来学习大多数输入值？如果我们有太多空的箱子，问题就开始显现出来。在这种情况下，密度是10
    / 5 = 2，这告诉我们每个箱子（平均）有2个样本。通过查看[图7-18](#figure7-18)，我们可以看到这是每个箱子中样本数的一个相当不错的估计。在一维数据中，密度为2让我们能够找到一个好的边界。
- en: Let’s now include the weight in each egg’s description. Because we now have
    two dimensions, we can pull our line segment of [Figure 7-18](#figure7-18) upward
    to make a 2D square, as in [Figure 7-19](#figure7-19).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将每个蛋的重量也包含进描述中。由于我们现在有了两个维度，我们可以将[图7-18](#figure7-18)中的线段向上拉，形成一个二维正方形，如[图7-19](#figure7-19)所示。
- en: '![f07019](Images/f07019.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![f07019](Images/f07019.png)'
- en: 'Figure 7-19: Our 10 samples are now each described by two measurements, or
    dimensions.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-19：我们的10个样本现在每个都由两个测量值或维度来描述。
- en: Breaking up each of the sides into 5 segments, as before, we have 25 bins inside
    the square. But we still have only 10 samples. That means most of the regions
    won’t have any data. The density now is 10 / (5 × 5) = 10 / 25 = 0.4, a big drop
    from the density of 2 we had in one dimension. As a result, we can draw lots of
    different boundary curves to split the data of [Figure 7-19](#figure7-19).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前一样，将每个边分成5个部分，我们得到了25个箱子在正方形内部。但我们仍然只有10个样本。这意味着大多数区域将没有数据。现在的密度是10 / (5
    × 5) = 10 / 25 = 0.4，远低于一维数据时的密度2。因此，我们可以绘制许多不同的边界曲线来划分[图7-19](#figure7-19)中的数据。
- en: Now let’s add a third dimension, such as the temperature at the time the egg
    was laid (scaled to a value from 0 to 1). To represent this third dimension, we
    push our square back into the page to form a 3D cube, as in [Figure 7-20](#figure7-20).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们添加第三个维度，比如蛋被产下时的温度（缩放到0到1之间的值）。为了表示这个第三个维度，我们将正方形推回页面，形成一个3D立方体，如[图7-20](#figure7-20)所示。
- en: '![F07020](Images/F07020.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![F07020](Images/F07020.png)'
- en: 'Figure 7-20: Now our 10 pieces of data are represented by three measurements,
    so we draw them in a 3D space.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-20：现在我们的10个数据点由三个测量值表示，因此我们将它们绘制在3D空间中。
- en: By splitting each axis into 5 pieces, we now have 125 little cubes, but we still
    have only 10 samples. Our density has dropped to 10 / (5 × 5 × 5) = 10 / 125 =
    0.08\. In other words, the average cell contains 0.08 samples. We’ve gone from
    a density of 2, to 0.4, to 0.08\. No matter where the data is located in 3D, the
    vast majority of the space is empty.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将每个轴分成5个部分，我们现在有了125个小立方体，但我们仍然只有10个样本。我们的密度已下降到10 / (5 × 5 × 5) = 10 / 125
    = 0.08。换句话说，平均每个单元格包含0.08个样本。我们的密度从2降到了0.4，再降到0.08。不管数据位于3D空间的哪个位置，绝大部分空间都是空的。
- en: Any classifier that splits this 3D data into two pieces with a boundary surface
    is going to have to make a big guess. The issue isn’t that it’s hard to separate
    the data, but rather that it’s too easy. It’s not clear how to best separate the
    data so that our system will *generalize*, that is, correctly classify points
    we get in the future. The classifier is going to have to classify lots of empty
    boxes as belonging to either one class or the other, and it just doesn’t have
    enough information to do that in a principled way.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 任何将这个3D数据分成两部分并设置边界面的分类器，都必须做出一个大猜测。问题不在于分隔数据困难，而在于太容易了。并不清楚如何最好地分隔数据，使我们的系统能够*泛化*，即正确分类我们未来得到的点。分类器将不得不将许多空的框分类为属于某一类，但它没有足够的信息以系统化的方式做到这一点。
- en: In other words, who knows where new samples are going to end up once our system
    is deployed? At this point, nobody. [Figure 7-21](#figure7-21) shows one guess
    at a boundary surface, but as we saw in [Figure 7-17](#figure7-17), we can fit
    all kinds of planes and curvy sheets through the big open spaces between the two
    sets of samples. Most of them are probably not going to generalize very well.
    Maybe this one is too close to the red balls. Or maybe it’s not close enough.
    Or maybe it should be a curved surface rather than a plane.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，谁知道我们的系统部署后，新样本会最终到达哪里？目前，没人知道。[图 7-21](#figure7-21) 显示了一个边界面猜测，但正如我们在[图
    7-17](#figure7-17)中看到的那样，我们可以通过两个样本集之间的大开放空间拟合各种平面和弯曲的面。它们中的大多数可能不会很好地进行泛化。也许这个面离红色小球太近了，或者也许它离得还不够近，或者也许它应该是一个弯曲的表面而不是平面。
- en: The expected low quality of this boundary surface when we use it to predict
    the classes for new data is not the fault of the classifier. This plane is a perfectly
    good boundary surface, given the data available to the classifier. The problem
    is that because the density of the samples is so low, the classifier just doesn’t
    have enough information to do a good job. The density drops like a rock with each
    added dimension, and as we add yet more features, the density continues to plummet.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们用这个边界面预测新数据的类别时，期望的低质量并不是分类器的错。这个平面是一个完全合适的边界面，考虑到分类器可用的数据。问题在于，由于样本的密度如此低，分类器根本没有足够的信息来做好工作。随着每增加一个维度，密度像石头一样急剧下降，随着我们添加更多特征，密度继续暴跌。
- en: '![F07021](Images/F07021.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![F07021](Images/F07021.png)'
- en: 'Figure 7-21: Passing a boundary surface through our cube, separating the red
    and blue samples'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-21：通过我们的立方体传递一个边界面，分隔红色和蓝色的样本
- en: It’s hard to draw pictures for spaces with more than three dimensions, but we
    can calculate their densities. [Figure 7-22](#figure7-22) shows a plot of the
    density of a space for our 10 samples as the number of dimensions goes up. Each
    curve corresponds to a different number of bins for each axis. Notice that as
    the number of dimensions goes up, the density drops to 0 no matter how many bins
    we use.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在三维以上的空间中很难画出图形，但我们可以计算它们的密度。[图 7-22](#figure7-22) 显示了随着维度数增加，我们的 10 个样本的空间密度变化图。每条曲线对应每个轴上不同的箱数。请注意，随着维度的增加，不管我们使用多少箱，密度都会下降到
    0。
- en: If we have fewer bins, we have a better chance of filling each one, but before
    long, the number of bins becomes irrelevant. As the number of dimensions goes
    up, our density always heads to 0\. This means our classifier ends up guessing
    at where the boundary should be. Including more features with our egg data improves
    the classifier for a while, because the boundary is better able to follow where
    the data is located. But eventually we need enormous amounts of data to keep up
    with the density demands made by those new features.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有更少的箱数，我们有更大的机会填满每个箱子，但很快，箱数的增加就变得无关紧要。随着维度数的增加，我们的密度总是趋向于 0。这意味着我们的分类器最终会在猜测边界应该在哪里。将更多特征与我们的蛋数据结合起来会在一段时间内改善分类器，因为边界能更好地跟随数据的位置。但最终，我们需要大量的数据来满足这些新特征所要求的密度需求。
- en: There are some special cases where the lack of density resulting from new features
    won’t cause problems. If our new features are redundant, then our existing boundaries
    are already fine and don’t need to change. Or if the ideal boundary is simple,
    like a plane, then increasing the number of dimensions doesn’t change that boundary’s
    shape. But in the general case, new features add refinement and details to our
    boundary. Because adding new dimensions to accommodate those features causes the
    density to drop toward 0, that boundary becomes harder to find, so the computer
    ends up essentially guessing at its shape and location.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些特殊情况，新特征所造成的密度缺失不会引发问题。如果我们的新特征是冗余的，那么现有的边界已经足够好，无需更改。或者如果理想的边界很简单，比如一个平面，那么增加维度的数量不会改变该边界的形状。但在一般情况下，新特征会为我们的边界增加精细化和细节。因为为适应这些特征而增加的新维度导致密度下降到接近
    0，这使得边界变得更难以找到，因此计算机会本质上在猜测它的形状和位置。
- en: '![F07022](Images/F07022.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![F07022](Images/F07022.png)'
- en: 'Figure 7-22: This is how the density of our points drops off as the number
    of dimensions increases for a fixed number of samples. Each of the colored curves
    shows a different number of bins along each axis.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-22：这是随着维度数增加，固定数量样本的点密度如何下降的示意图。每一条彩色曲线显示了沿每个轴的不同数量的箱数。
- en: The curse of dimensionality is a serious problem, and it could have made all
    attempts at classification fruitless. After all, without a good boundary, a classifier
    can’t do a good job of classifying. What saved the day is the *blessing of non-uniformity*
    (Domingos 2012), which we prefer to think of as the *blessing of structure*. This
    is the name for the observation that, in practice, the features that we typically
    measure, even in very high-dimensional spaces, tend not to spread around uniformly
    in the space of the samples. That is, they’re not distributed equally across the
    line, square, and cube we’ve seen, or the higher-dimensional versions of those
    shapes we can’t draw. Instead, they’re often clumped in small regions, or spread
    out on much simpler, lower-dimensional surfaces (such as a bumpy sheet or a curve).
    That means our training data and all future data will generally land in the same
    regions. Those regions will be dense, while the great majority of the rest of
    the space will be empty. This is good news, because it says that it doesn’t matter
    how we draw the boundary surface in those big empty regions, because no data will
    appear there. Having good density where the samples themselves actually fall is
    what we want and what we usually find.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 维度灾难是一个严重的问题，它可能让所有的分类尝试都变得徒劳无功。毕竟，没有一个好的边界，分类器就无法做好分类。挽救局面的是*非均匀性的祝福*（Domingos
    2012），我们更倾向于将其称为*结构的祝福*。这个术语描述了这样一个观察结果：在实际情况中，即使在非常高维的空间中，我们通常测量的特征也不会在样本的空间中均匀分布。也就是说，它们不会均匀分布在我们所看到的线、平面和立方体中，或者我们无法绘制的这些形状的高维版本中。相反，它们通常会聚集在小区域内，或者分布在更简单、更低维的表面上（例如一张起伏的纸面或一条曲线）。这意味着我们的训练数据和所有未来数据通常会落在相同的区域中。这些区域将是密集的，而其余大部分空间将是空的。这是个好消息，因为它表明，无论我们如何在那些大空区域中绘制边界面，都没关系，因为那里不会有数据出现。我们希望并通常能发现的，正是样本实际落入的地方的良好密度。
- en: Let’s see this structuring in action. In our cube of [Figure 7-20](#figure7-20),
    instead of having the samples spread around more or less uniformly throughout
    the cube, we might find that the samples from each class are located on the same
    horizontal plane. That means that any roughly horizontal boundary surface that
    splits the groups probably does a good job on new data, too, as long as those
    new values also tend to fall into those horizontal planes. [Figure 7-23](#figure7-23)
    shows the idea.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这种结构化的实际应用。在我们展示的[图7-20](#figure7-20)中，我们可能会发现每个类别的样本都位于同一水平面上，而不是在立方体中大致均匀地分布。这意味着，任何大致水平的边界面，分割这些群体的可能也能很好地适应新数据，只要这些新值也倾向于落在这些水平面上。[图7-23](#figure7-23)展示了这个想法。
- en: '![F07023](Images/F07023.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![F07023](Images/F07023.png)'
- en: 'Figure 7-23: In practice, our data often has some structure in the sample space.
    Left: Each group of samples is mostly in the same horizontal slice of the cube.
    Right: A boundary plane passed between the two sets of points.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-23：实际上，我们的数据在样本空间中通常有一些结构。左：每组样本大多位于立方体的同一水平切片中。右：一条边界面穿过了两组点之间。
- en: Most of the cube in [Figure 7-23](#figure7-23) is empty, and thus has low density,
    but the parts we’re interested in have high densities, so we can find a sensible
    boundary. Although the curse of dimensionality dooms us to have a low density
    in our overall space, even with enormous amounts of data, the blessing of structure
    says that we usually get reasonably high density where we need it. On the right
    side of [Figure 7-23](#figure7-23), we show a boundary plane across the middle
    of the cube. This does the job of separating the classes, but since they’re so
    well clumped, and since the space between them is empty, almost any boundary surface
    that splits the groups would probably do a good job of generalizing.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-23](#figure7-23)中的大部分立方体是空的，因此密度较低，但我们关心的部分具有较高的密度，因此我们可以找到一个合理的边界。尽管维度灾难注定使得我们在整体空间中拥有低密度，即使有大量数据，但结构的祝福告诉我们，我们通常会在需要的地方获得合理的高密度。在[图7-23](#figure7-23)的右侧，我们展示了一个穿过立方体中部的边界面。这可以完成分离类别的任务，但由于它们聚集得如此紧密，并且它们之间的空间是空的，几乎任何分割这些群体的边界面都可能在泛化时表现良好。'
- en: Note that both the curse and blessing are empirical observations, and not hard
    facts we can always rely on. So the best solution for this important practical
    problem is usually to fill out the sample space with as much data as we can get.
    In Chapter 10, we’ll see some ways to reduce the number of features in our data
    if it seems to be producing bad boundaries.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，维度灾难和维度优势都是经验观察，而不是我们可以始终依赖的硬性事实。因此，这个重要实际问题的最佳解决方案通常是尽可能多地填充样本空间，获取尽可能多的数据。在第10章中，我们将看到一些方法来减少数据中的特征数，如果它似乎导致不好的边界。
- en: The curse of dimensionality is one reason why machine learning systems are famous
    for requiring enormous amounts of data when they’re being trained. If the samples
    have lots of features (or dimensions), we need lots and lots of samples to get
    enough density to make good class predictions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 维度灾难是机器学习系统在训练时需要大量数据的原因之一。如果样本具有许多特征（或维度），我们需要大量的样本才能获得足够的密度，从而做出准确的分类预测。
- en: Suppose we want enough points to get a specific density, say 0.1 or 0.5\. How
    many points do we need as the number of dimensions increases? [Figure 7-24](#figure7-24)
    shows that the number of points needed explodes in a hurry.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要足够的点数来获得特定的密度，比如0.1或0.5。随着维度的增加，我们需要多少点呢？[图7-24](#figure7-24)显示了所需点数迅速激增的情况。
- en: '![F07024](Images/F07024.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![F07024](Images/F07024.png)'
- en: 'Figure 7-24: The number of points we need to achieve different densities, assuming
    we have five divisions on each axis'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-24：为了实现不同密度所需的点数，假设我们在每个轴上有五个划分
- en: Speaking generally, if the number of dimensions is low, and we have lots of
    points, then we probably have enough density for the classifier to find a surface
    that has a good chance of generalizing well to new data. The values for *low*
    and *lots* in that sentence depend on the algorithm we’re using and what our data
    looks like. There’s no hard and fast rule for predicting these values; we generally
    take a guess, see what performance we get, and then make adjustments. Generally
    speaking, when it comes to training data, more really is more. Get all the data
    you ethically can.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，如果维度较低，并且我们有大量的点，那么我们很可能已经有足够的密度，使得分类器可以找到一个有很大可能性能够很好地推广到新数据的表面。句子中的*低*和*大量*的具体值取决于我们使用的算法以及数据的特点。没有固定的规则来预测这些值；我们通常会做一个猜测，看看得到的性能如何，然后进行调整。一般来说，在训练数据方面，更多确实意味着更多。尽可能获取所有你道德上可以获得的数据。
- en: High-Dimensional Weirdness
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高维空间的奇异性
- en: 'Since the samples in real-world data usually have many features, we often work
    in spaces with many dimensions. We saw earlier that if the data is locally structured,
    we are often okay: our ignorance of the boundary in the big empty regions doesn’t
    work against us, since no input data lands there. But what if our data isn’t structured
    or clumped?'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现实世界中的数据样本通常具有许多特征，我们经常在具有多个维度的空间中工作。我们之前看到，如果数据具有局部结构，我们通常没问题：我们对大空旷区域边界的无知不会对我们产生负面影响，因为没有输入数据会落入那里。但如果我们的数据没有结构或没有聚类呢？
- en: 'It’s tempting to look at pictures like Figures 7-19 and 7-20 and let our intuition
    guide our choices in designing a learning system, thinking that spaces of many
    dimensions are like the spaces we’re used to, only bigger. This is very much not
    true! There’s a technical term for the characteristics of high-dimensional spaces:
    *weird*. Things simply work out in ways that we don’t expect. Let’s look at two
    cautionary tales about the weirdness of geometry in higher dimensions in order
    to train our intuitions not to jump to generalizations from the low-dimensional
    spaces we’re familiar with. This will help us stay on our toes when designing
    learning systems.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看图像，比如图7-19和7-20，凭直觉指导我们设计学习系统，认为多维空间就像我们习惯的空间，只是更大而已。但事实并非如此！对于高维空间的特征，有一个技术术语：*奇怪*。事物的发展往往出乎我们意料。让我们看两个关于高维几何奇异性的警示故事，以训练我们的直觉，避免从我们熟悉的低维空间跳跃到泛化的结论。这将帮助我们在设计学习系统时保持警觉。
- en: The Volume of a Sphere Inside a Cube
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个球体在立方体中的体积
- en: 'A famous example of the weirdness of high-dimensional spaces involves the volume
    of a sphere inside a cube (Spruyt 2014). The setup is simple: take a sphere and
    put it into a cube, then measure how much of the cube is occupied by the sphere.
    Let’s first do this in one, two, and three dimensions. Then we’ll keep going to
    higher dimensions and track the percentage of the cube occupied by the sphere
    as the number of dimensions goes up.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一个著名的高维空间怪异性例子涉及一个球体在立方体中的体积（Spruyt 2014）。设置很简单：取一个球体并放入一个立方体中，然后测量球体占据了立方体多少空间。让我们先在一维、二维和三维中做这个。然后我们继续向更高维度推进，跟踪随着维度增加，球体占据立方体的百分比。
- en: '[Figure 7-25](#figure7-25) gets us started in the 1D, 2D, and 3D cases.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-25](#figure7-25) 帮助我们了解 1D、2D 和 3D 的情况。'
- en: '![F07025](Images/F07025.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![F07025](Images/F07025.png)'
- en: 'Figure 7-25: Spheres in cubes. Left: A 1D “cube” is a line segment, and the
    “sphere” covers it completely. Middle: A 2D “cube” is a square, and the “sphere”
    is a circle that touches the edges. Right: A 3D cube surrounds a sphere, which
    touches the center of each face.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-25：立方体中的球体。左：一个 1D 的“立方体”是一个线段，而“球体”完全覆盖了它。中：一个 2D 的“立方体”是一个正方形，而“球体”是一个触及边缘的圆形。右：一个
    3D 的立方体包围着一个球体，球体接触到每个面的中心。
- en: In one dimension, our “cube” is just a line segment, and the sphere is a line
    segment that covers the whole thing. The ratio of the contents of the sphere to
    the contents of the “cube” is 1:1\. In 2D, our “cube” is a square, and the sphere
    is a circle that just touches the center of each of the four sides. The area of
    the circle divided by the area of the box is about 0.8\. In 3D, our cube is a
    normal 3D cube, and the sphere fits inside, just touching the center of each of
    the six faces. The volume of the sphere divided by the volume of the cube is about
    0.5.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在一维中，我们的“立方体”只是一个线段，而球体是一个覆盖整个线段的线段。球体内容与“立方体”内容的比率是 1:1。在二维中，我们的“立方体”是一个正方形，而球体是一个圆，正好触及四条边的中心。圆的面积与盒子面积的比大约是
    0.8。在三维中，我们的立方体是一个正常的 3D 立方体，球体正好适应其中，接触到六个面的中心。球体体积与立方体体积的比约为 0.5。
- en: The amount of space taken up by the sphere relative to the box enclosing it
    is dropping. If we work out the math and calculate the volume of the sphere to
    the volume of its cube for higher dimensions (where they’re called a *hypersphere*
    and *hypercube*), we get the results in [Figure 7-26](#figure7-26).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 球体相对于包围它的盒子所占的空间正在下降。如果我们进行数学计算，并计算更高维度下球体的体积与其立方体的体积（它们被称为 *超球体* 和 *超立方体*），我们得到的结果如
    [图 7-26](#figure7-26) 所示。
- en: '![F07026](Images/F07026.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![F07026](Images/F07026.png)'
- en: 'Figure 7-26: The amount of a box occupied by the largest sphere that fits into
    that box for different numbers of dimensions'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-26：不同维度下，最大球体在盒子中所占的体积比例
- en: The amount of volume taken up by the hypersphere drops down toward 0\. By the
    time we reach 10 dimensions, the largest sphere we can fit into its enclosing
    box takes up almost none of the box’s volume!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 超球体所占的体积逐渐接近 0。到了 10 维时，我们能放入它包围盒中的最大球体几乎不占据盒子的体积！
- en: This is not what most of us would expect based on our experience in the 3D world.
    We’ve found that if we take a hypercube (of many dimensions) and place inside
    of it the largest possible hypersphere (of the same number of dimensions) that
    will fit, the volume of that hypersphere is just about 0\.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在 3D 世界中的经验大相径庭。如果我们取一个超立方体（具有多个维度），并将其中能容纳的最大超球体（具有相同数量的维度）放入其中，那个超球体的体积几乎为
    0\。
- en: There’s no trick to this, and nothing’s going wrong. When we work out the math,
    this is what happens. We saw the pattern starting in the first three dimensions,
    but we can’t really draw the rest, so it’s hard to picture how in the heck this
    can be going on. But it does happen this way, because higher dimensions are weird.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这没有什么技巧，也没有出错。当我们计算数学时，结果就是这样。我们在前三个维度中看到了这个模式，但我们无法真正画出其余的维度，所以很难想象这到底是怎么发生的。但它确实是这样发生的，因为更高的维度很奇怪。
- en: Packing Hyperspheres into Hypercubes
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将超球体装入超立方体
- en: To make sure our intuitions get the message, let’s look at another weird result
    from packing hyperspheres into hypercubes. Let’s suppose we want to transport
    some particularly fancy oranges and make sure they’re not damaged in any way.
    Each orange’s shape is close to spherical, so we decide to ship them in cubical
    boxes protected by air-filled balloons. We put one balloon in each corner of the
    box, so that each balloon touches both the orange and the sides of the box. The
    balloons and orange are all perfect spheres. What’s the biggest orange we can
    put into a cube of a given size?
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们的直觉正确无误，让我们再来看一个来自将超球体装入超立方体的奇特结果。假设我们要运输一些特别精美的橙子，并确保它们在运输过程中不受任何损坏。每个橙子的形状接近球形，因此我们决定将它们装入立方体盒子中，并用充气气球进行保护。我们在盒子的每个角落放一个气球，使得每个气球都触碰到橙子和盒子的侧面。这些气球和橙子都是完美的球体。那么，我们能将一个多大的橙子放入给定尺寸的立方体盒子中呢？
- en: We want to answer this question for cubes (and balloons and oranges) with any
    number of dimensions, so let’s start with just two dimensions. Our box is then
    a 2D square of size 4 by 4, and the four balloons are each a circle of radius
    1, placed in the corners, as in [Figure 7-27](#figure7-27).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要回答这个问题，对于任何维度的立方体（以及气球和橙子），让我们从二维开始。我们的盒子是一个大小为 4×4 的二维正方形，四个气球是半径为 1 的圆形，放置在四个角落，如图
    [7-27](#figure7-27) 所示。
- en: '![F07027](Images/F07027.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![F07027](Images/F07027.png)'
- en: 'Figure 7-27: Shipping a circular 2D orange in a square box, surrounded by circular
    balloons in each corner. Left: The four balloons each have a radius of 1, so they
    fit perfectly into a square box of side 4\. Right: The orange sitting inside the
    balloons.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-27：将一个圆形二维橙子装入一个方形盒子中，盒子的四个角落被圆形气球包围。左图：四个气球的半径均为 1，因此它们可以完美地装入一个边长为 4 的方形盒子中。右图：橙子被放置在气球之间。
- en: Our orange in this 2D diagram is also a circle. In [Figure 7-27](#figure7-27)
    we show the biggest orange we can fit. A little geometry tells us that the radius
    of this orange circle is about 0.4.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个二维图中的橙子也是一个圆形。在图 [7-27](#figure7-27) 中，我们展示了能够放入的最大橙子。通过一点几何学推算，我们得知这个橙子圆的半径大约是
    0.4。
- en: Now let’s move to 3D, so we have a cube (again, 4 units on a side). We can now
    fit eight spherical balloons, each of radius 1, into the corners, as in [Figure
    7-28](#figure7-28). As always, the orange goes into the space in the middle of
    the balloons.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转到三维空间，因此我们有一个立方体（每边仍然是 4 单位）。我们现在可以将八个半径为 1 的球形气球放入立方体的角落，如图 [7-28](#figure7-28)
    所示。与往常一样，橙子被放入气球之间的中央空间。
- en: '![F07028](Images/F07028.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![F07028](Images/F07028.png)'
- en: 'Figure 7-28: Shipping a spherical orange in a cubical box surrounded by spherical
    balloons in each corner. Left: The box is 4 by 4 by 4, and the eight balloons
    each have a radius of 1\. Right: The orange sits in the middle of the balloons.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-28：将一个球形橙子装入一个立方体盒子中，盒子的每个角落都被球形气球包围。左图：盒子的尺寸为 4×4×4，八个气球的半径均为 1。右图：橙子坐落在气球的中央。
- en: Another bit of geometry (this time in 3D) tells us that this orange has a radius
    of about 0.7\. This is larger than the radius of the orange in the 2D case, because
    in 3D, there’s more room for the orange in the central gap between spheres.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个几何问题（这次是三维的）告诉我们，这个橙子的半径大约是 0.7。这比二维情况下的橙子半径要大，因为在三维空间中，橙子可以在球体之间的中央空隙中获得更多的空间。
- en: Let’s take this scenario into 4, 5, 6, and even more dimensions, where we have
    hypercubes, hyperspheres, and *hyperoranges*. For any number of dimensions, our
    hypercubes are always 4 units on every side, there is always a hypersphere balloon
    in every corner of the hypercube, and these balloons always have a radius of 1.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个情景扩展到四维、五维、六维甚至更多维度，在这些维度中我们有超立方体、超球体和*超橙子*。对于任何维度，我们的超立方体每一边的长度总是 4，超立方体的每个角落都有一个半径为
    1 的超球体气球。
- en: We can write a formula that tells us the radius of the biggest hyperorange we
    can fit for this scenario in any number of dimensions (Numberphile 2017). [Figure
    7-29](#figure7-29) plots this radius for different numbers of dimensions.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以写出一个公式，告诉我们在任何维度下，能装入此情景中的最大超橙子的半径（Numberphile 2017）。图 [7-29](#figure7-29)
    绘制了不同维度下的半径。
- en: We can see from [Figure 7-29](#figure7-29) that in four dimensions, the biggest
    hyperorange we can ship has a radius of exactly 1\. That means that it’s as large
    as the hyperballoons around it. That’s hard to picture, but it gets much stranger.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从图 [7-29](#figure7-29) 看出，在四维空间中，我们能够运输的最大超橙子的半径恰好是 1。这意味着它和周围的超气球一样大。这很难想象，但情况会更奇怪。
- en: '[Figure 7-29](#figure7-29) also tells us that in nine dimensions, the hyperorange
    has a radius of 2, so its diameter is 4\. That means the hyperorange is as large
    as the box itself, like the 3D sphere in [Figure 7-25](#figure7-25). That’s despite
    being surrounded by 512 hyperspheres of radius 1, each in one of the 512 corners
    of this 9D hypercube. If the orange is as large as the box, how are the balloons
    protecting anything?'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-29](#figure7-29) 同样告诉我们，在九维空间中，超橙子的半径为 2，因此其直径为 4。这意味着超橙子的大小与盒子本身一样，就像[图
    7-25](#figure7-25)中的三维球体一样。尽管它被 512 个半径为 1 的超球体包围，而这些超球体位于这个 9D 超立方体的 512 个角落，但超橙子的大小和盒子相同。那么，这些气球是如何保护任何东西的呢？'
- en: 'But things get much crazier. At 10 dimensions and higher, the hyperorange has
    a radius that’s *more* than 2\. The hyperorange is now *bigger* than the hypercube
    that was meant to protect it. It seems to be extending beyond the sides of the
    cube, even though we constructed it to fit both inside the box and the protective
    balloons that are still in every corner. It’s hard for those of us with 3D brains
    to picture 10 dimensions (or more), but the equations check out: the orange is
    simultaneously inside the box and extending beyond the box.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 但事情变得更疯狂了。在 10 维及更高维度时，超橙子的半径*大于* 2。超橙子现在*比*原本要保护它的超立方体还要*大*。它似乎超出了立方体的边界，尽管我们构建它时是为了让它既能适应盒子，也能适应每个角落的保护气球。对于我们这些有三维大脑的人来说，很难想象
    10 维（或更多）的空间，但方程式是成立的：橙子同时位于盒子内部，并且延伸出了盒子之外。
- en: '![F07029](Images/F07029.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![F07029](Images/F07029.png)'
- en: 'Figure 7-29: The radius of a hyperorange in a hypercube with sides 4, surrounded
    by hyperspheres of radius 1 in each corner of the cube'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-29：在一个边长为 4 的超立方体中，超橙子的半径为 2，周围有半径为 1 的超球体，每个角落都有一个。
- en: The moral here is that our intuition can fail us when we get into spaces of
    many dimensions (Aggarwal 2001). This is important because we routinely work with
    data that has tens or hundreds of features.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的教训是，当我们进入多维空间时，我们的直觉可能会失效（Aggarwal 2001）。这一点很重要，因为我们经常处理具有几十个或几百个特征的数据。
- en: Any time we work with data that has more than three features, we’ve entered
    the world of higher dimensions, and we should not reason by analogy with what
    we know from our experience with two and three dimensions. We need to keep our
    eyes open and rely on math and logic, rather than intuition and analogy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们处理具有超过三个特征的数据时，我们就进入了高维空间的世界，我们不应通过类比来推理，尤其是依据我们在二维和三维空间中的经验。我们需要睁大眼睛，依赖数学和逻辑，而不是直觉和类比。
- en: In practice, that means keeping a close eye on how a deep learning system is
    behaving during training when we have multidimensional data, and we should always
    be alert to seeing it act strangely. The techniques of Chapter 10 can reduce the
    number of features in our data, which may help. Throughout the book we’ll see
    other ways to improve a system that isn’t learning well, whether because of high-dimensional
    weirdness or other causes.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这意味着在处理多维数据时，我们需要密切关注深度学习系统在训练过程中的表现，并时刻警惕它出现异常的行为。《第 10 章》中的技术可以减少我们数据中的特征数量，这可能会有所帮助。在本书中，我们将看到其他改善系统的方式，无论是因为高维度的奇异性还是其他原因导致系统学习不佳。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we looked at the mechanics of classifying. We saw that classifiers
    can break up the space of data into domains that are separated by boundaries.
    A new piece of data is classified by identifying which domain it falls into. The
    domains may be fuzzy, representing probabilities, so the result of the classifier
    is a probability for each class. We also looked at an algorithm for clustering.
    Lastly, we saw that our intuition often gets it wrong when we work in spaces of
    more than three dimensions. Things frequently just don’t work the way we expect.
    Those higher-dimensional spaces are weird, and full of surprises. We learned that
    when working with multidimensional data, we should proceed carefully and monitor
    our system as it learns. We should never rely on guesswork based on our experience
    in 3D!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了分类的机制。我们看到，分类器可以将数据空间划分为由边界分隔的领域。新的数据通过识别它落入哪个领域来进行分类。这些领域可能是模糊的，表示概率，因此分类器的结果是每个类别的概率。我们还探讨了聚类算法。最后，我们发现，当我们在超过三维的空间中工作时，我们的直觉往往会出错。事物经常不会按我们预期的方式运行。这些高维空间很奇异，充满了惊喜。我们了解到，在处理多维数据时，我们应该谨慎行事，并在系统学习的过程中进行监控。我们永远不应依赖基于我们在
    3D 经验的猜测！
- en: In the next chapter, we’ll look at how to efficiently train a learning system,
    even when we don’t have a lot of data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨如何高效地训练一个学习系统，即使我们没有大量数据。
