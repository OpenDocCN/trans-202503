<html><head></head><body>
<h2 class="h2a" id="ch08">8<br/><span epub:type="pagebreak" id="page_165"/>COMPUTER VISION: FOLLOW A COLORED BALL</h2>&#13;
<p class="startpara">AS HUMANS, WE USE OUR EYES AND BRAINS TO SEE AND COMPREHEND THE WORLD AROUND US. ALTHOUGH THIS HAPPENS AUTOMATICALLY FOR US, VISION IS ACTUALLY AN IMMENSELY COMPLICATED SERIES OF PROCESSES.</p>&#13;
<p class="noindent"><em>Computer vision</em> <span epub:type="pagebreak" id="page_166"/>is an advanced field of computer science and engineering that aims to enable computers and machines to see and understand their surroundings at least as well as humans, if not better. In this chapter you’ll learn some principles of computer vision, and then I’ll show you how to use a camera to enable your robot to recognize and follow a colored ball.</p>&#13;
<h3 class="h3" id="ch08lev1sec1">THE COMPUTER VISION PROCESS</h3>&#13;
<p class="noindent">Consider the process behind seeing, recognizing, and reacting to a colorful item. First, the image of that item passes through your eye and strikes your retina. The retina does some elementary analysis and then converts the received light into neural signals, which are sent to your brain and analyzed thoroughly by your visual cortex. Your brain then identifies the item and gives instructions to your muscles.</p>&#13;
<p class="indent">What is remarkable is that this all happens in a fraction of a second and with no conscious effort.</p>&#13;
<p class="indent">Even with that simplified explanation, you can appreciate the complexity of vision. Getting computers to complete a similar series of tasks is something that people in the field of computer vision have worked on tirelessly for <em>decades</em>.</p>&#13;
<p class="indent">There are three distinct tasks any computer vision system must be able to do:</p>&#13;
<p class="list-plain"><strong>See</strong> Biological beings generally see through their eyes. Com­puters must use their digital equivalent: cameras. Cameras work by using a lens to focus light onto a digital sensor. This sensor then converts the light into digital information: an image or frame of a video.</p>&#13;
<p class="list-plain"><strong>Process</strong> After the input has been captured, it must be processed to extract information, recognize patterns, and manipulate data. In nature, this is the role of the brain. For computer vision, it is the role of code and algorithms.</p>&#13;
<p class="list-plain"><strong>Understand</strong> The information must then be understood. The computer may have detected and processed a pattern, but what is the pattern and what does it mean? Again, this important step relies on code and algorithms.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_167"/>When these three elements work together, the computer can handle all sorts of vision-based problems, including the one we’re going to tackle in this chapter: we’ll give your robot the ability to detect, recognize, and move toward a colored ball that might be anywhere in its surrounding environment.</p>&#13;
<h3 class="h3" id="ch08lev1sec2">THE PARTS LIST</h3>&#13;
<p class="noindent">You’ll need two new items for this next project:</p>&#13;
<ul>&#13;
<li class="noindent">A colored ball</li>&#13;
<li class="noindent">Standard Pi Camera Module</li>&#13;
</ul>&#13;
<p class="indent">Initially, you’ll also need access to another computer to remotely view images taken by the Pi Camera Module during the configuration stage of this project. This can be the computer you’re using to SSH into your Pi. If you haven’t been using SSH over the course of this book, then you can just hook up your Pi to an HDMI display during the configuration.</p>&#13;
<p class="indent">Let’s take a closer look at the new parts.</p>&#13;
<h4 class="h4" id="ch08lev2sec1">The Target: A Colored Ball</h4>&#13;
<p class="noindent">First you’ll need a colored ball to act as the target your robot will seek out and follow. Your ball should be a bright color that doesn’t appear much elsewhere in the room, to help your robot differentiate it from other objects. I’d recommend something distinctive and not too large, like the balls shown in <a href="ch08.xhtml#ch08fig1">Figure 8-1</a>. The one I’m using is a bright yellow stress ball roughly 2 inches in diameter. You probably already have something suitable lying around your house, but if not, you should be able to pick up a similar one online for a few dollars.</p>&#13;
<div class="image" id="ch08fig1"><img alt="image" src="../images/f167-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-1</strong> Example colored balls for targets; I’m using the yellow one on the left.</p>&#13;
<h4 class="h4" id="ch08lev2sec2"><span epub:type="pagebreak" id="page_168"/>The Official Raspberry Pi Camera Module</h4>&#13;
<p class="noindent">To give your robot the ability to see, you will need a camera. In this project, we’ll be using the official Raspberry Pi Camera Module, shown in <a href="ch08.xhtml#ch08fig2">Figure 8-2</a>.</p>&#13;
<div class="image" id="ch08fig2"><img alt="image" src="../images/f168-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-2</strong> The official Raspberry Pi Camera Module</p>&#13;
<p class="indent">The Camera Module is a Raspberry Pi add-on board designed and produced by the Raspberry Pi Foundation. The latest model features an 8-megapixel sensor and is less than 1 inch square in size! Not only does it take great still photos, but the Camera Module is also able to shoot full-HD 1080p video at 30 frames per second. If you have the older 5-megapixel model, don’t worry: it is fully compatible with this project and used in exactly the same way.</p>&#13;
<p class="indent">The 6-inch ribbon cable on the Camera Module attaches to the CSI (Camera Serial Interface) port on the Raspberry Pi, shown in <a href="ch08.xhtml#ch08fig3">Figure 8-3</a>. It is compatible with all models, including the latest version of the Pi Zero.</p>&#13;
<div class="image" id="ch08fig3"><img alt="image" src="../images/f168-02.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-3</strong> The CSI interface port on the Raspberry Pi</p>&#13;
<div class="note">&#13;
<p class="note1"><strong>NOTE</strong></p>&#13;
<p class="notep"><em>The Pi Zero has a mini-CSI connector rather than the full-size one found on all other Raspberry Pi models. To connect a Camera Module to the Zero, you’ll need to also purchase a mini-CSI-to-CSI cable. These are branded online as “Pi Zero Camera Cables” and cost around $5. Keep in mind that this project requires intense image processing and code that will run better on the faster full-size Raspberry Pi models than it will on the Zero or original Raspberry Pi models.</em></p>&#13;
</div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_169"/>You can buy the official Raspberry Pi Camera Module online at the usual retailers. It costs approximately $30.</p>&#13;
<p class="indent">When looking online you may notice that there are actually two different official Camera Modules: the normal camera and the <em>NoIR</em> version, which can be used for night vision. You need the <em>standard</em> Camera Module. You can easily tell the two boards apart by their color difference: the circuit board of the normal Camera Module is green, whereas the NoIR is black.</p>&#13;
<h3 class="h3" id="ch08lev1sec3">CONNECTING AND SETTING UP YOUR CAMERA MODULE</h3>&#13;
<p class="noindent">Before you attach the Camera Module, ensure that your Pi is switched off. Then follow this process:</p>&#13;
<ol>&#13;
<li class="noindent">Locate the CSI port on your Raspberry Pi. For all the full-size models of Raspberry Pi, this is found between the HDMI port and the 3.5 mm audio jack, and is helpfully labeled CAMERA.</li>&#13;
<li class="noindent">Next, open up the port by gently but firmly grabbing it from either side and pulling it upward (see <a href="ch08.xhtml#ch08fig4">Figure 8-4</a>). This can be a delicate operation, and it often helps to get your fingernails underneath the sides.&#13;
<div class="image" id="ch08fig4"><img alt="image" src="../images/f169-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-4</strong> The CSI port opened for cable insertion</p></li>&#13;
<li class="noindent">Insert the ribbon cable on your Camera Module all the way into the CSI port with the silver contacts facing <em>away</em> from the 3.5 mm audio jack and Ethernet port (see <a href="ch08.xhtml#ch08fig5">Figure 8-5</a>). This orientation is critical: if you insert the Module’s cable the other way around, it won’t be connected properly and you will not be able to use it!&#13;
<div class="image" id="ch08fig5"><img alt="image" src="../images/f170-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-5</strong> CSI port with the Camera Module’s ribbon cable inserted in the correct direction</p></li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_170"/>Then, while holding the ribbon cable in place, place a finger on both sides of the CSI port and push it back down at the same time. If both sides don’t close simultaneously, then one side will not close properly and the cable may come loose. <a href="ch08.xhtml#ch08fig6">Figure 8-6</a> shows a properly attached ribbon cable. Notice that a fraction of the silver contacts are just visible, and they’re all parallel with the board.&#13;
<div class="image" id="ch08fig6"><img alt="image" src="../images/f170-02.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-6</strong> My correctly connected Camera Module ribbon cable</p></li>&#13;
<li class="noindent">Finally, to ensure that the Camera Module is connected properly, give the ribbon cable a gentle tug near the CSI port. It should stay rigidly in place. If the cable comes detached or slips, don’t worry—just remove it and repeat these steps.</li>&#13;
</ol>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_171"/>If you want to connect the Camera Module to a Raspberry Pi Zero, the process is similar. Find the mini-CSI port on the right-hand side of the board and open it using a finger on either side of it. Then, make sure the silver contacts face downward toward the board when you insert the Pi Zero camera cable. See <a href="ch08.xhtml#ch08fig7">Figure 8-7</a>.</p>&#13;
<div class="image" id="ch08fig7"><img alt="image" src="../images/f171-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-7</strong> The correct orientation of the camera cable for the mini-CSI port on Pi Zero models</p>&#13;
<h4 class="h4" id="ch08lev2sec3">Mounting Your Camera</h4>&#13;
<p class="noindent">Now that you have the camera connected to the Pi on your robot, you need to mount it in an appropriate position. I recommend using some sticky tack to affix it to the front of your robot, relatively low to ensure it will have a clear view. To do this, I’ve used a 2 × 2 LEGO brick to create some mounting space (see <a href="ch08.xhtml#ch08fig8">Figure 8-8</a>). Also make sure that your camera is positioned the correct way up, like mine is in the picture.</p>&#13;
<div class="image" id="ch08fig8"><img alt="image" src="../images/f171-02.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-8</strong> My Camera Module mounted on my Raspberry Pi robot</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_172"/>The Camera Module is quite delicate, so handle it with caution. Try not to contort the cable too much and make sure to not leave any kinks in it. If at any point the ribbon cable comes loose from the Raspberry Pi, just reattach it the same way as before. If it comes loose from the connector on the actual module then you can also just reattach it. This is done in the same way: use your fingers to lever open the module’s CSI port and then insert the cable with the silver contacts facing down and toward the PCB.</p>&#13;
<h4 class="h4" id="ch08lev2sec4">Enabling the Camera and VNC, and Setting the Screen Resolution</h4>&#13;
<p class="noindent">To use the camera in Raspbian, you first need to enable it. If you followed all of the instructions in <a href="ch01.xhtml#ch01">Chapter 1</a>, you’ll already have done part of this. On top of that, we’ll need to enable VNC for this project and manually set up the right screen resolution. Here’s the full process.</p>&#13;
<p class="indent">To do this setup, we’ll use the configuration tool, <code>raspi-config</code>. Open the command line and enter the following:</p>&#13;
<p class="codesingle"><span class="green">pi@raspberrypi</span>:<span class="p-dark-blue">~/robot $</span> <span class="codestrong1">sudo raspi-config</span></p>&#13;
<p class="indent">You should see the same blue configuration screen you met when configuring the audio output of your Raspberry Pi a few chapters ago. Using the arrow keys, scroll down to <strong>Interfacing Options</strong> and then press <span class="small">ENTER</span>. This opens up a new menu shown in <a href="ch08.xhtml#ch08fig9">Figure 8-9</a>.</p>&#13;
<div class="image" id="ch08fig9"><img alt="image" src="../images/f172-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-9</strong> The Interfacing Options menu of the <code>raspi-config</code> tool</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_173"/>Select <strong>Camera</strong> by pressing <span class="small">ENTER</span> again. Then, when asked whether you’d like to enable the camera interface, use the left/right arrow keys to select <strong>Yes</strong> (see <a href="ch08.xhtml#ch08fig10">Figure 8-10</a>).</p>&#13;
<div class="image" id="ch08fig10"><img alt="image" src="../images/f173-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-10</strong> Enabling the camera using <code>raspi-config</code></p>&#13;
<p class="indent">You’ll see a message confirming that the camera interface has been enabled and then you’ll return to the original menu.</p>&#13;
<p class="indent">While you are in the <code>raspi-config</code> tool, it is also important to ensure that VNC is enabled. VNC will be fully explained in the next section, but for now just scroll down to <strong>Interfacing Options</strong> again, and then select <strong>VNC</strong> (see <a href="ch08.xhtml#ch08fig11">Figure 8-11</a>). Press <span class="small">ENTER</span> to enable it.</p>&#13;
<div class="image" id="ch08fig11"><img alt="image" src="../images/f173-02.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-11</strong> Selecting VNC from Interfacing Options</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_174"/>You’ll be sent back to the original menu. Before you can exit the configuration tool, you have to do one last thing: manually set the screen resolution of your Pi. This will ensure that when we use VNC later, the screen will be set up in the right way. To set the resolution, from the original menu, scroll down to <strong>Advanced Options</strong>, and then scroll down and select <strong>Resolution</strong> (see <a href="ch08.xhtml#ch08fig12">Figure 8-12</a>).</p>&#13;
<div class="image" id="ch08fig12"><img alt="image" src="../images/f174-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-12</strong> The screen resolution option inside Advanced Options</p>&#13;
<div class="note">&#13;
<p class="note1"><strong>NOTE</strong></p>&#13;
<p class="notep"><em>If you know that the resolution of your PC’s screen is</em> lower <em>than full HD, then set the screen resolution of your Pi to the closest lower option in the menu. You can always run</em> raspi-config <em>again to change the resolution at a later point</em>.</p>&#13;
</div>&#13;
<p class="indent">You will then be prompted to choose a screen resolution. Use the arrow keys to scroll down to the full HD option. It will look something like “DMT Mode 82 1920x1080 60Hz 16:9.” Press <span class="small">ENTER</span> on this option, and your screen resolution should now be set! You’ll be returned to the original menu.</p>&#13;
<p class="indent">Exit the configuration tool by pressing the right arrow key twice (highlighting <strong>Finish</strong>) and then pressing <span class="small">ENTER</span>. Reboot your Raspberry Pi if prompted to do so.</p>&#13;
<h3 class="h3" id="ch08lev1sec4">TAKING A TEST PHOTO</h3>&#13;
<p class="noindent">With your Camera Module connected and enabled, let’s test it by taking a photo. This is easy to do from a remote terminal with a simple command, but by using that method you won’t be able to view the image it took in the text-based environment! This is where VNC, the option you enabled previously, comes in.</p>&#13;
<h4 class="h4" id="ch08lev2sec5">Controlling Your Pi’s Desktop Remotely with VNC</h4>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_175"/><em>VNC</em> stands for <em>Virtual Network Computing</em>. It allows you to remotely view and control your Raspberry Pi’s desktop from another computer, a little bit like what you’ve been doing with SSH, but for the full graphical user interface (GUI) rather than just the terminal. Since we can view the Pi’s GUI using VNC, you’ll easily be able to see any photos that you take with the Raspberry Pi Camera Module using Raspbian’s built-in image viewer.</p>&#13;
<div class="note">&#13;
<p class="note1"><strong>NOTE</strong></p>&#13;
<p class="notep"><em>If you’ve been following along with this book and not using your Raspberry Pi wirelessly over SSH, don’t worry! You can still follow the steps after this section and view the results with your Pi connected to a monitor over HDMI.</em></p>&#13;
</div>&#13;
<h4 class="h4" id="ch08lev2sec6">Installing and Making a Connection with VNC Viewer</h4>&#13;
<p class="noindent">You have everything set up on the Raspberry Pi end, and now you need to download a VNC viewer on the computer you want to view the images on. We’ll use some free software called <em>VNC Viewer</em> from RealVNC, which is compatible with Windows, Mac, Linux, and more. To install the software, follow this process on your machine:</p>&#13;
<ol>&#13;
<li class="noindent">On your computer, open a web browser and go to <em><a href="https://www.realvnc.com/en/connect/download/viewer/">https://www.realvnc.com/en/connect/download/viewer/</a></em>. You should see the download page for the VNC Viewer software, shown in <a href="ch08.xhtml#ch08fig13">Figure 8-13</a>.&#13;
<div class="image" id="ch08fig13"><img alt="image" src="../images/f175-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-13</strong> The VNC Viewer software download page</p></li>&#13;
<li class="noindent">From here, select your operating system and click the <strong>Download</strong> button. Once the software has downloaded, go through the installation wizard and agree to the terms of service. A few minutes later, everything should be installed and ready!</li>&#13;
</ol>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_176"/>With VNC Viewer now installed, run it. You should see a window with a box at the top (<a href="ch08.xhtml#ch08fig14">Figure 8-14</a>); here, you’ll enter the IP address of your Raspberry Pi, which you should already know since you’ve been using it to connect to your Pi over SSH.</p>&#13;
<p class="indent">An authentication box will appear asking you for a username and password. Enter the login details of your Raspberry Pi and click <strong>OK</strong>. If you haven’t changed the default user, then the username will be <code>pi</code> and the password will be whatever you set it to in <a href="ch01.xhtml#ch01">Chapter 1</a>.</p>&#13;
<div class="image" id="ch08fig14"><img alt="image" src="../images/f176-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-14</strong> Setting up VNC Viewer to connect to my Raspberry Pi</p>&#13;
<p class="indent">A new window will appear displaying your Pi’s desktop (see <a href="ch08.xhtml#ch08fig15">Figure 8-15</a>). Here you can access and use everything just the same as if your Pi were plugged into an HDMI monitor.</p>&#13;
<div class="image" id="ch08fig15"><img alt="image" src="../images/f176-02.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-15</strong> A terminal and file manager in my Raspberry Pi desktop environment, viewed with VNC</p>&#13;
<h4 class="h4" id="ch08lev2sec7"><span epub:type="pagebreak" id="page_177"/>Taking and Viewing a Photo Using the Raspberry Pi Camera Module</h4>&#13;
<p class="noindent">Now that you have everything set up, you can take your test photo! We’ll use a built-in command-line tool called <code>raspistill</code>. Open up a terminal, either through an SSH connection or by using a terminal window in the desktop environment over your VNC connection, and enter the following command to take a photo:</p>&#13;
<p class="codesingle"><span class="green">pi@raspberrypi</span>:<span class="p-dark-blue">~ $</span> <span class="codestrong1">raspistill –o test.jpg</span></p>&#13;
<p class="indent">After a 5-second delay (to make sure you can get in front of your camera or frame your shot), this command will complete. If you see no output, that’s great news! There is no success message for this particular command. This instruction takes a picture and saves it as <em>test.jpg</em> in the directory where the command was run—in this case, the default home directory.</p>&#13;
<p class="indent">To view the image in the VNC desktop, click the <strong>File Manager</strong> icon in the VNC desktop environment (it looks like a collection of folders, as shown in <a href="ch08.xhtml#ch08fig16">Figure 8-16</a>).</p>&#13;
<div class="box">&#13;
<p class="headbox">CAMERA TROUBLESHOOTING</p>&#13;
<p class="noindent">If running the <code>raspistill</code> command gives you a scary error similar to the following, don’t worry!</p>&#13;
<pre><span class="p-green">pi@raspberrypi</span>:<span class="p-dark-blue">~ $</span> <span class="literal1"><span class="codestrong1">raspistill -o test.jpg</span></span><br/>mmal: mmal_vc_component_enable: failed to enable component:<br/>ENOSPC<br/>mmal: camera component couldn’t be enabled<br/>mmal: main: Failed to create camera component<br/>mmal: Failed to run camera app. Please check for firmware<br/>updates</pre>&#13;
<p class="indent">You most likely just haven’t connected your camera properly. If you get this error message, or any other, check the ribbon cable connections between the Camera Module and your Pi. Also ensure that you have enabled the camera interface properly—turn back to <a href="ch08.xhtml#page_172">page 172</a> for guidance.</p>&#13;
</div>&#13;
<div class="image" id="ch08fig16"><img alt="image" src="../images/f178-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-16</strong> The File Manager icon</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_178"/>Navigate to the directory you ran the <code>raspistill</code> command from (I ran mine in the default home directory), locate <em>test.jpg</em>, and double-click it. You should see the photo that you just took in the Image Viewer (see <a href="ch08.xhtml#ch08fig17">Figure 8-17</a>).</p>&#13;
<div class="image" id="ch08fig17"><img alt="Image" src="../images/f178-02.jpg"/></div>&#13;
<div class="image"><img alt="Image" src="../images/f178-03.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-17</strong> The <em>test.jpg</em> image in Raspbian’s Image Viewer over VNC (top); the <em>test.jpg</em> image, which features a stapler, Carl Sagan’s <em>Cosmos</em>, and two colored balls for use later (bottom)</p>&#13;
<h3 class="h3" id="ch08lev1sec5"><span epub:type="pagebreak" id="page_179"/>MAKE YOUR ROBOT SEEK AND FOLLOW A BALL</h3>&#13;
<p class="noindent">Now you have your camera hooked up and a successful image test under your belt, it’s time to move on to the advanced project in this chapter: making your robot recognize and follow a colored ball. But first: a quick lesson in some important theory.</p>&#13;
<h4 class="h4" id="ch08lev2sec8">Understanding the Theory Behind Colored-Object Recognition</h4>&#13;
<p class="noindent">How do we get a robot incapable of independent thought as we know it to detect and identify a particular object?</p>&#13;
<p class="indent">As your robot moves around, the position of the ball relative to your robot will be constantly changing, so the first thing we need is a continually refreshing view of what is in front of your robot. The Camera Module provides this view through a stream of images, often referred to as <em>video frames</em> or just <em>frames</em>.</p>&#13;
<p class="indent">Each image, like the one in <a href="ch08.xhtml#ch08fig18">Figure 8-18</a>, will need to be analyzed to identify whether or not it contains your colored ball. To do this, we will apply various image processing techniques.</p>&#13;
<div class="image" id="ch08fig18"><img alt="image" src="../images/f179-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-18</strong> An untouched image from the Camera Module, ready to be analyzed</p>&#13;
<p class="indent">The first step is to convert the image, which is in an RGB format, into an <em>HSV</em> format. We discussed RGB in <a href="ch06.xhtml#ch06">Chapter 6</a>, but I’ll briefly summarize here. <em>RGB</em> stands for red, green, and blue. Each pixel of the image from the Camera Module in <a href="ch08.xhtml#ch08fig18">Figure 8-18</a> is made out of <span epub:type="pagebreak" id="page_180"/>a combination of these three colors, represented as three numbers between 0 and 255—for example, [100,200,150].</p>&#13;
<p class="indent">Computers use RGB for <em>displaying</em> colors, but for processing images and the color data they contain, the HSV color format is much more appropriate. <em>HSV</em> stands for hue, saturation, and value, and it is just another way of digitally representing color with three parameters. HSV is a bit more complicated to understand and represent than RGB, but it is often easiest to get your head around when viewed as a cylinder (see <a href="ch08.xhtml#ch08fig19">Figure 8-19</a>).</p>&#13;
<div class="image" id="ch08fig19"><img alt="image" src="../images/f180-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-19</strong> An HSV cylinder</p>&#13;
<p class="indent"><em>Hue</em> is the color portion of the HSV model, and it is expressed as a number from 0 to 360 degrees. Different sections of this range represent the different colors (see <a href="ch08.xhtml#ch08tab1">Table 8-1</a>).</p>&#13;
<p class="tablecap" id="ch08tab1"><strong>TABLE 8-1</strong> The Hue Ranges of HSV</p>&#13;
<table class="topbot-d">&#13;
<tbody>&#13;
<tr>&#13;
<td class="border"><p class="table-h"><strong>COLOR</strong></p></td>&#13;
<td class="border1"><p class="table-h"><strong>ANGLE</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="border"><p class="table-parac"><span class="c1">Red</span></p></td>&#13;
<td class="border1"><p class="table-parac"><span class="c1">0–60</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="border"><p class="table-parac"><span class="c1">Yellow</span></p></td>&#13;
<td class="border1"><p class="table-parac"><span class="c1">60–120</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="border"><p class="table-parac"><span class="c1">Green</span></p></td>&#13;
<td class="border1"><p class="table-parac"><span class="c1">120–180</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="border"><p class="table-parac"><span class="c1">Cyan</span></p></td>&#13;
<td class="border1"><p class="table-parac"><span class="c1">180–240</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="border"><p class="table-parac"><span class="c1">Blue</span></p></td>&#13;
<td class="border1"><p class="table-parac"><span class="c1">240–300</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="border"><p class="table-parac"><span class="c1">Magenta</span></p></td>&#13;
<td class="border1"><p class="table-parac"><span class="c1">300–360</span></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_181"/><em>Saturation</em> is the level of white in a color, from 0 to 100%. <em>Value</em> works alongside saturation and can be thought of as brightness; it describes the intensity of the color from 0 to 100%.</p>&#13;
<p class="indent">By converting each image into an HSV format, your Pi is able to separate just the color component (hue) for further analysis. This means that the computer should still be able to recognize a colored object, regardless of the environment and its lighting effects. This would be incredibly difficult to achieve in an RGB color space. <a href="ch08.xhtml#ch08fig20">Figure 8-20</a> shows an RGB implementation of the HSV data.</p>&#13;
<div class="image" id="ch08fig20"><img alt="image" src="../images/f181-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-20</strong> HSV data of the image</p>&#13;
<p class="indent">As you can see in <a href="ch08.xhtml#ch08fig20">Figure 8-20</a>, the yellow ball my robot will follow is now clear and distinctive. There is no possible way that it could be confused with the red ball behind it, or any of the other objects in the frame. Remember, though, that this is an RGB <em>implementation</em> of HSV color. The hue value of a color is not something that we can see in the same way with our own eyes.</p>&#13;
<p class="indent">The next stage in the process is to look for and identify any color that matches the one we are searching for. In my case, I want to match all the parts of the image that are the same color as my yellow ball. This forms a <em>mask</em> (see <a href="ch08.xhtml#ch08fig21">Figure 8-21</a>) that simply keeps the parts of the image we want and removes the parts we don’t want. You can see that it keeps only the areas that contain my desired color, yellow.</p>&#13;
<div class="image" id="ch08fig21"><img alt="image" src="../images/f182-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-21</strong> Masking out the colors and areas of the image that are not the same shade of yellow as the ball. Notice that there are areas on the ball (bright reflections/shadowy regions) that aren’t picked up!</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_182"/>Now that the relevant colored objects have been isolated, the next step in the process is to identify the largest patch of color. Notice how there are other (albeit small) readings of yellow in <a href="ch08.xhtml#ch08fig21">Figure 8-21</a>? If you don’t program this right, your robot might get confused and head toward those instead of the desired ball. This could be disastrous—after all, you don’t want it to get distracted by distant bananas!</p>&#13;
<p class="indent">Assuming that the largest part of the mask is the colored ball, the next stage is to actually find that largest area. We do so by drawing a contour (like an outline; see <a href="ch08.xhtml#ch08fig22">Figure 8-22</a>) around each detected object in the mask. We can work out the area of each contour using some basic mathematics. The largest area is then identified and assumed to be our target ball!</p>&#13;
<div class="image" id="ch08fig22"><img alt="image" src="../images/f182-02.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-22</strong> Contour drawn around the largest single object in the mask</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_183"/>After this, we simply need to program the robot to move toward the object. If the target is to the right of the robot, move right. If it is to the left of the robot, move left. If it is in front of the robot, move forward.</p>&#13;
<p class="indent">And that’s all there is to making a ball-following computer vision system with your Raspberry Pi! It’s time to put it into practice.</p>&#13;
<h4 class="h4" id="ch08lev2sec9">Installing the Software</h4>&#13;
<p class="noindent">You’ll need a couple of Python libraries to enable computer vision. Most notably, we will be using OpenCV, a free and open source library of programming functions for real-time computer vision. You will also need the PiCamera Python library to manipulate and handle the Camera Module in Python, though this is included by default in the latest versions of Raspbian.</p>&#13;
<p class="indent">To install the dependencies for the OpenCV Python 3 library, enter the following command into the terminal:</p>&#13;
<p class="codesingle"><span class="green">pi@raspberrypi</span>:<span class="p-dark-blue">~ $</span> <span class="literal1"><span class="codestrong1">sudo apt-get install libblas-dev<br/>libatlas-base-dev libjasper-dev libqtgui4 libqt4-test</span></span></p>&#13;
<div class="note">&#13;
<p class="note1"><strong>NOTE</strong></p>&#13;
<p class="notep"><em>If you get any errors during this installation process, or it just doesn’t look like it has gone correctly, visit the book’s website at <a href="https://nostarch.com/raspirobots">https://nostarch.com/raspirobots</a> to check for any changes and get further guidance.</em></p>&#13;
</div>&#13;
<p class="indent">When prompted as to whether you want to continue, press <span class="small">Y</span> and then <span class="small">ENTER</span>. This command will take a few minutes to execute.</p>&#13;
<p class="indent">Now, you can use <code>pip</code> (the Python software management tool we have used previously) to install OpenCV for Python 3. Enter the following command:</p>&#13;
<p class="codesingle"><span class="green">pi@raspberrypi</span>:<span class="p-dark-blue">~ $</span> <span class="codestrong1">sudo pip3 install opencv-python</span></p>&#13;
<p class="indent">After the OpenCV installation has finished, check that you have the PiCamera library installed with the following command. It will most likely inform you that you already have the newest version, but if not, proceed with the installation:</p>&#13;
<p class="codesingle"><span class="green">pi@raspberrypi</span>:<span class="p-dark-blue">~ $</span> <span class="codestrong1">sudo apt-get install python3-picamera</span></p>&#13;
<p class="indent">And that’s all you will need!</p>&#13;
<h4 class="h4" id="ch08lev2sec10">Identifying the HSV Color of Your Colored Ball</h4>&#13;
<p class="noindent">To identify a ball of a specific color, your Raspberry Pi robot needs the HSV value of that color. The Pi will use this value to compare each part of each image to see whether it is the color of the ball that you want your robot to follow.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_184"/>Your ball is probably a different color than mine, so you’ll need to find out the exact HSV value for yours. Even if you have a yellow ball as I do, it may be a slightly different shade from mine!</p>&#13;
<p class="indent">There are various ways to identify the hue value you’ll need, but I’ve found that the best method is to try out various values on your Raspberry Pi and observe the effects. The aim is to find the one that matches your particular colored ball. To help you do this, I have created a test program, <em>hsv_tester.py</em>, which you can find in the software bundled with this book (<a href="https://nostarch.com/raspirobots/"><em>https://nostarch.com/raspirobots/</em></a>). The next section will walk you through running the program.</p>&#13;
<h4 class="h4" id="ch08lev2sec11">Running the HSV Test Program</h4>&#13;
<p class="noindent">Place your robot in a well-lit environment, with your colored ball approximately a meter in front of it. Then boot up the Pi on your robot and view its desktop remotely over VNC. Next, open up a terminal application in the desktop, locate the <em>hsv_tester.py</em> program, and run it using the command:</p>&#13;
<div class="note">&#13;
<p class="note1"><strong>NOTE</strong></p>&#13;
<p class="notep"><em>I won’t go into the specifics of how this tester program works, because it’s very similar to the actual ball-following code that you will use in a few pages. I’ll follow that code up with the usual detailed explanation.</em></p>&#13;
</div>&#13;
<p class="codesingle"><span class="green">pi@raspberrypi</span>:<span class="p-dark-blue">~/robot $</span> <span class="codestrong1">python3 hsv_tester.py</span></p>&#13;
<p class="indent">You’ll see a prompt asking you for a hue value between 10 and 245. Try approximating the hue value of your ball; the ranges in <a href="ch08.xhtml#ch08tab1">Table 8-1</a> should give you a rough idea of what end of the spectrum to start at. Mine is yellow, so I’m going to guess 40. When you enter the value, you’ll see four new windows appear, showing you the different stages of image processing discussed previously (see <a href="ch08.xhtml#ch08fig23">Figure 8-23</a>).</p>&#13;
<div class="image" id="ch08fig23"><img alt="image" src="../images/f184-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-23</strong> The four different windows of the HSV tester program</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_185"/>The first window, titled <em>Camera Output</em>, is a raw RGB video output directly from the Camera Module <span class="ent">➊</span>. The second window, titled <em>HSV</em>, is the same video converted into HSV format <span class="ent">➋</span>. Then, the window titled <em>Color Mask</em> displays the parts of the image that match the hue number you provided <span class="ent">➌</span>. Finally, the window titled <em>Final Result</em> overlays the color mask with the original video feed, showing you the isolated area <span class="ent">➍</span>.</p>&#13;
<p class="indent">If the mask in the Final Result window is more or less ball-shaped, you have your hue!</p>&#13;
<p class="indent">It is unlikely that you’ll get your hue value correct the first time, just as I haven’t—the hue didn’t match my ball but instead it got parts of the stapler in the frame! To try again, select any of the output windows (but not the terminal window), and press Q on your keyboard. This will freeze the video output, and you can then go back to the terminal and enter another value to try.</p>&#13;
<p class="indent">Play around and tweak the hue until you get a definitive match for your colored ball. After a little while, I found that my magic number was 28. When you have found yours, you should see that the majority of your ball and not much else is left in the frame, like mine in <a href="ch08.xhtml#ch08fig24">Figure 8-24</a>.</p>&#13;
<div class="image" id="ch08fig24"><img alt="image" src="../images/f185-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-24</strong> My output windows after I correctly identified my hue number as around 28</p>&#13;
<p class="indent">Make a note of the value, as you’ll need it soon. After you’ve found the right number, close the HSV tester program by pressing <span class="small">CTRL-</span>C in the terminal window.</p>&#13;
<h4 class="h4" id="ch08lev2sec12">Programming Your Raspberry Pi to Follow a Ball</h4>&#13;
<p class="noindent">With all of the groundwork in place, you can now program your Raspberry Pi robot to follow a ball! The program we’ll be using is relatively long and more advanced than anything you’ve met so far, <span epub:type="pagebreak" id="page_186"/>so I recommend downloading it from the software bundle rather than copying it from this book to minimize typos. The program is called <em>ball_follower.py</em> and you can check it out with the command:</p>&#13;
<p class="codesingle"><span class="green">pi@raspberrypi</span>:<span class="p-dark-blue">~/robot $</span> <span class="codestrong1">nano ball_follower.py</span></p>&#13;
<p class="indent">This program is 75 lines long, so for the explanation I’ve split it into sections and will explain how each part works. If you’re more interested in just running the code first and understanding how it works later, skip ahead to the next section on <a href="ch08.xhtml#page_193">page 193</a>.</p>&#13;
<h5 class="h5">Importing Packages and Setting Up the Camera Module</h5>&#13;
<p class="noindent">First we’ll import the packages we need and set a few things up, as shown in <a href="ch08.xhtml#ch08list1">Listing 8-1</a>.</p>&#13;
<pre><span class="ent">➊</span> <span class="p-blue">from</span> picamera.array <span class="p-blue">import</span> PiRGBArray<br/>   <span class="p-blue">from</span> picamera <span class="p-blue">import</span> PiCamera<br/>   <span class="p-blue">import</span> cv2<br/>   <span class="p-blue">import</span> numpy <span class="p-blue">as</span> np<br/>   <span class="p-blue">import</span> gpiozero<br/><br/><span class="ent">➋</span> camera = PiCamera()<br/><span class="ent">➌</span> image_width = 640<br/>   image_height = 480<br/><span class="ent">➍</span> camera.resolution = (image_width, image_height)<br/>   camera.framerate = 32<br/>   rawCapture = PiRGBArray(camera, size=(image_width, image_height))<br/><span class="ent">➎</span> center_image_x = image_width / 2<br/>   center_image_y = image_height / 2<br/><span class="ent">➏</span> minimum_area = 250<br/>   maximum_area = 100000</pre>&#13;
<p class="listing" id="ch08list1"><strong>LISTING 8-1</strong> Importing libraries and setting up the Camera Module</p>&#13;
<p class="indent">The first lines of the program <span class="ent">➊</span> import the necessary libraries, including various parts of the PiCamera library needed to allow us to use the Camera Module in Python. We also import the OpenCV library, <code>cv2</code>; <code>gpiozero</code> as usual; and the NumPy library, <code>np</code>. NumPy is a Python package used for scientific computing and will be useful for manipulating the image data later.</p>&#13;
<p class="indent">At <span class="ent">➋</span>, we initiate a PiCamera object and assign it to the <code>camera</code> variable for use throughout the program. Next we define the size <span class="ent">➌</span> and resolution <span class="ent">➍</span> of the images being fed from the camera. We won’t need full-HD video frames and they would just slow down speed and performance, so we downgrade the resolution to standard definition: 640×480 pixels. On the lines following this, we specify the frame rate of the camera and the raw capture setup.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_187"/>The two lines at <span class="ent">➎</span> work out where the center of the image is. This information will be used later to determine and compare where the ball is in the frame.</p>&#13;
<p class="indent">Then at <span class="ent">➏</span>, we set a minimum and maximum area for the colored ball. This prevents your robot from detecting and following any colored object smaller than 250 square pixels or larger than 100,000 square pixels. These are arbitrary numbers that I’ve found work pretty well, but if you want to change them later, feel free!</p>&#13;
<h5 class="h5">Setting Up the Robot and Color Values</h5>&#13;
<p class="noindent">This section deals with the final part of the setup process, shown in <a href="ch08.xhtml#ch08list2">Listing 8-2</a>.</p>&#13;
<pre>   robot = gpiozero.Robot(left=(17,18), right=(27,22))<br/><span class="ent">➊</span> forward_speed = 0.3<br/>   turn_speed = 0.2<br/><br/><span class="ent">➋</span> HUE_VAL = 28<br/><br/><span class="ent">➌</span> lower_color = np.array([HUE_VAL-10,100,100])<br/>   upper_color = np.array([HUE_VAL+10,255,255])</pre>&#13;
<p class="listing" id="ch08list2"><strong>LISTING 8-2</strong> Setting up the robot and color values</p>&#13;
<p class="indent">We set up the robot and its motor pins as before and then define two variables for both the forward and turning speeds <span class="ent">➊</span>, with the values 0.3 and 0.2, respectively. This will limit the speed of your robot when it moves toward your colored ball. Again, these are arbitrary numbers, so you can change them if you find that higher or lower values work better for you and your robot.</p>&#13;
<p class="indent">At <span class="ent">➋</span>, we set the number for the hue. This is a value you <em>must change</em> to the value you found earlier using the HSV tester program. I have set mine to 28.</p>&#13;
<p class="indent">Next we set a range of values for the robot to check for instead of a precise one <span class="ent">➌</span>. That way, changes in the environment, like the room’s lighting and how bright it is, will still fall within this small range and therefore the ball will continue to be detected. We do this by using arrays to create the upper and lower bounds of the color in HSV format.</p>&#13;
<p class="indent">In programming, an <em>array</em> is a collection of information where each piece of data in the array has an <em>index</em>, or location, associated with it. Arrays can be as long as you like and can store anything you want, from people’s names to types of animal to lists of numbers. In Python, the first piece of data in an array has an index of 0, the second piece has an index of 1, the third piece has an index of 2, and so on—in other words, Python starts counting the items in an <span epub:type="pagebreak" id="page_188"/>array at 0. This means that, in a Python program, you could ask for the piece of information stored in the array at index 3, for example, and it would return the data found in the fourth position.</p>&#13;
<p class="indent">In this situation, we use arrays to represent the HSV format, as each HSV color can be described with three numbers (the hue, saturation, and value). Notice that we actually search for hues ±10 in either direction, and that the range for the saturation and value components of the color goes from 100 to 255. This ensures that the robot will look for a broader range of colors in each frame from the camera and improves the odds that it will detect the target colored ball.</p>&#13;
<p class="indent">These arrays are available to use through the NumPy library we imported. We use NumPy here because it is a highly optimized library for fast array calculations. This gives us the speed necessary to access and analyze each pixel of each frame.</p>&#13;
<h5 class="h5">Analyzing the Camera Frames</h5>&#13;
<p class="noindent">The third section of the program is shown in <a href="ch08.xhtml#ch08list3">Listing 8-3</a>. This is where the bulk of the code and computer vision process starts.</p>&#13;
<pre><span class="ent">➊</span> <span class="p-blue">for</span> frame <span class="p-blue">in</span> camera.capture_continuous(rawCapture, format=<span class="green">"bgr"</span>, <img alt="Image" src="../images/common.jpg"/><br/>   use_video_port=<span class="p-purple">True</span>):<br/><br/>    <span class="ent">➋</span> image = frame.array<br/><br/>    <span class="ent">➌</span> hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)<br/><br/>    <span class="ent">➍</span> color_mask = cv2.inRange(hsv, lower_color, upper_color)<br/><br/>    <span class="ent">➎</span> image2, countours, hierarchy = cv2.findContours(color_mask, <br/>       cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)</pre>&#13;
<p class="listing" id="ch08list3"><strong>LISTING 8-3</strong> Starting capture <code>for</code> loop, converting image, and finding contours</p>&#13;
<p class="indent">At <span class="ent">➊</span>, we initiate a <code>for</code> loop that, translated into plain English, reads: “for each frame from the Camera Module, do this.”</p>&#13;
<p class="indent">Next, the information from the current frame is saved to the variable <code>image</code> as an array <span class="ent">➋</span>. The RGB data of image is then converted into an HSV format <span class="ent">➌</span> using an OpenCV function, <code>cvtColor()</code>.</p>&#13;
<p class="indent">Once the HSV data has been acquired, the color mask, which keeps <em>only</em> your desired color, is created <span class="ent">➍</span>. We use OpenCV’s <code>inRange()</code> function so that the mask keeps all of the colors that fall between the lower and upper bounds of your color choice.</p>&#13;
<p class="indent">The next stage of the process is to draw lines around each distinct object in the mask so that the area of each detected object can be compared later. We do this at <span class="ent">➎</span>, using OpenCV’s <code>findContours()</code> function.</p>&#13;
<h5 class="h5"><span epub:type="pagebreak" id="page_189"/>Comparing Contours to Find Your Ball</h5>&#13;
<p class="noindent">Next up is <a href="ch08.xhtml#ch08list4">Listing 8-4</a>, which compares each contour and identifies the largest one.</p>&#13;
<pre><span class="ent">➊</span> object_area = 0<br/>   object_x = 0<br/>   object_y = 0<br/><br/><span class="ent">➋</span> <span class="p-blue">for</span> contour <span class="p-blue">in</span> contours:<br/>    <span class="ent">➌</span> x, y, width, height = cv2.boundingRect(contour)<br/>    <span class="ent">➍</span> found_area = width * height<br/>    <span class="ent">➎</span> center_x = x + (width / 2)<br/>       center_y = y + (height / 2)<br/>    <span class="ent">➏</span> <span class="p-blue">if</span> object_area &lt; found_area:<br/>          object_area = found_area<br/>          object_x = center_x<br/>          object_y = center_y<br/><span class="ent">➐</span> <span class="p-blue">if</span> object_area &gt; 0:<br/>       ball_location = [object_area, object_x, object_y]<br/><span class="ent">➑</span> <span class="p-blue">else</span>:<br/>    ball_location = <span class="p-purple">None</span></pre>&#13;
<p class="listing" id="ch08list4"><strong>LISTING 8-4</strong> Comparing and finding the largest contour</p>&#13;
<p class="indent">We create three variables <span class="ent">➊</span> that will later be used to store the largest object’s area and center coordinates. Initially we set these to zero.</p>&#13;
<p class="indent">At <span class="ent">➋</span> we start a <code>for</code> loop that will loop through each and every detected contour. The code at <span class="ent">➌</span> will draw a rectangular box around a contour to approximate its shape. This is known as a <em>bounding box</em>, and it just makes the objects easier to work with. We assign the details of this bounding box to four new variables: <code>x</code>, <code>y</code>, <code>width</code>, and <code>height</code>. As you’d expect, <code>width</code> and <code>height</code> represent the width and height of the rectangle; <code>x</code> and <code>y</code> represent the x- and y-coordinates of the <em>top left</em> of the box.</p>&#13;
<p class="indent">Next, we calculate and store the area of the current contour using the formula for the area of a rectangle: width × height <span class="ent">➍</span>. Then we figure out the center coordinates of the current contour <span class="ent">➎</span> to let the program know where in the frame this particular object is located. Knowing the coordinates of the center of the object is far more useful than knowing where its top-left corner is.</p>&#13;
<p class="indent">At <span class="ent">➏</span> we compare the current contour’s area with the previous largest area already found. If the current contour is larger than the previous one, we assume that the larger area is most likely your colored ball. Consequently, the previous contour’s information is discarded, and the details of the new contour are overwritten on the <span epub:type="pagebreak" id="page_190"/>three variables used to store the largest object’s area and center coordinates.</p>&#13;
<p class="indent">Once the <code>for</code> loop <span class="ent">➋</span> has finished, and all of the contours have been compared, the program verifies that a suitable contour has been found and uses an <code>if</code> statement <span class="ent">➐</span> to check whether the area of the contour is larger than 0. If it is, the largest contour, and therefore probably the ball, has been found and its exact details are saved in a list (a type of basic array) to a variable called <code>ball_location</code>. If a contour hasn’t been found, the variable <code>ball_location</code> is set to <code>None</code> in the <code>else</code> clause <span class="ent">➑</span>.</p>&#13;
<h5 class="h5">Making Your Robot React to the Ball</h5>&#13;
<p class="noindent">The last part of the program, found in <a href="ch08.xhtml#ch08list5">Listing 8-5</a>, deals with making your robot move depending on where the colored ball has been detected in the frame.</p>&#13;
<pre><span class="ent">➊</span> <span class="p-blue">if</span> ball_location:<br/>    <span class="ent">➋</span> <span class="p-blue">if</span> (ball_location[0] &gt; minimum_area) <span class="p-blue">and</span> (ball_location[0] <img alt="Image" src="../images/common.jpg"/><br/>       &lt; maximum_area):<br/>        <span class="ent">➌</span> <span class="p-blue">if</span> ball_location[1] &gt; (center_image_x + <img alt="Image" src="../images/common.jpg"/><br/>           (image_width/3)):<br/>               robot.right(turn_speed)<br/>               <span class="p-blue">print</span>(<span class="green">"Turning right"</span>)<br/>        <span class="ent">➍</span> <span class="p-blue">elif</span> ball_location[1] &lt; (center_image_x - <img alt="Image" src="../images/common.jpg"/><br/>           (image_width/3)):<br/>               robot.left(turn_speed)<br/>               <span class="p-blue">print</span>(<span class="green">"Turning left"</span>)<br/>        <span class="ent">➎</span> <span class="p-blue">else</span>:<br/>               robot.forward(forward_speed)<br/>               <span class="p-blue">print</span>(<span class="green">"Forward"</span>)<br/>    <span class="ent">➏</span> <span class="p-blue">elif</span> (ball_location[0] &lt; minimum_area):<br/>            robot.left(turn_speed)<br/>            <span class="p-blue">print</span>(<span class="green">"Target isn't large enough, searching"</span>)<br/>    <span class="ent">➐</span> <span class="p-blue">else</span>:<br/>           robot.stop()<br/>           <span class="p-blue">print</span>(<span class="green">"Target large enough, stopping"</span>)<br/><span class="ent">➑</span> <span class="p-blue">else</span>:<br/>       robot.left(turn_speed)<br/>       <span class="p-blue">print</span>(<span class="green">"Target not found, searching"</span>)<br/><br/>    rawCapture.truncate(0)</pre>&#13;
<p class="listing" id="ch08list5"><strong>LISTING 8-5</strong> Making the robot move depending on where the ball is found</p>&#13;
<p class="indent">This part of the code features a lot of <code>if</code>, <code>elif</code>, and <code>else</code> statements and jumps around, so go through it carefully and pay attention to the indentation to understand how each statement is constructed.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_191"/>We declare a simple <code>if</code> statement <span class="ent">➊</span> that translates to “if there was a ball found, do this.” If there is no ball found, the program skips right down to the <code>else</code> statement <span class="ent">➑</span>. The program then tells the user that a suitable target wasn’t found and commands your robot to turn left in order to search for the ball in the surrounding environment.</p>&#13;
<p class="indent">If a ball was found in the current frame, an <code>if</code> statement <span class="ent">➋</span> detects whether the size of the ball (stored in the <code>ball_location</code> list at index 0) is within the area values you defined at the start of the program. If the object detected is too small, the robot starts to turn to see if it can find a larger target <span class="ent">➏</span>. If the object detected is too large (caught by the <code>else</code> statement <span class="ent">➐</span>), the robot moves no closer to the ball and stops.</p>&#13;
<p class="indent">If the ball <em>is</em> in frame and the robot <em>does</em> need to move, the code through <span class="ent">➏</span> handles this. At <span class="ent">➌</span> we use an <code>if</code> statement to detect whether the x-coordinate of the ball in the frame is greater than the center point <em>plus a third of the image width</em>. If this is the case, the ball must be in the right-hand side of the frame and consequently the robot moves right.</p>&#13;
<p class="indent">The code at <span class="ent">➍</span> does the same thing but sees whether the x-coordinate of the ball in the frame is less than the center point <em>plus a third of the image width</em>. In this case, the robot turns left.</p>&#13;
<p class="indent">Finally, if the ball is not in either the left or right of the image, it must be in front of your robot and therefore the robot is instructed to move forward <span class="ent">➎</span>.</p>&#13;
<p class="indent">As you can see in <a href="ch08.xhtml#ch08fig25">Figure 8-25</a>, the camera frame is essentially being split into three different areas by this code: left, right, and forward. We use a third of the whole frame’s width on either side of the center point to be the middle section. This means that the middle section represents two-thirds of each frame. If the ball is found here, the robot moves forward. If it’s found on either side, the robot moves left or right. The left and right sections are a sixth of each frame, respectively.</p>&#13;
<p class="indent">And that’s all there is to the ball-following program! The code may have appeared complicated at first, but as with most things in computer science, if you break it down and look at it carefully, you’ll find it is made out of the same simple concepts you’ve used before.</p>&#13;
<div class="image" id="ch08fig25"><img alt="image" src="../images/f192-01.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-25</strong> A diagram of how the frames are split into three areas for movement</p>&#13;
<h4 class="h4" id="ch08lev2sec13"><span epub:type="pagebreak" id="page_192"/>Running Your Program: Make Your Robot Follow a Colored Ball!</h4>&#13;
<p class="noindent">Now you can get to the exciting part of this project: actually making your robot follow your colored ball. Power your robot with its batteries and place it on the floor or another large flat surface. Also place your colored ball somewhere in this environment, as shown in <a href="ch08.xhtml#ch08fig26">Figure 8-26</a>.</p>&#13;
<div class="image" id="ch08fig26"><img alt="image" src="../images/f192-02.jpg"/></div>&#13;
<p class="caption"><strong>FIGURE 8-26</strong> My robot, ready to hunt my yellow ball!</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_193"/>When you have everything set up, run the program with this command:</p>&#13;
<p class="codesingle"><span class="green">pi@raspberrypi</span>:<span class="p-dark-blue">~/robot $</span> <span class="codestrong1">python3 ball_follower.py</span></p>&#13;
<p class="indent">Your robot should spring to life and start hunting down your colored ball. Play a game of fetch with your new smart pet!</p>&#13;
<p class="indent">As usual, press <span class="small">CTRL-</span>C to stop this program.</p>&#13;
<div class="box">&#13;
<p class="headbox">EXPERIMENTING WITH IMAGE PROCESSING</p>&#13;
<p class="noindent">As with the line-following project in the previous chapter, computer vision and image processing are areas of computer science and robotics that lend themselves to fine-tuning in order to improve results and capabilities. Here are some suggestions for you to play around with.</p>&#13;
<p class="boxhead1">Color and Object</p>&#13;
<p class="noindent">While your colored ball is a great starting target for your robot, you can easily take it even further. For example, why not introduce a second color by scanning each frame for a secondary set of HSV values? Make your robot follow both yellow and red objects, for example. Remember that you can go back to the HSV tester program to work out the hue and color codes of other shades!</p>&#13;
<p class="indent">You’re also not just restricted to balls. You can make your robot follow or seek anything that is primarily a single color. Experiment with other objects that you have lying around!</p>&#13;
<p class="boxhead1">Speed</p>&#13;
<p class="noindent">How fast your robot moves does have a big effect on the quality of its image processing: usually, the faster it goes, the more likely it is to miss your colored object. That said, feel free to play around with the speed values defined at the start of the ball-following program—you may be able to fine-tune and improve your robot’s performance!</p>&#13;
<p class="boxhead1">Minimum/Maximum Area of Object</p>&#13;
<p class="noindent">Experiment with the minimum and maximum area of the target object. Remember that your robot by default will not move toward anything smaller than 250 square pixels, and it will stop at anything larger than 100,000 square pixels.</p>&#13;
<p class="indent">By changing these numbers, you can make your robot move toward targets that are potentially smaller or even stop closer to the target. A fun idea is to increase the maximum area to a point where your robot won’t stop when it gets close to your colored ball. The result is that your robot usually ends up bumping into the ball and “kicking” it . . . only to trundle after it again and repeat the process!</p>&#13;
<p class="indent">Remember that each frame of the video feed from your Camera Module measures 640×480 pixels, so a value of 307,200 is the maximum number of square pixels possible.</p>&#13;
<p class="boxhead1">Avoidance Behavior</p>&#13;
<p class="noindent">At the moment, your robot loves your colored ball, but what if you made it so the opposite was true? Try editing the program so that your robot runs away from the object, rather than toward it.</p>&#13;
<p class="indent">An extension of this would be to make your robot move toward certain colored balls, but run away from others. For example, it could love red balls but be terrified of yellow ones!</p>&#13;
</div>&#13;
<h3 class="h3" id="ch08lev1sec6">SUMMARY</h3>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_194"/>In this chapter, you’ve given your robot the advanced ability to seek out, recognize, and follow a colored ball. You have learned the basics of image processing and implemented an entire computer vision process in Python using the official Raspberry Pi Camera Module.</p>&#13;
<p class="indent">And with that you have completed the projects section of this book! Your little robot is now all grown up, and you’re its proud parent. This isn’t the end of the line though; check out “Next Steps” on <a href="bm01.xhtml#page_195">page 195</a> for some guidance and suggestions for continuing your adventures in robotics, programming, and Raspberry Pi.</p>&#13;
</body></html>