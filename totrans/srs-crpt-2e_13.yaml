- en: <hgroup>
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">9</samp> <samp class="SANS_Dogma_OT_Bold_B_11">HARD
    PROBLEMS</samp>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: </hgroup>
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/opener.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hard computational problems are the cornerstone of modern cryptography. These
    are problems for which even the best algorithm wouldn’t find a solution before
    the sun burns out.
  prefs: []
  type: TYPE_NORMAL
- en: In the 1970s, the rigorous study of hard problems gave rise to a new field of
    science called *computational complexity theory*, which dramatically impacted
    cryptography and many other fields, including economics, physics, and biology.
    In this chapter, you’ll learn the conceptual tools from complexity theory necessary
    to understand the foundations of cryptographic security. I’ll also introduce the
    hard problems behind public-key schemes, such as RSA encryption and Diffie–Hellman
    key agreement. I’ll touch on some deep concepts, but I’ll minimize the technical
    details and scratch only the surface. Still, I hope you’ll see the beauty in how
    cryptography leverages computational complexity theory to maximize security assurance.
  prefs: []
  type: TYPE_NORMAL
- en: '### <samp class="SANS_Futura_Std_Bold_B_11">Computational Hardness</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: A computational problem is a question that one can answer by doing enough computation—for
    example, “Is 217 a prime number?” or “How many *i*s are in *incomprehensibilities*?”
    The first question is a decision problem, because it can be answered with “yes”
    or “no,” while the second is a search problem.
  prefs: []
  type: TYPE_NORMAL
- en: '*Computational hardness* is the property of computational problems for which
    there is no algorithm that will run in a reasonable amount of time. Such problems
    are also called *intractable*. Computational hardness is independent of the type
    of computing device used, be it a general-purpose central processing unit (CPU),
    a graphics processing unit (GPU), an integrated circuit, or a mechanical Turing
    machine. Indeed, one of the first findings of computational complexity theory
    is that all computing models are equivalent. If one computing device can solve
    a problem efficiently, any other device can efficiently solve it by porting the
    algorithm to the other device’s language—an exception is quantum computers, which
    we’ll discuss in [Chapter 14](chapter14.xhtml). Thus, I won’t need to specify
    the underlying computing device or hardware when discussing computational hardness;
    instead, we’ll just discuss algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate computational hardness, you first need a way to measure the complexity
    of an algorithm, or its running time. You then categorize running times as hard
    or easy.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Running Time</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *computational complexity* of an algorithm is the approximate number of
    operations it does, as a function of its input size. You can count the size in
    bits or in the number of elements taken as input. For example, take the algorithm
    in [Listing 9-1](chapter9.xhtml#Lis9-1), written in pseudocode. It searches for
    a value, *x*, within an array of *n* elements and then returns its index position,
    or –1, if *x* isn’t found.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 9-1: A simple search
    algorithm of complexity linear with respect to the array length</samp> <samp class="SANS_Futura_Std_Book_11">n</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm uses a <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp>
    loop to find a specific value, *x*, in an array, iterating over values of the
    variable *i*, starting with 0\. It checks whether the value of position *i* in
    <samp class="SANS_TheSansMonoCd_W5Regular_11">array</samp> is equal to the value
    of *x*. If so, it returns the position *i*. Otherwise, it increments *i* and tries
    the next position until it reaches *n* – 1, the last position in the array, at
    which point it returns –1.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this kind of algorithm, you count complexity as the number of iterations
    of the <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop: 1 in the
    best case (if *x* is equal to <samp class="SANS_TheSansMonoCd_W5Regular_11">array[0]</samp>),
    *n* in the worst case (if *x* is equal to <samp class="SANS_TheSansMonoCd_W5Regular_11">array[</samp><samp
    class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">n -</samp> <samp class="SANS_TheSansMonoCd_W5Regular_11">1</samp>]
    or if *x* isn’t found in <samp class="SANS_TheSansMonoCd_W5Regular_11">array</samp>),
    and *n*/2 on average if *x* is uniformly randomly distributed in one of the *n*
    cells of the array. With an array 10 times as large, the algorithm will be 10
    times as slow. Complexity is therefore proportional to *n*, or “linear” in *n*.
    A complexity linear with respect to its input size is considered fast, as opposed
    to exponential complexities. In this example, although processing larger input
    values is slower, the computational cost won’t blow up exponentially but remains
    proportional to the table size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, many useful algorithms are slower than that and have a complexity
    higher than linear. The textbook example is sorting algorithms: given a list of
    *n* values in a random order, you need in the worst case *n* × log *n* basic operations
    to sort the list, which is sometimes called *linearithmic complexity*. Since *n*
    × log *n* grows faster than *n*, sorting speed slows down faster than proportionally
    to *n*. Yet such sorting algorithms remain in the realm of *practical* computation,
    or computation that one can carry out in a reasonable amount of time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What’s usually *not* reasonable are complexities exponential in the input size.
    At some point, you’ll hit the ceiling of what’s feasible even for relatively small
    input lengths. Take the simplest example from cryptanalysis: the brute-force search
    for a secret key. Recall from [Chapter 1](chapter1.xhtml) that given a plaintext
    *P* and a ciphertext *C* = **E**(*K*, *P*), it takes at most 2*^n* attempts to
    recover an *n*-bit symmetric key because there are 2*^n* possible keys—an example
    of a complexity that grows exponentially. A problem with *exponential complexity*
    is practically impossible to solve because as *n* grows, the effort rapidly becomes
    infeasible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may object that we’re comparing oranges and apples here: in the <samp class="SANS_TheSansMonoCd_W5Regular_11">search()</samp>
    function in [Listing 9-1](chapter9.xhtml#Lis9-1), we counted the number of <samp
    class="SANS_TheSansMonoCd_W5Regular_11">if (array[i]</samp> <samp class="SANS_TheSansMonoCd_W5Regular_11">==</samp>
    <samp class="SANS_TheSansMonoCd_W5Regular_11">x)</samp> operations, whereas key
    recovery counts the number of encryptions, each thousands of times slower than
    a single <samp class="SANS_TheSansMonoCd_W5Regular_11">==</samp> comparison. This
    seeming inconsistency can make a difference if you compare two algorithms with
    very similar complexities, but most of the time it won’t matter because the number
    of operations has a greater impact than the cost of an individual operation. Also,
    complexity estimates ignore *constant factors*: when we say that an algorithm
    takes time in the order of *n*³ operations (which is *cubic complexity*), it may
    actually take 41 × *n*³ operations, or even 12,345 × *n*³ operations—but again,
    as *n* grows, the constant factors lose significance to the point that you can
    ignore them. Complexity analysis is about theoretical hardness as a function of
    the input size; it doesn’t care about the exact number of CPU cycles it takes
    on your computer.'
  prefs: []
  type: TYPE_NORMAL
- en: You can often use the *O*() notation (*big O*) to express complexities. For
    example, *O*(*n*³) means that complexity grows no faster than *n*³, ignoring potential
    constant factors. *O*() denotes the *upper bound* of an algorithm’s complexity.
    The notation *O*(1) means that an algorithm runs in *constant time* —that is,
    the running time doesn’t depend on the input length. For example, the algorithm
    that determines an integer’s parity by looking at its least significant bit (LSB)
    and returning “even” if it’s zero and “odd” otherwise will do the same thing at
    the same cost, whatever the integer’s length.
  prefs: []
  type: TYPE_NORMAL
- en: To see the difference between linear, quadratic, and exponential time complexities,
    look at how complexity grows for *O*(*n*) (linear) versus *O*(*n*²) (quadratic)
    versus *O*(2*^n*) (exponential) in [Figure 9-1](chapter9.xhtml#fig9-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/fig9-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 9-1: The growth of exponential,
    quadratic, and linear complexities, from the fastest to the slowest growing</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: Exponential complexity means the problem is practically impossible to solve,
    and linear complexity means the solution is feasible, whereas quadratic complexity
    is somewhere between the two.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Polynomial vs. Superpolynomial
    Time</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The quadratic *O*(*n*²) complexity (the middle curve in [Figure 9-1](chapter9.xhtml#fig9-1))
    is a special case of the broader class of polynomial complexities, or *O*(*n**^k*),
    where *k* is some fixed number such as 3, 2.373, 7/10, or the square root of 17\.
    Polynomial-time algorithms are eminently important in complexity theory and in
    cryptography because they’re the very definition of practically feasible. When
    an algorithm runs in *polynomial time*, or *polytime* for short, it completes
    in a decent amount of time even if the input is large. That’s why polynomial time
    is synonymous with “efficient” for complexity theorists and cryptographers.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, you can view algorithms running in *superpolynomial time*—that
    is, in *O*(*f*(*n*)), where *f*(*n*) is any function that grows faster than any
    polynomial—as impractical. I’m saying superpolynomial, and not just exponential,
    because there are complexities in between polynomial and the well-known exponential
    complexity *O*(2*^n*), such as *O*(*n*^(log()*^n*^)), as [Figure 9-2](chapter9.xhtml#fig9-2)
    shows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/fig9-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 9-2: The growth of the
    2</samp> <samp class="SANS_Futura_Std_Book_SUP_11">n</samp><samp class="SANS_Futura_Std_Book_Oblique_I_11">,</samp>
    <samp class="SANS_Futura_Std_Book_11">n</samp><samp class="SANS_Futura_Std_Book_Oblique_I-SUP_11">log(</samp><samp
    class="SANS_Futura_Std_Book_SUP_11">n</samp><samp class="SANS_Futura_Std_Book_Oblique_I-SUP_11">)</samp><samp
    class="SANS_Futura_Std_Book_Oblique_I_11">, and</samp> <samp class="SANS_Futura_Std_Book_11">n</samp><samp
    class="SANS_Futura_Std_Book_Oblique_I-SUP_11">2</samp> <samp class="SANS_Futura_Std_Book_Oblique_I_11">functions,
    from the fastest to the slowest growing</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: '*O*(*n*²) or *O*(*n*³) may be efficient, but *O*(*n*^(99,999,999,999)) obviously
    isn’t. In other words, polytime is fast in practice as long as the exponent isn’t
    too large. Fortunately, all polynomial-time algorithms found to solve actual problems
    have small exponents. For example, *O*(*n*^(2.373)) is the time complexity of
    the best known algorithm for multiplying two *n* × *n* matrices in theory, as
    this algorithm is never used in practice. The 2002 breakthrough polytime deterministic
    algorithm for identifying *n*-bit prime numbers initially had a complexity *O*(*n*^(12)),
    but it was later improved to *O*(*n*⁶). Polynomial time thus may not be the perfect
    definition of a practical time for an algorithm, but it’s the best we have.'
  prefs: []
  type: TYPE_NORMAL
- en: By extension, you can consider a problem that can’t be solved by a polynomial-time
    algorithm impractical, or *hard*. As an example, for a straightforward key search,
    there’s no way to beat the *O*(2*^n*) complexity unless the cipher is somehow
    broken.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Dogma_OT_Bold_B_15">NOTE</samp>
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Exponential complexity* O*(2*^n*) is not the worst you can get. Some complexities
    grow even faster and thus characterize algorithms even slower to compute—for example,
    the complexity* O*(*n^n*) or the* exponential factorial O*(*n*^(f()*^n *^(– 1))**),
    where for any* x*, the function* f *is here recursively defined as* f*(*x*) =*
    x*^(f()*^x *^(– 1))**. In practice, you’ll never encounter algorithms with such
    preposterous complexities.*'
  prefs: []
  type: TYPE_NORMAL
- en: You know that there’s no way to beat the *O*(2*^n*) complexity of a brute-force
    key search (as long as the cipher is secure), but you won’t always know the fastest
    way to solve a computational problem in general. A large portion of the research
    in complexity theory is about proving complexity *bounds* on the running time
    of algorithms solving a given problem. To make their job easier, complexity theorists
    have categorized computational problems in different groups, or *classes*, according
    to the effort needed to solve them.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">Complexity Classes</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In mathematics, a *class* is a group of objects with some similar attribute.
    For example, all computational problems solvable in time *O*(*n*²), which complexity
    theorists simply denote **TIME**(*n*²), are one class. Likewise, **TIME**(*n*³)
    is the class of problems solvable in time *O*(*n*³), **TIME**(2*^n*) is the class
    of problems solvable in time *O*(2*^n*), and so on. For the same reason that a
    supercomputer can compute whatever a laptop can compute, any problem solvable
    in *O*(*n*²) is also solvable in *O*(*n*³). Hence, any problem in the class **TIME**(*n*²)
    also belongs to the class **TIME**(*n*³), which also both belong to the class
    **TIME**(*n*⁴), and so on. The union of all the classes **TIME**(*n**^k*), for
    all constants *k*, is **P**, which stands for polynomial time.
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve programmed a computer, you’ll know that seemingly fast algorithms
    may still crash your system by eating all its memory resources. When selecting
    an algorithm, you should consider not only its time complexity but also how much
    memory it uses, or its *space complexity*. This is especially important because
    a single memory access is usually orders of magnitudes slower than a basic arithmetic
    operation in a CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, you define an algorithm’s memory consumption as a function of its
    input length, *n*, in the same way you defined time complexity. The class of problems
    solvable using *f*(*n*) bits of memory is **SPACE**(*f*(*n*)). For example, **SPACE**(*n*³)
    is the class of problems solvable using of the order of *n*³ bits of memory. Just
    as you had **P** as the union of all **TIME**(*n**^k*), the union of all **SPACE**(*n**^k*)
    problems is **PSPACE**.
  prefs: []
  type: TYPE_NORMAL
- en: While the lower the memory the better, a polynomial amount of memory doesn’t
    necessarily imply that an algorithm is practical. Take, for example, a brute-force
    key search, which takes negligible memory but is slow as hell. More generally,
    an algorithm can take forever, even if it uses just a few bytes of memory.
  prefs: []
  type: TYPE_NORMAL
- en: Any problem solvable in time *f*(*n*) needs at most *f*(*n*) memory, so **TIME**(*f*(*n*))
    is included in **SPACE**(*f*(*n*)). In time *f*(*n*), you can write up to *f*(*n*)
    bits, and no more, because writing (or reading) 1 bit is assumed to take one unit
    of time; therefore, any problem in **TIME**(*f*(*n*)) can’t use more than *f*(*n*)
    space. As a consequence, **P** is a subset of **PSPACE**.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Nondeterministic
    Polynomial Time</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**NP**, *nondeterministic* polynomial time, is the second most important complexity
    class, after the class **P** of all polynomial-time algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NP** is the class of decision problems for which you can verify a solution
    in polynomial time—that is, efficiently—even though the solution may be hard to
    find. By *verified*, I mean that given a potential solution, you can run some
    polynomial-time algorithm that checks whether you’ve found an actual solution.
    For example, the problem of deciding whether there exists a key *K* such that
    *C* = **E**(*K*, *P*) given *P* and *C* for a symmetric cryptosystem **E** is
    in **NP**. This is because given a candidate key *K*[0], you can check that *K*[0]
    is the correct key by verifying that **E**(*K*[0], *P*) equals *C*. You can’t
    find a potential key (the solution), if it exists, in polynomial time, but you
    can check whether a key is correct.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now for a counterexample: What about known-ciphertext attacks? This time, you
    get only some **E**(*K*, *P*) values for random unknown plaintext *P*s. If you
    don’t know what the *P*s are, then there’s no way to verify whether a potential
    key, *K*[0], is the right one. In other words, the key-recovery problem under
    known-ciphertext attacks is not in **NP** (let alone in **P**), as you can’t express
    it as a decision problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Another example of a problem not in **NP** is that of verifying the *absence*
    of a solution to a problem. Verifying that a solution is correct boils down to
    computing some algorithm with the candidate solution as an input and then checking
    the return value. However, to verify that *no* solution exists, you may need to
    go through all possible inputs. If there’s an exponential number of inputs, you
    won’t be able to efficiently prove that no solution exists. The absence of a solution
    is hard to show for the hardest problems in the class **NP**—the so-called **NP**-complete
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">NP-Complete Problems</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '***NP****-complete* problems are the hardest decision problems in the class
    **NP**; you won’t know how to solve the worst-case instances of these problems
    in polynomial time. As complexity theorists discovered in the 1970s when they
    developed the theory of **NP**-completeness, **NP**’s hardest problems are all
    fundamentally equally hard. This was proven by showing that you can turn any efficient
    solution to any of the **NP**-complete problems into an efficient solution for
    any of the other **NP**-complete problems. In other words, if you can solve any
    **NP**-complete problem efficiently, you can solve all of them, as well as all
    problems in **NP**. How can this be?'
  prefs: []
  type: TYPE_NORMAL
- en: '**NP**-complete problems come in different guises, but they’re fundamentally
    similar from a mathematical perspective. In fact, you can reduce any **NP**-complete
    problem to any other **NP**-complete problem such that the capability to solve
    the second implies the capability to solve the first. Remember that **NP** contains
    decision problems, not search problems. You can efficiently transform an algorithm
    able to solve a search problem into an algorithm able to solve the corresponding
    decision problem, though the converse direction is not always possible. Fortunately,
    this is the case for NP-complete problems, which explains why people often mix
    up the two.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of **NP**-complete problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The traveling salesman problem **Given a set of points on a map (such as
    cities) with the distances between each point from each other point and given
    a maximum distance *x*, decide whether there is a path that visits every point
    such that the total distance is smaller than *x*. (Note that you can find such
    a path with essentially the same complexity as the decision problem, but deciding
    whether a path is optimal is not in **NP**, because you can’t efficiently verify
    a solution’s correctness.)'
  prefs: []
  type: TYPE_NORMAL
- en: '**The clique problem **Given a number, *x*, and a graph (a set of nodes connected
    by edges, as in [Figure 9-3](chapter9.xhtml#fig9-3)), determine whether there’s
    a set of at most *x* nodes that are all connected to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/fig9-3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 9-3: A graph containing
    a clique of four nodes. The general problem of finding a clique (set of nodes
    all connected to each other) of a given size in a graph is</samp> <samp class="SANS_Futura_Std_Bold_Oblique_BI_11">NP</samp><samp
    class="SANS_Futura_Std_Book_Oblique_I_11">-complete.</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: '**The knapsack problem **Given two numbers, *x* and *y*, and a set of items,
    each of a known value and weight, decide if there is a group of items such that
    the total value is at least *x* and the total weight is at most *y.*'
  prefs: []
  type: TYPE_NORMAL
- en: You can find such **NP**-complete problems everywhere, from scheduling (given
    jobs of some priority and duration and one or more processors, assign jobs to
    the processors by respecting the priority while minimizing total execution time)
    to constraint-satisfaction (determine values that satisfy a set of mathematical
    constraints, such as logical equations). Even the task of winning certain video
    games was proven to be **NP**-hard (for famous games including *Tetris*, *Super
    Mario Bros.*, *Pokémon*, and *Candy Crush Saga*). For example, the article “Classic
    Nintendo Games are (Computationally) Hard” considers “the decision problem of
    reachability” to determine the possibility of reaching the goal point from a particular
    starting point (*[https://<wbr>arxiv<wbr>.org<wbr>/abs<wbr>/1203<wbr>.1895](https://arxiv.org/abs/1203.1895)*).
  prefs: []
  type: TYPE_NORMAL
- en: Some of these video game problems are at least as hard as **NP**-complete problems
    and are called **NP**-*hard*. A (not necessarily decisional) problem is **NP**-hard
    when it’s at least as hard as **NP**-complete (decision) problems and if any method
    to solve it can be efficiently used to solve **NP**-complete problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**NP**-complete problems must be decisional problems; that is, problems with
    a yes or no answer. Therefore, strictly speaking, problems of computing the “best”
    value of a solution cannot be **NP**-complete but may be **NP**-hard. For example,
    take the traveling salesman problem: the problem “Is there a path visiting all
    points with a distance less than *X*?” is **NP**-complete, whereas “Find the faster
    path visiting all points” is **NP**-hard.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that not all *instances* of **NP**-hard problems are actually hard to solve.
    You may be able to efficiently solve some instances because they’re small or have
    a specific structure. Take, for example, the graph in [Figure 9-3](chapter9.xhtml#fig9-3).
    You can quickly spot the clique, which is the top four connected nodes—even though
    the aforementioned clique-finding problem is **NP**-hard, there’s nothing hard
    here. Being **NP**-hard doesn’t mean that all instances of a given problem are
    hard but that as the problem size grows, some of them are. This is why cryptographers
    are more interested in problems that are hard on average, not just in the worst
    case.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">The P vs. NP Problem</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you could solve the hardest **NP** problems in polynomial time, then you
    could solve *all* **NP** problems in polynomial time, and therefore **NP** would
    equal **P**. Such an equality sounds preposterous: Aren’t there problems for which
    a solution is easy to verify but hard to find? For example, isn’t it obvious that
    exponential-time brute force is the fastest way to recover the key of a symmetric
    cipher, and therefore that the problem can’t be in **P**?'
  prefs: []
  type: TYPE_NORMAL
- en: 'As crazy as it sounds, no one has proved that **P** is different from **NP**,
    despite a bounty of $1 million. The Clay Mathematics Institute will award this
    to anyone who proves that either **P** ≠ **NP** or **P** = **NP**. This problem,
    known as **P** vs. **NP**, was called “one of the deepest questions that human
    beings have ever asked” by renowned complexity theorist Scott Aaronson. Think
    about it: if **P** were equal to **NP**, then any easily checked solution would
    also be easy to find. All cryptography used in practice would be insecure because
    you could recover symmetric keys and invert hash functions efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But don’t panic: most complexity theorists believe **P** isn’t equal to **NP**
    and therefore that **P** is instead a strict subset of **NP**, as [Figure 9-4](chapter9.xhtml#fig9-4)
    shows, where **NP**-complete problems are another subset of **NP** not overlapping
    with **P**. In other words, problems that look hard actually are hard. It’s just
    difficult to prove this mathematically. While proving that **P** = **NP** requires
    only a polynomial-time algorithm for an **NP**-complete problem, proving the nonexistence
    of such an algorithm is fundamentally harder. This didn’t stop mathematicians
    from coming up with simple proofs that, while usually obviously wrong, often make
    for funny reads; for an example, see “The P-versus-NP page” (*[https://<wbr>www<wbr>.win<wbr>.tue<wbr>.nl<wbr>/~wscor<wbr>/woeginger<wbr>/P<wbr>-versus<wbr>-NP<wbr>.htm](https://www.win.tue.nl/~wscor/woeginger/P-versus-NP.htm)*).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/fig9-4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 9-4: The classes</samp>
    <samp class="SANS_Futura_Std_Bold_Oblique_BI_11">NP</samp> <samp class="SANS_Futura_Std_Book_Oblique_I_11">and</samp>
    <samp class="SANS_Futura_Std_Bold_Oblique_BI_11">P</samp> <samp class="SANS_Futura_Std_Book_Oblique_I_11">and
    the set of</samp> <samp class="SANS_Futura_Std_Bold_Oblique_BI_11">NP</samp><samp
    class="SANS_Futura_Std_Book_Oblique_I_11">-complete problems</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we’re almost sure that hard problems exist in **NP**, what about leveraging
    them to build strong, provably secure crypto? Imagine a proof that breaking some
    cipher is **NP**-hard and therefore that the cipher is unbreakable as long as
    **P** isn’t equal to **NP**. But reality is disappointing: the search versions
    of **NP**-complete problems have proved difficult to use for crypto purposes because
    the very structure that makes them hard in the worst case can make them easy in
    specific cases that sometimes occur in crypto. Instead, cryptography often relies
    on problems that are *probably not* **NP**-hard.'
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">The Factoring Problem</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The factoring problem consists of finding the prime numbers *p* and *q* given
    a large number, *N* = *p* × *q*. The widely used RSA algorithms are based on the
    difficulty of factoring: RSA encryption and signature schemes are secure because
    factoring is a hard problem. Before we see how RSA leverages the factoring problem
    in [Chapter 10](chapter10.xhtml), I’d like to convince you that the decision version
    of this problem (“Does *N* have a factor smaller than *k* that is not equal to
    1?”) is indeed hard yet probably not **NP**-complete.'
  prefs: []
  type: TYPE_NORMAL
- en: First, some basic math. A *prime number* isn’t divisible by any other number
    but itself and 1\. For example, the numbers 3, 7, and 11 are prime; the numbers
    4 (that is, 2 × 2), 6 (2 × 3), and 12 (2 × 2 × 3) are not. A fundamental theorem
    of number theory says that you can write any integer number uniquely as a product
    of primes, a representation called the *factorization* of that number. For example,
    the factorization of 123,456 is 2⁶ × 3 × 643, the factorization of 1,234,567 is
    127 × 9,721, and so on. Any integer has a unique factorization, or a unique way
    to write it as a product of prime numbers. Polynomial-time primality testing algorithms
    allow us to efficiently test whether a given number is prime or a given factorization
    contains only prime numbers. Getting from a number to its prime factors, however,
    is another matter.
  prefs: []
  type: TYPE_NORMAL
- en: '#### <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Factoring Large
    Numbers</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: So how do you go from a number to its factorization—namely, its decomposition
    as a product of prime numbers? The most basic way to factor a number, *N*, is
    to try dividing it by all the numbers lower than it until you find a number, *x*,
    that divides *N*. Then attempt to divide *N* with the next number, *x* + 1, and
    so on. You’ll end up with a list of factors of *N*. What’s the time complexity
    of this? First, remember that we express complexities as a function of the input’s
    *length*. The bit length of the number *N* is *n* = log[2] *N*. By definition
    of the logarithm, this means that *N* = 2*^n*. Because all the numbers less than
    *N*/2 are reasonable guesses for possible factors of *N*, there are about *N/*2
    = 2*^n*/2 values to try. The complexity of our naive factoring algorithm is therefore
    *O*(2*^n*), ignoring the 1/2 coefficient in the *O*() notation.
  prefs: []
  type: TYPE_NORMAL
- en: While many numbers are easy to factor by first finding any small factors (2,
    3, 5, and so on) and then iteratively factoring any other nonprime factors, here
    we’re interested in numbers of the form *N* = *p* × *q*, where *p* and *q* are
    large, as found in cryptography.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s be a bit smarter: as we don’t need to test all numbers lower than *N*/2,
    but rather only the prime numbers, we can start by trying those smaller than the
    square root of *N*. If *N* isn’t a prime number, then it must have at least one
    factor lower than its square root √*N.* This is because if both of *N*’s factors
    *p* and *q* were greater than √*N*, then their product would be greater than √*N*
    × √*N* = *N*, which is impossible. For example, if *N* = 100, its factors *p*
    and *q* can’t both be greater than 10 because that would result in a product greater
    than 100\. Either *p* or *q* has to be less than or equal to √*N*.'
  prefs: []
  type: TYPE_NORMAL
- en: So what’s the complexity of testing only the primes less than √*N*? The *prime
    number theorem* states that there are approximately *N*/log *N* primes smaller
    than *N*. Hence, there are approximately √*N*/log √*N* primes smaller than √*N*.
    Expressing this value in terms of *n* = log[2] *N*, we get approximately 2*^n*^(/2
    + 1)/*n* possible prime factors and therefore a complexity of *O*(2*^n*^(/2)/*n*),
    since √*N* = 2*^n*^(/2) and 1/log √*N* = 1/(*n*/2) = 2/*n*. This is faster than
    testing all prime numbers, but it’s still painfully slow—on the order of 2^(120)
    operations for a 256-bit number. That’s quite an impractical computational effort.
    But we can do much better.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fastest factoring algorithm is the *general number field sieve (GNFS)*,
    which I won’t describe here because it requires the introduction of several advanced
    mathematical concepts. A rough estimate of GNFS’s complexity is exp(1.91 × *n*^(1/3)
    (log *n*)^(2/3)), where *n* = log[2] *N* is the bit length of *N* and exp(. .
    .) is just a different notation for the exponential function *e* *^x*, with *e*
    the exponential constant approximately equal to 2.718\. However, it’s difficult
    to get an accurate estimate of GNFS’s actual complexity for a given number size.
    Therefore, we must rely on heuristic complexity estimates, which show how security
    increases with a longer *n*. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Factoring a **1,024-bit** number, which has two prime factors of approximately
    500 bits each, takes on the order of 2^(70) basic operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factoring a **2,048-bit** number, which has two prime factors of approximately
    1,000 bits each, takes on the order of 2^(90) basic operations—about a million
    times slower than for a 1,024-bit number.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can estimate that reaching 128-bit security requires at least 4,096 bits.
    Take these values with a grain of salt, as researchers don’t always agree on these
    estimates. The following experimental results reveal the actual cost of factoring:'
  prefs: []
  type: TYPE_NORMAL
- en: In 2005, after about 18 months of computation—and thanks to the power of a cluster
    of 80 processors, with a total effort equivalent to 75 years of computation on
    a single processor—a group of researchers factored a **663-bit** (200-decimal
    digit) number.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2009, after about two years and using several hundred processors, with a
    total effort equivalent to about 2,000 years of computation on a single processor,
    another group of researchers factored a **768-bit** (232-decimal digit) number.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2020, after a few months of computation, using tens of thousands of processors
    and a supercomputer, for a total effort equivalent to around 2,700 years of calculation
    on a single processor, another team factored an **829-bit** (250-decimal digit)
    number.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the numbers factored by researchers are shorter than those in
    real applications, which are at least 1,024-bit and often more than 2,048-bit.
    As I write this, no one has reported the factoring of a 1,024-bit number, but
    many speculate that well-funded organizations such as the NSA can do it.
  prefs: []
  type: TYPE_NORMAL
- en: In sum, you should view 1,024-bit RSA as insecure and use RSA with at least
    a 2,048-bit value, but preferably a 4,096-bit one to ensure higher security.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Factoring Is Probably
    Not NP-Hard</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We don’t know how to factor large numbers efficiently, which suggests that the
    factoring problem doesn’t belong to **P**. However, the decision version of factoring
    is clearly in **NP**, because given a factorization, we can verify the solution
    by checking that all factors are prime numbers, thanks to the aforementioned primality
    testing algorithm, and that when multiplied together, the factors give the expected
    number. For example, to check that 3 × 5 is the factorization of 15, confirm that
    both 3 and 5 are prime and that 3 times 5 equals 15.
  prefs: []
  type: TYPE_NORMAL
- en: 'So we have a problem that is in **NP** and that looks hard, but is it as hard
    as the hardest **NP** problems? In other words, is the decision version of factoring
    **NP**-complete, and as a consequence, is factoring **NP**-hard? Spoiler alert:
    probably not.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s no mathematical proof that factoring isn’t **NP**-hard, but we have
    a few pieces of soft evidence. First, all known **NP**-hard problems in **NP**
    can have one solution, more than one solution, or no solution at all. In contrast,
    factoring always has exactly one solution. Also, the factoring problem has a mathematical
    structure that allows for the GNFS algorithm to significantly outperform a naive
    algorithm, a structure that **NP**-hard problems don’t have. Factoring would be
    easy with a *quantum computer*, a computing model that exploits quantum mechanical
    phenomena to run different kinds of algorithms and that would have the capability
    to factor large numbers efficiently (not because it’d run the algorithm faster
    but because it could run a quantum algorithm dedicated to factoring large numbers).
    Such a quantum computer doesn’t exist yet, though—and might never. Regardless,
    a quantum computer is believed to be useless in tackling **NP**-hard problems
    because it’d be no faster than a classical one (see [Chapter 14](chapter14.xhtml)).
  prefs: []
  type: TYPE_NORMAL
- en: Factoring may be slightly easier than solving **NP**-hard problems in theory,
    but as far as cryptography is concerned, it’s hard enough, and even more reliable
    than **NP**-hard problems. Indeed, it’s easier to build cryptosystems on top of
    the factoring problem than search versions of **NP**-complete problems because
    it’s difficult to know exactly how hard it is to break a cryptosystem based on
    such problems—in other words, how many bits of security you’d get. As mentioned
    earlier, this relates to the fact that **NP** concerns worst-case hardness, and
    cryptographers are looking for average-case hardness.
  prefs: []
  type: TYPE_NORMAL
- en: The factoring problem is one of several problems you can use in cryptography
    as a *hardness assumption*, which is an assumption that some problem is computationally
    hard. You can use this assumption when proving that breaking a cryptosystem’s
    security is at least as hard as solving said problem. Another problem you can
    use as a hardness assumption, the *discrete logarithm problem (DLP)*, is actually
    a family of problems.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">The Discrete Logarithm Problem</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Discrete Logarithm Problem (DLP) predates the factoring problem in the official
    history of cryptography. Whereas RSA appeared in 1977, another cryptographic breakthrough,
    the Diffie–Hellman key agreement (see [Chapter 11](chapter11.xhtml)), came about
    a year earlier, grounding its security on the hardness of the DLP. Like the factoring
    problem, the DLP deals with large numbers, but it’s less straightforward and requires
    more math than factoring. Let’s begin by introducing the mathematical notion of
    a group in the context of discrete logarithms.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Groups</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a mathematical context, a *group* is a set of elements (typically, numbers)
    that relate to each other according to certain well-defined rules. An example
    of a group is the set of nonzero integers modulo a prime number *p* (that is,
    numbers between 1 and *p* – 1) with modular multiplication being the group operation.
    We note such a group as (**Z**p^*, ×), or just **Z**p^* when it’s clear that the
    group operation is multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: For *p* = 5, you get the group **Z**[5]^* = {1, 2, 3, 4}. In the group **Z**[5]^*,
    operations are carried out modulo 5; hence, you don’t have 3 × 4 = 12 but instead
    3 × 4 = 2, because 12 mod 5 = 2\. You can nonetheless use the same sign (×) that
    you use for normal integer multiplication. You can also use the exponent notation
    to denote a group element’s multiplication with itself mod *p*, a common operation
    in cryptography. For example, in the context of **Z**[5]^*, 2³ = 2 × 2 × 2 = 3
    rather than 8, because 8 mod 5 is equal to 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be a group, a mathematical set and its operation must have the following
    characteristics, which are called *group axioms*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Closure **For any two group elements *x* and *y*, *x* × *y* is in the group
    too. In **Z**[5]^*, 2 × 3 = 1 (because 6 = 1 mod 5), 2 × 4 = 3, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Associativity **For any group elements *x*, *y*, *z*, (*x* × *y*) × *z* =
    *x* × (*y* × *z*). In **Z**[5]^*, (2 × 3) × 4 = 1 × 4 = 2 × (3 × 4) = 2 × 2 =
    4.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identity existence **The group includes an element *e* such that *e* × *x*
    = *x* × *e* = *x*. Such an element is called the *identity*. In any **Z**p^*,
    the identity element is 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inverse existence **For any *x* in the group, there’s a *y* such that *x*
    × *y* = *y* × *x* = *e*. In **Z**[5]^*, the inverse of 2 is 3, and the inverse
    of 3 is 2, while 4 is its own inverse because 4 × 4 = 16 = 1 mod 5.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, a group is *commutative*, or *abelian*, if *x* × *y* = *y* × *x*
    for any group elements *x* and *y*. That’s also true for any multiplicative group
    of integers **Z**p^*. In particular, **Z**[5]^* is commutative: 3 × 4 = 4 × 3,
    2 × 3 = 3 × 2, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: A group is *cyclic* if there’s at least one element *g* such that its powers
    (*g*¹, *g*², *g*³, and so on) mod *p* span all distinct group elements. The element
    *g* is then a *generator* of the group. **Z**[5]^* is cyclic and has two generators,
    2 and 3, because 2¹ = 2, 2² = 4, 2³ = 3, 2⁴ = 1, and 3¹ = 3, 3² = 4, 3³ = 2, 3⁴
    = 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that I’m using multiplication as a group operator, but you can also get
    groups from other operators. For example, the most straightforward group is the
    set of all integers, positive and negative, with addition as a group operation.
    Let’s check that the group axioms hold with addition, in the preceding order:
    the number *x* + *y* is an integer if *x* and *y* are integers (closure); (*x*
    + *y*) + *z* = *x* + (*y* + *z*) for any *x*, *y*, and *z* (associativity); zero
    is the identity element; and the inverse of any number *x* in the group is –*x*
    because *x* + (–*x*) = 0 for any integer *x*. A big difference, though, is that
    this group of integers is of infinite size, whereas in crypto you’ll deal with
    only *finite groups*, or groups with a finite number of elements, for implementation
    reasons. Typically, you’ll use groups **Z**p^*, where *p* is *thousands* of bits
    long (that is, groups that contain on the order of 2*^m* numbers if *p* is *m*-bit
    long).'
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">The Hard Thing</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The discrete logarithm problem as initially used in cryptography consists of
    finding the *y* for which *g**^y* = *x*, given a generator *g* within some group
    **Z**p^*, where *p* is a prime number, and given a group element *y*. We often
    express the DLP in additive rather than multiplicative notation, as in groups
    of elliptic curves. In this case, the problem is to find the multiplicative factor
    *k* such that *k* × *P* = *Q*, where you know the points *P* and *Q*. This is
    called the *elliptic curve DLP (ECDLP)*.
  prefs: []
  type: TYPE_NORMAL
- en: The DLP is *discrete* because you’re dealing with countable integers as opposed
    to uncountable real numbers, and it’s a *logarithm* because you’re looking for
    the logarithm of *x* in base *g*. For example, the logarithm of 256 in base two
    is 8 because 2⁸ = 256.
  prefs: []
  type: TYPE_NORMAL
- en: Factoring is about as equally hard, thus as secure, as a discrete logarithm.
    In fact, algorithms to solve DLP bear similarities with those factoring integers,
    and you get about the same security level with *n*-bit hard-to-factor numbers
    as with discrete logarithms in an *n*-bit group. For the same reason as factoring,
    DLP isn’t **NP**-hard. (There are certain groups where the DLP is easier to solve,
    but these aren’t used in cryptography, at least not where DLP hardness is needed.)
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">How Things Can Go Wrong</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More than 40 years later, we still don’t know how to efficiently factor large
    numbers or solve discrete logarithms. In the absence of mathematical proof, it’s
    always possible to speculate that they’ll be broken one day. But we also don’t
    have proof that **P** ≠ **NP**, so you can speculate that **P** may be equal to
    **NP**; however, according to experts, that surprise is unlikely. Most public-key
    crypto deployed today relies on either factoring (RSA) or DLP (Diffie–Hellman,
    ElGamal, elliptic curve cryptography). Although math may not fail us, real-world
    concerns and human error can sneak in.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">When Factoring Is
    Easy</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Factoring large numbers isn’t always hard. For example, take the following
    1,024-bit number *N*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/pg191-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For 1,024-bit numbers in RSA encryption or signature schemes where *N* = *pq*,
    we expect the best factoring algorithms to need around 2^(70) operations, as we
    discussed earlier. But you can factor this sample number in seconds using SageMath,
    a piece of Python-based mathematical software. Using SageMath’s <samp class="SANS_TheSansMonoCd_W5Regular_11">factor()</samp>
    function on my 2023 MacBook, it took less than a second to find the following
    factorization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/pg191-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Right, I cheated. This number isn’t of the form *N* = *pq* because it doesn’t
    have just two large prime factors but rather five, including very small ones,
    which makes it easy to factor. First, you identify the 2^(800) × 641 × 6,700,417
    part by trying small primes from a precomputed list of prime numbers, which leaves
    you with a 192-bit number that’s much easier to factor than a 1,024-bit number
    with two large factors.
  prefs: []
  type: TYPE_NORMAL
- en: Factoring can be easy not only when *n* has small prime factors but also when
    *N* or its factors *p* and *q* have particular forms—for example, when *N* = *pq*
    with *p* and *q* both close to some 2*^b*, when *N* = *pq* and some bits of *p*
    or *q* are known, or when *N* is of the form *N* = *p**^r**q**^s* and *r* is greater
    than log *p*. However, detailing the reasons for these weaknesses is too technical
    for this book.
  prefs: []
  type: TYPE_NORMAL
- en: The upshot is that the RSA encryption and signature algorithms (see [Chapter
    10](chapter10.xhtml)) need to work with a value of *N* = *pq*, where *p* and *q*
    are carefully chosen, to avoid easy factorization of *N*, which can result in
    a security disaster.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Small Hard Problems
    Aren’t Hard</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Computationally hard problems, and even exponential-time algorithms, become
    practical when they’re small enough. A symmetric cipher may be secure in the sense
    that there’s no faster attack than the 2*^n*-time brute force, but if the key
    length is *n* = 32, you’ll break the cipher in minutes. This sounds obvious, and
    you’d think that no one would be naive enough to use small keys, but in reality,
    there are plenty of reasons why this could happen. The following are two true
    stories.
  prefs: []
  type: TYPE_NORMAL
- en: Say you’re a developer who knows nothing about crypto but has some API to encrypt
    with RSA and has been told to encrypt with 128-bit security. What RSA key size
    would you pick? I’ve seen real cases of 128-bit RSA, or RSA based on a 128-bit
    number *N* = *pq*. However, although factoring is impractically hard for an *N*
    thousands of bits long, factoring a 128-bit number is easy. Using the SageMath
    software, the commands in [Listing 9-2](chapter9.xhtml#Lis9-2) complete instantaneously.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 9-2: Generating an
    RSA modulus by picking two random prime numbers and factoring it instantaneously</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 9-2](chapter9.xhtml#Lis9-2) shows that you can easily factor a 128-bit
    number taken randomly as the product of two 64-bit prime numbers on a typical
    laptop. However, if I chose 1,024-bit prime numbers instead by using <samp class="SANS_TheSansMonoCd_W5Regular_11">p
    = random _prime(2**1024)</samp>, the command <samp class="SANS_TheSansMonoCd_W5Regular_11">factor(p*q)</samp>
    would never complete, at least not in my lifetime.'
  prefs: []
  type: TYPE_NORMAL
- en: To be fair, the tools available don’t help prevent the naive use of insecurely
    short parameters. For example, the OpenSSL toolkit used to let you generate RSA
    keys as short as 31 bits without any warning; such short keys are totally insecure,
    as [Listing 9-3](chapter9.xhtml#Lis9-3) shows. OpenSSL has since been fixed, and
    its version 1.1.1t (from February 2023) returns an error “key size too small”
    if you request a key shorter than 512 bits.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 9-3: Generating an
    insecure RSA private key using an older version of the OpenSSL toolkit</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: When reviewing cryptography, you should check not only the type of algorithms
    used but also their parameters and the length of their secret values. However,
    as you’ll see in the following story, what’s secure enough today may be insecure
    tomorrow.
  prefs: []
  type: TYPE_NORMAL
- en: In 2015, researchers discovered that many HTTPS servers and email servers supported
    an older, insecure version of the Diffie–Hellman key agreement protocol. Namely,
    the underlying TLS implementation supported Diffie–Hellman within a group, **Z**p^*,
    defined by a prime number, *p*, of only 512 bits, where the discrete logarithm
    problem was no longer practically impossible to compute.
  prefs: []
  type: TYPE_NORMAL
- en: Not only did servers support a weak algorithm, but also attackers could force
    a benign client to use that algorithm by injecting malicious traffic within the
    client’s session. Even better for attackers, the largest part of the attack could
    be carried out once and recycled to attack multiple clients. After about a week
    of computations to attack a specific group, **Z**p^*, it took only 70 seconds
    to break individual sessions of different users.
  prefs: []
  type: TYPE_NORMAL
- en: A secure protocol is worthless if it’s undermined by a weakened algorithm, and
    a reliable algorithm is useless if sabotaged by weak parameters. In cryptography,
    you should always read the fine print.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details about this story, check the research article “Imperfect Forward
    Secrecy: How Diffie–Hellman Fails in Practice” (*[https://<wbr>weakdh<wbr>.org<wbr>/imperfect<wbr>-forward<wbr>-secrecy<wbr>-ccs15<wbr>.pdf](https://weakdh.org/imperfect-forward-secrecy-ccs15.pdf)*).'
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">Further Reading</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I encourage you to look deeper into the foundational aspects of computation
    in the context of computability (what functions can be computed?) and complexity
    (at what cost?) and how they relate to cryptography. I’ve talked mostly about
    the classes **P** and **NP**, but there are many more classes and points of interest
    for cryptographers. I highly recommend the book *Quantum Computing Since Democritus*
    by Scott Aaronson (Cambridge University Press, 2013). It’s in large part about
    quantum computing, but its first chapters brilliantly introduce complexity theory
    and cryptography.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the cryptography research literature, you’ll find other hard computational
    problems. I’ll mention them in later chapters, but here are some examples that
    illustrate the diversity of problems leveraged by cryptographers:'
  prefs: []
  type: TYPE_NORMAL
- en: The Diffie–Hellman problem (given *g**^x* and *g* *^y*, find *g**^(xy)*) is
    a variant of the discrete logarithm problem and is widely used in key agreement
    protocols.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lattice problems, such as the shortest vector problem (SVP) as well as the short
    integer solutions (SIS) and learning with errors (LWE) problems, can be **NP**-hard,
    depending on their parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding problems rely on the hardness of decoding error-correcting codes with
    insufficient information and have been studied since the late 1970s. These can
    also be **NP**-hard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multivariate problems are about solving nonlinear systems of equations and are
    potentially **NP**-hard, but they’ve failed to provide reliable cryptosystems
    because hard versions are too big and slow, and practical versions were found
    to be insecure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Chapter 10](chapter10.xhtml), we’ll continue exploring hard problems, especially
    factoring and its main variant, the RSA problem.
  prefs: []
  type: TYPE_NORMAL
