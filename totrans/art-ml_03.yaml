- en: '**2'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CLASSIFICATION MODELS**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The last chapter briefly introduced *classification applications*, in which
    we predict dummy or categorical variables. These differ from the *numeric applications*
    we’ve analyzed, such as predicting the number of bike riders, which is a numeric
    entity. For instance, in a marketing application, we might wish to predict whether
    a customer will purchase a certain product. In that case, we’d represent the “Y”
    outcome with a dummy variable, using 1 for buying the item and 0 for not buying
    it. There are two *classes* here: Buy and Not Buy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We discussed an example of categorical *Y* in the “Some Terminology” box in
    [Section 1.1](ch01.xhtml#ch01lev1). In that example, a physician has divided patients
    into three classes—that is, three categories—depending on their spinal condition:
    normal (NO), disk hernia (DH), and spondylolisthesis (SL). Here *Y* is the class,
    or category, for the given patient. Thus *Y* is a categorical variable with three
    categories. If *Y* is coded as an R *factor*, which is usually the case, then
    the factor will have three levels. On the other hand, we might code *Y* as a vector
    of dummy variables, with one for each class.'
  prefs: []
  type: TYPE_NORMAL
- en: We will analyze this vertebrae data in detail in [Section 2.3.1](ch02.xhtml#ch02lev3sec1),
    stating where to obtain it and so on. But let’s do a sneak preview to illustrate
    the previously described notions of class, or category.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The vertebral condition is in the last column. We see that in this dataset,
    there were 60 patients of class DH and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter goes into more detail regarding classification applications, beginning
    with a brief discussion of a conceptual issue, the notion of a *regression function*,
    and then getting right to the data analysis. We’ll bring in some new datasets,
    again analyzing them using `qeKNN()` but showing some special issues that arise
    in classification contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Classification Is a Special Case of Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classification applications are quite common in ML. In fact, they probably form
    the majority of ML applications. How does the regression function *r*(*t*) (see
    [Section 1.6](ch01.xhtml#ch01lev6)) play out in such contexts?
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the regression function relates mean *Y* to *X*. If we are predicting
    weight from height and age, then *r*(71, 25) means the mean weight of all people
    of height 71 inches and age 25\. But how does this work if *Y* is a dummy variable?
  prefs: []
  type: TYPE_NORMAL
- en: In classification settings, the regression function, a conditional mean, becomes
    a conditional probability. To see why, consider a classification application in
    which the outcome *Y* is represented by a dummy variable coded 1 or 0, such as
    the marketing example at the start of this chapter. After collecting our *k*-nearest
    neighbors, we average their 1s and 0s. Say, for example, that *k* = 8, and the
    outcomes for those 8 neighbors are 0,1,1,0,0,0,1,0\. The average is then (0 +
    1 + 1 + 0 + 0 + 0 + 1 + 0) / 8 = 3/8 = 0.375\. Since this means that 3/8 of the
    outcomes were 1s, you can think of the average of 0-or-1 outcomes as the *probability*
    of a 1.
  prefs: []
  type: TYPE_NORMAL
- en: In the marketing example, the regression function is the probability that a
    customer will buy a certain product, conditioned on the customer’s feature values,
    such as age, gender, income, and so on. We then guess either Buy or Not Buy according
    to whichever class has the larger probability. Since there are just two classes
    here, that’s equivalent to saying we guess Buy if and only if its class probability
    is greater than 0.5\. (This strategy minimizes the overall probability of misclassification.
    However, other criteria are possible, which is a point we will return to later.)
  prefs: []
  type: TYPE_NORMAL
- en: The same is true in multiclass settings. Consider the medical application above.
    For a new patient whose vertebral status is to be predicted, ML would give the
    physician three probabilities—one for each class. As above, these probabilities
    come from averaging 1s and 0s in dummy variables, with one dummy for each class.
    The preliminary diagnosis would be the spinal class with the highest estimated
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary:'
  prefs: []
  type: TYPE_NORMAL
- en: The function *r*() defined for use in predicting numeric quantities applies
    to classification settings as well. In those settings, mean values reduce to probabilities,
    and we use those to predict class.
  prefs: []
  type: TYPE_NORMAL
- en: This view is nice as a unifying concept.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The reader may wonder why we use three dummy variables to classify patients’
    spinal conditions. As noted in [Section 1.4](ch01.xhtml#ch01lev4), just two should
    suffice here; however, certain other methods covered later on require more than
    two. For consistency, we’ll always take the approach of using as many dummies
    as classes. Note that this convention is for* Y*; categorical features in* X *will
    still usually have one fewer dummy than values that the feature can take on.*'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, classification problems are really special cases of regression, which
    is a point we’ll often return to in this book. However, the field uses some confusing
    terminology, which one must be aware of.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we distinguished between, on the one hand, *numerical* applications,
    say, predicting the *number* of bike riders, and, on the other hand, *classification*
    applications, such as the earlier marketing and medical examples where we are
    predicting a *class*.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is customary in the ML field to call numeric applications *regression
    problems*. This, of course, is a source of confusion, since both numeric and classification
    applications involve the regression function! Sigh . . . As long as you are aware
    of this, it’s not a big issue, but in this book we will use the term *numeric-Y
    applications* for clarity.
  prefs: []
  type: TYPE_NORMAL
- en: '2.2 Example: The Telco Churn Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For our first example of a classification model, we’ll take the Telco Customer
    Churn dataset. In marketing circles, the term *churn* refers to customers moving
    from one purveyor of a service to another. A service will then hope to identify
    customers who are likely “flight risks,” or those with a substantial probability
    of leaving. So, we have two classes: Churn or No Churn (that is, Leave or Stay).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download and learn more about the dataset at [*https://www.kaggle.com/blastchar/telco-customer-churn*](https://www.kaggle.com/blastchar/telco-customer-churn).
    Let’s load it and take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: That last column is the response; `Yes` in the `Churn` column means yes, the
    customer bolted.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to data preparation, such as checking for NA values. This is a
    rather complex dataset, necessitating extra prep—a bonus for learners of data
    science like readers of this book!
  prefs: []
  type: TYPE_NORMAL
- en: '***2.2.1 Pitfall: Factor Data Read as Non-factor***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many features in the telco dataset are R factors (that is, nonnumeric quantities
    such as `gender` and `InternetService`). Most R ML packages, including the `qe*`-series
    functions, allow factors. But wait . . . they’re not factors after all. By default,
    `read.csv()` treats nonnumeric items as character strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Since our software expects R factors, we need to tell R to treat nonnumeric
    items as factors. (Actually, depending on your version of R, and your default
    settings, this may actually be your default value. If you are not sure, go ahead
    and set this in your call.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Failure to do this will result in character or numeric values, causing problems
    when we run the ML functions.
  prefs: []
  type: TYPE_NORMAL
- en: '***2.2.2 Pitfall: Retaining Useless Features***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In some datasets, some columns have no predictive value and should be removed.
    The `customerID` feature is of no predictive value (though it might be if we had
    multiple data points for each customer), so we’ll delete the first column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Retaining useless features can lead to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '***2.2.3 Dealing with NA Values***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As noted in [Section 1.16](ch01.xhtml#ch01lev16), many datasets contain NA
    values for missing (not available) data. Let’s see if this is the case here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: There are indeed 11 NA values present.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use listwise deletion here as a first-level analysis (see [Section 1.16](ch01.xhtml#ch01lev16)),
    but if we were to pursue the matter further, we may look more closely at the NA
    pattern. R actually has a `complete.cases()` function, which returns `TRUE` for
    the rows that are intact. Let’s delete the other cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If we don’t need to know which particular cases are excluded, we could simply
    run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How many cases are left?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: It’s generally a good idea to check this.
  prefs: []
  type: TYPE_NORMAL
- en: Among other things, the number of remaining rows affects the size of the value
    we choose for the number of near neighbors *k* (see [Section 1.12.4](ch01.xhtml#ch01lev12sec4)).
  prefs: []
  type: TYPE_NORMAL
- en: '***2.2.4 Applying the k-Nearest Neighbors Method***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s see how to call the `qeKNN()` function in classification problems, say,
    by setting *k* to 75:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As an example of prediction, say we have a new case to predict, such as a hypothetical
    customer like the one in row 8 of the data but who is male and a senior citizen.
    To set this up, we’ll copy row 8 of `tc` and make the stated changes in the `gender`
    and `SeniorCitizen` columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note, too, that since this revised row will be our “X,” we need to remove the
    “Y” portion—that is, remove the `Churn` column. We saw earlier that `Churn` is
    column 21, but remember, we removed column 1, so it’s now in column 20.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, we could have used the `subset()` function in base R,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: the `data.table` package, or the tidyverse. As noted on [page 11](ch01.xhtml#ch01lev5sec1),
    it is up to readers to choose whichever R constructs they feel most comfortable
    with; our focus in this book is on ML, with R playing only a supporting role.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we make the prediction for the new case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The class of interest to us is Churn, so we check the probability of Churn for
    this case, which is 0.32\. Since it’s under 0.5, we guess No Churn.
  prefs: []
  type: TYPE_NORMAL
- en: Calls to `qeKNN()` for classification problems are essentially the same as for
    numeric-outcome applications. The form of the output is slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: But . . . how does `qeKNN()` know that this is a classification application
    rather than a numeric- *Y* problem? Our specified *Y* variable, `Churn`, is an
    R factor, signaling to `qeKNN()` that we are running a classification application.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s check the classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We have a misclassification rate of about 22 percent, which is not too bad.
    Once again, however, I must emphasize that those numbers are subject to sampling
    variation, which we will discuss further in [Chapter 3](ch03.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: '***2.2.5 Pitfall: Overfitting Due to Features with Many Categories***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose we had not removed the `customerID` column in our original data, `telco`.
    How many distinct IDs are there?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The number of IDs is also the number of rows, as there’s one record per customer.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that `qeKNN()`, like functions in many R packages, internally converts
    factors to dummy variables. If we had not removed this column, there would have
    been 7,042 columns in the internal version of `tc` just stemming from this ID
    column! Not only would the result be unwieldy, but the presence of all these columns
    would dilute the power of k-NN.
  prefs: []
  type: TYPE_NORMAL
- en: This latter phenomenon is known as *overfitting*. Using too many features will
    actually reduce accuracy in predicting future cases. [Chapter 3](ch03.xhtml) covers
    this in greater detail, but for now, one might loosely think of the data as being
    “shared” by too many features, with not much data available to each one. Note
    that overfitting is a concern both in classification and numeric-*Y* applications.
  prefs: []
  type: TYPE_NORMAL
- en: There also may be computational issues if we were to include the ID. Having
    7,000 customer IDs would mean 7,000 dummy variables. That means the internal data
    matrix has over 7000 × 7000 entries—about 50 million. And at 8 bytes each, that
    means something like 0.4GB of RAM. We’ve got to remove this column.
  prefs: []
  type: TYPE_NORMAL
- en: Even with ML packages that directly accept factor data, one must keep an eye
    on what the package is doing—and what we are feeding into it. It’s good practice
    to watch for R factors with a large number of levels. They may appear useful,
    and may in fact be so. But they can also lead to overfitting and computational
    or memory problems.
  prefs: []
  type: TYPE_NORMAL
- en: '2.3 Example: Vertebrae Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider another UCI dataset, Vertebral Column Dataset,^([1](footnote.xhtml#ch2fn1))
    which is on diseases of the vertebrae. It is described by the curator as a dataset
    that contains “values for six biomechanical features used to classify orthopaedic
    patients into 3 classes (normal, disk hernia, or spondilolysthesis [ *sic* ]).”
    They abbreviate the three classes as NO, DH, and SL.
  prefs: []
  type: TYPE_NORMAL
- en: This example is similar to the last one, but with three classes instead of two.
    Recall that in a two-class problem, we predict on the basis of whether the probability
    of the class of interest is greater than 0.5\. In the telco example, that class
    was Churn, so we predict either Churn or No Churn, depending on whether the Churn
    probability is greater than 0.5\. That probability turned out to be 0.32, so we
    predicted No Churn. But with three or more classes, none of the probabilities
    might be above 0.5; we simply choose the class with the largest probability.
  prefs: []
  type: TYPE_NORMAL
- en: '***2.3.1 Analysis***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s read in the data and, as usual, take a look around.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The patient status is column `V7`. We see, by the way, that the curator of the
    dataset decided to group the rows by patient class. That’s why the `qe*`-series
    functions randomly choose the holdout sets.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s fit the model. The question of how to choose *k* is still open, but we
    need to take into account the size of our dataset. We only have 310 cases here,
    in contrast to the *n* = 7032 we had in the customer churn example. Recall from
    [Section 1.12.4](ch01.xhtml#ch01lev12sec4) that the larger our number of data
    points *n* is, the larger we can set the number of nearest neighbors *k*, so with
    this small dataset, let’s try a small value here, say, *k* = 5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As an example of prediction, consider a patient similar to the first one in
    our data but with `V2` being 25 rather than 22.55\. What would our predicted class
    be?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We’d predict the DH class, with an estimated probability of 0.6\. Now let’s
    find the overall accuracy of our predictions using this model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We would have an error rate of about 19 percent. Note, though, that due to the
    small sample size (310), our predictions would, in this case, be quite susceptible
    to sampling variation.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4 Pitfall: Error Rate Improves Only Slightly Using the Features'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When working with any ML method, on any dataset, it’s important to check whether
    your predictions have a better chance of success than those based on random chance,
    without using the features. Recall our analysis along those lines in [Section
    1.12.2](ch01.xhtml#ch01lev12sec2). Let’s see an example in the classification
    realm.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the telco dataset in [Section 2.2](ch02.xhtml#ch02lev2). We found that
    in using 19 features to predict whether a customer will be loyal or not, we would
    err about 22 percent of the time. Is 22 percent good?
  prefs: []
  type: TYPE_NORMAL
- en: To answer that, consider what happens if we don’t have any feature to predict
    from. Then we would be forced to predict on the basis of what most customers do.
    What percentage of them bolt?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This says about 27 percent of the customers leave. We will often do computations
    of this form, so let’s review how it works. The expression
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: evaluates to a bunch of TRUEs and FALSEs. But in R, as in most computer languages,
    TRUE and FALSE are taken to be 1 and 0, respectively. So, we are taking the mean
    of a bunch of 1s and 0s, giving us the proportion of 1s. That’s the proportion
    of `Yes` entries.
  prefs: []
  type: TYPE_NORMAL
- en: So, without customer information, we would simply predict everyone to stay—and
    we would be wrong 27 percent of the time. In other words, using customer information
    reduces our error rate from 27 to 22 percent—helpful, yes, but not dramatically
    so. Of course, we might do better with other values of *k*, and the reader is
    urged to try some, but it does put our analysis in perspective.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, a seemingly “good” error rate may be little or no better than
    random chance. Always remember to check the unconditional class probabilities—that
    is, the ones computed without *X*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider the example in [Section 2.3](ch02.xhtml#ch02lev3) in this regard.
    We achieved an error rate of about 26 percent. If we didn’t use the features,
    guessing instead the most prevalent class overall, would our error rate increase?
  prefs: []
  type: TYPE_NORMAL
- en: To that end, let’s see what proportion each class has, ignoring our six features.
    We can answer this question easily.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The call to `table()` gives us the category counts, so dividing by the total
    number of counts gives us the proportions.
  prefs: []
  type: TYPE_NORMAL
- en: If we did not use the features, we’d always guess the SL class, as it is the
    most common. We would then be wrong a proportion of 1 − 0.4838710 = 0.516129 of
    the time, which is much worse than the 26 percent error rate we attained using
    the features. Using the features greatly enhances our predictive ability in this
    case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, the `qe*`-series functions in `regtools` compute the featureless
    error rate for us. In the vertebrae example above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the result is a little different from the earlier figure of 0.516129\.
    This is because the latter was computed on the full dataset, while this one was
    computed with holdout: the mean came from the training set while the *Y* values
    were from the holdout set.'
  prefs: []
  type: TYPE_NORMAL
- en: As seen here, in classification settings, `baseAcc` will show the overall misclassification
    rate if one does not use the features. Of course, the same comparison—error rates
    using the features and not—is of interest in numeric *Y* settings. Therefore,
    if we were to ignore the features, our guess for a new *Y* would be the overall
    mean value of *Y* in the training set, which is analogous to our using the conditional
    mean if we do use the features. Without the features, the analog of MAPE is then
    the mean absolute difference between the overall mean *Y* and the actual *Y* in
    the test set. This is reported in `baseAcc`.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider again the bike ridership data from [Chapter 1.](ch01.xhtml)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, using our chosen set of 5 predictors, MAPE was about 1,204, versus 1,785
    using no predictors.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 The Confusion Matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In multiclass problems, the overall error rate is only the start of the story.
    We might also calculate the (unfortunately named) *confusion matrix*, which computes
    per-class error rates. Let’s see how this plays out in the vertebrae data.
  prefs: []
  type: TYPE_NORMAL
- en: The `qe*`-series functions include the confusion matrix in the return value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Of the 6 + 2 + 0 = 8 data points with actual class DH, 6 were correctly classified
    as DH, but 2 were misclassified as NO, though none were wrongly predicted as SL.
  prefs: []
  type: TYPE_NORMAL
- en: This type of analysis enables a more finely detailed assessment of our predictive
    power. It’s quite frequently used by MLers to identify potential areas of weakness
    of one’s model.
  prefs: []
  type: TYPE_NORMAL
- en: '2.6 Clearing the Confusion: Unbalanced Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we will discuss issues regarding *unbalanced data*, a common situation
    in classification problems that is much discussed in ML circles.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that in our customer churn example earlier in this chapter, about 73
    percent of the customers were “loyal,” while 27 percent moved to another telco.
    With the 7,032 cases in our data, those figures translate to 5,141 loyal cases
    and 1,901 cases of churn. The loyal cases outnumber the churn ones by a ratio
    of more than 2.5 to 1\. Often, this ratio can be 100 to 1 or even more. Such a
    situation is termed *unbalanced*. (In this section, our discussion will mainly
    cover the two-class case, but multiclass cases are similar.)
  prefs: []
  type: TYPE_NORMAL
- en: Many analysts recommend that if one’s dataset has unbalanced class sizes, one
    should modify the data to create equal class counts. Their reasoning is that application
    of ML methods to unbalanced data will result in almost all predictions being that
    we guess the class of a new data point to be the dominant class—for instance,
    we always guess No Churn rather than Churn in the telco data. That is not very
    informative!
  prefs: []
  type: TYPE_NORMAL
- en: Illustrations of the problem and offered remedies appear in numerous parts of
    the ML literature, ranging from web tutorials^([2](footnote.xhtml#ch2fn2)) to
    major CRAN packages, such as `caret`, `parsnp`, and `mlr3`. In spite of warnings
    by statisticians,^([3](footnote.xhtml#ch2fn3)) all of these sources recommend
    that you artificially equalize the class counts in your data, say, by discarding
    “excess” data from the dominant class.
  prefs: []
  type: TYPE_NORMAL
- en: While it is true that unbalanced data will result in always, or almost always,
    predicting the dominant class, remedying by artificially equalizing the class
    sizes is unwarranted and, in many cases, harmful. Clearly, discarding data is
    generally not a good idea; it will always weaken one’s analysis. Moreover, depending
    on the goals of the given application, it may actually be desirable to always
    guess the dominant class.
  prefs: []
  type: TYPE_NORMAL
- en: '***2.6.1 Example: The Kaggle Appointments Dataset***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To illustrate how best to deal with unbalanced data, let’s look at a dataset
    from Kaggle,^([4](footnote.xhtml#ch2fn4)) a firm with the curious business model
    of operating data science competitions. The goal is to use this dataset to predict
    whether a patient will fail to keep a doctor’s appointment; if the medical office
    can flag the patients at risk of not showing up, staff can make extra efforts
    to avoid the economic losses resulting from no-shows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Should we remove the patient ID, as we did in the telco data, to avoid overfitting?
    We see that on average, each patient appears less than twice in the data, meaning
    there is not much data per patient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'So, yes, we probably should not include this feature. Using the same reasoning,
    it makes sense to remove the appointment ID, neighborhood, appointment day, and
    scheduled day variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'About 20 percent of cases are no-shows (counterintuitively, `Yes` here means
    “Yes, the patient didn’t show up”):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Yes, it is indeed unbalanced data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the concern is that our predicted *Y*s will also be unbalanced—that
    is, most or all will predict the patient to show up. Let’s check this by inspecting
    the output of running `qeKNN()`. Here is the call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'There is a lot of information in most `qe*`-series function return values.
    Here are the components of the `qeKNN()` function output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'That `holdoutPreds` component is actually the return value of `regtools::kNN()`,
    discussed in [Section 1.17](ch01.xhtml#ch01lev17). (The R notation `p::e` means
    the entity `e` in the package `p`.) Let’s see what is in there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Checking the documentation, we find that `predClasses` is the vector of predicted
    *Y*s in the holdout set, which is just what we need. Let’s tabulate them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: First, how do we deal with *two* dollar signs (`$`) in this expression?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Remember, the notation `u$v` means component `v` within the object `u`. So,
    `u$v$w` means the component *w* within the object `u$v` ! So yes, we have an R
    list within an R list here, which is common in the R world.
  prefs: []
  type: TYPE_NORMAL
- en: At any rate, we see that with our model here, we predict in the vast majority
    of cases that the patient will show up (again, Yes means a no-show), confirming
    the concern many people have about unbalanced data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, the predictions are even more unbalanced than the data itself; we
    saw about 20 percent of the *Y*s in the overall dataset were no-shows, while here
    that is true for only 1.3 percent of the predicted holdout *Y*s. Actually, this
    is to be expected: remember, if the model finds the probability of breaking the
    appointment to be greater than 0.5, the prediction will be that they don’t show
    up, even if they actually did.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is exactly the problem cited by the sources mentioned earlier who recommend
    artificially balancing the data. They recommend altering the data in a manner
    that makes all classes represented equally in the data. They suggest one of the
    following remedies (or variants) to equalize the class sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Downsample**'
  prefs: []
  type: TYPE_NORMAL
- en: Replace the No cases in our data with 22,319 randomly chosen elements from the
    original 88,208\. We then will have 22,319 Yes records and 22,139 No records,
    thus achieving balance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Upsample**'
  prefs: []
  type: TYPE_NORMAL
- en: Replace the Yes cases with 88,208 randomly selected elements from the original
    22,319 (with replacement). We then will have 88,208 Yes records and 88,208 No
    records, thus achieving balance.
  prefs: []
  type: TYPE_NORMAL
- en: One would apply one’s chosen ML method, say, k-NN, to the modified data, and
    then predict new cases to be no-shows according to whether the estimated conditional
    probability is larger than 0.5 or not.
  prefs: []
  type: TYPE_NORMAL
- en: '***2.6.2 A Better Approach to Unbalanced Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Again, downsampling is undesirable; data is precious and shouldn’t be discarded.
    The other approach to balancing, upsampling, doesn’t make sense either—why would
    adding fully duplicate data help?
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, balancing assumes equal adverse impact from false negatives and
    false positives, which is unlikely in applications like the appointments data.
    One could set up formal utility values here for the relative costs of false negatives
    and false positives. But in many applications, we need more flexibility than what
    a fully mechanical algorithm gives us. For instance, consider credit card fraud.
    As noted in global accounting network PricewaterhouseCoopers’s publication *Fraud:
    A Guide to Its Prevention,* *Detection and Investigation*, “Every fraud incident
    is different, and reactive responses will vary depending on the facts that are
    unique to each case.”^([5](footnote.xhtml#ch2fn5))'
  prefs: []
  type: TYPE_NORMAL
- en: The easier and better solution is to simply have our ML algorithm flag the cases
    in which there is a substantial probability of fraud, say, above some specified
    threshold, then take it “by hand” from there. Once the algorithm has selected
    a set of possible instances of fraud, the (human) auditor will take into account
    that estimated probability—now worrying not only that it is larger than the threshold
    but also *how much larger*—as well as such factors as the amount of the charge,
    special characteristics not measured in the available features, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The auditor may not give priority, for instance, to a case in which the probability
    is above the threshold but in which the monetary value of the transaction is small.
    On the other hand, the auditor may give this transaction a closer look, even if
    the monetary value is small, if the probability is much higher than the threshold
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the practical solution to the unbalanced-data “problem” is not to artificially
    resample the data but instead to identify individual cases of interest in the
    context of the application. This then means cases of sufficiently high probability
    to be of concern, again in the context of the given application.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the `probs` component of a `qe*`-series call in a classification
    application gives the estimated probabilities of the various classes—exactly what
    we need. For instance, in the missed-appointments dataset, we can check `probs`
    to find the probability a patient will be a no-show.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `probs` has one row for each case to be predicted. Since the missed-appointments
    data has two classes, there will be two columns. We saw previously that the Yes
    class (that is, missed appointments) is listed second. Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'There are quite a few patients who, despite being more likely than not to keep
    the appointment, still have a substantial risk of no-show. For instance, 18,427
    people have a 0.2 estimated probability of failing to keep their appointment.
    An additional 28,435 patients have more than a 25 percent chance of not showing
    up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The reasonable approach would be to decide on a threshold for no-show probability,
    then determine which patients fail to meet that threshold. With a threshold of
    0.75, for instance, any patient whose probability of keeping the appointment is
    greater than that level might be given special attention. We would make extra
    phone calls to them, explain penalties for missed appointments, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: If those calls prove too burdensome, we can increase the threshold. Or, if the
    calls turn out to be highly effective, we can reduce it. Either way, the point
    is that the power is now in the hands of the end user of the data, where it ought
    to be. Artificially balancing the data denies the user that power.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Receiver Operating Characteristic and Area Under Curve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have seen MAPE used as a measure of predictive power in numeric-*Y* problems,
    while overall misclassification error (OME) is used in classification applications.
    Both are very popular, but in the classification case, there are other common
    measures, two of which we will discuss now.
  prefs: []
  type: TYPE_NORMAL
- en: '***2.7.1 Details of ROC and AUC***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many analysts use the *Area Under Curve (AUC)* value as an overall measure of
    predictive power in classification problems. The curve under consideration is
    the *Receiver Operating Characteristic (ROC)* curve.
  prefs: []
  type: TYPE_NORMAL
- en: To understand ROC, recall [Section 2.6.2](ch02.xhtml#ch02lev6sec2), where we
    spoke of a threshold for class prediction. If the estimated class probability
    is on one side of the threshold, we predict Class 1, and on the other side, we
    predict Class 0\. Threshold values can be anywhere from 0 to 1; we choose our
    value based on our goals in the particular application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ROC curve then explores various scenarios. How well would we predict if,
    say, we take the threshold to be 0.4? What about 0.7? And so on? The question
    “How well would we predict?” is addressed by two numbers: the *true positive rate
    (TPR)* and the *false positive rate (FPR)*.'
  prefs: []
  type: TYPE_NORMAL
- en: TPR, also known as the *sensitivity*, is the probability that we guess Class
    1, given that the class actually is Class 1\. FPR, the *specificity*, is the probability
    that we guess Class 1, given that the true class is Class 0\. Note that the threshold
    value determines FPR and TPR. The ROC curve then plots TPR against FPR as the
    threshold varies.
  prefs: []
  type: TYPE_NORMAL
- en: The AUC is then the total area under the ROC curve. It takes on values between
    0 and 1\. The higher the curve, the better, as it implies that for any fixed FPR,
    TPR is high. Thus, the closer AUC is to 1.0, the better the predictive ability.
  prefs: []
  type: TYPE_NORMAL
- en: '***2.7.2 The qeROC() Function***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `qeROC()` function wraps `roc()` in the `pROC` package. It performs ROC
    analysis on the output of a `qe*-` function, say, `qeKNN()`, using the call form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Here, `dataIn` is the dataset on which the `qe*-` function had been called,
    `qeOut` is the output of that function, `yName` is the name of *Y* in `dataIn`,
    and `yLevelName` is the *Y* level (in the R factor sense) of interest. Note that
    the latter allows for the multiclass case.
  prefs: []
  type: TYPE_NORMAL
- en: As noted, `qeROC()` calls `pPROC::roc()`. The return value of the former consists
    of the return value of the latter. It may be useful to assign the return value
    to a variable for further use (see below), but if this is not done, the ROC curve
    is plotted and the AUC value is printed out.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `qeROC()` operates on the holdout set. This is important for the same
    reasons that the `testAcc` output of the `qe*`-series functions use the holdout
    set (see [Section 1.12](ch01.xhtml#ch01lev12)).
  prefs: []
  type: TYPE_NORMAL
- en: '***2.7.3 Example: Telco Churn Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s see what the ROC curve has to say about the Telco Churn data, continuing
    our earlier k-NN analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The plot is shown in [Figure 2-1](ch02.xhtml#ch02fig01). Let’s see how to interpret
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch02fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2-1: ROC, Telco Churn data*'
  prefs: []
  type: TYPE_NORMAL
- en: The 45-degree line is drawn to represent pure guessing, with our rate of prediction
    of a positive class being the same, regardless of whether we have a true positive
    or not. Again, the further the ROC curve is above this line, the better.
  prefs: []
  type: TYPE_NORMAL
- en: '***2.7.4 Example: Vertebrae Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `qeROC()` function can also be used in multiclass settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Since this is a small dataset, we opted for larger holdout (under the 10 percent
    default, the holdout would only have 31 cases). But even then, one must keep in
    mind that those AUC values are subject to considerable sample variation. Thus
    we must be cautious in concluding that we are less accurate in predicting the
    `'NO'` cases.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, what do these numbers mean in this multiclass context? Actually, they
    are the same ones one would get by running three individual ROC analyses (on the
    same holdout set); `qeROC()` is just a convenience function in this case, alleviating
    the user of the need to run `roc()` three times. And since no new computation
    is done (the class probabilities are scaled to sum to 1.0), this also saves the
    user computation time if the dataset is large.
  prefs: []
  type: TYPE_NORMAL
- en: '***2.7.5 Pitfall: Overreliance on AUC***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: AUC can be a useful number to complement OME, and many analysts regard it as
    an integral part of their ML toolkits. However, one should use it cautiously.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, AUC is the average value of ROC over all possible threshold
    values. But recall, each threshold value corresponds implicitly to a relative
    utility. In a credit card fraud dataset, for instance, we may believe it is far
    worse to decide a transaction is legitimate when it is actually fraudulent, compared
    to vice versa. The problem, then, is that there are some threshold values that
    we would never even consider using for a particular application. Yet they are
    averaged into the AUC value, thus rendering the latter less meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: 2.8 Conclusions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now have a solid foundation in the two basic types of ML problems: numeric-*Y*
    and classification. Along the way, we’ve picked up some miscellaneous skills,
    such as removing useless features and dealing with NA values.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s now time to take a serious look at how to choose the value of *k* and,
    more generally, hyperparameters in all ML methods, which is a topic that we have
    been glossing over so far. We will look at this in detail in the next two chapters.
  prefs: []
  type: TYPE_NORMAL
