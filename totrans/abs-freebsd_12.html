<html><head></head><body>
<h2 class="h2" id="ch12"><span epub:type="pagebreak" id="page_257"/><strong><span class="big">12</span><br/>THE Z FILE SYSTEM</strong></h2>
<div class="image1"><img src="../images/common01.jpg" alt="image"/></div>
<p class="noindent">Most filesystems are, in computing terms, ancient. We discard 5-year-old hardware because it’s painfully slow, but we format the replacement’s hard drive with a 40-year-old filesystem. While we’ve improved those filesystems and made them more robust, they still use the same basic architecture. And every time a filesystem breaks, we curse and scramble to fix it while desperately wishing for something better.</p>
<p class="indent">ZFS is something better.</p>
<p class="indent">It’s not that ZFS uses revolutionary technology. All the individual pieces of ZFS are well understood. There’s no mystery to hashes or data trees or indexing. But ZFS combines all of these well-understood principles into a single cohesive, well-engineered whole. It’s designed with the future in mind. Today’s hashing algorithm won’t suffice 15 years from now, but ZFS is designed so that new algorithms and techniques can be added to newer versions without losing backward compatibility.</p>
<p class="indent">This chapter won’t cover all there is to know about ZFS. ZFS is almost an operating system on its own, or perhaps a special-purpose database. <span epub:type="pagebreak" id="page_258"/>Entire books have been written about using and managing ZFS. You’ll learn enough about how ZFS works to use it on a server, though, and understand its most important features.</p>
<p class="indent">While ZFS expects to be installed directly on a disk partition, you can use other GEOM providers as ZFS storage. The most common example is when you do an install with encrypted disks. FreeBSD puts a geli(8) geom on the disk and installs ZFS atop that geom. This chapter calls any storage provider a “disk,” even though it could be a file or an encrypted provider or anything else.</p>
<p class="indent">If you’ve never worked with ZFS before, install a ZFS-based FreeBSD system on a virtual machine and follow along. The installer automatically handles prerequisites, like setting <code>zfs_load=YES</code> in <em>loader.conf</em> and <code>zfs_enable=YES</code> in <em>rc.local</em>; all you need concern yourself with is the filesystem.</p>
<div class="sidebar">
<p class="sidebart"><strong>WHAT DOES ZFS STAND FOR?</strong></p>
<p class="spara">The <em>Z File System</em>. Yes, seriously. Once upon a time, it meant <em>Zettabyte File System</em>, but that acronym has been retconned away.</p>
</div>
<p class="indent">ZFS blends a whole bunch of well-understood technologies into a combination volume manager and filesystem. It expects to handle everything from the permissions on a file down to tracking which blocks on which storage provider get which information. As the sysadmin, you tell ZFS which hardware you have and how you want it configured, and ZFS takes it from there.</p>
<p class="indent">ZFS has three main components: datasets, pools, and virtual devices.</p>
<h3 class="h3" id="lev424"><strong>Datasets</strong></h3>
<p class="noindent">A <em>dataset</em> is defined as a named chunk of ZFS data. The most common dataset resembles a partitioned filesystem, but ZFS supports other types of datasets for other uses. A snapshot (see “<a href="ch12.xhtml#lev453">Snapshots</a>” on <a href="ch12.xhtml#page_271">page 271</a>) is a dataset. ZFS also includes block devices for virtualization and iSCSI targets, clones, and more; all of those are datasets. This book focuses on filesystem datasets. Traditional filesystems like UFS have a variety of small programs to manage filesystems, but you manage all ZFS datasets with zfs(8).</p>
<p class="indent">View your existing datasets with <code>zfs list</code>. The output looks a lot like mount(8).</p>
<pre>   # <span class="codestrong1">zfs list</span><br/>   NAME                     USED  AVAIL  REFER  MOUNTPOINT<br/><span class="ent">➊</span> zroot                   4.71G   894G    88K  none<br/><span class="ent">➋</span> zroot/ROOT              2.40G   894G    88K  none<br/><span epub:type="pagebreak" id="page_259"/><span class="ent">➌</span> zroot/ROOT/2018-11-17      8K   894G  1.51G  /<br/><span class="ent">➍</span> zroot/ROOT/default      2.40G   894G  1.57G  /<br/><span class="ent">➎</span> zroot/usr               1.95G   894G    88K  /usr<br/><span class="ent">➏</span> zroot/usr/home           520K   894G   520K  /usr/home<br/>   --<span class="codeitalic1">snip</span>--</pre>
<p class="indent">Each line starts with the dataset name, starting with the storage pool—or <em>zpool</em>—that the dataset is on. The first entry is called <em>zroot</em> <span class="ent">➊</span>. This entry represents the pool’s <em>root dataset</em>. The rest of the dataset tree dangles off this dataset.</p>
<p class="indent">The next two columns show the amount of space used and available. The pool <em>zroot</em> has used 4.71GB and has 894GB available. While the available space is certainly correct, the 4.71GB is more complicated than it looks. The amount of space a dataset shows under USED includes everything on that dataset <em>and</em> on all of its children. A root dataset’s children include all the other datasets in that zpool.</p>
<p class="indent">The <code>REFER</code> column is special to ZFS. This column shows the amount of data accessible on this specific dataset, which isn’t necessarily the same as the amount of space used. Some ZFS features, such as snapshots, share data between themselves. This dataset has used 4.71GB of data but refers to only 88KB. Without its children, this dataset has only 88KB of data on it.</p>
<p class="indent">At the end, we have the dataset’s mount point. This root dataset doesn’t have a mount point; it’s not mounted.</p>
<p class="indent">Look at the next dataset, <em>zroot/ROOT</em> <span class="ent">➋</span>. This is a dataset created for the root directory and associated files. That seems sensible, but if you look at the <code>REFER</code> column, you’ll see it also has only 88KB of data inside it, and there’s no mount point. Shouldn’t the root directory exist?</p>
<p class="indent">The next two lines explain why . . . sort of. The dataset <em>zroot/ROOT/2018-11-17</em> <span class="ent">➌</span> has a mountpoint of <em>/</em>, so it’s a real root directory. The next dataset, <em>zroot/ROOT/default</em> <span class="ent">➍</span>, also has a mountpoint of <em>/</em>. No, ZFS doesn’t let you mount multiple datasets at the same mount point. A ZFS dataset records a whole bunch of its settings within the dataset. The mount point is one of those settings.</p>
<p class="indent">Consider these four datasets for a moment. The <em>zroot/ROOT</em> dataset is a child of the zroot dataset. The <em>zroot/ROOT/2018-11-17</em> and <em>zroot/ROOT/default</em> datasets are children of <em>zroot/ROOT</em>. Each dataset has its children’s space usage billed against it.</p>
<p class="indent">Why do this? When you boot a FreeBSD ZFS host, you can easily choose between multiple root directories. Each bootable root directory is called a <em>boot environment</em>. Suppose you apply a patch and reboot the system, but the new system won’t boot. By booting into an alternate boot environment, you can easily access the defective root directory and try to figure out the problem.</p>
<p class="indent">The next dataset, <em>zroot/usr</em> <span class="ent">➎</span>, is a completely different child of <em>zroot</em>. It has its own child, <em>zroot/usr/home</em> <span class="ent">➏</span>. The space used in <em>zroot/usr/home</em> gets charged against <em>zroot/usr</em>, and both get charged against its parent, but their allocation doesn’t affect <em>zroot/ROOT</em>.</p>
<h4 class="h4" id="lev425"><span epub:type="pagebreak" id="page_260"/><strong><em>Dataset Properties</em></strong></h4>
<p class="noindent">Beyond some accounting tricks, datasets so far look a lot like partitions. But a partition is a logical subdivision of a disk, filling very specific LBAs on a storage device. Partitions have no awareness of the data on the partition. Changing a partition means destroying the filesystem on it.</p>
<p class="indent">ZFS tightly integrates the filesystem and the lower storage layers. It can dynamically divide storage space between the various filesystems as needed. Where partitions control the number of available blocks to constrain disk usage, datasets can use quotas for the same effect. Without those quotas, though, if a pool has space, you can use it.</p>
<p class="indent">The amount of space a dataset can use is a ZFS <em>property</em>. ZFS supports dozens of properties, from the <code>quotas</code> property that controls how large a dataset can grow to the <code>mounted</code> property that shows whether a dataset is mounted.</p>
<h5 class="h5" id="lev426"><strong>Viewing and Changing Dataset Properties</strong></h5>
<p class="noindent">Use <code>zfs set</code> to change properties.</p>
<pre># <span class="codestrong1">zfs set quota=2G zroot/usr/home</span></pre>
<p class="indent">View a property with <code>zfs get</code>. You can either specify a particular property or use <code>all</code> to view all properties. You can list multiple properties by separating them with commas. If you specify a dataset name, you affect only that dataset.</p>
<pre># <span class="codestrong1">zfs get mounted zroot/ROOT</span><br/>NAME       PROPERTY  VALUE    SOURCE<br/>zroot/ROOT  mounted     no    -</pre>
<p class="indent">Here, we have the dataset’s name, the property, the property value, and something called source. (We’ll talk about that last one in “<a href="ch12.xhtml#lev427">Property Inheritance</a>” on <a href="ch12.xhtml#page_261">page 261</a>.)</p>
<p class="indent">My real question is, which dataset is mounted as the root directory? I could check the two datasets with a mount point of <em>/</em>, but when I get dozens of boot environments, that will drive me nuts. Check a property for a dataset and all of its children by adding the <code>-r</code> flag.</p>
<pre># <span class="codestrong1">zfs get -r mounted zroot/ROOT</span><br/>NAME                                   PROPERTY  VALUE    SOURCE<br/>zroot/ROOT                              mounted     no    -<br/>zroot/ROOT/2018-11-17                   mounted     no    -<br/>zroot/ROOT/default                      mounted   <span class="ent">➊</span>yes    -</pre>
<p class="indent">Of the three datasets, only <em>zroot/ROOT/default</em> <span class="ent">➊</span> is mounted. That’s our active boot environment.</p>
<h5 class="h5" id="lev427"><span epub:type="pagebreak" id="page_261"/><strong>Property Inheritance</strong></h5>
<p class="noindent">Many properties are inheritable. You set them on the parent dataset and they percolate down through the children. Inheritance doesn’t make sense for properties like mount points, but it’s right for certain more advanced features. While we’ll look at what the <code>compression</code> property does in “<a href="ch12.xhtml#lev457">Compression</a>” on <a href="ch12.xhtml#page_273">page 273</a>, we’ll use it as an example of inheritance here.</p>
<pre># zfs get compression<br/>NAME                     PROPERTY     VALUE     SOURCE<br/>zroot                    compression  lz4       local<br/>zroot/ROOT               compression  lz4       inherited from zroot<br/>zroot/ROOT/2018-11-17    compression  lz4       inherited from zroot<br/>zroot/ROOT/default       compression  lz4       inherited from zroot<br/>zroot/tmp                compression  lz4       inherited from zroot<br/>--<span class="codeitalic1">snip</span>--</pre>
<p class="indent">The root dataset, zroot, has the <code>compression</code> property set to lz4. The source is local, meaning that this property is set on this dataset. Now look at <em>zroot/ROOT</em>. The <code>compression</code> property is also lz4, but the source is inherited from zroot. This dataset inherited this property setting from its parent.</p>
<h4 class="h4" id="lev428"><strong><em>Managing Datasets</em></strong></h4>
<p class="noindent">ZFS uses datasets much as traditional filesystems use partitions. Manage datasets with zfs(8). You’ll want to create, remove, and rename datasets.</p>
<h5 class="h5" id="lev429"><strong>Create Datasets</strong></h5>
<p class="noindent">Create datasets with <code>zfs create</code>. Create a filesystem dataset by specifying the pool and the dataset name. Here, I create a new dataset for my packages. (Note that this breaks boot environments, as we’ll see later this chapter.)</p>
<pre># <span class="codestrong1">zfs create zroot/usr/local</span></pre>
<p class="indent">Each dataset must have a parent dataset. A default FreeBSD install has a <em>zroot/usr</em> dataset, so I can create a <em>zroot/usr/local</em>. I’d like to have a dataset for <em>/var/db/pkg</em>, but while FreeBSD comes with a <em>zroot/var</em> dataset, there’s no <em>zroot/var/db</em>. I’d need to create <em>zroot/var/db</em> and then <em>zroot/var/db/pkg</em>.</p>
<p class="indent">Note that datasets are stackable, just like UFS. If I have files in my <em>/usr/local</em> directory and I create a dataset over that directory, ZFS will mount the dataset over the directory. I will lose access to those files. You must shuffle files around to duplicate existing directories.</p>
<h5 class="h5" id="lev430"><strong>Destroying and Renaming Datasets</strong></h5>
<p class="noindent">That new <em>zroot/usr/local</em> dataset I created? It hid the contents of my <em>/usr/local</em> directory. Get rid of it with <code>zfs destroy</code> and try again.</p>
<pre># <span class="codestrong1">zfs destroy zroot/usr/local</span></pre>
<p class="indent"><span epub:type="pagebreak" id="page_262"/>The contents of <em>/usr/local</em> reappear. Or, I could rename that dataset instead, using <code>zfs rename</code>.</p>
<pre># <span class="codestrong1">zfs rename zroot/usr/local zroot/usr/new-local</span></pre>
<p class="indent">I like boot environments, though, so I’m going to leave <em>/usr/local</em> untouched. Sometimes you really need a <em>/usr/local</em> dataset, though . . .</p>
<h5 class="h5" id="lev431"><strong>Unmounted Parent Datasets</strong></h5>
<p class="noindent">As a Postgres user, I want a separate dataset for my Postgres data. FreeBSD’s Postgres 9.6 package uses <em>/var/db/pgsql/data96</em>. I can’t create that dataset without having a dataset for <em>/var/db</em>, and I can’t have <em>that</em> without breaking boot environment support for packages. What to do?</p>
<p class="indent">The solution is to create a dataset for <em>/var/db</em>, but not to use it, by setting the <code>canmount</code> dataset property. This property controls whether or not a dataset can be mounted. FreeBSD uses an unmounted dataset for <em>/var</em> for exactly this reason. New datasets automatically set <code>canmount</code> to <code>on</code>, so you normally don’t have to worry about it. Use the <code>-o</code> flag to set a property at dataset creation.</p>
<pre># <span class="codestrong1">zfs create -o canmount=off zroot/var/db</span></pre>
<p class="indent">The dataset for <em>/var/db</em> exists, but it can’t be mounted. Check the contents of your <em>/var/db</em> directory to verify everything’s still there. You can now create a dataset for <em>/var/db/postgres</em> and even <em>/var/db/pgsql/data96</em>.</p>
<pre># <span class="codestrong1">zfs create zroot/var/db/postgres</span><br/># <span class="codestrong1">zfs create zroot/var/db/postgres/data96</span><br/># <span class="codestrong1">chown -R postgres:postgres /var/db/postgres</span></pre>
<p class="indent">You have a dataset for your database, and you still have the files in <em>/var/db</em> itself as part of the root dataset. Now initialize your new Postgres database and go!</p>
<p class="indent">As you explore ZFS, you’ll find many situations where you might want to set properties at dataset creation or use unmounted parent datasets.</p>
<h5 class="h5" id="lev432"><strong>Moving Files to a New Dataset</strong></h5>
<p class="noindent">If you need to create a new dataset for an existing directory, you’ll need to copy the files over. I recommend you create a new dataset with a slightly different name, copy the files to that dataset, rename the directory, and then rename the dataset. Here, I want a dataset for <em>/usr/local</em>, so I create it with a different name.</p>
<pre># <span class="codestrong1">zfs create zroot/usr/local/pgsql-new</span></pre>
<p class="indent">Copy the files with tar(1), exactly as you would for a new UFS partition (see <a href="ch11.xhtml#ch11">Chapter 11</a>).</p>
<pre><span epub:type="pagebreak" id="page_263"/># <span class="codestrong1">tar cfC - /usr/local/pgsql . | tar xpfC - /usr/local/pgsql-new</span></pre>
<p class="indent">Once it finishes, move the old directory out of the way and rename the dataset.</p>
<pre># <span class="codestrong1">mv /usr/local/pgsql /usr/local/pgsql-old</span><br/># <span class="codestrong1">zfs rename zroot/usr/local/pgsql-new zroot/usr/local/pgsql</span></pre>
<p class="indent">My Postgres data now lives on its own dataset.</p>
<h3 class="h3" id="lev433"><strong>ZFS Pools</strong></h3>
<p class="noindent">ZFS organizes its underlying storage in pools, rather than by disk. A ZFS storage pool, or <em>zpool</em>, is an abstraction of the underlying storage devices, letting you separate the physical medium and the user-visible filesystem on top of it.</p>
<p class="indent">View and manage a host’s ZFS pools with zpool(8). Here, I use <code>zpool list</code> to see the pools from one of my hosts.</p>
<pre># <span class="codestrong1">zpool list</span><br/>NAME      SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT<br/>zroot     928G  4.72G   923G         -     0%     0%  1.00x  ONLINE  -<br/>jail      928G  2.70G   925G         -     0%     0%  1.00x  ONLINE  -<br/>scratch   928G  5.94G   922G         -     0%     0%  1.00x  ONLINE  -</pre>
<p class="indent">This host has three pools: <em>zroot</em>, <em>jail</em>, and <em>scratch</em>. Each has its own line.</p>
<p class="indent">The <code>SIZE</code> column shows us the total capacity of the pool. All of these pools can hold 928GB. The <code>ALLOC</code> column displays how much of each pool is in use, while <code>FREE</code> shows how much space remains. These disks are pretty much empty, which makes sense as I installed this host only about three hours ago.</p>
<p class="indent">The <code>EXPANDSZ</code> column shows whether the underlying storage providers have any free space. When a pool has virtual device redundancy (which we’ll discuss in the next section), you can replace individual storage devices in the pool and make the pool larger. It’s like swapping out the 5TB drives in your RAID array with 10TB drives to make it bigger.</p>
<p class="indent">The <code>FRAG</code> column shows how much fragmentation this pool has. You’ve heard over and over that fragmentation slows performance. ZFS minimizes the impact of fragmentation, though.</p>
<p class="indent">The <code>CAP</code> column shows what percentage of the available space is used.</p>
<p class="indent">The <code>DEDUP</code> column shows whether this pool uses deduplication. While many people trumpet deduplication as a ZFS feature, it’s not as useful as you might hope.</p>
<p class="indent">The <code>HEALTH</code> column displays whether the pool is working well or the underlying disks have a problem.</p>
<h4 class="h4" id="lev434"><span epub:type="pagebreak" id="page_264"/><strong><em>Pool Details</em></strong></h4>
<p class="noindent">You can get more detail on pools, or on a single pool, by running <code>zpool status</code>. If you omit the pool name, you’ll see this information for all of your pools. Here, I check the status of my <em>jail</em> pool.</p>
<pre># <span class="codestrong1">zpool status jail</span><br/>  pool: jail<br/> state: ONLINE<br/>  scan: none requested<br/>config:<br/><br/>        NAME               STATE     READ WRITE CKSUM<br/>        jail               ONLINE       0     0     0<br/>          mirror-0         ONLINE       0     0     0<br/>            gpt/da2-jail   ONLINE       0     0     0<br/>            gpt/ada2-jail  ONLINE       0     0     0<br/><br/>errors: No known data errors</pre>
<p class="indent">We start with the pool name. The state is much like the <code>HEALTH</code> column; it displays any problems with the pool. The scan field shows information on scrubs (see “<a href="ch12.xhtml#lev458">Pool Integrity and Repair</a>” on <a href="ch12.xhtml#page_273">page 273</a>).</p>
<p class="indent">We then have the pool configuration. The configuration shows the layout of the virtual devices in the pool. We’ll dive into that when we create our pools.</p>
<h4 class="h4" id="lev435"><strong><em>Pool Properties</em></strong></h4>
<p class="noindent">Much like datasets, zpools have properties that control and display the pool’s settings. Some properties are inherently informational, such as the <code>free</code> property that expresses how much free space the pool has. You can change others.</p>
<h4 class="h4" id="lev436"><strong><em>Viewing Pool Properties</em></strong></h4>
<p class="noindent">To view all of a pool’s properties, use <code>zpool get</code>. Add the property <code>all</code> to view every property. You can add a pool name to include only that pool.</p>
<pre># <span class="codestrong1">zpool get all zroot</span><br/>NAME   PROPERTY       VALUE                          SOURCE<br/>zroot  size           928G                           -<br/>zroot  capacity       0%                             -<br/>zroot  health         ONLINE                         -<br/>zroot  guid           7955546176707282768            default<br/>--<span class="codeitalic1">snip</span>--</pre>
<p class="indent">Some of this information gets pulled into commands like <code>zpool status</code> and <code>zpool list</code>. You can also query for individual properties across all pools by using the property name.</p>
<pre><span epub:type="pagebreak" id="page_265"/># <span class="codestrong1">zpool get readonly</span><br/>NAME     PROPERTY  VALUE   SOURCE<br/>zroot    readonly  off     -<br/>jail     readonly  off     -<br/>scratch  readonly  off     -</pre>
<p class="indent">Unlike dataset properties, most pool properties are set when you create or import the pool.</p>
<h3 class="h3" id="lev437"><strong>Virtual Devices</strong></h3>
<p class="noindent">A <em>virtual device (VDEV)</em> is a group of storage devices. You might think of a VDEV as a RAID container: a big RAID-5 presents itself to the operating system as a huge device, even though the sysadmin knows it’s really a bunch of smaller disks. The virtual device is where ZFS’s magic happens. You can arrange pools for different levels of redundancy or abandon redundancy and maximize space.</p>
<p class="indent">ZFS’s automated error correction takes place at the VDEV level. Everything in ZFS, from znodes (index nodes) to data blocks, is checksummed to verify integrity. If your pool has sufficient redundancy, ZFS will notice that data is damaged and restore it from a good copy. If your pool lacks redundancy, ZFS will notify you that the data is damaged and you can restore from backup.</p>
<p class="indent">A zpool consists of one or more identical VDEVs. The pool stripes data across all the VDEVs, with no redundancy. The loss of a VDEV means the loss of the pool. If you have a pool with a whole bunch of disks, make sure to use redundant VDEVs.</p>
<h4 class="h4" id="lev438"><strong><em>VDEV Types and Redundancy</em></strong></h4>
<p class="noindent">ZFS supports several different types of VDEV, each differentiated by the degree and style of redundancy they offer. The common mirrored disk, where each disk copies what’s on another disk, is one type of VDEV. Piles of disks with no redundancy is another type of VDEV. And ZFS includes three different varieties of sophisticated parity-based redundancy, called <em>RAID-Z</em>.</p>
<p class="indent">Using multiple VDEVs in a pool creates systems similar to advanced RAID arrays. A RAID-Z2 array looks an awful lot like RAID-6, but a ZFS pool with two RAID-Z2 VDEVs resembles RAID-60. Mirrored VDEVs work like RAID-1, but multiple mirrors in a pool behave like RAID-10. In both of these cases, ZFS stripes the data across the VDEV with no redundancy. The individual VDEVs provide the redundancy.</p>
<p class="indent">Choose your VDEV type carefully.</p>
<h5 class="h5" id="lev439"><strong>Striped VDEVs</strong></h5>
<p class="noindent">A VDEV composed of a single disk is called a <em>stripe</em> and has no redundancy. Losing the disk means losing your data. While a pool can contain multiple striped VDEVs, each disk is its own VDEV. Much like RAID-0, losing one disk means losing the whole pool.</p>
<h5 class="h5" id="lev440"><span epub:type="pagebreak" id="page_266"/><strong>Mirror VDEVs</strong></h5>
<p class="noindent">A mirror VDEV stores a complete copy of all the VDEV’s data on every disk. You can lose all but one of the drives in the VDEV and still access your data. A mirror can contain any number of disks.</p>
<p class="indent">ZFS can read data from all of the mirrored disks simultaneously, so reading data is fast. When you write data, though, ZFS must write that data to all of the disks simultaneously. The write isn’t complete until the slowest disk finishes. Write performance suffers.</p>
<h5 class="h5" id="lev441"><strong>RAID-Z</strong></h5>
<p class="noindent">RAID-Z spreads data and parity information across all of the disks, much like conventional RAID. If a disk in a RAID-Z dies or starts giving corrupt data, RAID-Z uses the parity information to recalculate the missing data. A RAID-Z VDEV must contain at least three disks and can withstand the loss of any single disk. RAID-Z is sometimes called <em>RAID-Z1</em>.</p>
<p class="indent">You can’t add or remove disks in a RAID-Z. If you create a five-disk RAID-Z, it will remain a five-disk RAID-Z forever. Don’t go thinking you can add an additional disk to a RAID-Z for more storage. You can’t.</p>
<p class="indent">If you’re using disks over 2TB, there’s a nontrivial chance of a second drive failing as you repair the first drive. For large disks, you should probably consider RAID-Z2.</p>
<h5 class="h5" id="lev442"><strong>RAID-Z2</strong></h5>
<p class="noindent">RAID-Z2 stripes parity and data across every disk in the VDEV, much like RAID-Z1, but doubles the amount of parity information. This means a RAID-Z2 can withstand the loss of up to two disks. You can’t add or remove disks from a RAID-Z2. It is slightly slower than RAID-Z.</p>
<p class="indent">A RAID-Z2 must have four or more disks.</p>
<h5 class="h5" id="lev443"><strong>RAID-Z3</strong></h5>
<p class="noindent">Triple parity is for the most important data or those sysadmins with a whole bunch of disks and no time to fanny about. You can lose up to three disks in your RAID-Z3 without losing data. As with any other RAID-Z, you can’t add or remove disks from a RAID-Z3.</p>
<p class="indent">A RAID-Z3 must have five or more disks.</p>
<h5 class="h5" id="lev444"><strong>Log and Cache VDEVs</strong></h5>
<p class="noindent">Pools can improve performance with special-purpose VDEVs. Only adjust or implement these if performance problems demand them; don’t add them proactively.<sup><a href="footnote.xhtml#ch12fn1" id="ch12fn1a">1</a></sup> Most people don’t need them, so I won’t go into details, but you should know they exist in case you get unlucky.</p>
<p class="indent"><span epub:type="pagebreak" id="page_267"/>The <em>Separate Intent Log (SLOG</em> or <em>ZIL)</em> is ZFS’s filesystem journal. Pending writes get dumped to the SLOG and then arranged more properly in the primary pool. Every pool dedicates a chunk of disk space for a SLOG, but you can use a separate device for the SLOG instead. You need faster writes? Install a really fast drive and dedicate it to the SLOG. The pool will dump all its initial writes to the fast disk device and then migrate those writes to the slower media as time permits. A dedicated fast SLOG will also smooth out bursty I/O.</p>
<p class="indent">The <em>Level 2 Adaptive Replacement Cache (L2ARC)</em> is like the SLOG but for reads. ZFS keeps the most recently accessed and the most frequently accessed data in memory. By adding a really fast device as an L2ARC, you expand the amount of data ZFS can provide from cache instead of calling from slow disk. An L2ARC is slower than memory but faster than the slow disk.</p>
<h5 class="h5" id="lev445"><strong>RAID-Z and Pools</strong></h5>
<p class="noindent">You can add VDEVs to a pool. You can’t add disks to a RAID-Z VDEV. Think about your storage needs and your hardware before creating your pools.</p>
<p class="indent">Suppose you have a server that can hold 20 hard drives, but you have only 12 drives. You create a single RAID-Z2 VDEV out of those 12 drives, thinking that you’ll add more drives to the pool later if you need them. You haven’t even finished installing the server, and already you’ve failed.</p>
<p class="indent">You can add multiple identical VDEVs to a pool. If you create a pool with a 12-disk VDEV, and the host can hold only another 8 disks, there’s no way to create a second identical VDEV. A 12-disk RAID-Z2 isn’t identical to an 8-disk RAID-Z2. You can force ZFS to accept the different VDEVs, but performance will suffer. Adding a VDEV to a pool is irreversible.</p>
<p class="indent">Plan ahead. Look at your physical gear. Decide how you will expand your storage. This 20-drive server would be fine with two 10-disk RAID-Z2 VDEVs, or one 12-disk pool and a separate 8-disk pool. Don’t sabotage yourself.</p>
<p class="indent">Once you know what sort of VDEV you want to use, you can create a pool.</p>
<h3 class="h3" id="lev446"><strong>Managing Pools</strong></h3>
<p class="noindent">Now that you understand the different VDEV types and have indulged in planning your storage, let’s create some different types of zpools. Start by setting your disk block size.</p>
<h4 class="h4" id="lev447"><strong><em>ZFS and Disk Block Size</em></strong></h4>
<p class="noindent"><a href="ch10.xhtml#ch10">Chapter 10</a> covered how modern disks have two different sector sizes, 512 bytes and 4KB. While a filesystem can safely assume a disk has 4KB sectors, if your filesystem assumes the disk has 512-byte sectors and the disk really has 4KB sectors, your performance will plunge. ZFS, of course, assumes that disks have 512-byte sectors. If your disk really has 512-byte sectors, you’re good. If you’re not sure what size the physical sectors are, though, err on the side of caution and tell ZFS to use 4KB sectors. Control <span epub:type="pagebreak" id="page_268"/>ZFS’s disk sector assumptions with the <em>ashift</em> property. An ashift of 9 tells ZFS to use 512-byte sectors, while an ashift of 12 indicates 4KB sectors. Control ashift with the sysctl <code>vfs.zfs.min_auto_ashift</code>.</p>
<pre># <span class="codestrong1">sysctl vfs.zfs.min_auto_ashift=12</span></pre>
<p class="indent">Make this permanent by setting it in <em>/etc/sysctl.conf</em>.</p>
<p class="indent">You <em>must</em> set ashift before creating a pool. Setting it after pool creation has no effect.</p>
<p class="indent">If you’re not sure what size sectors your disks have, use an ashift of 12. That’s what the FreeBSD installer does. You’ll lose a small amount of performance, but using an ashift of 9 on 4KB disks will drain system performance.</p>
<p class="indent">Now create your pools.</p>
<h4 class="h4" id="lev448"><strong><em>Creating and Viewing Pools</em></strong></h4>
<p class="noindent">Create a pool with the zpool <code>create</code> command.</p>
<pre># <span class="codestrong1">zpool create</span> <span class="codestrongitalic1">poolname vdevtype disks...</span></pre>
<p class="indent">If the command succeeds, you get no output back.</p>
<p class="indent">Here, I create a pool named <em>db</em>, using a mirror VDEV and two GPT-labeled partitions:</p>
<pre># <span class="codestrong1">zpool create db mirror gpt/zfs3 gpt/zfs4</span></pre>
<p class="indent">The structure we assign gets reflected in the pool status.</p>
<pre># <span class="codestrong1">zpool status db</span><br/>--<span class="codeitalic1">snip</span>--<br/>config:<br/><br/>        NAME          STATE     READ WRITE CKSUM<br/>        db            ONLINE       0     0     0<br/>        <span class="ent">➊</span> mirror-0    ONLINE       0     0     0<br/>          <span class="ent">➋</span> gpt/zfs3  ONLINE       0     0     0<br/>          <span class="ent">➌</span> gpt/zfs4  ONLINE       0     0     0<br/>--<span class="codeitalic1">snip</span>--</pre>
<p class="indent">The pool db contains a single VDEV, named <em>mirror-0</em> <span class="ent">➊</span>. It includes two partitions with GPT labels, <em>/dev/gpt/zfs3</em> <span class="ent">➋</span> and <em>/dev/gpt/zfs</em> <span class="ent">➌</span>. All of those partitions are online.</p>
<p class="indent">If you don’t include a VDEV name, zpool(8) creates a striped pool with no redundancy. Here, I create a striped pool called <em>scratch</em>:</p>
<pre># <span class="codestrong1">zpool create scratch gpt/zfs3 gpt/zfs4</span></pre>
<p class="indent"><span epub:type="pagebreak" id="page_269"/>The pool status shows each VDEV, named after the underlying disk.</p>
<pre>--<span class="codeitalic1">snip</span>--<br/>        NAME        STATE     READ WRITE CKSUM<br/>        garbage     ONLINE       0     0     0<br/>          gpt/zfs3  ONLINE       0     0     0<br/>          gpt/zfs4  ONLINE       0     0     0<br/>--<span class="codeitalic1">snip</span>--</pre>
<p class="indent">Creating any type of RAID-Z looks much like creating a mirror. Just use the correct VDEV type.</p>
<pre># <span class="codestrong1">zpool create db raidz gpt/zfs3 gpt/zfs4 gpt/zfs5</span></pre>
<p class="indent">The pool status closely resembles that of a mirror, but with more disks in the VDEV.</p>
<h4 class="h4" id="lev449"><strong><em>Multi-VDEV Pools</em></strong></h4>
<p class="noindent">When you’re creating a pool, the keywords <code>mirror</code>, <code>raidz</code>, <code>raidz2</code>, and <code>raidz3</code> all tell zpool(8) to create a new VDEV. Any disks listed after one of those keywords goes into creating a new VDEV. To create a pool with multiple VDEVs, you’d do something like this:</p>
<pre># <span class="codestrong1">zpool create</span> <span class="codestrongitalic1">poolname vdevtype disks... vdevtype disks...</span></pre>
<p class="indent">Here, I create a pool containing two RAID-Z VDEVs, each with three disks:</p>
<pre># <span class="codestrong1">zpool create db raidz gpt/zfs3 gpt/zfs4 gpt/zfs5 raidz gpt/zfs6 gpt/zfs7 gpt/zfs8</span></pre>
<p class="indent">A zpool status on this new pool will look a little different.</p>
<pre>--<span class="codeitalic1">snip</span>--<br/>        NAME          STATE     READ WRITE CKSUM<br/>        db            ONLINE       0     0     0<br/>        <span class="ent">➊</span> raidz1-0    ONLINE       0     0     0<br/>            gpt/zfs3  ONLINE       0     0     0<br/>            gpt/zfs4  ONLINE       0     0     0<br/>            gpt/zfs5  ONLINE       0     0     0<br/>        <span class="ent">➋</span> raidz1-1    ONLINE       0     0     0<br/>            gpt/zfs6  ONLINE       0     0     0<br/>            gpt/zfs7  ONLINE       0     0     0<br/>            gpt/zfs8  ONLINE       0     0     0<br/>--<span class="codeitalic1">snip</span>--</pre>
<p class="indent">This pool contains a VDEV called <code>raidz1-0</code> <span class="ent">➊</span> with three disks in it. There’s a second VDEV, named <code>raidz1-1</code> <span class="ent">➋</span>, with three disks in it. It’s very clear that these are identical pools. Data gets striped across both VDEVs.</p>
<h4 class="h4" id="lev450"><span epub:type="pagebreak" id="page_270"/><strong><em>Destroying Pools</em></strong></h4>
<p class="noindent">To destroy a pool, use <code>zpool destroy</code> and the pool name.</p>
<pre># <span class="codestrong1">zpool destroy db</span></pre>
<p class="indent">Note that zpool doesn’t ask whether you’re really sure before destroying the pool. Being sure you want to destroy the pool is your problem, not zpool(8)’s.</p>
<h4 class="h4" id="lev451"><strong><em>Errors and -f</em></strong></h4>
<p class="noindent">If you enter a command that doesn’t make sense, zpool(8) will complain.</p>
<pre># <span class="codestrong1">zpool create db raidz gpt/zfs3 gpt/zfs4 gpt/zfs5 raidz gpt/zfs6 gpt/zfs7</span><br/>invalid vdev specification<br/>use '-f' to override the following errors:<br/>mismatched replication level: both 3-way and 2-way raidz vdevs are present</pre>
<p class="indent">The first thing you see when reading the error message is “use <code>-f</code> to override this error.” Many sysadmins read this as “<code>-f</code> makes this problem go away.” What ZFS is really saying, though, is “Your command line is a horrible mistake. Add <code>-f</code> to do something unfixable, harmful to system stability, and that you’ll regret as long as this system lives.”</p>
<p class="indent">Most zfs(8) and zpool(8) error messages are meaningful, but you have to read them carefully. If you don’t understand the message, fall back on the troubleshooting instructions in <a href="ch01.xhtml#ch01">Chapter 1</a>. Often, reexamining what you typed will expose the problem.</p>
<p class="indent">In this example, I asked zpool(8) to create a pool with a RAID-Z VDEV containing three disks and a second RAID-Z VDEV containing only two disks. I screwed up this command line. Adding <code>-f</code> and proceeding to install my database to the new malformed <em>db</em> pool would only ensure that I have to recreate this pool and reinstall the database at a later date.<sup><a href="footnote.xhtml#ch12fn2" id="ch12fn2a">2</a></sup> If you find yourself in this situation, investigate <code>zfs send</code> and <code>zfs recv</code>.</p>
<h3 class="h3" id="lev452"><strong>Copy-On-Write</strong></h3>
<p class="noindent">In both ordinary filesystems and ZFS, files exist as blocks on the disk. When you edit a file in a traditional filesystem, the filesystem picks up the block, modifies it, and sets it back down in the same place on the disk. A system problem halfway through that write can cause a <em>shorn write</em>: a file that’s 50 percent the old version, 50 percent the new version, and probably 100 percent unusable.</p>
<p class="indent">ZFS never overwrites the existing blocks in a file. When a file changes, ZFS identifies the blocks that must change and writes them to a new chunk <span epub:type="pagebreak" id="page_271"/>of disk space. The old version is left intact. This is called <em>copy-on-write (COW)</em>. With copy-on-write, a short write might lose the newest changes to the file, but the previous version of the file will remain intact.</p>
<p class="indent">Never corrupting files is a great benefit to copy-on-write, but COW opens up other possibilities. The metadata blocks are also copy-on-write, all the way up to the <em>uberblocks</em> that form the root of the ZFS pool’s data tree. ZFS creates snapshots by tracking the blocks that contain old versions of a file. While that sounds simple, the details are what will lead you astray.</p>
<h3 class="h3" id="lev453"><strong>Snapshots</strong></h3>
<p class="noindent">A snapshot is a copy of a dataset as it existed at a specific instant. Snapshots are read-only and never change. You can access the contents of a snapshot to access older versions of files or even deleted files. While snapshots are read-only, you can roll the dataset back to the snapshot. Take a snapshot before upgrading a system, and if the upgrade goes horribly wrong, you can fall back to the snapshot. ZFS uses snapshots to provide many features, such as boot environments (see “<a href="ch12.xhtml#lev464">Boot Environments</a>” on <a href="ch12.xhtml#page_276">page 276</a>). Best of all, depending on your data, snapshots can take up only tiny amounts of space.</p>
<p class="indent">Every dataset has a bunch of metadata, all built as a tree from a top-level block. When you create a snapshot, ZFS duplicates that top-level block. One of those metadata blocks goes with the dataset, while the other goes with the snapshot. The dataset and the snapshot share the data blocks within the dataset.</p>
<p class="indent">Deleting, modifying, or overwriting a file on the live dataset means allocating new blocks for the new data and disconnecting blocks containing the old data. Snapshots need some of those old data blocks, however. Before discarding an old block, ZFS checks to see whether a snapshot still needs it. If a snapshot needs a block, but the dataset no longer does, ZFS keeps the block.</p>
<p class="indent">So, a snapshot is merely a list of which blocks the dataset used at the time the snapshot was taken. Creating a snapshot tells ZFS to preserve those blocks, even if the dataset no longer needs those blocks.</p>
<h4 class="h4" id="lev454"><strong><em>Creating Snapshots</em></strong></h4>
<p class="noindent">Use the <code>zfs snapshot</code> command to create snapshots. Specify the dataset by its full path, then add <code>@</code> and a snapshot name. I habitually name my snapshots after the date and time I create the snapshot, for reasons that will become clear by the end of this chapter.</p>
<p class="indent">I’m about to do maintenance on user home directories, removing old stuff to free up space. I’m pretty sure that someone will whinge about me removing their files,<sup><a href="footnote.xhtml#ch12fn3" id="ch12fn3a">3</a></sup> so I want to create a snapshot before cleaning up.</p>
<pre># <span class="codestrong1">zfs snapshot zroot/usr/home@2018-07-21-13:09:00</span></pre>
<p class="indent"><span epub:type="pagebreak" id="page_272"/>I don’t get any feedback. Did anything happen? View all your snapshots with the <code>-t snapshot</code> argument to <code>zfs list</code>.</p>
<pre># <span class="codestrong1">zfs list -t snapshot</span><br/>NAME                                    USED  AVAIL    REFER     MOUNTPOINT<br/>zroot/usr/home@2018-07-21-13:09:00       <span class="ent">➊</span>0     <span class="ent">➋</span>-   <span class="ent">➌</span>4.68G   <span class="ent">➍</span>-</pre>
<p class="indent">The snapshot exists. The <code>USED</code> column shows that it uses zero disk space <span class="ent">➊</span>: it’s identical to the dataset it came from. As snapshots are read-only, available space <span class="ent">➋</span> shown by <code>AVAIL</code> is just not relevant. The <code>REFER</code> column shows that this snapshot pulls in 4.68GB of disk space <span class="ent">➌</span>. If you check, you’ll see that’s the size of <em>zroot/usr/home</em>. Finally, the <code>MOUNTPOINT</code> column shows that this snapshot isn’t mounted <span class="ent">➍</span>.</p>
<p class="indent">This is an active system, and other people are logged into it. I wait a moment and check my snapshots again.</p>
<pre># <span class="codestrong1">zfs list -t snapshot</span><br/>NAME                                    USED  AVAIL  REFER  MOUNTPOINT<br/>zroot/usr/home@2018-07-21-13:09:00      <span class="ent">➊</span>96K      -  4.68G  -</pre>
<p class="indent">The snapshot now uses 96KB <span class="ent">➊</span>. A user changed something on the dataset, and the snapshot gets charged with the space needed to maintain the difference.</p>
<p class="indent">Now I go on my rampage, and get rid of the files I think are garbage.</p>
<pre># <span class="codestrong1">zfs list -t snapshot</span><br/>NAME                                    USED  AVAIL  REFER  MOUNTPOINT<br/>zroot/usr/home@2018-07-21-13:09:00     1.62G      -  4.68G  -</pre>
<p class="indent">This snapshot now uses 1.62GB of space. Those are files that I’ve deleted but that are still available in the snapshot. I’ll keep this snapshot for a little while to give the users a chance to complain.</p>
<h4 class="h4" id="lev455"><strong><em>Accessing Snapshots</em></strong></h4>
<p class="noindent">Every ZFS dataset has a hidden <em>.zfs</em> directory in its root. It won’t show up in ls(1); you have to know it exists. That directory has a snapshot directory, which contains a directory named after each snapshot. The contents of the snapshot are in that directory.</p>
<p class="indent">For our snapshot <em>zroot/usr/home@2018-07-21-13:09:00</em>, we’d go to <em>/usr/home/.zfs/snapshot/2018-07-21-13:09:00</em>. While the <em>.zfs</em> directory doesn’t show up in ls(1), once you’re in it, ls(1) works normally. That directory contains every file as it existed when I created the snapshot, even if I’ve deleted or changed that file since creating that snapshot.</p>
<p class="indent">Recovering a file from the snapshot requires only copying the file from the snapshot to a read-write location.</p>
<h4 class="h4" id="lev456"><span epub:type="pagebreak" id="page_273"/><strong><em>Destroying Snapshots</em></strong></h4>
<p class="noindent">A snapshot is a dataset, just like a filesystem-style dataset. Remove it with <code>zfs</code> <code>destroy</code>.</p>
<pre># <span class="codestrong1">zfs destroy zroot/usr/home@2017-07-21-13:09:00</span></pre>
<p class="indent">The space used by the snapshot is now available for more junk files.</p>
<h3 class="h3" id="lev457"><strong>Compression</strong></h3>
<p class="noindent">Snapshots aren’t the only way ZFS can save space. ZFS uses on-the-fly compression, transparently inspecting the contents of each file and squeezing its size if possible. With ZFS, your programs don’t need to compress their log files: the filesystem will do it for you in real time. While FreeBSD enables compression by default at install time, you’ll use it more effectively if you understand how it works.</p>
<p class="indent">Compression changes system performance, but probably not in the way you think it would. You’ll need CPU time to compress and decompress data as it goes to and from the disk. Most disk requests are smaller than usual, however. You essentially exchange processor time for disk I/O. Every server I manage, whether bare metal or virtual, has far, far more processor capacity than disk I/O, so that’s a trade I’ll gleefully make. The end result is that using ZFS compression most often <em>increases</em> performance.</p>
<p class="indent">Compression works differently on different datasets. Binary files are already pretty tightly compressed; compressing <em>/usr/bin</em> doesn’t save much space. Compressing <em>/var/log</em>, though, often results in reducing file size by a factor of six or seven. Check the property <code>compressratio</code> to see how effectively compression shrinks your data. My hosts write to logs far more often than they write binaries. I’ll gleefully accept a sixfold performance increase for the most common task.</p>
<p class="indent">ZFS supports many compression algorithms, but the default is <em>lz4</em>. The lz4 algorithm is special in that in quickly recognizes incompressible files. When you write a binary to disk, lz4 looks at it and says, “Nope, I can’t help you,” and immediately quits trying. This eliminates pointless CPU load. It effectively compresses files that can be compressed, however.</p>
<h3 class="h3" id="lev458"><strong>Pool Integrity and Repair</strong></h3>
<p class="noindent">Every piece of data in a ZFS pool has an associated cryptographic hash stored in its metadata to verify integrity. Every time you access a piece of data, ZFS recomputes the hash of every block in that data. When ZFS discovers corrupt data in a pool with redundancy, it transparently corrects that data and proceeds. If ZFS discovers corrupt data in a pool without redundancy, it gives a warning and refuses to serve the data. If your pool has identified any data errors, they’ll show up in <code>zpool status</code>.</p>
<h4 class="h4" id="lev459"><span epub:type="pagebreak" id="page_274"/><strong><em>Integrity Verification</em></strong></h4>
<p class="noindent">In addition to the on-the-fly verification, ZFS can explicitly walk the entire filesystem tree and verify every chunk of data in the pool. This is called a <em>scrub</em>. Unlike UFS’s fsck(8), scrubs happen while the pool is online and in use. If you’ve previously run a scrub, that will also show up in the pool status.</p>
<pre>  scan: scrub repaired 0 in 8h3m with 0 errors on Fri Jul 21 14:17:29 2017</pre>
<p class="indent">To scrub a pool, run <code>zpool scrub</code> and give the pool name.</p>
<pre># <span class="codestrong1">zpool scrub zroot</span></pre>
<p class="indent">You can watch the progress of the scrub with <code>zpool status</code>.</p>
<p class="indent">Scrubbing a pool reduces its performance. If your system is already pushing its limits, scrub pools only during off hours. You can cancel a scrub<sup><a href="footnote.xhtml#ch12fn4" id="ch12fn4a">4</a></sup> with the <code>-s</code> option.</p>
<pre># <span class="codestrong1">zpool scrub -s zroot</span></pre>
<p class="indent">Run another scrub once the load drops.</p>
<h4 class="h4" id="lev460"><strong><em>Repairing Pools</em></strong></h4>
<p class="noindent">Disks fail. That’s what they’re for. The point of redundancy is that you can replace failing or flat-out busted disks with working disks and restore redundancy.</p>
<p class="indent">Mirror and RAID-Z virtual devices are specifically designed to reconstruct the data lost when a disk fails. They’re much like RAID in that regard. If one disk in a ZFS mirror dies, you replace the dead disk, and ZFS copies the surviving mirror onto the new disk. If a disk in a RAID-Z VDEV fails, you replace the busted drive, and ZFS rebuilds the data on that disk from parity data.</p>
<p class="indent">In ZFS, this reconstruction is called <em>resilvering</em>. Like other ZFS integrity operations, resilvering takes place only on live filesystems. Resilvering isn’t quite like rebuilding a RAID disk from parity, as ZFS leverages its knowledge of the filesystem to optimize repopulating the replacement device. Resilvering begins automatically when you replace a failed device. ZFS resilvers at a low priority so that it doesn’t interfere with normal operations.</p>
<h4 class="h4" id="lev461"><strong><em>Pool Status</em></strong></h4>
<p class="noindent">The <code>zpool status</code> command shows the health of the underlying storage hardware in the <code>STATE</code> field. We’ve seen a couple examples of healthy pools, so let’s take a look at an unhealthy pool.</p>
<pre><span epub:type="pagebreak" id="page_275"/># <span class="codestrong1">zpool status db</span><br/>  pool: db<br/> state: <span class="ent">➊</span>DEGRADED<br/>status: One or more devices could not be opened.  Sufficient replicas exist for<br/>        the pool to continue functioning in a degraded state.<br/>action: Attach the missing device and online it using 'zpool online'.<br/>   see: http://illumos.org/msg/ZFS-8000-2Q<br/>  scan: none requested<br/>config:<br/><br/>        NAME                        STATE     READ WRITE CKSUM<br/>        db                          DEGRADED     0     0     0<br/>        <span class="ent">➋</span>mirror-0                   DEGRADED     0     0     0<br/>            gpt/zfs1                ONLINE       0     0     0<br/>          <span class="ent">➌</span>14398195156659397932   <span class="ent">➍</span>UNAVAIL      0     0     0   <span class="ent">➎</span>was /dev/gpt/zfs3<br/><br/>errors: No known data errors</pre>
<p class="indent">The pool state is <code>DEGRADED</code> <span class="ent">➊</span>. If you look further down the output, you’ll see more <code>DEGRADED</code> entries and an <code>UNAVAIL</code> <span class="ent">➍</span>. What exactly does that mean?</p>
<p class="indent">Errors in a pool percolate upward. The pool state is a summary of the health of the pool as a whole. The whole pool shows up as <code>DEGRADED</code> because the pool’s virtual device <em>mirror-0</em> <span class="ent">➋</span> is <code>DEGRADED</code>. This error comes from an underlying disk being in the <code>UNAVAIL</code> state. We get the ZFS GUID <span class="ent">➌</span> for this disk, and the label used to create the pool <span class="ent">➎</span>.</p>
<p class="indent">ZFS pools show an error when an underlying device has an error. When a pool has a state other than <code>ONLINE</code>, dig through the VDEV and disk listings until you find the real problem.</p>
<p class="indentb">Pools, VDEVs, and disks can have six states:</p>
<p class="hang"><strong>ONLINE</strong> The device is functioning normally.</p>
<p class="hang"><strong>DEGRADED</strong> The pool or VDEV has at least one provider missing, offline, or generating errors more quickly than ZFS tolerates. Redundancy is handling the error, but you need to address this right now.</p>
<p class="hang"><strong>FAULTED</strong> A faulted disk is corrupt or generating errors more quickly than ZFS can tolerate. A faulted VDEV takes the last known good copy of the data. A two-disk mirror with two bad disks faults.</p>
<p class="hang"><strong>UNAVAIL</strong> ZFS can’t open the disk. Maybe it’s been removed, shut off, or that iffy cable finally failed. It’s not there, so ZFS can’t use it.</p>
<p class="hang"><strong>OFFLINE</strong> This device has been deliberately turned off.</p>
<p class="hang"><strong>REMOVED</strong> Some hardware detects when a drive is physically removed while the system is running, letting ZFS set the REMOVED flag. When you plug the drive back in, ZFS tries to reactivate the disk.</p>
<p class="indentt">Our missing disk is in the UNAVAIL state. For whatever reason, ZFS can’t access <em>/dev/gpt/zfs3</em>, but the disk mirror is still serving data because it has a working disk. Here’s where you get to run around to figure out where that disk went. How you manage ZFS depends on what you discover.</p>
<h5 class="h5" id="lev462"><span epub:type="pagebreak" id="page_276"/><strong>Reattaching and Detaching Drives</strong></h5>
<p class="noindent">Unavailable drives might not be dead. They might be disconnected. If you wiggle a drive tray and suddenly get a green light, the disk is fine but the connection is faulty. You should address that hardware problem, yes, but in the meantime, you can reactivate the drive. You can also reactivate deliberately removed drives. Use the <code>zpool online</code> command with the pool name and the GUID of the missing disk as arguments. If the disk in my example pool were merely disconnected, I could reactivate it like so:</p>
<pre># <span class="codestrong1">zpool online db 14398195156659397932</span></pre>
<p class="indent">ZFS resilvers the drive and resumes normal function.</p>
<p class="indent">If you want to remove a drive, you can tell ZFS to take it offline with zpool offline. Give the pool and disk names as arguments.</p>
<pre># <span class="codestrong1">zpool offline db gpt/zfs6</span></pre>
<p class="indent">Bringing disks offline, physically moving them, bringing them back online, and allowing the pools to resilver will let you migrate large storage arrays from one SAS cage to another without downtime.</p>
<h5 class="h5" id="lev463"><strong>Replacing Drives</strong></h5>
<p class="noindent">If the drive isn’t merely loose but flat-out busted, you’ll need to replace it with a new drive. ZFS lets you replace drives in several ways, but the most common is using <code>zpool replace</code>. Use the pool name, the failed provider, and the new provider as arguments. Here, I replace the db pool’s <em>/dev/gpt/zfs3</em> disk with <em>/dev/gpt/zfs6</em>:</p>
<pre># <span class="codestrong1">zpool replace db gpt/zfs3 gpt/zfs6</span></pre>
<p class="indent">The pool will resilver itself and resume normal operation.</p>
<p class="indent">In a large storage array, you can also use successive <code>zpool replace</code> operations to empty a disk shelf. Only do this if your organization’s operation requirements don’t allow you to offline and online disks.</p>
<h3 class="h3" id="lev464"><strong>Boot Environments</strong></h3>
<p class="noindent">ZFS helps us cope with one of the most dangerous things sysadmins do. No, not our eating habits. No, not a lack of exercise. I’m talking about system upgrades. When an upgrade goes well, everybody’s happy. When the upgrade goes poorly, it can ruin your day, your weekend, or your job. Nobody likes restoring from backup when the mission-critical software chokes on the new version of a shared library. Nobody likes to restore from backup.</p>
<p class="indent">Through the magic of <em>boot environments</em>, ZFS takes advantage of snapshots to let you fall back from a system upgrade with only a reboot. A boot <span epub:type="pagebreak" id="page_277"/>environment is a clone of the root dataset. It includes the kernel, the base system userland, the add-on packages, and the core system databases. Before running an upgrade, create a boot environment. If the upgrade goes well, you’re good. If the upgrade goes badly, though, you can reboot into the boot environment. This restores service while you investigate how the upgrade failed and what you can do to fix those problems.</p>
<p class="indent">Boot environments do not work when a host requires a separate boot pool. The installer handles boot pools for you. They appear when combining UEFI and GELI, or when using ZFS on an MBR-partitioned disk.</p>
<p class="indent">Using boot environments requires a boot environment manager. I recommend beadm(8), available as a package.</p>
<pre># <span class="codestrong1">pkg install beadm</span></pre>
<p class="indent">You’re now ready to use boot environments.</p>
<h4 class="h4" id="lev465"><strong><em>Viewing Boot Environments</em></strong></h4>
<p class="noindent">Each boot environment is a dataset under <em>zroot/ROOT</em>. A system where you’ve just installed beadm should have only one boot environment. Use <code>beadm list</code> to view them all.</p>
<pre>  # <span class="codestrong1">beadm list</span><br/>  BE        Active    Mountpoint  Space   Created<br/> <span class="ent">➊</span>default  <span class="ent">➋</span>NR      <span class="ent">➌</span>/          <span class="ent">➍</span>2.4G   <span class="ent">➎</span>2018-05-04 13:13</pre>
<p class="indent">This host has one boot environment, named <em>default</em> <span class="ent">➊</span>, after the dataset <em>zroot/ROOT/default</em>.</p>
<p class="indent">The Active column <span class="ent">➋</span> shows whether this boot environment is in use. An <code>N</code> means that the environment is now in use. An <code>R</code> means that this environment will be active after a reboot. They appear together when the default environment is running.</p>
<p class="indent">The Mountpoint column <span class="ent">➌</span> shows the location of this boot environment’s mount point. Most boot environments aren’t mounted unless they’re in use, but you can use beadm(8) to mount an unused boot environment.</p>
<p class="indent">The Space column <span class="ent">➍</span> shows the amount of disk space this boot environment uses. It’s built on a snapshot, so the dataset probably has more data than this amount in it.</p>
<p class="indent">The Created column <span class="ent">➎</span> shows the date this boot environment was created. In this case, it’s the date the machine was installed.</p>
<p class="indent">Before changing the system, create a new boot environment.</p>
<h4 class="h4" id="lev466"><strong><em>Creating and Accessing Boot Environments</em></strong></h4>
<p class="noindent">Each boot environment needs a name. I recommend names based on the current operating system version and patch level or the date. Names like “beforeupgrade” and “dangitall,” while meaningful in the moment, will only confuse you later.</p>
<p class="indent"><span epub:type="pagebreak" id="page_278"/>Use <code>beadm create</code> to make your new boot environment. Here, I check the current FreeBSD version, and use that to create the boot environment name:</p>
<pre># <span class="codestrong1">freebsd-version</span><br/>11.0-RELEASE-p11<br/># <span class="codestrong1">beadm create 11.0-p11</span><br/>Created successfully</pre>
<p class="indent">I now have two identical boot environments.</p>
<pre># <span class="codestrong1">beadm list</span><br/>BE  a       Active Mountpoint  Space Created<br/>default    NR     /           12.3G 2015-04-28 11:53<br/>11.0-p11   -      -          236.0K 2018-07-21 14:57</pre>
<p class="indent">You might notice that the new boot environment already takes up 236KB. This is a live system. Between when I created the boot environment and when I listed those environments, the filesystem or its metadata changed.</p>
<p class="indent">The Active column shows that we’re currently using the default boot environment and that we’ll be using that on the next boot. If I change my installed packages or upgrade the base system, those changes will affect the default environment.</p>
<p class="indent">Each boot environment is available as a snapshot under <em>zroot/ROOT</em>. If you want to access a boot environment read-write, use <code>beadm mount</code> to temporarily mount the boot environment under <em>/tmp</em>. Unmount those environments with <code>beadm umount</code>.</p>
<h4 class="h4" id="lev467"><strong><em>Activating Boot Environments</em></strong></h4>
<p class="noindent">Suppose you upgrade your packages and the system goes belly-up. Fall back to an earlier operating system install by activating a boot environment and rebooting. Activate a boot environment with <code>beadm activate</code>.</p>
<pre># <span class="codestrong1">beadm activate 11.0-p11</span><br/>Activated successfully<br/># <span class="codestrong1">beadm list</span><br/>BE         Active Mountpoint  Space Created<br/>default    N      /           12.4G 2015-04-28 11:53<br/>11.0-p11   R      -          161.8M 2018-07-21 14:57</pre>
<p class="indent">The default boot environment has its Active flag set to <code>N</code>, meaning it’s now running. The 11.0-p11 environment has the <code>R</code> flag, so after a reboot it will be live.</p>
<p class="indent">Reboot the system and suddenly you’ve fallen back to the previous operating system install, without the changes that destabilized your system. That’s much simpler than restoring from backup.</p>
<h4 class="h4" id="lev468"><span epub:type="pagebreak" id="page_279"/><strong><em>Removing Boot Environments</em></strong></h4>
<p class="noindent">After a few upgrades, you’ll find that you’ll never fall back to some of the existing boot environments. Once I upgrade this host to, say, 12.2-RELEASE-p29, chances are I’ll never ever reboot into 11.0-p11 again. Remove obsolete boot environments and free up their disk space with <code>beadm destroy</code>.</p>
<pre># <span class="codestrong1">beadm destroy 11.0-p11</span><br/>Are you sure you want to destroy '11.0-p11'?<br/>This action cannot be undone (y/[n]): <span class="codestrong1">y</span><br/>Destroyed successfully</pre>
<p class="indent">Answer <code>y</code> when prompted, and beadm will remove the boot environment.</p>
<h4 class="h4" id="lev469"><strong><em>Boot Environments at Boot</em></strong></h4>
<p class="noindent">So you’ve truly hosed your operating system. Forget getting to multiuser mode, you can’t even hit <em>single</em>-user mode without generating a spew of bizarre error messages. You can select a boot environment right at the loader prompt. This requires console access, but so would any other method of rescuing yourself.</p>
<p class="indent">The boot loader menu includes an option to select a boot environment. Choose that option. You’ll get a new menu listing every boot environment on the host by name. Choose your new boot environment and hit <small>ENTER</small>. The system will boot into that environment, giving you a chance to figure out why everything went sideways.</p>
<h4 class="h4" id="lev470"><strong><em>Boot Environments and Applications</em></strong></h4>
<p class="noindent">It’s not enough that your upgrade failed. It might take your application data with it.</p>
<p class="indent">Most applications store their data somewhere in the root dataset. MySQL uses <em>/var/db/mysql</em>, while Apache uses <em>/usr/local/www</em>. This means that falling back to an earlier boot environment can revert your application data with the environment. Depending on your application, you might want that reversion—or not.</p>
<p class="indent">If an application uses data that shouldn’t be included in the boot environment, you need to create a new dataset for that data. I provided an example in “<a href="ch12.xhtml#lev431">Unmounted Parent Datasets</a>” on <a href="ch12.xhtml#page_262">page 262</a> earlier this chapter. Consider your application’s need and separate out your data as appropriate.</p>
<p class="indent">While ZFS has many more features, this covers the topics every sysadmin <em>must</em> know. Many of you would find clones, delegations, or replication useful. You might find the books <em>FreeBSD Mastery: ZFS</em> (Tilted Windmill Press, 2015) and <em>FreeBSD Mastery: Advanced ZFS</em> (Tilted Windmill Press, 2016) by Allan Jude and yours truly helpful. You’ll also find many resources on the internet documenting all of these topics.</p>
<p class="indent">Now let’s consider some other filesystems FreeBSD administrators find useful.<span epub:type="pagebreak" id="page_280"/></p>
</body></html>