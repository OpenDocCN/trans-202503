- en: '13'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HEALTH PROBES
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Having a reliable application is about more than just keeping application components
    running. Application components also need to be able to respond to requests in
    a timely way and get data from and make requests of dependencies. This means that
    the definition of a “healthy” application component is different for each individual
    component.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, Kubernetes needs to know when a Pod and its containers are
    healthy so that it can route traffic to only healthy containers and replace failed
    ones. For this reason, Kubernetes allows configuration of custom health checks
    for containers and integrates those health checks into management of workload
    resources such as Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll look at how to define health probes for our applications.
    We’ll look at both network-based health probes and probes that are internal to
    a container. We’ll see how Kubernetes runs these health probes and how it responds
    when a container becomes unhealthy.
  prefs: []
  type: TYPE_NORMAL
- en: About Probes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kubernetes supports three different types of probes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exec** Run a command or script to check on a container.'
  prefs: []
  type: TYPE_NORMAL
- en: '**TCP** Determine whether a socket is open.'
  prefs: []
  type: TYPE_NORMAL
- en: '**HTTP** Verify that an HTTP GET succeeds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we can use any of these three types of probes for any of three
    different purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Liveness** Detect and restart failed containers.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Startup** Give extra time before starting liveness probes.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Readiness** Avoid sending traffic to containers when they are not prepared
    for it.'
  prefs: []
  type: TYPE_NORMAL
- en: Of these three purposes, the most important is the liveness probe because it
    runs during the primary life cycle of the container and can result in container
    restarts. We’ll look closely at liveness probes and use that knowledge to understand
    how to use startup and readiness probes.
  prefs: []
  type: TYPE_NORMAL
- en: Liveness Probes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A *liveness* probe runs continuously as soon as the container has started running.
    Liveness probes are created as part of the container definition, and a container
    that fails its liveness probe will be restarted automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Exec Probes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s begin with a simple liveness probe that runs a command inside the container.
    Kubernetes expects the command to finish before a timeout and return zero to indicate
    success, or a non-zero code to indicate a problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s illustrate this with an NGINX web server container. We’ll use this Deployment
    definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-exec.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `exec` section of the `livenessProbe` tells Kubernetes to run a command
    inside the container. In this case, `curl` is used with a `-q` flag so that it
    doesn’t print the page contents but just returns a zero exit code on success.
    Additionally, the `-f` flag causes `curl` to return a non-zero exit code for any
    HTTP error response (that is, any response code of 300 or above).
  prefs: []
  type: TYPE_NORMAL
- en: The `curl` command runs every 5 seconds based on the `periodSeconds`; it starts
    10 seconds after the container is started, based on `initialDelaySeconds`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The automated scripts for this chapter add the *nginx-exec.yaml* file to */opt*.
    Create this Deployment as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting Pod status doesn’t look any different from a Pod without a liveness
    probe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'However, in addition to the regular NGINX server process, `curl` is being run
    inside the container every 5 seconds, verifying that it is possible to connect
    to the server. The detailed output from `kubectl describe` shows this configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Because a liveness probe is defined, the fact that the Pod continues to show
    a `Running` status and no restarts indicates that the check is successful. The
    `#success` field shows that one successful run is sufficient for the container
    to be considered live, whereas the `#failure` value shows that three consecutive
    failures will cause the Pod to be restarted.
  prefs: []
  type: TYPE_NORMAL
- en: We used `-q` to discard the logs from `curl`, but even without that flag, any
    logs from a successful liveness probe are discarded. If we want to save the ongoing
    log information from a probe, we need to send it to a file or use a logging library
    to ship it across the network.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to another type of probe, let’s see what happens if a liveness
    probe fails. We’ll patch the `curl` command to try to retrieve a nonexistent path
    on the server, which will cause `curl` to return a non-zero exit code, so our
    probe will fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used a patch file in [Chapter 9](ch09.xhtml#ch09) when we edited a Service
    type. Let’s do that again here to make the change:'
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-404.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Although a patch file allows us to update only the specific fields we care about,
    in this case the patch file has several lines because we need to specify the full
    hierarchy, and we also must specify the name of the container we want to modify
    ➊, so Kubernetes will merge this content into the existing definition for that
    container.
  prefs: []
  type: TYPE_NORMAL
- en: 'To patch the Deployment, use the `kubectl patch` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we changed the Pod specification within the Deployment, Kubernetes
    needs to terminate the old Pod and create a new one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Initially, the new Pod shows a `Running` status. However, if we check back
    again in about 30 seconds, we get an indication that the Pod has an issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We didn’t change the initial delay or the period for our liveness probe, so
    the first probe started after 10 seconds and the probe runs every 5 seconds. It
    takes three failures to trigger a restart, so it’s not surprising that we see
    one restart after 25 seconds have elapsed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Pod’s event log indicates the reason for the restart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The event log helpfully provides the output from `curl` telling us the reason
    for the failed liveness probe. Kubernetes will continue to restart the container
    every 25 seconds as each new container starts running and then fails three consecutive
    liveness probes.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP Probes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ability to run a command within a container to check health allows us to
    perform custom probes. However, for a web server like this one, we can take advantage
    of the HTTP probe capability within Kubernetes, avoiding the need for `curl` inside
    our container image and also verifying connectivity from outside the Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s replace our NGINX Deployment with a new configuration that uses an HTTP
    probe:'
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-http.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With this configuration, we tell Kubernetes to connect to port 80 of our Pod
    and do an HTTP GET at the root path of */*. Because our NGINX server is listening
    on port 80 and will serve a welcome file for the root path, we can expect this
    to work.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve specified the entire Deployment rather than using a patch, so we’ll use
    `kubectl apply` to update the Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We could use a patch to make this change as well, but it would be more complex
    this time, because a patch file is merged into the existing configuration. As
    a result, we would require two commands: one to remove the existing liveness probe
    and one to add the new HTTP liveness probe. Better to just replace the resource
    entirely.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The kubectl patch command is a valuable command for debugging, but production
    applications should have YAML resource files under version control to allow for
    change tracking and peer review, and the entire file should always be applied
    every time to ensure that the cluster reflects the current content of the repository.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve applied the new Deployment configuration, Kubernetes will make
    a new Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For an HTTP probe, `kubelet` has the responsibility of running an HTTP GET request
    on the appropriate schedule and confirming the result. By default, any HTTP return
    code in the 200 or 300 series is considered a successful response.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NGINX server is logging all of its requests, so we can use the container
    logs to see the probes taking place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We didn’t specify `periodSeconds` this time, so `kubelet` is probing the server
    at the default rate of once every 10 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s clean up the NGINX Deployment before moving on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We’ve looked at two of the three types of probes; let’s finish by looking at
    TCP.
  prefs: []
  type: TYPE_NORMAL
- en: TCP Probes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A database server such as PostgreSQL listens for network connections, but it
    does not use HTTP for communication. We can still create a probe for these kinds
    of containers using a TCP probe. It won’t provide the configuration flexibility
    of an HTTP or exec probe, but it will verify that a container in the Pod is listening
    for connections on the specified port.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a PostgreSQL Deployment with a TCP probe:'
  prefs: []
  type: TYPE_NORMAL
- en: '*postgres-tcp.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We saw the requirement for the `POSTGRES_PASSWORD` environment variable in [Chapter
    10](ch10.xhtml#ch10). The only configuration that’s changed for this example is
    the `livenessProbe`. We specify a TCP socket of 5432, as this is the standard
    port for PostgreSQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we can create this Deployment and, after a while, observe that it’s
    running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, it is the job of `kubelet` to perform the probe. It does this solely
    by making a TCP connection to the port and then disconnecting. PostgreSQL doesn’t
    emit any logging when this happens, so the only way we know that the probe is
    working is to check that the container continues to run and doesn’t show any restarts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we move on, let’s clean up the Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We’ve now looked at all three types of probes. And although we used these three
    types to create liveness probes, the same three types will work with both startup
    and readiness probes as well. The only difference is the change in the behavior
    of our cluster when a probe fails.
  prefs: []
  type: TYPE_NORMAL
- en: Startup Probes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unhealthy containers can create all kinds of difficulties for an application,
    including lack of responsiveness, errors responding to requests, or bad data,
    so we want Kubernetes to respond quickly when a container becomes unhealthy. However,
    when a container is first started, it can take time before it is fully initialized.
    During that time, it might not be able to respond to liveness probes.
  prefs: []
  type: TYPE_NORMAL
- en: Because of that delay, we’re left with a need to have a long timeout before
    a container fails a probe, so we can give our container enough time for initialization.
    However, at the same time, we need to have a short timeout in order to detect
    a failed container quickly and restart it. The solution is to configure a separate
    *startup probe*. Kubernetes will use the startup probe configuration until the
    probe is successful; then it will switch over to the liveness probe.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we might configure our NGINX server Deployment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Given this configuration, Kubernetes would start checking the container 30 seconds
    after startup. It would continue checking every 10 seconds until the probe is
    successful or until there are 60 failed attempts. The effect is that the container
    has 10 minutes to finish initialization and respond to a probe successfully. If
    the container does not have a successful probe in that time, it will be restarted.
  prefs: []
  type: TYPE_NORMAL
- en: As soon as the container has one successful probe, Kubernetes will switch to
    the configuration for `livenessProbe`. Because we didn’t override any timing parameters,
    this will transition to a probe every 10 seconds, with three consecutive failed
    probes leading to a restart. We give the container 10 minutes to be live initially,
    but after that we will allow no more than 30 seconds before restarting it.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the `startupProbe` is defined completely separately means that
    it is possible to create a different check for startup from the one used for liveness.
    Of course, it’s important to choose wisely so that the container doesn’t pass
    its startup probe before the liveness probe would also pass, because that would
    result in inappropriate restarts.
  prefs: []
  type: TYPE_NORMAL
- en: Readiness Probes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third probe purpose is to check the *readiness* of the Pod. The term *readiness*
    might seem redundant with the startup probe. However, even though completing initialization
    is an important part of readiness for a piece of software, an application component
    might not be ready to do work for many reasons, especially in a highly available
    microservice architecture where components can come and go at any time.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than being used for initialization, readiness probes should be used for
    any case in which the container cannot perform any work because of a failure outside
    its control. It may be a temporary situation, as retry logic somewhere else could
    fix the failure. For example, an API that relies on an external database might
    fail its readiness probe if the database is unreachable, but that database might
    return to service at any time.
  prefs: []
  type: TYPE_NORMAL
- en: This also creates a valuable contrast with startup and liveness probes. As we
    examined earlier, Kubernetes will restart a container if it fails the configured
    number of startup or liveness probes. But it makes no sense to do that if the
    issue is a failed or missing external dependency, given that restarting the container
    won’t fix whatever is wrong externally.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, if a container is missing a required external dependency,
    it can’t do work, so we don’t want to send any work to it. In that situation,
    the best thing to do is to leave the container running and give it an opportunity
    to reestablish the connections it needs, but avoid sending any requests to it.
    In the meantime, we can hope that somewhere in the cluster another Pod for the
    same Deployment is working as expected, making our application as a whole resilient
    to a localized failure.
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly how readiness probes work in Kubernetes. As we saw in [Chapter
    9](ch09.xhtml#ch09), a Kubernetes Service continually watches for Pods that match
    its selector and configures load balancing for its cluster IP that routes traffic
    to those Pods. If a Pod reports itself as not ready, the Service will stop routing
    traffic to it, but `kubelet` will not trigger any other action such as a container
    restart.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s illustrate this situation. We want to have individual control over Pod
    readiness, so we’ll use a somewhat contrived example rather than a real external
    dependency to determine readiness. We’ll deploy a set of NGINX Pods, this time
    with a corresponding Service:'
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-ready.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This Deployment keeps its `livenessProbe` as an indicator that NGINX is working
    correctly and adds a `readinessProbe`. The Service definition is identical to
    what we saw in [Chapter 9](ch09.xhtml#ch09) and will route traffic to our NGINX
    Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'This file has already been written to */opt*, so we can apply it to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After these Pods are up and running, they stay running because the liveness
    probe is successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, the Service we created has been allocated a cluster IP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we aren’t able to use that IP address to reach any Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This is because, at the moment, there is nothing for NGINX to serve on the
    */ready* path, so it’s returning `404`, and the readiness probe is failing. A
    detailed inspection of the Pod shows that it is not ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, the Service does not have any Endpoints to which to route traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Because the Service has no Endpoints, it has configured `iptables` to reject
    all traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: To fix this, we’ll need at least one Pod to become ready to ensure that NGINX
    has something to serve on the */ready* path. We’ll use the container’s hostname
    to keep track of which Pod is serving our request.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make one of our Pods ready, let’s first get the list of Pods again, just
    to have the Pod names handy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we’ll choose one and make it report that it is ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Our Service will start to show a valid Endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Even better, we can now reach an NGINX instance via the cluster IP, and the
    content corresponds to the hostname:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note the `/ready` at the end of the URL so the response is the hostname. If
    we run this command many times, we’ll see that the hostname is the same every
    time. This is because the one Pod that is passing its readiness probe is handling
    all of the Service traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make the other two Pods ready as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Our Service now shows all three Endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the `curl` command multiple times shows that the traffic is now being
    distributed across multiple Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The embedded command `$(seq 1 5)` returns the numbers one through five, causing
    the `for` loop to run `curl` five times. If you run this same `for` loop several
    times, you will see a different distribution of hostnames. As described in [Chapter
    9](ch09.xhtml#ch09), load balancing is based on a random uniform distribution
    wherein each endpoint has an equal chance of being selected for each new connection.
  prefs: []
  type: TYPE_NORMAL
- en: A good practice is to offer an HTTP readiness endpoint for each application
    that checks the current state of the application and its dependencies and returns
    an HTTP success code (such as `200`) if the component is healthy, and an HTTP
    error code (such as `500`) if not. Some application frameworks such as Spring
    Boot provide application state management that automatically exposes liveness
    and readiness endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes offers the ability to check on our containers and make sure they
    are working as expected, not just that the process is running. These probes can
    include any arbitrary command run inside the container, verifying that a port
    is open for TCP connections, or that the container responds correctly to an HTTP
    request. To build resilient applications, we should define both a liveness probe
    and a readiness probe for each application component. The liveness probe is used
    to restart an unhealthy container; the readiness probe determines whether the
    Pod can handle Service traffic. Additionally, if a component needs extra time
    for initialization, we should also define a startup probe to make sure that give
    it the required initialization time while responding quickly to failure as soon
    as initialization is complete.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, for our containers to run as expected, other containers in the cluster
    must also be well behaved, not using too many of the cluster’s resources. In the
    next chapter, we’ll look at how we can limit our containers in their use of CPU,
    memory, disk space, and network bandwidth, as well as how we can control the maximum
    amount of total resources available to a user. This ability to specify limits
    and quotas is important to ensure that our cluster can support multiple applications
    with reliable performance.
  prefs: []
  type: TYPE_NORMAL
