- en: '13'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '13'
- en: HEALTH PROBES
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 健康探针
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Having a reliable application is about more than just keeping application components
    running. Application components also need to be able to respond to requests in
    a timely way and get data from and make requests of dependencies. This means that
    the definition of a “healthy” application component is different for each individual
    component.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个可靠的应用程序不仅仅是让应用组件保持运行。应用组件还需要能够及时响应请求，并从依赖项中获取数据并发出请求。这意味着，“健康”应用组件的定义对于每个组件都是不同的。
- en: At the same time, Kubernetes needs to know when a Pod and its containers are
    healthy so that it can route traffic to only healthy containers and replace failed
    ones. For this reason, Kubernetes allows configuration of custom health checks
    for containers and integrates those health checks into management of workload
    resources such as Deployment.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，Kubernetes需要知道Pod及其容器的健康状态，以便只将流量路由到健康的容器，并替换掉失败的容器。因此，Kubernetes允许为容器配置自定义健康检查，并将这些健康检查集成到工作负载资源的管理中，例如Deployment。
- en: In this chapter, we’ll look at how to define health probes for our applications.
    We’ll look at both network-based health probes and probes that are internal to
    a container. We’ll see how Kubernetes runs these health probes and how it responds
    when a container becomes unhealthy.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何为我们的应用程序定义健康探针。我们将研究基于网络的健康探针和容器内部的探针。我们还将了解Kubernetes如何运行这些健康探针，并且当容器变得不健康时如何响应。
- en: About Probes
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于探针
- en: 'Kubernetes supports three different types of probes:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes支持三种不同类型的探针：
- en: '**Exec** Run a command or script to check on a container.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**Exec** 运行一个命令或脚本来检查容器的状态。'
- en: '**TCP** Determine whether a socket is open.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**TCP** 确定一个套接字是否打开。'
- en: '**HTTP** Verify that an HTTP GET succeeds.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**HTTP** 验证HTTP GET是否成功。'
- en: 'In addition, we can use any of these three types of probes for any of three
    different purposes:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以将这三种探针中的任何一种用于三种不同的用途：
- en: '**Liveness** Detect and restart failed containers.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**Liveness** 检测并重启失败的容器。'
- en: '**Startup** Give extra time before starting liveness probes.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**Startup** 在启动活性探针之前，给容器额外的时间。'
- en: '**Readiness** Avoid sending traffic to containers when they are not prepared
    for it.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**Readiness** 在容器尚未准备好时避免向其发送流量。'
- en: Of these three purposes, the most important is the liveness probe because it
    runs during the primary life cycle of the container and can result in container
    restarts. We’ll look closely at liveness probes and use that knowledge to understand
    how to use startup and readiness probes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三种用途中，最重要的是活性探针，因为它在容器的主要生命周期内运行，并可能导致容器重启。我们将详细了解活性探针，并利用这些知识理解如何使用启动探针和就绪探针。
- en: Liveness Probes
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活性探针
- en: A *liveness* probe runs continuously as soon as the container has started running.
    Liveness probes are created as part of the container definition, and a container
    that fails its liveness probe will be restarted automatically.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*活性*探针会在容器启动后持续运行。活性探针作为容器定义的一部分创建，任何未通过活性探针检查的容器将会被自动重启。'
- en: Exec Probes
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Exec 探针
- en: Let’s begin with a simple liveness probe that runs a command inside the container.
    Kubernetes expects the command to finish before a timeout and return zero to indicate
    success, or a non-zero code to indicate a problem.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先从一个简单的活性探针开始，该探针会在容器内运行一个命令。Kubernetes期望命令在超时前完成，并返回零表示成功，或者返回非零代码表示存在问题。
- en: '**NOTE**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**NOTE**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例仓库在* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。
    *关于如何设置，请参见[第xx页](ch00.xhtml#ch00lev1sec2)中的“运行示例”。*'
- en: 'Let’s illustrate this with an NGINX web server container. We’ll use this Deployment
    definition:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个NGINX web服务器容器来说明这一点。我们将使用这个Deployment定义：
- en: '*nginx-exec.yaml*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx-exec.yaml*'
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `exec` section of the `livenessProbe` tells Kubernetes to run a command
    inside the container. In this case, `curl` is used with a `-q` flag so that it
    doesn’t print the page contents but just returns a zero exit code on success.
    Additionally, the `-f` flag causes `curl` to return a non-zero exit code for any
    HTTP error response (that is, any response code of 300 or above).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The `curl` command runs every 5 seconds based on the `periodSeconds`; it starts
    10 seconds after the container is started, based on `initialDelaySeconds`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'The automated scripts for this chapter add the *nginx-exec.yaml* file to */opt*.
    Create this Deployment as usual:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The resulting Pod status doesn’t look any different from a Pod without a liveness
    probe:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'However, in addition to the regular NGINX server process, `curl` is being run
    inside the container every 5 seconds, verifying that it is possible to connect
    to the server. The detailed output from `kubectl describe` shows this configuration:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Because a liveness probe is defined, the fact that the Pod continues to show
    a `Running` status and no restarts indicates that the check is successful. The
    `#success` field shows that one successful run is sufficient for the container
    to be considered live, whereas the `#failure` value shows that three consecutive
    failures will cause the Pod to be restarted.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: We used `-q` to discard the logs from `curl`, but even without that flag, any
    logs from a successful liveness probe are discarded. If we want to save the ongoing
    log information from a probe, we need to send it to a file or use a logging library
    to ship it across the network.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to another type of probe, let’s see what happens if a liveness
    probe fails. We’ll patch the `curl` command to try to retrieve a nonexistent path
    on the server, which will cause `curl` to return a non-zero exit code, so our
    probe will fail.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'We used a patch file in [Chapter 9](ch09.xhtml#ch09) when we edited a Service
    type. Let’s do that again here to make the change:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-404.yaml*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Although a patch file allows us to update only the specific fields we care about,
    in this case the patch file has several lines because we need to specify the full
    hierarchy, and we also must specify the name of the container we want to modify
    ➊, so Kubernetes will merge this content into the existing definition for that
    container.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'To patch the Deployment, use the `kubectl patch` command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Because we changed the Pod specification within the Deployment, Kubernetes
    needs to terminate the old Pod and create a new one:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Initially, the new Pod shows a `Running` status. However, if we check back
    again in about 30 seconds, we get an indication that the Pod has an issue:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We didn’t change the initial delay or the period for our liveness probe, so
    the first probe started after 10 seconds and the probe runs every 5 seconds. It
    takes three failures to trigger a restart, so it’s not surprising that we see
    one restart after 25 seconds have elapsed.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'The Pod’s event log indicates the reason for the restart:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The event log helpfully provides the output from `curl` telling us the reason
    for the failed liveness probe. Kubernetes will continue to restart the container
    every 25 seconds as each new container starts running and then fails three consecutive
    liveness probes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: HTTP Probes
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ability to run a command within a container to check health allows us to
    perform custom probes. However, for a web server like this one, we can take advantage
    of the HTTP probe capability within Kubernetes, avoiding the need for `curl` inside
    our container image and also verifying connectivity from outside the Pod.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s replace our NGINX Deployment with a new configuration that uses an HTTP
    probe:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-http.yaml*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With this configuration, we tell Kubernetes to connect to port 80 of our Pod
    and do an HTTP GET at the root path of */*. Because our NGINX server is listening
    on port 80 and will serve a welcome file for the root path, we can expect this
    to work.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve specified the entire Deployment rather than using a patch, so we’ll use
    `kubectl apply` to update the Deployment:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We could use a patch to make this change as well, but it would be more complex
    this time, because a patch file is merged into the existing configuration. As
    a result, we would require two commands: one to remove the existing liveness probe
    and one to add the new HTTP liveness probe. Better to just replace the resource
    entirely.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '*The kubectl patch command is a valuable command for debugging, but production
    applications should have YAML resource files under version control to allow for
    change tracking and peer review, and the entire file should always be applied
    every time to ensure that the cluster reflects the current content of the repository.*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve applied the new Deployment configuration, Kubernetes will make
    a new Pod:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For an HTTP probe, `kubelet` has the responsibility of running an HTTP GET request
    on the appropriate schedule and confirming the result. By default, any HTTP return
    code in the 200 or 300 series is considered a successful response.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'The NGINX server is logging all of its requests, so we can use the container
    logs to see the probes taking place:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We didn’t specify `periodSeconds` this time, so `kubelet` is probing the server
    at the default rate of once every 10 seconds.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s clean up the NGINX Deployment before moving on:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We’ve looked at two of the three types of probes; let’s finish by looking at
    TCP.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: TCP Probes
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A database server such as PostgreSQL listens for network connections, but it
    does not use HTTP for communication. We can still create a probe for these kinds
    of containers using a TCP probe. It won’t provide the configuration flexibility
    of an HTTP or exec probe, but it will verify that a container in the Pod is listening
    for connections on the specified port.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a PostgreSQL Deployment with a TCP probe:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '*postgres-tcp.yaml*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We saw the requirement for the `POSTGRES_PASSWORD` environment variable in [Chapter
    10](ch10.xhtml#ch10). The only configuration that’s changed for this example is
    the `livenessProbe`. We specify a TCP socket of 5432, as this is the standard
    port for PostgreSQL.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we can create this Deployment and, after a while, observe that it’s
    running:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Again, it is the job of `kubelet` to perform the probe. It does this solely
    by making a TCP connection to the port and then disconnecting. PostgreSQL doesn’t
    emit any logging when this happens, so the only way we know that the probe is
    working is to check that the container continues to run and doesn’t show any restarts:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Before we move on, let’s clean up the Deployment:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We’ve now looked at all three types of probes. And although we used these three
    types to create liveness probes, the same three types will work with both startup
    and readiness probes as well. The only difference is the change in the behavior
    of our cluster when a probe fails.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Startup Probes
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unhealthy containers can create all kinds of difficulties for an application,
    including lack of responsiveness, errors responding to requests, or bad data,
    so we want Kubernetes to respond quickly when a container becomes unhealthy. However,
    when a container is first started, it can take time before it is fully initialized.
    During that time, it might not be able to respond to liveness probes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Because of that delay, we’re left with a need to have a long timeout before
    a container fails a probe, so we can give our container enough time for initialization.
    However, at the same time, we need to have a short timeout in order to detect
    a failed container quickly and restart it. The solution is to configure a separate
    *startup probe*. Kubernetes will use the startup probe configuration until the
    probe is successful; then it will switch over to the liveness probe.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we might configure our NGINX server Deployment as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Given this configuration, Kubernetes would start checking the container 30 seconds
    after startup. It would continue checking every 10 seconds until the probe is
    successful or until there are 60 failed attempts. The effect is that the container
    has 10 minutes to finish initialization and respond to a probe successfully. If
    the container does not have a successful probe in that time, it will be restarted.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: As soon as the container has one successful probe, Kubernetes will switch to
    the configuration for `livenessProbe`. Because we didn’t override any timing parameters,
    this will transition to a probe every 10 seconds, with three consecutive failed
    probes leading to a restart. We give the container 10 minutes to be live initially,
    but after that we will allow no more than 30 seconds before restarting it.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the `startupProbe` is defined completely separately means that
    it is possible to create a different check for startup from the one used for liveness.
    Of course, it’s important to choose wisely so that the container doesn’t pass
    its startup probe before the liveness probe would also pass, because that would
    result in inappropriate restarts.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`startupProbe`被完全独立定义，这意味着可以为启动创建与活跃检查不同的检查。当然，重要的是要明智选择，以确保容器不会在活跃探针通过之前就通过其启动探针，因为那样会导致不适当的重启。'
- en: Readiness Probes
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 就绪探针
- en: The third probe purpose is to check the *readiness* of the Pod. The term *readiness*
    might seem redundant with the startup probe. However, even though completing initialization
    is an important part of readiness for a piece of software, an application component
    might not be ready to do work for many reasons, especially in a highly available
    microservice architecture where components can come and go at any time.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个探针的目的是检查Pod的*就绪性*。*就绪性*这个术语可能与启动探针显得有些重复。然而，尽管完成初始化是软件就绪性的一个重要部分，一个应用组件可能由于多种原因无法准备好执行工作，尤其是在一个高可用的微服务架构中，组件可能随时进出。
- en: Rather than being used for initialization, readiness probes should be used for
    any case in which the container cannot perform any work because of a failure outside
    its control. It may be a temporary situation, as retry logic somewhere else could
    fix the failure. For example, an API that relies on an external database might
    fail its readiness probe if the database is unreachable, but that database might
    return to service at any time.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 就绪探针应当用于任何容器无法执行工作因为超出其控制的故障的情况，而不是用于初始化。这个问题可能是暂时的，因为其他地方的重试逻辑可能会修复这个故障。例如，一个依赖外部数据库的API如果数据库无法访问，可能会失败其就绪探针，但该数据库可能随时恢复服务。
- en: This also creates a valuable contrast with startup and liveness probes. As we
    examined earlier, Kubernetes will restart a container if it fails the configured
    number of startup or liveness probes. But it makes no sense to do that if the
    issue is a failed or missing external dependency, given that restarting the container
    won’t fix whatever is wrong externally.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这也与启动和活跃探针形成了有价值的对比。如前所述，Kubernetes将在容器未通过配置的启动或活跃探针次数时重启容器。但如果问题是外部依赖失败或缺失，重启容器毫无意义，因为重启容器无法解决外部的问题。
- en: At the same time, if a container is missing a required external dependency,
    it can’t do work, so we don’t want to send any work to it. In that situation,
    the best thing to do is to leave the container running and give it an opportunity
    to reestablish the connections it needs, but avoid sending any requests to it.
    In the meantime, we can hope that somewhere in the cluster another Pod for the
    same Deployment is working as expected, making our application as a whole resilient
    to a localized failure.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，如果一个容器缺少所需的外部依赖项，它就无法执行工作，因此我们不希望向它发送任何工作。在这种情况下，最好的做法是保持容器运行，并给予它重新建立所需连接的机会，但避免向它发送任何请求。与此同时，我们可以希望集群中其他地方有一个相同部署的Pod正常工作，使我们的应用整体对局部故障具有弹性。
- en: This is exactly how readiness probes work in Kubernetes. As we saw in [Chapter
    9](ch09.xhtml#ch09), a Kubernetes Service continually watches for Pods that match
    its selector and configures load balancing for its cluster IP that routes traffic
    to those Pods. If a Pod reports itself as not ready, the Service will stop routing
    traffic to it, but `kubelet` will not trigger any other action such as a container
    restart.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是Kubernetes中就绪探针的工作原理。如我们在[第9章](ch09.xhtml#ch09)中所看到的，Kubernetes服务持续监视与其选择器匹配的Pod，并为其集群IP配置负载均衡，将流量路由到这些Pod。如果Pod报告自己未就绪，服务将停止将流量路由到它，但`kubelet`不会触发任何其他操作，如容器重启。
- en: 'Let’s illustrate this situation. We want to have individual control over Pod
    readiness, so we’ll use a somewhat contrived example rather than a real external
    dependency to determine readiness. We’ll deploy a set of NGINX Pods, this time
    with a corresponding Service:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来举一个例子。我们希望对Pod的就绪性进行单独控制，因此我们将使用一个稍微做作的例子，而不是一个真实的外部依赖来决定就绪性。我们将部署一组NGINX
    Pod，并且这次会有一个相应的服务：
- en: '*nginx-ready.yaml*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx-ready.yaml*'
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This Deployment keeps its `livenessProbe` as an indicator that NGINX is working
    correctly and adds a `readinessProbe`. The Service definition is identical to
    what we saw in [Chapter 9](ch09.xhtml#ch09) and will route traffic to our NGINX
    Pods.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此部署将其`livenessProbe`保留为NGINX正常工作的指标，并添加了一个`readinessProbe`。服务定义与[第9章](ch09.xhtml#ch09)中看到的完全相同，并将流量路由到我们的NGINX
    Pod。
- en: 'This file has already been written to */opt*, so we can apply it to the cluster:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件已写入*/opt*，因此我们可以将其应用到集群中：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After these Pods are up and running, they stay running because the liveness
    probe is successful:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些Pod运行后会保持运行状态，因为存活探针成功：
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In addition, the Service we created has been allocated a cluster IP:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们创建的服务已分配了一个集群IP：
- en: '[PRE22]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'However, we aren’t able to use that IP address to reach any Pods:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们无法使用该IP地址访问任何Pod：
- en: '[PRE23]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This is because, at the moment, there is nothing for NGINX to serve on the
    */ready* path, so it’s returning `404`, and the readiness probe is failing. A
    detailed inspection of the Pod shows that it is not ready:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为当前NGINX在*/ready*路径上没有内容可提供，因此返回`404`，就绪探针失败。对Pod的详细检查显示它尚未准备就绪：
- en: '[PRE24]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As a result, the Service does not have any Endpoints to which to route traffic:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，服务没有任何端点可用于路由流量：
- en: '[PRE25]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Because the Service has no Endpoints, it has configured `iptables` to reject
    all traffic:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因为服务没有端点，已配置`iptables`拒绝所有流量：
- en: '[PRE26]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: To fix this, we’ll need at least one Pod to become ready to ensure that NGINX
    has something to serve on the */ready* path. We’ll use the container’s hostname
    to keep track of which Pod is serving our request.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决此问题，我们需要至少一个Pod准备就绪，以确保NGINX有内容可以提供给*/ready*路径。我们将使用容器的主机名来跟踪哪个Pod正在处理我们的请求。
- en: 'To make one of our Pods ready, let’s first get the list of Pods again, just
    to have the Pod names handy:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要使其中一个Pod准备就绪，让我们首先再次获取Pod列表，只是为了方便获取Pod名称：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, we’ll choose one and make it report that it is ready:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将选择一个并使其报告为准备就绪：
- en: '[PRE28]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Our Service will start to show a valid Endpoint:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务将开始显示一个有效的端点：
- en: '[PRE29]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Even better, we can now reach an NGINX instance via the cluster IP, and the
    content corresponds to the hostname:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，我们现在可以通过集群IP访问NGINX实例，内容与主机名对应：
- en: '[PRE30]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note the `/ready` at the end of the URL so the response is the hostname. If
    we run this command many times, we’ll see that the hostname is the same every
    time. This is because the one Pod that is passing its readiness probe is handling
    all of the Service traffic.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意URL末尾的`/ready`，因此响应是主机名。如果多次运行此命令，我们会看到每次主机名都相同。这是因为通过存活探针的唯一Pod正在处理所有服务流量。
- en: 'Let’s make the other two Pods ready as well:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也使其他两个Pod变为准备就绪状态：
- en: '[PRE31]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Our Service now shows all three Endpoints:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务现在展示所有三个端点：
- en: '[PRE32]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Running the `curl` command multiple times shows that the traffic is now being
    distributed across multiple Pods:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 多次运行`curl`命令显示流量现在分布在多个Pod之间：
- en: '[PRE33]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The embedded command `$(seq 1 5)` returns the numbers one through five, causing
    the `for` loop to run `curl` five times. If you run this same `for` loop several
    times, you will see a different distribution of hostnames. As described in [Chapter
    9](ch09.xhtml#ch09), load balancing is based on a random uniform distribution
    wherein each endpoint has an equal chance of being selected for each new connection.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入命令`$(seq 1 5)`返回数字一至五，导致`for`循环运行`curl`五次。如果多次运行相同的`for`循环，你将看到主机名的不同分布。如[第9章](ch09.xhtml#ch09)所述，负载均衡基于随机均匀分布，每个端点被选中作为新连接的概率相等。
- en: A good practice is to offer an HTTP readiness endpoint for each application
    that checks the current state of the application and its dependencies and returns
    an HTTP success code (such as `200`) if the component is healthy, and an HTTP
    error code (such as `500`) if not. Some application frameworks such as Spring
    Boot provide application state management that automatically exposes liveness
    and readiness endpoints.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一个良好的实践是为每个应用程序提供一个HTTP准备就绪端点，检查应用程序及其依赖项的当前状态，并在组件健康时返回HTTP成功代码（如`200`），否则返回HTTP错误代码（如`500`）。某些应用框架（如Spring
    Boot）提供自动公开存活和就绪端点的应用程序状态管理。
- en: Final Thoughts
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结思路
- en: Kubernetes offers the ability to check on our containers and make sure they
    are working as expected, not just that the process is running. These probes can
    include any arbitrary command run inside the container, verifying that a port
    is open for TCP connections, or that the container responds correctly to an HTTP
    request. To build resilient applications, we should define both a liveness probe
    and a readiness probe for each application component. The liveness probe is used
    to restart an unhealthy container; the readiness probe determines whether the
    Pod can handle Service traffic. Additionally, if a component needs extra time
    for initialization, we should also define a startup probe to make sure that give
    it the required initialization time while responding quickly to failure as soon
    as initialization is complete.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了检查我们的容器并确保它们按预期工作的能力，而不仅仅是进程正在运行。这些探针可以包括在容器内运行任意命令，验证端口是否开放以进行
    TCP 连接，或者容器是否正确响应 HTTP 请求。为了构建弹性应用程序，我们应为每个应用程序组件定义一个存活探针和一个就绪探针。存活探针用于重新启动不健康的容器；就绪探针确定
    Pod 是否能处理服务流量。此外，如果组件需要额外的初始化时间，我们还应定义一个启动探针，以确保在初始化完成后能够给予其所需的初始化时间，并在初始化完成后迅速响应失败。
- en: Of course, for our containers to run as expected, other containers in the cluster
    must also be well behaved, not using too many of the cluster’s resources. In the
    next chapter, we’ll look at how we can limit our containers in their use of CPU,
    memory, disk space, and network bandwidth, as well as how we can control the maximum
    amount of total resources available to a user. This ability to specify limits
    and quotas is important to ensure that our cluster can support multiple applications
    with reliable performance.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为了使我们的容器按预期运行，集群中的其他容器也必须表现良好，不能使用过多的集群资源。在下一章中，我们将看看如何限制我们的容器在使用 CPU、内存、磁盘空间和网络带宽方面，以及如何控制用户可用的总资源量。指定限制和配额的能力对于确保我们的集群能够支持多个应用程序并保持可靠的性能至关重要。
