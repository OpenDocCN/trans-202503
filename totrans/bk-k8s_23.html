<html><head></head><body>
<h2 class="h2" id="ch20"><span epub:type="pagebreak" id="page_323"/><span class="big">20</span><br/>APPLICATION RESILIENCY</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">Over the course of this book, we’ve seen how containers and Kubernetes enable scalable, resilient applications. Using containers, we can encapsulate application components so that processes are isolated from one another, have separate virtualized network stacks, and a separate filesystem. Each container can then be rapidly deployed without interfering with other containers. When we add Kubernetes as a container orchestration layer on top of the container runtime, we are able to include many separate hosts into a single cluster, dynamically scheduling containers across available cluster nodes with automatic scaling and failover, distributed networking, traffic routing, storage, and configuration.</p>&#13;
<p class="indent">All of the container and Kubernetes features we’ve seen in this book work together to provide the necessary infrastructure to deploy scalable, resilient applications, but it’s up to us to configure our applications correctly to take advantage of what the infrastructure provides. In this chapter, we’ll take another look at the <code>todo</code> application we deployed in <a href="ch01.xhtml#ch01">Chapter 1</a>. This <span epub:type="pagebreak" id="page_324"/>time, however, we’ll deploy it across multiple nodes in a Kubernetes cluster, eliminating single points of failure and taking advantage of the key features that Kubernetes has to offer. We’ll also explore how to monitor the performance of our Kubernetes cluster and our deployed application so that we can identify performance issues before they lead to downtime for our users.</p>&#13;
<h3 class="h3" id="ch00lev1sec81">Example Application Stack</h3>&#13;
<p class="noindent">In <a href="ch01.xhtml#ch01">Chapter 1</a>, we deployed <code>todo</code> onto a Kubernetes cluster running <code>k3s</code> from Rancher. We already had some amount of scalability and failover available. The web layer was based on a Deployment, so we were able to scale the number of server instances with a single command. Our Kubernetes cluster was monitoring those instances so failed instances could be replaced. However, we still had some single points of failure. We had not yet introduced the idea of a highly available Kubernetes control plane, so we chose to run <code>k3s</code> only in a single-node configuration. Additionally, even though we used a Deployment for our PostgreSQL database, it was lacking in any of the necessary configuration for high availability. In this chapter, we’ll see the details necessary to correct those limitations, and we’ll also take advantage of the many other Kubernetes features we’ve learned.</p>&#13;
<h4 class="h4" id="ch00lev2sec112">Database</h4>&#13;
<p class="noindent">Let’s begin by deploying a highly available PostgreSQL database. <a href="ch17.xhtml#ch17">Chapter 17</a> demonstrated how the Kubernetes Operator design pattern uses CustomResourceDefinitions to extend the behavior of a cluster, making it easy to package and deploy advanced functionality. We’ll use the Postgres Operator we introduced in that chapter to deploy our database.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up. This chapter uses a larger six-node cluster to provide room for the application and all the monitoring components that we’ll be deploying. See the <em>README.md</em> file for this chapter for more information.</em></p>&#13;
</div>&#13;
<p class="indent">The automation for this chapter has already deployed the Postgres Operator together with its configuration. You can inspect the Postgres Operator and its configuration by looking at the files in <em>/etc/kubernetes/components</em>. The operator is running in the <code>todo</code> Namespace, where the <code>todo</code> application is also deployed. Many operators prefer to run in their own Namespace and operate across the cluster, but the Postgres Operator is designed to be deployed directly into the Namespace where the database will reside.</p>&#13;
<p class="indent">Because we’re using the Postgres Operator, we can create a highly available PostgreSQL database by applying a custom resource to the cluster:</p>&#13;
<p class="noindent6"><em>database.yaml</em></p>&#13;
<pre> ---&#13;
 apiVersion: "acid.zalan.do/v1"&#13;
 kind: postgresql&#13;
 <span epub:type="pagebreak" id="page_325"/>metadata:&#13;
<span class="ent">➊</span> name: todo-db&#13;
 spec:&#13;
   teamId: todo&#13;
   volume:&#13;
     size: 1Gi&#13;
     storageClass: longhorn&#13;
<span class="ent">➋</span> numberOfInstances: 3&#13;
   users:&#13;
  <span class="ent">➌</span> todo:&#13;
     - superuser&#13;
     - createdb&#13;
   databases:&#13;
  <span class="ent">➍</span> todo: todo&#13;
   postgresql:&#13;
     version: "14"</pre>&#13;
<p class="indent">All of the files shown in this walkthrough have been staged to the <em>/etc/kubernetes/todo</em> directory so that you can explore them and experiment with changes. The <code>todo</code> application is automatically deployed, but it can take several minutes for all the components to reach a healthy state.</p>&#13;
<p class="indent">The Postgres Operator has the job of creating the Secrets, StatefulSets, Services, and other core Kubernetes resources needed to deploy PostgreSQL. We’re only required to supply the configuration it should use. We start by identifying the name for this database, <code>todo-db</code> <span class="ent">➊</span>, which will be used as the name of the primary Service that we’ll use to connect to the primary database instance, so we’ll see this name again in the application configuration.</p>&#13;
<p class="indent">We want a highly available database, so let’s specify three instances <span class="ent">➋</span>. We also ask the Postgres Operator to create a <code>todo</code> user <span class="ent">➌</span> and to create a <code>todo</code> database with the <code>todo</code> user as the owner <span class="ent">➍</span>. This way, our database is already set up and we only need to populate the tables to store the application data.</p>&#13;
<p class="indent">We can verify that the database is running in the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n todo get sts</span>&#13;
NAME      READY   AGE&#13;
todo-db   3/3     6m1s</pre>&#13;
<p class="indent">The <code>todo-db</code> StatefulSet has three Pods, all of which are ready.</p>&#13;
<p class="indent">Because the Postgres Operator is using a StatefulSet, as we saw in <a href="ch15.xhtml#ch15">Chapter 15</a>, a PersistentVolumeClaim is allocated for the database instances as they are created:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n todo get pvc</span>&#13;
NAME               STATUS   ... CAPACITY   ACCESS MODES   STORAGECLASS   AGE&#13;
pgdata-todo-db-0   Bound    ... 1Gi        RWO            longhorn       10m&#13;
pgdata-todo-db-1   Bound    ... 1Gi        RWO            longhorn       8m44s&#13;
pgdata-todo-db-2   Bound    ... 1Gi        RWO            longhorn       7m23s</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_326"/>These PersistentVolumeClaims will be reused if one of the database instance Pods fails and must be re-created, and the Longhorn storage engine is distributing its storage across our entire cluster, so the database will retain the application data even if we have a node failure.</p>&#13;
<p class="indent">Note that when we requested the Postgres Operator to create a <code>todo</code> user, we didn’t specify a password. For security, the Postgres Operator automatically generates a password. This password is placed into a Secret based on the name of the user and the name of the database. We can see the Secret created for the <code>todo</code> user:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n todo get secret</span>&#13;
NAME                                                    TYPE    DATA   AGE&#13;
...&#13;
todo.todo-db.credentials.postgresql.acid.zalan.do       Opaque  2      8m30s</pre>&#13;
<p class="indent">We’ll need to use this information to configure the application so that it can authenticate to the database.</p>&#13;
<p class="indent">Before we look at the application configuration, let’s inspect the Service that the Postgres Operator created:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n todo get svc todo-db</span>&#13;
NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE&#13;
todo-db   ClusterIP   10.110.227.34   &lt;none&gt;        5432/TCP   59m</pre>&#13;
<p class="indent">This is a <code>ClusterIP</code> Service, meaning that it is reachable from anywhere inside the cluster but is not externally exposed. That matches perfectly with what we want for our application, as our web service component is the only user-facing component and thus the only one that will be exposed outside the cluster.</p>&#13;
<h4 class="h4" id="ch00lev2sec113">Application Deployment</h4>&#13;
<p class="noindent">All of our application’s data is in the PostgreSQL database, so the web server layer is stateless. For this stateless component, we’ll use a Deployment and set up automatic scaling.</p>&#13;
<p class="indent">The Deployment has a lot of information, so let’s look at it step by step. To see the entire Deployment configuration and get a sense of how it all fits together, you can look at the file <em>/etc/kubernetes/todo/application.yaml</em> on any of the cluster nodes.</p>&#13;
<p class="indent">The first section tells Kubernetes that we’re creating a Deployment:</p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: todo&#13;
  labels:&#13;
    app: todo</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_327"/>This part is simple because we’re only specifying the metadata for the Deployment. Note that we don’t include the <code>namespace</code> in the metadata. Instead, we provide it to Kubernetes directly when we apply this Deployment to the cluster. This way, we can reuse the same Deployment YAML for development, test, and production versions of this application, keeping each in a separate Namespace to avoid conflict.</p>&#13;
<p class="indent">The <code>label</code> field is purely informational, though it also provides a way for us to query the cluster for all of the resources associated with this application by matching on the label.</p>&#13;
<p class="indent">The next part of the Deployment YAML specifies how the cluster should handle updates:</p>&#13;
<pre>spec:&#13;
  replicas: 3&#13;
  strategy:&#13;
    type: RollingUpdate&#13;
    rollingUpdate:&#13;
      maxUnavailable: 30%&#13;
      maxSurge: 50%</pre>&#13;
<p class="indent">The <code>replicas</code> field tells Kubernetes how many instances to create initially. The autoscaling configuration will automatically adjust this.</p>&#13;
<p class="indent">The <code>strategy</code> field allows us to configure this Deployment for updates without any application downtime. We can choose either <code>RollingUpdate</code> or <code>Recreate</code> as a strategy. With <code>Recreate</code>, when the Deployment changes, all of the existing Pods are terminated, and then the new Pods are created. With <code>RollingUpdate</code>, new Pods are immediately created, and old Pods are kept running to ensure that this application component can continue functioning while it is updated.</p>&#13;
<p class="indent">We can control how the rolling update operates using the <code>maxUnavailable</code> and <code>maxSurge</code> fields, which we can specify either as integer numbers or as a percentage of the current number of replicas. In this case, we specified 30 percent for <code>maxUnavailable</code>, so the Deployment will throttle the rolling update process to prevent us from falling below 70 percent of the current number of replicas. Additionally, because we set <code>maxSurge</code> at 50 percent, the Deployment will immediately start new Pods until the number of Pods that are running or in the creation process reaches 150 percent of the current number of replicas.</p>&#13;
<p class="indent">The <code>RollingUpdate</code> strategy is the default, and by default, both <code>maxSurge</code> and <code>maxUnavailable</code> are 25 percent. Most Deployments should use the <code>RollingUpdate</code> strategy unless it is absolutely necessary to use <code>Recreate</code>.</p>&#13;
<p class="indent">The next part of the Deployment YAML links the Deployment to its Pods:</p>&#13;
<pre>  selector:&#13;
    matchLabels:&#13;
      app: todo&#13;
  template:&#13;
<span epub:type="pagebreak" id="page_328"/>    metadata:&#13;
      labels:&#13;
        app: todo</pre>&#13;
<p class="indent">The <code>selector</code> and the <code>labels</code> in the Pod <code>metadata</code> must match. As we saw in <a href="ch07.xhtml#ch07">Chapter 7</a>, the Deployment uses the <code>selector</code> to track its Pods.</p>&#13;
<p class="indent">With this part, we’ve now begun defining the <code>template</code> for the Pods this Deployment creates. The rest of the Deployment YAML completes the Pod template, which consists entirely of configuration for the single container this Pod runs:</p>&#13;
<pre>    spec:&#13;
      containers:&#13;
      - name: todo&#13;
        image: bookofkubernetes/todo:stable</pre>&#13;
<p class="indent">The container name is mostly informational, though it is essential for Pods with multiple containers so that we can choose a container when we need to retrieve logs and use <code>exec</code> to run commands. The <code>image</code> tells Kubernetes what container image to retrieve in order to run this container.</p>&#13;
<p class="indent">The next section of the Pod template specifies the environment variables for this container:</p>&#13;
<pre>        env:&#13;
        - name: NODE_ENV&#13;
          value: production&#13;
        - name: PREFIX&#13;
          value: /&#13;
        - name: PGHOST&#13;
          value: todo-db&#13;
        - name: PGDATABASE&#13;
          value: todo&#13;
        - name: PGUSER&#13;
          valueFrom:&#13;
            secretKeyRef:&#13;
              name: todo.todo-db.credentials.postgresql.acid.zalan.do&#13;
              key: username&#13;
              optional: false&#13;
        - name: PGPASSWORD&#13;
          valueFrom:&#13;
            secretKeyRef:&#13;
              name: todo.todo-db.credentials.postgresql.acid.zalan.do&#13;
              key: password&#13;
              optional: false</pre>&#13;
<p class="indent">Some of the environment variables have static values; they’re expected to remain the same for all uses of this Deployment. The <code>PGHOST</code> environment variable matches the name of the PostgreSQL database. The Postgres Operator has created a Service with the name <code>todo-db</code> in the <code>todo</code> Namespace <span epub:type="pagebreak" id="page_329"/>where these Pods will run, so the Pods are able to resolve this hostname to the Service IP address. Traffic destined for the Service IP address is then routed to the primary PostgreSQL instance using the <code>iptables</code> configuration we saw in <a href="ch09.xhtml#ch09">Chapter 9</a>.</p>&#13;
<p class="indent">The final two variables provide the credentials for the application to authenticate to the database. We’re using the ability to fetch configuration from a Secret and provide it as an environment variable to a container, similar to what we saw in <a href="ch16.xhtml#ch16">Chapter 16</a>. However, in this case, we need the environment variable to have a different name from the key name in the Secret, so we use a slightly different syntax that allows us to specify each variable name separately.</p>&#13;
<p class="indent">Finally, we declare the resource requirements of this container and the port it exposes:</p>&#13;
<pre>        resources:&#13;
          requests:&#13;
            memory: "128Mi"&#13;
            cpu: "50m"&#13;
          limits:&#13;
            memory: "128Mi"&#13;
            cpu: "50m"&#13;
        ports:&#13;
        - name: web&#13;
          containerPort: 5000</pre>&#13;
<p class="indent">The <code>ports</code> field in a Pod is purely informational; the actual traffic routing will be configured in the Service.</p>&#13;
<p class="indent">Within the <code>resources</code> field, we set the <code>requests</code> and <code>limits</code> to be the same for this container. As we saw in <a href="ch19.xhtml#ch19">Chapter 19</a>, this means that Pod will be placed in the <code>Guaranteed</code> Quality of Service class. The web service component is stateless and easy to scale, so it makes sense to use a relatively low CPU limit, in this case, 50 millicores, or 5 percent of a core, and rely on the autoscaling to create new instances if the load becomes high.</p>&#13;
<h4 class="h4" id="ch00lev2sec114">Pod Autoscaling</h4>&#13;
<p class="noindent">To automatically scale the Deployment to match the current load, we use a HorizontalPodAutoscaler, as we saw in <a href="ch07.xhtml#ch07">Chapter 7</a>. Here’s the configuration for the autoscaler:</p>&#13;
<p class="noindent6"><em>scaler.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: autoscaling/v2&#13;
kind: HorizontalPodAutoscaler&#13;
metadata:&#13;
  name: todo&#13;
  labels:&#13;
    app: todo&#13;
spec:&#13;
<span epub:type="pagebreak" id="page_330"/>  scaleTargetRef:&#13;
    apiVersion: apps/v1&#13;
    kind: Deployment&#13;
    name: todo&#13;
  minReplicas: 3&#13;
  maxReplicas: 10&#13;
  metrics:&#13;
  - type: Resource&#13;
    resource:&#13;
      name: cpu&#13;
      target:&#13;
        type: Utilization&#13;
        averageUtilization: 50</pre>&#13;
<p class="indent">As we did in our earlier example, we apply a label to this resource purely for informational purposes. Three key configuration items are necessary for this autoscaler. First, the <code>scaleTargetRef</code> specifies that we want to scale the <code>todo</code> Deployment. Because this autoscaler is deployed to the <code>todo</code> Namespace, it finds the correct Deployment to scale.</p>&#13;
<p class="indent">Second, we specify a range for <code>minReplicas</code> and <code>maxReplicas</code>. We choose <code>3</code> as the minimum number of replicas, as we want to make sure the application is resilient even if we have a Pod failure. For simplicity, we didn’t apply the anti-affinity configuration we saw in <a href="ch18.xhtml#ch18">Chapter 18</a>, but this may also be a good practice to avoid having all of the instances on a single node. We choose a maximum number of replicas based on the size of our cluster; for a production application, we would measure our application load and choose based on the highest load we expect to handle.</p>&#13;
<p class="indent">Third, we need to specify the metric that the autoscaler will use to decide how many replicas are needed. We base this autoscaler on CPU utilization. If the average utilization across the Pods is greater than 50 percent of the Pod’s <code>requests</code>, the Deployment will be scaled up. We set the <code>requests</code> at 50 millicores, so this means that an average utilization greater than 25 millicores will cause the autoscaler to increase the number of replicas.</p>&#13;
<p class="indent">To retrieve the average CPU utilization, the autoscaler relies on a cluster infrastructure component that retrieves metrics data from the <code>kubelet</code> service running on each node and exposes that metrics data via an API. For this chapter, we have some extra cluster monitoring functionality to demonstrate, so the automation has skipped the regular metrics server component we described in <a href="ch06.xhtml#ch06">Chapter 6</a>. We’ll deploy an alternative later in this chapter.</p>&#13;
<h4 class="h4" id="ch00lev2sec115">Application Service</h4>&#13;
<p class="noindent">The final cluster resource for our application is the Service. <a href="ch20.xhtml#ch20list1">Listing 20-1</a> presents the definition we’re using for this chapter.</p>&#13;
<p class="noindent6"><em>service.yaml</em></p>&#13;
<pre>---&#13;
kind: Service&#13;
apiVersion: v1&#13;
<span epub:type="pagebreak" id="page_331"/>metadata:&#13;
  name: todo&#13;
  labels:&#13;
    app: todo&#13;
spec:&#13;
  type: NodePort&#13;
  selector:&#13;
    app: todo&#13;
  ports:&#13;
  - name: web&#13;
    protocol: TCP&#13;
    port: 5000&#13;
    nodePort: 5000</pre>&#13;
<p class="caption" id="ch20list1"><em>Listing 20-1: Todo Service</em></p>&#13;
<p class="indent">We use the same <code>selector</code> that we saw in the Deployment to find the Pods that will receive traffic sent to this Service. As we saw in <a href="ch09.xhtml#ch09">Chapter 9</a>, the <code>ports</code> field of a Service is essential because <code>iptables</code> traffic routing rules are configured only for the ports we identify. In this case, we declare the <code>port</code> to be 5000 and don’t declare a <code>targetPort</code>, so this Service will send to port 5000 on the Pods, which matches the port on which our web server is listening. We also configure a <code>name</code> on this port, which will be important later when we configure monitoring.</p>&#13;
<p class="indent">For this chapter, we’re exposing our application Service using <code>NodePort</code>, which means that all of our cluster’s nodes will be configured to route traffic to the Service that is sent to the <code>nodePort</code> for any host interface. Thus, we can access port 5000 on any of our cluster’s nodes and we’ll be routed to our application:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">curl -v http://host01:5000/</span>&#13;
...&#13;
&lt; HTTP/1.1 200 OK&#13;
&lt; X-Powered-By: Express&#13;
...&#13;
&lt;html lang="en" data-framework="backbonejs"&gt;&#13;
    &lt;head&gt;&#13;
        &lt;meta charset="utf-8"&gt;&#13;
        &lt;title&gt;Todo-Backend client&lt;/title&gt;&#13;
        &lt;link rel="stylesheet" href="css/vendor/todomvc-common.css"&gt;&#13;
        &lt;link rel="stylesheet" href="css/chooser.css"&gt;&#13;
    &lt;/head&gt;&#13;
...&#13;
&lt;/html&gt;</pre>&#13;
<p class="indent">This Service traffic routing works on any host interface, so the <code>todo</code> application can be accessed from outside the cluster as well. The URL is different depending on whether you’re using the Vagrant or Amazon Web <span epub:type="pagebreak" id="page_332"/>Services configuration, so the automation for this chapter includes a message with the URL to use.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><strong>NODEPORT, NOT INGRESS</strong></p>&#13;
<p class="noindents">When we deployed <code>todo</code> in <a href="ch01.xhtml#ch01">Chapter 1</a>, we exposed the Service using an Ingress. The Ingress, as we saw in <a href="ch09.xhtml#ch09">Chapter 9</a>, consolidates multiple Services such that they can all be exposed outside the cluster without requiring each Service to have a separate externally routable IP address. We’ll expose a monitoring service later in this chapter, so we have multiple Services to expose outside the cluster. However, because we’re working with an example cluster on a private network, we don’t have the underlying network infrastructure available to use an Ingress to its full potential. By using a <code>NodePort</code> instead, we’re able to expose multiple Services outside the cluster in a way that works well with both the Vagrant and Amazon Web Services configurations.</p>&#13;
</div>&#13;
<p class="indent">We’ve now looked all of the components in the <code>todo</code> application, using what we’ve learned in this book to eliminate single points of failure and maximize scalability.</p>&#13;
<p class="indent">You can also explore the source code for the <code>todo</code> application at <em><a href="https://github.com/book-of-kubernetes/todo">https://github.com/book-of-kubernetes/todo</a></em>, including the <em>Dockerfile</em> that’s used to build the application’s container image and the GitHub Actions that automatically build it and publish it to Docker Hub whenever the code changes.</p>&#13;
<p class="indent">However, although our Kubernetes cluster will now do its best to keep this application running and performing well, we can do more to monitor both the <code>todo</code> application and the Kubernetes cluster.</p>&#13;
<h3 class="h3" id="ch00lev1sec82">Application and Cluster Monitoring</h3>&#13;
<p class="noindent">Proper application and cluster monitoring is essential for applications, for multiple reasons. First, our Kubernetes cluster will try to keep the applications running, but any hardware or cluster failures could leave an application in a non-working or degraded state. Without monitoring, we would be dependent on our users to tell us when the application is down or behaving badly, which is poor user experience. Second, if we do see failures or performance issues with our application, we’re going to need data to diagnose them or to try to identify a pattern in order to find a root cause. It’s a lot easier to build in monitoring ahead of time than to try to apply it after we’re already seeing problems. Finally, we may have problems with our cluster or application that occurs below the level at which users notice, but that indicates potential performance or stability issues. Integrating proper monitoring allows us to detect those kinds of issues before they become a bigger headache. It also allows us to measure an application over time to make sure that added features aren’t degrading its performance.</p>&#13;
<p class="indent">Fortunately, although we do need to think about monitoring at the level of each of our application components, we don’t need to build a monitoring framework ourselves. Many mature monitoring tools are already designed to work in a Kubernetes cluster, so we can get up and running quickly. In <span epub:type="pagebreak" id="page_333"/>this chapter, we’ll look at <code>kube-prometheus</code>, a complete stack of tools that we can deploy to our cluster and use to monitor both the cluster and the <code>todo</code> application.</p>&#13;
<h4 class="h4" id="ch00lev2sec116">Prometheus Monitoring</h4>&#13;
<p class="noindent">The core component of <code>kube-prometheus</code> is, as the name implies, the open source Prometheus monitoring software. Prometheus deploys as a server that periodically queries various metrics sources and accumulates the data it receives. It supports a query language that is optimized for “time series” data, which makes it easy to collect individual data points showing a system’s performance at a moment in time. It then aggregates those data points to get a picture of the system’s load, resource utilization, and responsiveness.</p>&#13;
<p class="indent">For each component that exposes metrics, Prometheus expects to reach out to a URL and receive data in return in a standard format. It’s common to use the path <em>/metrics</em> to expose metrics to Prometheus. Following this convention, the Kubernetes control plane components already expose metrics in the format that Prometheus is expecting.</p>&#13;
<p class="indent">To illustrate, we can use <code>curl</code> to visit the <em>/metrics</em> path on the API server to see the metrics that it provides. To do this, we’ll need to authenticate to the API server, so let’s use a script that collects a client certificate for authentication:</p>&#13;
<p class="noindent6"><em>api-metrics.sh</em></p>&#13;
<pre>#!/bin/bash&#13;
conf=/etc/kubernetes/admin.conf&#13;
...&#13;
curl --cacert $ca --cert $cert --key $key https://192.168.61.10:6443/metrics&#13;
...</pre>&#13;
<p class="indent">Running this script returns a wealth of API server metrics:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">/opt/api-server-metrics.sh</span>&#13;
...&#13;
# TYPE rest_client_requests_total counter&#13;
rest_client_requests_total{code="200",host="[::1]:6443",method="GET"} 9051&#13;
rest_client_requests_total{code="200",host="[::1]:6443",method="PATCH"} 25&#13;
rest_client_requests_total{code="200",host="[::1]:6443",method="PUT"} 21&#13;
rest_client_requests_total{code="201",host="[::1]:6443",method="POST"} 179&#13;
rest_client_requests_total{code="404",host="[::1]:6443",method="GET"} 155&#13;
rest_client_requests_total{code="404",host="[::1]:6443",method="PUT"} 1&#13;
rest_client_requests_total{code="409",host="[::1]:6443",method="POST"} 5&#13;
rest_client_requests_total{code="409",host="[::1]:6443",method="PUT"} 62&#13;
rest_client_requests_total{code="500",host="[::1]:6443",method="GET"} 18&#13;
rest_client_requests_total{code="500",host="[::1]:6443",method="PUT"} 1&#13;
...</pre>&#13;
<p class="indent">This example illustrates only a few of the hundreds of metrics that are collected and exposed. Each line of this response provides one data point to Prometheus. We can include additional parameters for the metric in curly <span epub:type="pagebreak" id="page_334"/>braces, allowing for more complex queries. For example, the API server data in the preceding example can be used to determine not only the total number of client requests served by the API server but also the raw number and percentage of requests that resulted in an error. Most systems are resilient to a few HTTP error responses, but a sudden increase in error responses is often a good indication of a more serious issue, so this is valuable in configuring a reporting threshold.</p>&#13;
<p class="indent">In addition to all of the data that the Kubernetes cluster is already providing to Prometheus, we can also configure our application to expose metrics. Our application is based on Node.js, so we do this using the <code>prom-client</code> library. As demonstrated in <a href="ch20.xhtml#ch20list2">Listing 20-2</a>, our <code>todo</code> application is exposing metrics at <em>/metrics</em>, like the API server.</p>&#13;
<pre>root@host01:~# <span class="codestrong1">curl http://host01:5000/metrics/</span>&#13;
# HELP api_success Successful responses&#13;
# TYPE api_success counter&#13;
api_success{app="todo"} 0&#13;
&#13;
# HELP api_failure Failed responses&#13;
# TYPE api_failure counter&#13;
api_failure{app="todo"} 0&#13;
...&#13;
# HELP process_cpu_seconds_total Total user and system CPU time ...&#13;
# TYPE process_cpu_seconds_total counter&#13;
process_cpu_seconds_total{app="todo"} 0.106392&#13;
...</pre>&#13;
<p class="caption" id="ch20list2"><em>Listing 20-2: Todo metrics</em></p>&#13;
<p class="indent">The response includes some default metrics that are relevant to all applications. It also includes some counters that are specific to the <code>todo</code> application and track API usage and responses over time.</p>&#13;
<h4 class="h4" id="ch00lev2sec117">Deploying kube-prometheus</h4>&#13;
<p class="noindent">At this point, our Kubernetes cluster and our application are ready to provide these metrics on demand, but we don’t yet have a Prometheus server running in the cluster to collect them. To fix this, we’ll deploy the complete <code>kube-prometheus</code> stack. This includes not only a Prometheus Operator that makes it easy to deploy and configure Prometheus but also other useful tools, such as Alertmanager, which can trigger notifications in response to cluster and application alerts, and Grafana, a dashboard tool that we’ll use to see the metrics we’re collecting.</p>&#13;
<p class="indent">To deploy <code>kube-prometheus</code>, we’ll use a script that’s been installed in <em>/opt</em>. This script downloads a current <code>kube-prometheus</code> release from GitHub and applies the manifests.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_335"/>Run the script as follows:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">/opt/install-kube-prometheus.sh</span>&#13;
...</pre>&#13;
<p class="indent">These manifests also include a Prometheus Adapter. The Prometheus Adapter implements the same Kubernetes metrics API as the <code>metrics-server</code> we deployed to the clusters throughout <a href="part02.xhtml#part02">Part II</a>, so it exposes CPU and memory data obtained from <code>kubelet</code>, enabling our HorizontalPodAutoscaler to track CPU utilization of our <code>todo</code> application. However, it also exposes that utilization data to Prometheus so that we can observe it in Grafana dashboards. For this reason, we use the Prometheus Adapter in this chapter in place of the regular <code>metrics-server</code>.</p>&#13;
<p class="indent">We can see the Prometheus Adapter and the other components by listing Pods in the <code>monitoring</code> Namespace:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n monitoring get pods</span>&#13;
NAME                                   READY   STATUS    RESTARTS   AGE&#13;
alertmanager-main-0                    2/2     Running   0          14m&#13;
alertmanager-main-1                    2/2     Running   0          14m&#13;
alertmanager-main-2                    2/2     Running   0          14m&#13;
blackbox-exporter-6b79c4588b-pgp5r     3/3     Running   0          15m&#13;
grafana-7fd69887fb-swjpl               1/1     Running   0          15m&#13;
kube-state-metrics-55f67795cd-mkxqv    3/3     Running   0          15m&#13;
node-exporter-4bhhp                    2/2     Running   0          15m&#13;
node-exporter-8mc5l                    2/2     Running   0          15m&#13;
node-exporter-ncfd2                    2/2     Running   0          15m&#13;
node-exporter-qp7mg                    2/2     Running   0          15m&#13;
node-exporter-rtn2t                    2/2     Running   0          15m&#13;
node-exporter-tpg97                    2/2     Running   0          15m&#13;
prometheus-adapter-85664b6b74-mglp4    1/1     Running   0          15m&#13;
prometheus-adapter-85664b6b74-nj7hp    1/1     Running   0          15m&#13;
prometheus-k8s-0                       2/2     Running   0          14m&#13;
prometheus-k8s-1                       2/2     Running   0          14m&#13;
prometheus-operator-6dc9f66cb7-jtrqd   2/2     Running   0          15m</pre>&#13;
<p class="indent">In addition to the Prometheus Adapter, we see Pods for Alertmanager, Grafana, and various <code>exporter</code> Pods, which collect metrics from the cluster infrastructure and expose it to Prometheus. We also see Pods for Prometheus itself and for the Prometheus Operator. The Prometheus Operator automatically updates Prometheus whenever we change the custom resources that the Prometheus Operator is monitoring. The most important of those custom resources is the Prometheus resource shown in <a href="ch20.xhtml#ch20list3">Listing 20-3</a>.</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n monitoring describe prometheus</span>&#13;
Name:         k8s&#13;
Namespace:    monitoring&#13;
...&#13;
API Version:  monitoring.coreos.com/v1&#13;
<span epub:type="pagebreak" id="page_336"/>Kind:         Prometheus&#13;
...&#13;
Spec:&#13;
...&#13;
  Image:  quay.io/prometheus/prometheus:v2.32.1&#13;
...&#13;
  Service Account Name:  prometheus-k8s&#13;
  Service Monitor Namespace Selector:&#13;
  Service Monitor Selector:&#13;
...</pre>&#13;
<p class="caption" id="ch20list3"><em>Listing 20-3: Prometheus configuration</em></p>&#13;
<p class="indent">The Prometheus custom resource allows us to configure which Namespaces will be watched for Services to monitor. The default configuration presented in <a href="ch20.xhtml#ch20list3">Listing 20-3</a> does not specify a value for the Service Monitor Namespace Selector or the Service Monitor Selector. For this reason, by default the Prometheus Operator will be looking for monitoring configuration in all Namespaces, with any metadata label.</p>&#13;
<p class="indent">To identify specific Services to monitor, the Prometheus Operator keeps an eye out for another custom resource, <em>ServiceMonitor</em>, as demonstrated in <a href="ch20.xhtml#ch20list4">Listing 20-4</a>.</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n monitoring get servicemonitor</span>&#13;
NAME                      AGE&#13;
alertmanager-main         20m&#13;
blackbox-exporter         20m&#13;
coredns                   20m&#13;
grafana                   20m&#13;
kube-apiserver            20m&#13;
kube-controller-manager   20m&#13;
kube-scheduler            20m&#13;
kube-state-metrics        20m&#13;
kubelet                   20m&#13;
node-exporter             20m&#13;
prometheus-adapter        20m&#13;
prometheus-k8s            20m&#13;
prometheus-operator       20m</pre>&#13;
<p class="caption" id="ch20list4"><em>Listing 20-4: Default ServiceMonitors</em></p>&#13;
<p class="indent">When we installed <code>kube-prometheus</code>, it configured multiple ServiceMonitor resources. As a result, our Prometheus instance is already watching the Kubernetes control plane components and the <code>kubelet</code> services running on our cluster nodes. Let’s see the targets from which Prometheus is scraping metrics and see how those metrics are used to populate dashboards in Grafana.</p>&#13;
<h4 class="h4" id="ch00lev2sec118"><span epub:type="pagebreak" id="page_337"/>Cluster Metrics</h4>&#13;
<p class="noindent">The installation script patched the Grafana and Prometheus Services in the <code>monitoring</code> Namespace to expose them as <code>NodePort</code> Services. The automation scripts print the URL you can use to access Prometheus. The initial page looks like <a href="ch20.xhtml#ch20fig1">Figure 20-1</a>.</p>&#13;
<div class="image" id="ch20fig1"><img alt="Image" src="../images/f0337-01.jpg"/></div>&#13;
<p class="figcap"><em>Figure 20-1: Prometheus initial page</em></p>&#13;
<p class="indent">Click the <strong>Targets</strong> item underneath the <strong>Status</strong> menu on the top menu bar to see which components in the cluster Prometheus is currently scraping. Click <strong>Collapse All</strong> to get a consolidated list, as shown in <a href="ch20.xhtml#ch20fig2">Figure 20-2</a>.</p>&#13;
<div class="image" id="ch20fig2"><img alt="Image" src="../images/f0337-02.jpg"/></div>&#13;
<p class="figcap"><em>Figure 20-2: Prometheus targets</em></p>&#13;
<p class="indent">This list matches the list of ServiceMonitors we saw in <a href="ch20.xhtml#ch20list4">Listing 20-4</a>, showing us that Prometheus is scraping Services as configured by the Prometheus Operator.</p>&#13;
<p class="indent">We can use the Prometheus web interface to query data directly, but Grafana has already been configured with some useful dashboards, so we <span epub:type="pagebreak" id="page_338"/>can more easily see the data there. The automation scripts print the URL you can use to access Grafana. Log in using the default <code>admin</code> as the username and <code>admin</code> as the password. You will be prompted to change the password; you can just click <em>Skip</em>. At this point you should see the Grafana initial page, as shown in <a href="ch20.xhtml#ch20fig3">Figure 20-3</a>.</p>&#13;
<div class="image" id="ch20fig3"><img alt="Image" src="../images/f0338-01.jpg"/></div>&#13;
<p class="figcap"><em>Figure 20-3: Grafana initial page</em></p>&#13;
<p class="indent">From this page, choose the <strong>Browse</strong> item under <strong>Dashboards</strong> in the menu. There are many dashboards in the <em>Default</em> folder. For example, by selecting <strong>Default</strong> and then selecting <strong>Kubernetes</strong> ▸ <strong>Compute Resources</strong> ▸ <strong>Pod</strong>, you can see a dashboard, depicted in <a href="ch20.xhtml#ch20fig4">Figure 20-4</a>, that shows CPU and memory usage over time for any Pod in the cluster.</p>&#13;
<div class="image" id="ch20fig4"><img alt="Image" src="../images/f0338-02.jpg"/></div>&#13;
<p class="figcap"><em>Figure 20-4: Pod compute resources</em></p>&#13;
<p class="indent">All of the <code>todo</code> database and application Pods are selectable in this dashboard by first selecting the <code>todo</code> Namespace, so we can already get valuable information about our application by using nothing more than the default <span epub:type="pagebreak" id="page_339"/>monitoring configuration. This is possible because the Prometheus Adapter is pulling data from the <code>kubelet</code> services, which includes resource utilization for each of the running Pods. The Prometheus Adapter is then exposing a <em>/metrics</em> endpoint for Prometheus to scrape and store, and Grafana is querying Prometheus to build the chart showing usage over time.</p>&#13;
<p class="indent">There are numerous other Grafana dashboards to explore in the default installation of <code>kube-prometheus</code>. Choose the <em>Browse</em> menu item again to select other dashboards and see what data is available.</p>&#13;
<h4 class="h4" id="ch00lev2sec119">Adding Monitoring for Services</h4>&#13;
<p class="noindent">Although we are already getting useful metrics for our <code>todo</code> application, Prometheus is not yet scraping our application Pods to pull in the Node.js metrics we saw in <a href="ch20.xhtml#ch20list2">Listing 20-2</a>. To configure Prometheus to scrape our <code>todo</code> metrics, we’ll need to provide a new ServiceMonitor resource to the Prometheus Operator, informing it about our <code>todo</code> Service.</p>&#13;
<p class="indent">In a production cluster, the team deploying an application like our <code>todo</code> application wouldn’t have the permissions to create or update resources in the <code>monitoring</code> Namespace. However, the Prometheus Operator looks for ServiceMonitor resources in all Namespaces by default, so we can create a ServiceMonitor in the <code>todo</code> Namespace instead.</p>&#13;
<p class="indent">First, though, we need to give Prometheus permission to see the Pods and Services we’ve created in the <code>todo</code> Namespace. As this access control configuration needs to apply only in a single Namespace, we’ll do this by creating a Role and a RoleBinding. Here’s the Role configuration we’ll use:</p>&#13;
<p class="noindent6"><em>rbac.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: rbac.authorization.k8s.io/v1&#13;
kind: Role&#13;
metadata:&#13;
...&#13;
  name: prometheus-k8s&#13;
rules:&#13;
  - apiGroups:&#13;
    - ""&#13;
    resources:&#13;
    - services&#13;
    - endpoints&#13;
    - pods&#13;
    verbs:&#13;
    - get&#13;
    - list&#13;
    - watch&#13;
...</pre>&#13;
<p class="indent">We need to make sure we allow access to Services, Pods, and Endpoints, so we confirm that these are listed in the <code>resources</code> field. The Endpoint resource records the current Pods that are receiving traffic for a Service, which <span epub:type="pagebreak" id="page_340"/>will be critical for Prometheus to identify all of the Pods it scrapes. Because Prometheus needs only read-only access, we specify only the <code>get</code>, <code>list</code>, and <code>watch</code> verbs.</p>&#13;
<p class="indent">After we have this Role, we need to bind it to the ServiceAccount that Prometheus is using. We do that with this RoleBinding:</p>&#13;
<p class="noindent6"><em>rbac.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: rbac.authorization.k8s.io/v1&#13;
kind: RoleBinding&#13;
metadata:&#13;
...&#13;
  name: prometheus-k8s&#13;
roleRef:&#13;
  apiGroup: rbac.authorization.k8s.io&#13;
  kind: Role&#13;
  name: prometheus-k8s&#13;
subjects:&#13;
  - kind: ServiceAccount&#13;
    name: prometheus-k8s&#13;
    namespace: monitoring</pre>&#13;
<p class="indent">The <code>roleRef</code> matches the Role we just declared in the preceding example, whereas the <code>subjects</code> field lists the ServiceAccount Prometheus is using, based on the information we saw in <a href="ch20.xhtml#ch20list3">Listing 20-3</a>.</p>&#13;
<p class="indent">Both of these YAML resources are in the same file, so we can apply them both to the cluster at once. We need to make sure we apply them to the <code>todo</code> Namespace, as that’s the Namespace where we want to enable access by Prometheus:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n todo apply -f /opt/rbac.yaml</span>&#13;
role.rbac.authorization.k8s.io/prometheus-k8s created&#13;
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created</pre>&#13;
<p class="indent">Now that we’ve granted permission to Prometheus to see our Pods and Services, we can create the ServiceMonitor. Here’s that definition:</p>&#13;
<p class="noindent6"><em>svc-mon.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: monitoring.coreos.com/v1&#13;
kind: ServiceMonitor&#13;
metadata:&#13;
  name: todo&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: todo&#13;
  endpoints:&#13;
    - port: web</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_341"/>A ServiceMonitor uses a selector, similar to a Service or a Deployment. We previously applied the <code>app: todo</code> label to the Service, so the <code>matchLabels</code> field will cause Prometheus to pick up the Service. The <code>endpoints</code> field matches the name of the port we declared in the Service in <a href="ch20.xhtml#ch20list1">Listing 20-1</a>. Prometheus requires us to name the port in order to match it.</p>&#13;
<p class="indent">Let’s apply this ServiceMonitor to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n todo apply -f /opt/svc-mon.yaml</span>&#13;
servicemonitor.monitoring.coreos.com/todo created</pre>&#13;
<p class="indent">As before, we need to make sure we deploy this to the <code>todo</code> Namespace because Prometheus will be configured to look for Services with the appropriate label in the same Namespace as the ServiceMonitor.</p>&#13;
<p class="indent">Because the Prometheus Operator is watching for new ServiceMonitor resources, using the API we saw in <a href="ch17.xhtml#ch17">Chapter 17</a>, it picks up this new resource and immediately reconfigures Prometheus to start scraping the Service. Prometheus then takes a few minutes to register the new targets and start scraping them. If we go back to the Prometheus Targets page after this is complete, the new Service shows up, as illustrated in <a href="ch20.xhtml#ch20fig5">Figure 20-5</a>.</p>&#13;
<div class="image" id="ch20fig5"><img alt="Image" src="../images/f0341-01.jpg"/></div>&#13;
<p class="figcap"><em>Figure 20-5: Prometheus monitoring todo</em></p>&#13;
<p class="indent">If we click the <strong>show more</strong> button next to the <code>todo</code> Service, we see its three Endpoints, shown in <a href="ch20.xhtml#ch20fig6">Figure 20-6</a>.</p>&#13;
<div class="image" id="ch20fig6"><span epub:type="pagebreak" id="page_342"/><img alt="Image" src="../images/f0342-01.jpg"/></div>&#13;
<p class="figcap"><em>Figure 20-6: Todo Endpoints</em></p>&#13;
<p class="indent">It may be surprising that we created a ServiceMonitor, specifying the <code>todo</code> Service as the target, and yet Prometheus is scraping Pods. However, it’s essential that Prometheus works this way. Because Prometheus is using a regular HTTP request to scrape metrics, and because Service traffic routing chooses a random Pod for every new connection, Prometheus would get metrics from a random Pod each time it did scraping. By reaching behind the Service to identify the Endpoints, Prometheus is able to scrape metrics from all the Service’s Pods, enabling aggregation of metrics for the entire application.</p>&#13;
<p class="indent">We’ve successfully incorporated the Node.js and custom metrics for the <code>todo</code> application into Prometheus, in addition to the default resource utilization metrics already collected. Before we finish our look at application monitoring, let’s run a Prometheus query to demonstrate that the data is being pulled in. First, you should interact with the <code>todo</code> application using the URL printed out by the automation scripts. This will ensure that there are metrics to display and that enough time has passed for Prometheus to scrape that data. Next, open the Prometheus web interface again, or click <strong>Prometheus</strong> at the top of any Prometheus web page to go back to the main page. Then, type <strong>api_success</strong> into the query box and press ENTER. Custom <code>todo</code> metrics should appear, as illustrated in <a href="ch20.xhtml#ch20fig7">Figure 20-7</a>.</p>&#13;
<div class="image" id="ch20fig7"><span epub:type="pagebreak" id="page_343"/><img alt="Image" src="../images/f0343-01.jpg"/></div>&#13;
<p class="figcap"><em>Figure 20-7: Todo metric query</em></p>&#13;
<p class="indent">We’re now able to monitor both the Kubernetes cluster and the <code>todo</code> application.</p>&#13;
<h3 class="h3" id="ch00lev1sec83">Final Thoughts</h3>&#13;
<p class="noindent">In this chapter, we’ve explored how the various features of containers and Kubernetes come together to enable us to deploy a scalable, resilient application. We’ve used everything we learned about containers—Deployments, Services, networking, persistent storage, Kubernetes Operators, and role-based access control—to not only deploy the <code>todo</code> application but also configure Prometheus monitoring of our cluster and our application.</p>&#13;
<p class="indent">Kubernetes is a complex platform with many different capabilities, and new capabilities are being added all the time. The purpose of this book is not only to show you the most important features you need to run an application on Kubernetes, but also to give you the tools to explore a Kubernetes cluster for troubleshooting and performance monitoring. As a result, you should be equipped to explore new features as they are added to Kubernetes and to conquer the challenges of deploying complex applications and getting them to perform well.<span epub:type="pagebreak" id="page_344"/></p>&#13;
</body></html>