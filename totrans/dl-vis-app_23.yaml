- en: '19'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '19'
- en: Recurrent Neural Networks
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: In most of this book we’ve considered every sample as an isolated entity, unrelated
    to any other samples. This makes sense for things like photographs. If we’re classifying
    an image and decide that we’re looking at a cat, it doesn’t matter if the image
    before or after this one is a dog, a squirrel, or an airplane. The images are
    independent of each other. But if an image is a frame of a movie, then it can
    be helpful to look at it in the context of the other images around it. For example,
    we can track objects that might be temporarily obscured.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大多数内容中，我们将每个样本视为一个孤立的实体，与任何其他样本无关。这对于像照片这样的东西是有道理的。如果我们在对一张图像进行分类并决定它是一只猫，那之前或之后的图像是狗、松鼠还是飞机都无关紧要。图像彼此独立。但如果图像是电影中的一帧，那么将其放在其他图像的上下文中进行观察就变得有帮助。例如，我们可以追踪那些可能暂时被遮挡的物体。
- en: When we work with multiple samples whose order matters, we call that a *sequence*.
    The flow of words in any human language are an important type of sequence and
    will be our focus in this chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理多个顺序重要的样本时，我们称之为*序列*。任何人类语言中单词的流动都是一种重要的序列类型，并且将是本章的重点。
- en: 'Algorithms that understand and process sequences have another bonus: they are
    frequently capable of *generating*, or creating, new sequences. Trained systems
    can generate stories (Deutsch 2016a) or TV scripts (Deutsch 2016b), Irish jigs
    (Sturm 2015b), polyphonic melodies (LISA Lab 2018), and complex songs (Johnson
    2015; O’Brien and Román 2017). We can create lyrics (Krishan 2016) for pop music
    (Chu, Urtasun, and Fidler 2016), folk music (Sturm 2015a), rap (Barrat 2018),
    or country (Moocarme 2020). We can turn speech into text (Geitgey 2016; Graves,
    Mohamed, and Hinton 2013) and write captions for images and video (Karpathy and
    Li 2013; Mao et al. 2015).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 能够理解和处理序列的算法还有一个额外的好处：它们通常能够*生成*，或创造，新的序列。经过训练的系统可以生成故事（Deutsch 2016a）或电视剧本（Deutsch
    2016b），爱尔兰民间舞曲（Sturm 2015b），复调旋律（LISA Lab 2018），以及复杂的歌曲（Johnson 2015；O’Brien和Román
    2017）。我们可以为流行音乐（Krishan 2016）、民谣音乐（Sturm 2015a）、说唱（Barrat 2018）或乡村音乐（Moocarme
    2020）创作歌词。我们可以将语音转换为文本（Geitgey 2016；Graves、Mohamed和Hinton 2013），并为图像和视频写字幕（Karpathy和Li
    2013；Mao等，2015）。
- en: In this chapter, we look at a method for handling sequences based on remembering
    something about each element as it comes by. The models we build are called recurrent
    neural networks (RNNs).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一种基于记住每个元素特征的序列处理方法。我们所构建的模型被称为递归神经网络（RNN）。
- en: When we work with sequences, each element of the input is called a *token*.
    A token represents a word, or a fragment of a word, a measurement, or anything
    else we can represent numerically. In this chapter, we use language as our most
    frequent source of data, and we focus on whole words, so we use *word* and *token*
    interchangeably.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理序列时，每个输入元素称为*token*。一个token代表一个单词、一个单词的片段、一项测量，或任何其他可以用数字表示的东西。在本章中，我们使用语言作为最常见的数据来源，并专注于整个单词，因此我们将*word*和*token*互换使用。
- en: Working with Language
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言的应用
- en: The general field that studies natural language is called *natural language
    understanding*, or *NLU*. Most of today’s algorithms are unconcerned with any
    kind of actual understanding of the language they process. Instead, they extract
    statistics from the data and use those statistics as the basis for tasks like
    answering questions or generating text. These techniques are generally called
    *natural language processing*, or *NLP*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 研究自然语言的整体领域称为*自然语言理解*，或*NLU*。今天的大多数算法并不关心它们所处理的语言的实际理解。相反，它们从数据中提取统计信息，并以这些统计信息为基础进行诸如回答问题或生成文本等任务。这些技术通常被称为*自然语言处理*，或*NLP*。
- en: We saw in Chapters 16 and 17 that convolutional neural networks, or CNNs, can
    recognize objects in photos without having any actual understanding of the photo.
    They just process the statistics of the pixels. In the same way, NLP systems don’t
    understand the language they manipulate. Instead, they assign numbers to words
    and find useful statistical relationships between those numbers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在第16章和第17章中，我们看到卷积神经网络（CNN）可以识别照片中的物体，而无需真正理解照片的内容。它们只处理像素的统计数据。同样，NLP系统并不理解它们所处理的语言。相反，它们为单词分配数字，并找到这些数字之间有用的统计关系。
- en: In a fundamental sense, these systems have no knowledge that there is even a
    thing such as language, or that the objects they manipulate have semantic meanings.
    As always, the system is using statistics to generate outputs that we declare
    to be acceptable in a given situation, without even a glimmer of comprehension
    of what it’s doing or what the outputs might mean to a person.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上说，这些系统并不知道语言的存在，或它们操作的对象具有语义含义。像往常一样，系统使用统计学来生成我们在特定情境下认为可接受的输出，甚至没有一点理解它在做什么或这些输出对人类意味着什么。
- en: Common Natural Language Processing Tasks
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常见的自然语言处理任务
- en: 'The applications of natural language algorithms are commonly called *tasks*.
    Here are some popular tasks:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言算法的应用通常被称为*任务*。以下是一些流行的任务：
- en: '**Sentiment Analysis:** Given opinionated text like a movie review, determine
    whether the overall sense is positive or negative.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**情感分析：** 给定带有观点的文本，如电影评论，判断整体情感是积极的还是消极的。'
- en: '**Translation:** Turn text into another language.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**翻译：** 将文本转换成另一种语言。'
- en: '**Answer Questions:** Answer questions about the text, like who is the hero,
    or what actions occurred.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**回答问题：** 回答关于文本的问题，比如谁是英雄，或者发生了什么行动。'
- en: '**Summarize or Paraphrase:** Provide a short overview of the text, emphasizing
    the main points.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**总结或改写：** 提供文本的简短概述，强调主要观点。'
- en: '**Generate New Text:** Given some starting text, write more text that seems
    to follow from it.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成新文本：** 给定一些起始文本，编写更多似乎与其相关的文本。'
- en: '**Logical Flow:** If a sentence first asserts a premise and the following sentence
    asserts a conclusion based on that premise, determine whether the conclusion logically
    follows from the premise.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**逻辑流：** 如果一个句子首先提出一个前提，接下来的句子根据该前提提出一个结论，判断结论是否从前提中合乎逻辑地得出。'
- en: 'In this chapter and the next, we focus mainly on two tasks: translation and
    text generation. The other tasks have much in common with these (Rajpurkar, Jia,
    and Liang 2018; Roberts, Raffel, and Shazeer, 2020). In particular, logical flow
    is extra difficult and benefits from human-computer partnerships (Full Fact 2020).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章及下一章中，我们主要关注两个任务：翻译和文本生成。其他任务与这两者有许多相似之处（Rajpurkar, Jia, and Liang 2018;
    Roberts, Raffel, and Shazeer, 2020）。特别是，逻辑流的处理非常困难，需要借助人机合作（Full Fact 2020）。
- en: Translation requires, at a minimum, the text we want to translate, and the source
    and target languages. We might also want to know some context to help us understand
    idioms and other language features that change from one place to another or over
    time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译至少需要我们提供要翻译的文本，以及源语言和目标语言。我们可能还希望了解一些上下文信息，以帮助我们理解习语和其他随时间或地点变化的语言特征。
- en: Text generation typically starts with a *seed* or *prompt*. The algorithm takes
    that as the start of the text and then builds from there. Typically, it does this
    one word at a time. Given a prompt, it predicts the next word. That word is added
    to the end of the prompt, and the system uses that new, longer prompt to predict
    the next word after it. We can repeat this process endlessly to produce a sentence,
    essay, or book. We call this technique *autoregression* because we’re predicting,
    or regressing, the next word in the sequence by automatically appending previous
    outputs together and using them as the input. Autoregressive systems are called
    *autoregressors*. More generally, creating text algorithmically is called *natural
    language generation*, or *NLG*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成通常从一个*种子*或*提示*开始。算法将其作为文本的起点，并从那里构建。通常，它一次生成一个词。给定一个提示，它会预测下一个词。该词被添加到提示的末尾，系统使用新的、更长的提示来预测下一个词。我们可以无限重复这个过程来生成一个句子、文章或书籍。我们称这种技术为*自回归*，因为我们通过自动将之前的输出连接在一起并将其作为输入来预测或回归序列中的下一个词。自回归系统被称为*自回归器*。更一般地说，按算法创建文本被称为*自然语言生成*，或*NLG*。
- en: Both translation and text generation make use of a concept called a *language
    model*. This is any kind of computation that takes a sequence of words as an input
    and tells us how likely it is that the sequence is a well-formed sentence. Note
    that it doesn’t tell us if it’s a particularly well-written sentence, or even
    if it’s meaningful or true. It’s often convenient to refer to a trained neural
    network as itself being a language model (Jurafsky 2020).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译和文本生成都使用了一个叫做*语言模型*的概念。这是一种计算方法，它将一系列单词作为输入，并告诉我们该序列形成一个完整句子的可能性有多大。请注意，它并不会告诉我们该句子是否写得特别好，甚至是否有意义或是否真实。通常，我们会将训练好的神经网络本身称为语言模型（Jurafsky
    2020）。
- en: Transforming Text into Numbers
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将文本转化为数字
- en: To build systems that can help us with translation and text generation, we have
    to first transform our text into a form that’s useful to the computer. As usual,
    we’ll turn everything into numbers. There are two popular ways to do this.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建可以帮助我们进行翻译和文本生成的系统，我们必须首先将文本转化为计算机可用的形式。像往常一样，我们将一切转化为数字。这里有两种常见的方法来做到这一点。
- en: The first is *character based*, where we number all the symbols that can appear
    in our text. The most extensive tabulation of written characters in human language
    is called Unicode. The most recent version, Unicode 13.0.0, encompasses 154 written
    human languages and identifies 143,859 distinct characters (Unicode Consortium
    2020). We can assign every symbol from any of these writing systems a unique number
    from 0 to about 144,000\. In this chapter, we keep things simple and show a few
    examples of text generation using the 89 characters most common in English text.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是*基于字符的*，我们为文本中可能出现的所有符号编号。人类语言中最广泛的书写字符表叫做Unicode。最新版本Unicode 13.0.0包含154种人类书写语言，识别了143,859个不同的字符（Unicode
    Consortium 2020）。我们可以为这些书写系统中的每个符号分配一个从0到大约144,000的唯一编号。在本章中，我们保持简洁，展示一些使用89个最常见的英语字符的文本生成示例。
- en: The second approach is *word based*, where we number all the words that can
    appear in our text. Counting all the words in all the languages of the world would
    be a daunting task. In this book, we stick to English, but even there, we have
    no definitive count of the number of words. Most modern English dictionaries have
    about 300,000 entries (Dictionary.com 2020). Imagine working through the dictionary
    and assigning each entry a unique number starting at 0\. These words and their
    corresponding numbers would then make up our *vocabulary*. Most of the examples
    in this chapter take a word-based approach.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是*基于词语的*，我们为文本中可能出现的所有单词编号。统计世界上所有语言中的所有单词将是一项艰巨的任务。在本书中，我们坚持使用英语，但即便如此，我们也没有明确统计过单词的数量。大多数现代英语词典大约有300,000个词条（Dictionary.com
    2020）。试想，逐字翻阅词典并为每个条目分配一个从0开始的唯一编号。这些单词及其对应的数字便构成了我们的*词汇表*。本章中的大多数示例都采用基于词语的方法。
- en: Now we can create a computer-friendly, numerical representation of any sentence.
    We can generate more text by handing this list of numbers to a trained autoregressive
    network. The network predicts the number of the next word, that word gets appended
    to the words used as its input, the network then predicts the next word, which
    again gets appended to the words used as its input, and so on. For us to see what
    text this corresponds to, we can turn each number back into its corresponding
    word. For many of our discussions in the following pages, we take these transformations
    into numbers as a given and illustrate our inputs and outputs as words, not numbers.
    We’ll see later that while a single number is workable, there’s a much richer
    way to represent words that includes their context and how they’re used in a sentence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以为任何句子创建一个计算机友好的数字表示。我们可以通过将这组数字传递给一个经过训练的自回归网络来生成更多的文本。网络预测下一个词的数字，然后将这个词附加到作为输入的词汇上，网络接着预测下一个词，这个词再次附加到输入的词汇中，依此类推。为了让我们看到与之对应的文本，我们可以将每个数字转回其对应的单词。在接下来的讨论中，我们将这些数字转换过程视为理所当然，并将输入和输出表示为单词，而不是数字。稍后我们将看到，虽然单个数字是可行的，但有一种更丰富的方式来表示单词，包含它们的上下文以及它们在句子中的使用方式。
- en: Fine-Tuning and Downstream Networks
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调与下游网络
- en: It’s often useful to train a system on a generic database and then specialize
    it. For example, we might enhance a general-purpose image classifier into one
    that can recognize leaf shapes and tell us what kind of tree they came from. The
    process is called *transfer learning*. When used with a classifier, it often involves
    freezing the existing network, adding a few new layers at the end of the classification
    section, and training those. That way, the new layers can make use of all the
    information that the existing network has learned to extract from each image.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个系统时，通常会先在一个通用数据库上进行训练，然后再进行专业化。例如，我们可以将一个通用的图像分类器增强为一个能够识别叶子形状并告诉我们它们来自哪种树的分类器。这个过程被称为*迁移学习*。在分类器中使用时，通常涉及冻结现有的网络，在分类部分的末尾添加一些新的层，并训练这些新层。这样，新层就可以利用现有网络从每个图像中提取到的所有信息。
- en: In NLP, we say that a system that has learned from a general database is *pretrained*.
    Then when we want to learn a new type of specialized language, like the language
    used in law, poetry, or engineering, we *fine-tune* the network with the new data.
    Unlike transfer learning, we typically modify all the weights in the system when
    we fine-tune.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，我们说一个从通用数据库中学习的系统是*预训练的*。然后，当我们想学习一种新的专业语言时，比如法律、诗歌或工程领域的语言，我们会用新的数据对网络进行*微调*。与迁移学习不同，当我们进行微调时，通常会修改系统中的所有权重。
- en: If we don’t want to retrain the system, we can create a second model to take
    the language system’s output and turn it into something more useful to us, which
    is close in spirit to transfer learning. Here the language model is frozen, and
    its output is fed to a new model. We call this second model a *downstream network*,
    which carries out a *downstream task*. Some language models are designed to create
    rich, dense summaries of their input text so they can be used to drive a wide
    variety of downstream tasks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不想重新训练系统，我们可以创建第二个模型，接受语言系统的输出并将其转换为对我们更有用的内容，这与迁移学习的精神相近。这里语言模型被冻结，其输出被传递给新模型。我们将这个第二个模型称为*下游网络*，它执行*下游任务*。一些语言模型旨在创建其输入文本的丰富、密集的摘要，从而可以用于推动各种下游任务。
- en: These two approaches of fine-tuning and downstream training are useful conceptual
    distinctions, but in practice, many systems blend together some of both techniques.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 微调和下游训练这两种方法是有用的概念区分，但在实践中，许多系统将两种技术结合使用。
- en: Fully Connected Prediction
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完全连接预测
- en: 'As we’ve discussed, we’re going to treat language as sequences of numbers.
    To get a feeling for working with such sequences in general, let’s set aside language
    for a moment and focus just on the numbers. We’ll build a tiny network that learns
    to take in a few numbers from a sequence, and produce the next number. We’ll do
    it in perhaps the simplest possible way, with just two layers: a fully connected
    layer of a mere five neurons, followed by a fully connected layer with a single
    neuron, as in [Figure 19-1](#figure19-1). We’ll use a leaky ReLU activation function
    with slope 0.1 on the first layer and no activation function on the output layer.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所讨论的，我们将语言视为一系列数字。为了更好地理解如何处理这种序列，让我们暂时放下语言，专注于数字。我们将构建一个微型网络，该网络学习从序列中获取一些数字，并生成下一个数字。我们将以最简单的方式来做：一个完全连接的层，只有五个神经元，接着是一个完全连接的层，只有一个神经元，如[图
    19-1](#figure19-1)所示。我们将在第一层使用一个斜率为 0.1 的泄漏 ReLU 激活函数，在输出层不使用激活函数。
- en: '![F19001](Images/F19001.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![F19001](Images/F19001.png)'
- en: 'Figure 19-1: A tiny network for sequence prediction'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19-1：用于序列预测的微型网络
- en: Testing Our Network
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试我们的网络
- en: To try out this tiny network, let’s use a synthetic dataset created by adding
    a bunch of sine waves together. The first 500 samples are shown in [Figure 19-2](#figure19-2).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尝试这个微型网络，让我们使用一个合成数据集，该数据集是通过将一堆正弦波相加生成的。前 500 个样本如[图 19-2](#figure19-2)所示。
- en: '![F19002](Images/F19002.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![F19002](Images/F19002.png)'
- en: 'Figure 19-2: Synthetic training data'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19-2：合成训练数据
- en: To train our system, we’ll take the first five values from our dataset and ask
    our little network to produce the sixth value. Then we’ll take values 2 through
    6 of the dataset and ask it to predict the seventh value. We say that we’re using
    a *sliding window* to choose each set of inputs, as in [Figure 19-3](#figure19-3).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的系统，我们将从数据集中取出前五个值，并让我们的微型网络生成第六个值。然后，我们将取出数据集中的第 2 到第 6 个值，并让它预测第七个值。我们说我们使用*滑动窗口*来选择每一组输入，如[图
    19-3](#figure19-3)所示。
- en: '![F19003](Images/F19003.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![F19003](Images/F19003.png)'
- en: 'Figure 19-3: Using a sliding window to create training samples, shown in blue,
    from 5-element sequences of the training data. The value we want to predict for
    each sample is in red.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19-3：使用滑动窗口从训练数据的 5 元素序列中创建训练样本，蓝色表示训练样本。我们希望为每个样本预测的值用红色表示。
- en: From our starting 500 values, we can make 495 samples in this way. We trained
    our little network on these samples for 50 epochs. When we run the training data
    through again and ask for predictions, we get the results on the left of [Figure
    19-4](#figure19-4), showing the original training data in blue, and the predictions
    in orange. Not bad!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们开始的 500 个值中，我们可以以这种方式生成 495 个样本。我们用这些样本训练了我们的微型网络 50 个时期。当我们再次运行训练数据并请求预测时，我们得到[图
    19-4](#figure19-4)左侧的结果，图中显示了原始训练数据（蓝色）和预测值（橙色）。还不错！
- en: '![F19004](Images/F19004.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![F19004](Images/F19004.png)'
- en: 'Figure 19-4: Left: Training data and predictions. Right: Test data and predictions.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19-4：左侧：训练数据和预测结果。右侧：测试数据和预测结果。
- en: Let’s now run this on 250 points of test data from later in the curves. The
    data and predictions are shown on the right of [Figure 19-4](#figure19-4). The
    predictions aren’t perfect, but they are pretty great, considering how small our
    network is.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在后续曲线的 250 个测试数据点上运行这个模型。数据和预测结果显示在[图 19-4](#figure19-4)的右侧。虽然预测结果并不完美，但考虑到我们的网络非常小，预测已经相当不错。
- en: This was easy data, though, since it was so smooth. Let’s try a more realistic
    dataset composed of the average number of sunspots recorded monthly from 1749
    to 2018 (Kaggle 2020). [Figure 19-5](#figure19-5) shows the inputs and outputs
    using the same arrangement as in [Figure 19-4](#figure19-4). The peaks and valleys
    correspond to the roughly 11-year solar cycle. Though it doesn’t quite reach the
    extremes of the data, our tiny regressor seems to follow the general ups and downs
    of the data quite well.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些数据比较简单，因为它们变化平缓。让我们尝试一个更现实的数据集，数据来源于 1749 到 2018 年间每月记录的太阳黑子平均数量（Kaggle
    2020）。[图 19-5](#figure19-5)显示了使用与[图 19-4](#figure19-4)相同排列方式的输入和输出。数据中的波峰波谷对应于大约
    11 年的太阳周期。虽然它并没有完全达到数据的极端值，但我们的小型回归模型似乎能很好地跟踪数据的整体波动。
- en: '![F19005](Images/F19005.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![F19005](Images/F19005.png)'
- en: 'Figure 19-5: Left: Training sunspot data and predictions. Right: Test data
    and predictions.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19-5：左侧：训练太阳黑子数据和预测结果。右侧：测试数据和预测结果。
- en: Unfortunately, this little network is not going to be able to generate enjoyable
    novels. To see why, let’s change our data to numbered words. For our text, we’ll
    use the first six chapters of Charles Dickens’ novel *A Tale of Two Cities* (Dickens
    1859). To make processing easier, we stripped out all the punctuation and turned
    everything into lowercase.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个小型网络无法生成令人愉快的小说。为了理解原因，让我们将数据改为编号的单词。我们将使用查尔斯·狄更斯小说《双城记》（Dickens 1859）的前六章作为文本。为了便于处理，我们去除了所有标点符号并将所有内容转换为小写字母。
- en: Since we’re going to work at word level, we need to assign a number to every
    word we’ll use. Numbering an entire dictionary would be overkill, and we’d miss
    all the people and place names in the text. Instead, let’s build our vocabulary
    from the book itself. Let’s assign the value 0 to the first word in the book and
    then work our way forward one word at a time. Each time we see a word we haven’t
    seen before, we assign it the next available number. This opening chunk of the
    novel contains 17,267 words total but has a vocabulary of only 3,458 unique words,
    so our words have values from 0 to 3,457\.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将按单词级别工作，我们需要为每个将要使用的单词分配一个编号。为整个词典编号显得过于繁琐，并且会错过文本中的所有人名和地名。相反，让我们从书本本身建立词汇表。我们将为书中的第一个单词分配编号
    0，然后逐个单词向前推进。每次遇到一个之前未见过的单词时，我们就为它分配下一个可用的编号。这部分小说包含 17,267 个单词，但只有 3,458 个独特的单词，所以我们的单词编号从
    0 到 3,457。
- en: Now that every word in this part of the novel has a number, we split the database
    into training and test sets. At the end of the training data, we have only seen
    about 3,000 unique words. So that we don’t ask the network to predict word numbers
    it hasn’t been trained with, we removed all sequences in the test set where any
    word numbers (or the target) are above this value. That is, the test data consists
    of sequences that only use words that are present in the training data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于小说中的每个单词都有了编号，我们将数据库分为训练集和测试集。在训练数据的末尾，我们只看到了大约 3,000 个独特的单词。为了避免网络预测它没有训练过的单词编号，我们移除了测试集中任何包含超过此值的单词编号（或目标）的所有序列。也就是说，测试数据只包含使用训练数据中已出现的单词的序列。
- en: We repeated the previous experiment and fed windows of five consecutive word
    numbers to the little network of [Figure 19-1](#figure19-1), collecting from the
    output its prediction of the next word. We told it to train for 50 epochs, but
    the error quickly stopped improving and early stopping brought training to a close
    after 8 epochs, giving us the results in [Figure 19-6](#figure19-6). As we can
    see in the training data on the left, the word numbers gradually increase as we
    get further into the book. The orange lines are the word numbers predicted by
    the system in response to each set of five inputs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复了前面的实验，向[图19-1](#figure19-1)中的小网络输入了五个连续的单词编号窗口，从输出中收集了它对下一个单词的预测。我们让它训练了50个epoch，但误差很快停止改进，并且早期停止在8个epoch后结束了训练，给出了[图19-6](#figure19-6)中的结果。正如我们在左侧的训练数据中看到的，随着我们深入到书中，单词编号逐渐增加。橙色线条是系统根据每组五个输入预测的单词编号。
- en: '![F19006](Images/F19006.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![F19006](Images/F19006.png)'
- en: 'Figure 19-6: Left: Training and predictions for the first roughly 12,000 words
    from the first six chapters of *A**Tale**of**Two**Cities.*Right: Test data and
    predictions for roughly 2,000 more words.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图19-6：左：来自《**双城记**》前六章大约12,000个单词的训练和预测。右：大约2,000个更多单词的测试数据和预测。
- en: That’s not good at all. The predictions definitely aren’t matching either the
    training or test data. The structure of the test data and predictions is easier
    to see in the close-up shown in [Figure 19-7](#figure19-7).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 结果一点也不好。预测结果显然与训练数据或测试数据都不匹配。从[图19-7](#figure19-7)中的特写可以更容易地看到测试数据和预测的结构。
- en: '![F19007](Images/F19007.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![F19007](Images/F19007.png)'
- en: 'Figure 19-7: Close-up of 500 pieces of test data and predictions from [Figure
    19-6](#figure19-6)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图19-7：来自[图19-6](#figure19-6)的500个测试数据和预测结果的特写
- en: The predictions appear to vaguely follow the targets, but they’re way off.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果似乎模糊地跟随目标，但它们完全偏离了。
- en: Why Our Network Failed
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么我们的网络失败了
- en: 'Let’s turn the numbers of [Figure 19-7](#figure19-7) back into words. Here’s
    a typical extract:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把[图19-7](#figure19-7)中的数字转回成单词。以下是一个典型的摘录：
- en: pricked hollows mud crosses argument ripples loud want joints upon harness followed
    side three intensely atop fired wrote pretence
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: pricked hollows mud crosses argument ripples loud want joints upon harness followed
    side three intensely atop fired wrote pretence
- en: That’s not great literature, even if we put some punctuation back in. A number
    of things went wrong here. First, this one little network clearly doesn’t have
    anywhere near enough power for this job. We’d need many more neurons, maybe on
    many layers, to get anywhere near readable text.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这不算是伟大的文学作品，即使我们加上了标点符号。这里有不少问题。首先，这个小网络显然没有足够的能力来完成这项任务。我们可能需要更多的神经元，也许需要更多的层，才能接近可读的文本。
- en: Even much larger fully connected networks will struggle with this task, though,
    because they have no way of capturing the structure of the text, also called its
    *semantics*. The structure of language is fundamentally different than that of
    the curves and sunspot data we saw before. Consider the five-word string Just
    yesterday, I saw a. This fragment can be completed by any noun. By one estimate,
    the number of nouns in English runs to at least tens of thousands (McCrae 2018).
    How could any network possibly guess the one we want? One answer is to make the
    window bigger, so the network has more preceding words and may be able to make
    a more informed choice. For example, given the input, I’ve been spending my time
    watching tigers very closely. Just yesterday, I saw a, most English nouns can
    now be reasonably ruled out as unlikely.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是更大的全连接网络也会在这项任务上遇到困难，因为它们无法捕捉文本的结构，也就是所谓的*语义*。语言的结构与我们之前看到的曲线和太阳黑子数据有本质的不同。考虑这个五个单词的字符串：Just
    yesterday, I saw a。这个片段可以通过任何名词来完成。根据一项估计，英语中的名词数量至少有成千上万（McCrae 2018）。任何网络怎么可能猜出我们想要的那个名词呢？一个答案是将窗口增大，这样网络就有了更多的前置单词，可能能够做出更有根据的选择。例如，给定输入：I’ve
    been spending my time watching tigers very closely. Just yesterday, I saw a，大多数英语名词现在可以合理地排除为不太可能。
- en: Let’s try this out. We enlarged our little network in [Figure 19-1](#figure19-1)
    to have 20 neurons on the first layer. We gave it 20 elements at a time and asked
    it to predict the 21st. The results for the curve data are shown in [Figure 19-8](#figure19-8).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来试试这个。我们将[图19-1](#figure19-1)中的小网络扩大，第一层有20个神经元。我们一次给它20个元素，并要求它预测第21个。曲线数据的结果显示在[图19-8](#figure19-8)中。
- en: Though the training data is still pretty okay, the test results are much worse.
    To handle all the information coming from this bigger window, we need a far bigger
    network. Making the window bigger means we need a bigger network, which means
    it needs more training data, more memory, more compute power, more electricity,
    and more training time.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管训练数据仍然相当不错，但测试结果却差得多。为了处理来自这个更大窗口的所有信息，我们需要一个更大的网络。增大窗口意味着我们需要一个更大的网络，这也意味着它需要更多的训练数据、更大的内存、更强的计算能力、更高的电力消耗和更多的训练时间。
- en: '![F19008](Images/F19008.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![F19008](Images/F19008.png)'
- en: 'Figure 19-8: An enlarged network predicting sine wave data using a window of
    20 elements'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19-8：一个放大的网络，使用 20 个元素的窗口预测正弦波数据
- en: 'But there’s an even bigger problem that won’t improve just by using a bigger
    network. The issue is that even a tiny error in the prediction leads to incomprehensible
    text. To see this, let’s arbitrarily look at the words that were assigned values
    1,003 and 1,004\. These numbers correspond to the words keep and flint. The words
    seem entirely unrelated, but searching the text turns up this passage near the
    start of the book: he had only to shut himself up inside, keep the flint and steel
    sparks well off the straw. The word the has already appeared as the third word
    of the book, so since neither keep nor flint had appeared earlier, when we numbered
    the book’s words, keep and flint were assigned successive numbers.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 但还有一个更大的问题，仅仅通过使用更大的网络是无法改善的。问题在于，即使是预测中的一个微小错误，也会导致无法理解的文本。为了看清这一点，我们可以随便查看赋值为
    1,003 和 1,004 的单词。这些数字分别对应单词 keep 和 flint。这两个单词似乎完全无关，但在搜索文本时，我们会找到书的开头附近有这样一段话：he
    had only to shut himself up inside, keep the flint and steel sparks well off the
    straw。单词 the 已经出现在书中的第三个位置，所以由于 keep 和 flint 之前都没有出现，当我们对书中的单词进行编号时，keep 和 flint
    被赋予了相邻的编号。
- en: Suppose that in response to some input, our network predicts the next word to
    be 1,003.49\. We need to turn this into an integer to look up the corresponding
    word. The nearest integer is 1,003, giving us keep. But if the system predicts
    the slightly larger value 1,003.51, the nearest integer is 1,004, giving us flint.
    These two words are entirely unrelated. This demonstrates that even a tiny numerical
    difference in the prediction can create nonsensical output.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的网络在响应某些输入时，预测下一个单词为 1,003.49。我们需要将其转换为整数，以便查找相应的单词。最接近的整数是 1,003，给我们的是
    keep。但如果系统预测略大的值 1,003.51，最接近的整数是 1,004，给我们的是 flint。这两个词完全不相关。这表明，即使是预测中的微小数值差异，也会产生荒谬的输出。
- en: Looking back on our predictions in the graphs for this network, we can see lots
    of errors that didn’t seem too terrible for the curve and sunspot data, but would
    wreak havoc on language data. Throwing more compute power at this problem will
    reduce it, but our need for pinpoint accuracy won’t go away.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们在网络图中的预测，我们可以看到很多错误，这些错误在曲线和太阳黑子数据中似乎不那么严重，但对于语言数据来说会造成严重的混乱。增加计算能力可以减少这些错误，但我们对精确度的需求不会消失。
- en: 'Our little network of [Figure 19-1](#figure19-1) is hiding another flaw: it
    doesn’t track the locations of the words in its input. Suppose we are given the
    sentence, Bob told John that he was hungry, and we want to know who the pronoun
    he refers to. The answer is Bob. But word order matters, because if we instead
    were given the sentence, John told Bob that he was hungry, then he would refer
    to John. The need for accuracy would encourage us to extend the network with more
    fully connected layers, and we’d lose the implicit ordering of the words when
    they arrived at the first layer. Later layers wouldn’t have any chance at working
    out which word corresponds to he.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图 19-1](#figure19-1)中的小型网络隐藏了另一个缺陷：它无法跟踪输入中单词的位置。假设我们给出句子“Bob 告诉 John 他饿了”，并且我们想知道代词
    he 指的是谁。答案是 Bob。但单词顺序很重要，因为如果我们改为给出句子“John 告诉 Bob 他饿了”，那么 he 就会指代 John。对准确性的需求会促使我们通过更多的全连接层来扩展网络，但当单词进入第一层时，我们就会失去它们的隐含顺序。后续的层将无法判断哪个单词对应
    he。
- en: To address these issues, and many others, we want something more sophisticated
    than fully connected layers and words represented by single numbers. We might
    try using a CNN, and there has been some work on using CNNs to handle sequence
    data (Chen and Wu 2017; van den Oord et al. 2016), but those tools are still developing.
    Instead, let’s look at something explicitly designed to handle sequences.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题以及许多其他问题，我们需要比全连接层和由单一数字表示的单词更复杂的东西。我们可能会尝试使用CNN，且已经有一些关于使用CNN处理序列数据的研究（Chen
    and Wu 2017；van den Oord 等，2016），但这些工具仍在发展中。相反，让我们看看一些专门设计来处理序列的东西。
- en: Recurrent Neural Networks
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: A better way to handle language is to build a network that is explicitly designed
    to manage words as an ordered sequence. One such type of network, and the focus
    of this chapter, is the recurrent neural network, or RNN. Such networks build
    on a few concepts we haven’t looked at before, so let’s consider them now and
    then use them to build an RNN.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 处理语言的更好方法是构建一个明确设计来管理单词顺序的网络。这样的一种网络，本章的重点，是循环神经网络（RNN）。这种网络建立在我们之前没有探讨过的一些概念之上，因此让我们现在考虑它们，然后用它们来构建一个RNN。
- en: Introducing State
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入状态
- en: 'RNNs make use of an idea called *state*. This is just a description of a system
    (such as a neural network) at any given time. For example, imagine preheating
    an oven. In this process, the oven takes on three unique states: off; preheating;
    and at the desired temperature. The state can also contain additional information.
    For example, as the oven warms up, we can pack three pieces of information into
    the oven’s state: its current status (such as preheating); the temperature it’s
    currently at; and the temperature it’s aiming for. So, a state can represent the
    current condition of a system, plus any other information it’s convenient to remember.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RNN利用了一个叫做*状态*的概念。这只是描述系统（如神经网络）在任何给定时刻的状态。例如，想象一下预热烤箱。在这个过程中，烤箱会经历三种独特的状态：关闭；预热中；以及达到所需温度。状态也可以包含其他信息。例如，随着烤箱加热，我们可以将三条信息打包到烤箱的状态中：它当前的状态（例如，正在预热）；它目前的温度；以及它的目标温度。因此，状态可以表示系统的当前状态，以及任何其他便于记忆的信息。
- en: Because state is so important, let’s see some of its subtleties with another
    example.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于状态非常重要，让我们通过另一个例子来看看它的一些微妙之处。
- en: Suppose that you’re working at an ice cream shop and you’re learning how to
    make a simple fudge sundae. In this story, you play the role of the system, and
    the recipe you’re building up in your head is your state.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在一家冰淇淋店工作，正在学习如何制作一个简单的巧克力酱圣代。在这个故事中，你扮演的是系统的角色，而你脑海中逐步建立的配方就是你的状态。
- en: Before getting any instructions, your *starting state* or *initial state* would
    be “An empty cup.” So, let’s say you have an empty cup. Your starting state is
    shown at the far left of [Figure 19-9](#figure19-9).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在得到任何指示之前，你的*起始状态*或*初始状态*将是“一个空杯子”。所以，假设你有一个空杯子。你的起始状态如[图19-9](#figure19-9)所示，在最左侧。
- en: '![F19009](Images/F19009.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![F19009](Images/F19009.png)'
- en: 'Figure 19-9: Your evolving state, or recipe, as you learn to make a dessert'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图19-9：你在学习做甜点时，逐步变化的状态或配方
- en: Your manager says that the first step is to put in some vanilla ice cream. So,
    you update your internal recipe, or state, to “An empty cup with three scoops
    of vanilla ice cream.” You put three scoops of ice cream into the cup.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你的经理说第一步是放一些香草冰淇淋。所以，你更新了内部配方或状态为“一个空杯子，里面有三勺香草冰淇淋。”你将三勺冰淇淋放入杯中。
- en: Your manager says that’s too much, and you should remove one scoop. You do so,
    and you update your state to “An empty cup with two scoops of vanilla ice cream.”
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你的经理说这样太多了，你应该去掉一勺冰淇淋。你照做了，并将状态更新为“一个空杯子，里面有两勺香草冰淇淋。”
- en: Now your manager says to pour on enough chocolate syrup to cover the ice cream.
    You do this, and update your state to “an empty cup with two scoops of vanilla
    ice cream covered in chocolate syrup.” But this reminds you of your friend Marty,
    because this is his favorite dessert. So, you simplify your state by throwing
    out what you had, now remembering only “Marty’s favorite.”
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的经理说要倒入足够的巧克力糖浆，覆盖住冰淇淋。你这么做，并将状态更新为“一个空杯子，里面有两勺香草冰淇淋，覆盖着巧克力糖浆。”但这让你想起了你的朋友Marty，因为这是他最喜欢的甜点。所以，你通过抛弃之前的状态来简化你的状态，现在只记得“马蒂最喜欢的”。
- en: Finally, your manager says you should place a cherry on the top. So, you update
    your state to “Marty’s favorite with a cherry on top.” Congratulations, your sundae
    is complete!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: There are a few key things to take away from this story and the concept of state.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: First, your state is not simply a snapshot of the current situation or a list
    of the information you were given. It captures both of those ideas, perhaps in
    a compressed or modified form. For example, instead of remembering to put in three
    scoops of ice cream and then removing one, you remembered instead to put in two
    scoops.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Second, after receiving new information at each step, you updated your state
    and produced an output. The output depends on the input you received and your
    internal state, but an outside observer can’t see your state, and so they might
    not understand how your output resulted from the input you just received. In fact,
    outside observers usually don’t get to see a system’s internal state. We emphasize
    this by sometimes referring to a system’s state as its *hidden state*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the order of the inputs matters. This is the essential aspect of this
    example that makes it about a sequence, rather than just a bunch of inputs, and
    thus distinguishes it from our simple, fully connected layer at the start of the
    chapter. If you’d put the chocolate in the cup first, you’d have made quite a
    different dessert, and you probably wouldn’t have created a reference to your
    friend Marty in your state.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: We call each input a *time step*. This makes sense when the inputs represent
    events in time, as they were here. Other sequences might not have a time component,
    like a sequence describing the depth of a river at successive points along its
    length from its source to its terminus. In particular, words in a sentence have
    a time component when they’re spoken aloud, but that idea doesn’t really apply
    when they’re printed. Nevertheless, the term *time step* is widely used to refer
    to each successive element of a sequence.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Rolling Up Our Diagram
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we had a long sequence of inputs to process, a drawing like [Figure 19-9](#figure19-9)
    can consume a lot of space on the page. So, we usually draw something like this
    in a more compact form, as in [Figure 19-10](#figure19-10). We’ve put hyphens
    between the words here to suggest that each little phrase is to be understood
    as a single chunk of information.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![F19010](Images/F19010.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19-10: The rolled-up version of [Figure 19-9](#figure19-9)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: The loop on the right represents the state between one input and the next. After
    each input, the system (represented by the big, light blue box) creates a new
    state, which goes into the black square. This square is called the *delay*, and
    we can think of it as a little piece of memory. When the next input arrives, the
    system pulls the state out of the delay and computes an output and a new state.
    That new state again emerges from the system and sits in the delay until the next
    input arrives. The purpose of the delay is to make it clear that the state produced
    during each time step is not immediately used again in some way, but is held until
    it’s needed to process the next input.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的循环表示了一个输入与下一个输入之间的状态。每次输入后，系统（由大而浅蓝色的框表示）会创建一个新的状态，该状态进入黑色方框。这个方框称为*延迟*，我们可以将其视为一小块记忆。当下一个输入到达时，系统会从延迟中取出状态并计算输出和新状态。新的状态再次从系统中产生，并停留在延迟中，直到下一个输入到达。延迟的目的是为了清楚地表明，在每个时间步中产生的状态不会立即以某种方式被再次使用，而是被保持直到需要处理下一个输入。
- en: We say that the diagram in [Figure 19-9](#figure19-9) is the *unrolled* version
    of the process. The more compact version in [Figure 19-10](#figure19-10) is called
    the *rolled-up* or *rolled* version.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说[图19-9](#figure19-9)中的图示是过程的*展开*版本。[图19-10](#figure19-10)中的更紧凑的版本称为*卷起*或*收缩*版本。
- en: In deep learning, we implement the process of managing state and presenting
    output by packaging everything up into a recurrent cell, as shown in [Figure 19-11](#figure19-11).
    The word *recurrent* refers to the fact that we use the state memory over and
    over, even though its contents usually change from one input to the next (note
    that this is not the word *recursive*, which sounds similar but means something
    quite different). The workings of the cell are usually managed by multiple neural
    networks. As usual, these networks learn how to do their job when we train the
    complete network, which contains [Figure 19-11](#figure19-11) as a layer.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们通过将一切打包成一个循环单元来实现管理状态和输出呈现的过程，如[图19-11](#figure19-11)所示。*循环*一词指的是我们反复使用状态记忆，尽管它的内容通常会随着每次输入的不同而变化（注意，这与*递归*一词不同，尽管这两个词听起来相似，但意义完全不同）。单元的工作通常由多个神经网络管理。像往常一样，当我们训练包含[图19-11](#figure19-11)作为一层的完整网络时，这些网络会学习如何执行它们的任务。
- en: We’ll see that even though a cell’s internal state is usually private, some
    networks can make good use of this information, so here we show the exported state
    as a dashed line, suggesting that it’s available, but can be ignored if not needed.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到，尽管一个单元的内部状态通常是私有的，但一些网络可以很好地利用这些信息，因此这里我们将导出的状态表示为虚线，表示它是可用的，但如果不需要可以忽略。
- en: We often place a recurrent cell on a layer of its own and call that a *recurrent
    layer*. A network that is dominated by recurrent layers is called a *recurrent
    neural network*, or *RNN*. The same term is frequently applied to the recurrent
    layers themselves, and sometimes even the recurrent cells, since they have neural
    networks inside them. The correct interpretation is usually clear from context.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常将一个循环单元放在单独的层上，并称之为*循环层*。以循环层为主的网络被称为*循环神经网络*，或*RNN*。这个术语也常常应用于循环层本身，有时甚至是循环单元，因为它们内部包含神经网络。正确的解释通常可以从上下文中明确得知。
- en: '![F19011](Images/F19011.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![F19011](Images/F19011.png)'
- en: 'Figure 19-11: A recurrent neural cell. The hidden state can be exported outside
    of the cell if needed.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图19-11：一个循环神经单元。如果需要，隐藏状态可以被导出到单元外部。
- en: The internal state of a recurrent cell is saved as a tensor. Because this tensor
    is frequently just a one-dimensional list of numbers, we sometimes speak of the
    *width* or *size* of a recurrent cell, referring to the number of memory elements
    in the state. If all cells in a network have the same width, we sometimes refer
    to it as the network’s width.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 循环单元的内部状态以张量的形式保存。由于这个张量通常只是一个一维的数字列表，我们有时会谈到循环单元的*宽度*或*大小*，指的是状态中记忆元素的数量。如果网络中所有单元的宽度相同，我们有时会将其称为网络的宽度。
- en: The left side of [Figure 19-12](#figure19-12) shows our icon for a recurrent
    cell, which we usually use in unrolled diagrams. The right side shows the icon
    when we place the cell in a layer, where we roll it up for convenience. In the
    layer version, we don’t draw the cell’s internal state.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19-12](#figure19-12)的左侧显示了我们用来表示循环单元的图标，这通常出现在展开的图示中。右侧则显示了当我们将单元放入一个层时的图标，为了方便，我们将其收起。在层版本中，我们不绘制单元的内部状态。'
- en: '![F19012](Images/F19012.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![F19012](Images/F19012.png)'
- en: 'Figure 19-12: Left: Our icon for a recurrent cell. Right: Our icon for a recurrent
    layer.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图19-12：左：我们为递归单元设计的图标。右：我们为递归层设计的图标。
- en: We could use the bare-bones recurrent cell in [Figure 19-11](#figure19-11) to
    build up a language model. Suppose that the box marked “neural networks” holds
    a small neural network, built from any layers we like. We could feed the cell
    sequences of words (in numerical form). After each word, the cell would produce
    an output predicting the next word to come, and update its internal state to remember
    the words that have come so far. To replicate our experiment from the start of
    this chapter, we could feed the cell five words in a row, ignoring the cell’s
    outputs for the first four. Its output after the fifth input would be its prediction
    for the sixth word. If we’re training and the prediction isn’t correct, then as
    usual, we use backpropagation and optimization to improve the values of the weights
    in the neural networks inside the cell and continue training. The goal is that
    eventually the networks will become so good at interpreting input and controlling
    the state that they will be able to make good predictions.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用[图19-11](#figure19-11)中的基本递归单元来构建一个语言模型。假设标有“神经网络”的框中包含一个小型神经网络，由我们喜欢的任何层构建。我们可以将单元喂入一系列的单词（以数字形式）。每个单词之后，单元会生成一个预测，预测下一个单词，并更新其内部状态以记住到目前为止出现的单词。为了复现本章开始时的实验，我们可以连续输入五个单词，忽略前四个单元的输出。第五个输入后的输出将是它对第六个单词的预测。如果我们正在训练，并且预测不正确，那么像往常一样，我们使用反向传播和优化来改进神经网络中权重的值，并继续训练。目标是最终这些网络能够非常擅长于解读输入并控制状态，从而能够做出良好的预测。
- en: Recurrent Cells in Action
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 递归单元的应用
- en: Let’s see how a recurrent cell might predict the next word of a five-word sequence.
    We can see the inputs and possible outputs with an unrolled diagram, shown in
    [Figure 19-13](#figure19-13).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个递归单元如何预测五个单词序列的下一个单词。我们可以通过展开的图示来看到输入和可能的输出，如[图19-13](#figure19-13)所示。
- en: '![F19013](Images/F19013.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![F19013](Images/F19013.png)'
- en: 'Figure 19-13: A recurrent cell predicting words. The diagram is in unrolled
    form. Predictions come out of the top of the cell, whereas state is indicated
    by the open horizontal arrow.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图19-13：递归单元预测单词。图示为展开形式。预测从单元的顶部输出，而状态由水平箭头表示。
- en: We begin with a cell whose hidden state has been initialized to something generic
    like all zeros, representing that nothing has been learned yet. That’s the open
    circle at the far left. The first word, it, arrives. The cell considers the input
    and its hidden state, and predicts the next word, swam. The cell is telling us
    that the sentence that begins with it is most likely to continue with the word
    swam, but we ignore this because we only care about the prediction after the fifth
    word.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个隐藏状态已被初始化为零的单元开始，表示尚未学到任何东西。那就是最左边的空圆圈。第一个单词“it”到达，单元考虑输入和隐藏状态，并预测下一个单词“swam”。单元告诉我们，以“it”开头的句子最可能接着是“swam”这个单词，但我们忽略这个预测，因为我们只关心第五个单词后的预测。
- en: Now comes the interesting part. Using the information it learned during training,
    the RNN updates its hidden state to contain some representation of the fact that
    it received the word it as input, and produced swam as output.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入有趣的部分。利用在训练过程中学到的信息，RNN更新其隐藏状态，以包含它接收到单词“it”作为输入，并生成“swam”作为输出的某种表示。
- en: Now comes the second word from the text, was. Again, the cell consults its hidden
    state and the input, and produces a new output prediction. Here it’s night, completing
    the phrase it was night. The cell updates its hidden state to remember receiving
    it and then was and then predicting night. Again, we ignore the prediction of
    night.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在出现第二个单词：was。单元再次根据其隐藏状态和输入进行计算，并生成新的输出预测。这里是“night”，完成了短语“it was night”。单元更新其隐藏状态，记住接收了“it”，然后是“was”，然后是预测“night”。我们再次忽略了对“night”的预测。
- en: This goes on until we provide the fifth word, of. If we’re near the start of
    training, the system might produce something like jellyfish, completing the sentence
    it was the best of jellyfish. But after enough training on the original text,
    the networks inside the recurrent cell will have learned how to represent the
    consecutive words of the phrase it was the best of inside the hidden state in
    such a way that the word times has a high probability.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程持续进行，直到我们提供第五个单词“of”。如果我们在训练初期，系统可能会生成像“jellyfish”这样的内容，完成句子“它是最好的jellyfish”。但在对原始文本进行了足够的训练后，递归单元内部的网络将学会如何在隐藏状态中表示“它是最好的”这个短语的连续单词，从而使得单词“times”具有很高的概率。
- en: Training a Recurrent Neural Network
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练递归神经网络
- en: Suppose that we’re at the start of training the recurrent cell in [Figure 19-13](#figure19-13).
    We give it the five words of input, and then compute an error by comparing the
    cell’s final prediction with the next word from the text. If the prediction doesn’t
    match the text, we run backprop and then optimization as usual. Looking at the
    diagram, we start by finding the gradients in the rightmost cell in the diagram,
    then we propagate the gradients to the preceding cell to its left, then propagate
    the gradients again to the cell preceding that, and so on. It’s important to apply
    backprop in sequence because these are sequential steps of processing.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在开始训练[图19-13](#figure19-13)中的递归单元。我们给它输入五个单词，然后通过将单元的最终预测与文本中的下一个单词进行比较来计算误差。如果预测与文本不匹配，我们会像往常一样执行反向传播和优化。查看图示，我们首先在图中最右侧的单元中找到梯度，然后将梯度传播到左边的前一个单元，再将梯度传播到它前面的单元，依此类推。按顺序应用反向传播非常重要，因为这些是处理的顺序步骤。
- en: But we can’t really apply optimization to each box in [Figure 19-13](#figure19-13)
    because these are all the same cell! To the system, it looks like just one instance
    of [Figure 19-11](#figure19-11) sitting on a layer of its own, rather than some
    unrolled list of repeated uses of the same cell. Somehow we have to apply backprop
    to the same layer repeatedly, which can create a confusing mess of bookkeeping.
    To handle this, we use a special variant of backpropagation, called *backpropagation
    through time*, or *BPTT*. It handles these details so that we can interpret [Figure
    19-13](#figure19-13) literally for the purposes of training.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们不能对[图19-13](#figure19-13)中的每个框进行优化，因为这些都是相同的单元！对于系统来说，这看起来就像[图19-11](#figure19-11)的一个实例，坐落在自己的层上，而不是一些展开的、重复使用相同单元的列表。我们必须以某种方式对同一层应用反向传播，这可能会造成混乱的记录工作。为了处理这个问题，我们使用了一种特殊的反向传播变体，叫做*反向传播通过时间*（BPTT）。它处理这些细节，使我们能够在训练时字面上解释[图19-13](#figure19-13)。
- en: BPTT allows us to train a recurrent cell efficiently, but it doesn’t solve the
    training problem completely. Suppose that while using BPTT, we compute a gradient
    for a particular weight in the rightmost cell in [Figure 19-13](#figure19-13).
    Then as we propagate the gradient left, we find that the gradient for that same
    weight in the previous cell is smaller. This means that as we push the gradient
    to the left, through the same cell over and over, the same process will repeat
    and the gradient will get smaller and smaller. If the gradient gets 60 percent
    smaller each time, then after just eight cells, it is down to less than a thousandth
    of its original size. All it takes for this process to get started is for a gradient
    to become smaller as we move backward, which is common. Then it inevitably gets
    smaller by the same percentage on every step backward.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: BPTT（反向传播通过时间）允许我们高效地训练递归单元，但它并不能完全解决训练问题。假设在使用BPTT时，我们为[图19-13](#figure19-13)中最右侧单元的某个特定权重计算了一个梯度。然后，当我们向左传播梯度时，发现前一个单元中该权重的梯度更小。这意味着，当我们一次又一次地通过同一个单元将梯度向左传播时，相同的过程会不断重复，梯度会变得越来越小。如果梯度每次减少60％，那么在经过八个单元后，梯度将降到原来值的千分之一以下。这个过程的开始只需要梯度在向后传播时变小，这是很常见的情况。然后它不可避免地在每次向后传播时按相同比例变小。
- en: This is very bad news. Recall that when a gradient becomes very small, learning
    slows down, and if a gradient becomes zero, learning stops entirely. This is not
    only bad for the recurrent cell, which stops learning, but for neurons on the
    layers that precede it, because they lose the opportunity to improve, too. The
    whole learning process can grind to a halt long before we’ve reached the network’s
    smallest possible error.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常糟糕的消息。回想一下，当梯度变得非常小的时候，学习进程会变慢；如果梯度变为零，学习就完全停止。这不仅对递归单元不利，因为它停止了学习，而且对其之前层的神经元也有不利影响，因为它们失去了改进的机会。整个学习过程可能在我们达到网络的最小误差之前就彻底停滞。
- en: This phenomenon is called the *vanishing gradient* problem (Hochreiter et al.
    2001; Pascanu, Mikolov, and Bengio 2013). A similar problem comes up if the gradient
    gets larger every time we step backward through the unrolled diagram. After the
    same eight steps, a gradient that grows by 60 percent on each step is almost 43
    times larger by the time it reaches the first cell. This is called the *exploding
    gradient* problem (R2RT 2016). These are serious problems that can prevent a network
    from learning.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这一现象被称为*梯度消失*问题（Hochreiter et al. 2001；Pascanu, Mikolov 和 Bengio 2013）。如果梯度在每次反向传播时增大，类似的问题也会出现。在经过相同的八步之后，每步增长60%的梯度到达第一个单元时已经大约大了43倍。这就是*梯度爆炸*问题（R2RT
    2016）。这些是严重的问题，可能会阻止网络的学习。
- en: Long Short-Term Memory and Gated Recurrent Networks
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长短期记忆和门控递归网络
- en: We can avoid both vanishing and exploding gradients with a fancier recurrent
    cell, called a *long short-term memory*, or *LSTM*. The name can be confusing,
    but it refers to the fact that the internal state changes frequently, so it can
    be considered a short-term memory. But sometimes we can choose to keep some information
    in the state for a long time. It might make more sense to think of this as a *selectively
    persistent short-term memory*. A block diagram of an LSTM is shown in [Figure
    19-14](#figure19-14).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个更复杂的递归单元——*长短期记忆*（LSTM）来避免梯度消失和梯度爆炸的问题。这个名字可能让人困惑，但它指的是内部状态会频繁变化，因此可以视为短期记忆。但有时我们也可以选择将某些信息在状态中保持较长时间。将其看作是*选择性持久的短期记忆*可能更有意义。LSTM的框图见[图19-14](#figure19-14)。
- en: '![F19014](Images/F19014.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![F19014](Images/F19014.png)'
- en: 'Figure 19-14: A block diagram of a long short-term memory, or LSTM'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图19-14：长短期记忆（LSTM）的框图
- en: The LSTM uses three internal neural networks. The first is used to remove (or
    forget) information from the state that is no longer needed. The second inserts
    new information the cell wants to remember. The third network presents a version
    of the internal state as the cell’s output.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM使用三个内部神经网络。第一个用来移除（或忘记）不再需要的状态信息。第二个用来插入单元想要记住的新信息。第三个网络将内部状态的一个版本作为单元的输出。
- en: The convention is that “forgetting” a number simply means moving it toward zero,
    and remembering a new number means adding it in to the appropriate location in
    the state memory.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 约定俗成地，“忘记”一个数字意味着将其移向零，而记住一个新数字则意味着将其添加到状态记忆的适当位置。
- en: The LSTM doesn’t require repeated copies of itself, like the basic recurrent
    cell of [Figure 19-11](#figure19-11), so it avoids the problems of vanishing and
    exploding gradients. We can place this LSTM cell on a layer and train the neural
    networks inside it using normal backprop and optimization. A practical implementation
    has many details that we’ve skipped over here, but they follow this general flow
    (Hochreiter et al. 2001; Olah 2015).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM不需要像[图19-11](#figure19-11)中的基本递归单元那样重复自身，因此它避免了梯度消失和梯度爆炸的问题。我们可以将这个LSTM单元放在一个层上，并使用常规的反向传播和优化方法训练其中的神经网络。实际的实现有许多细节在这里没有展开，但它们遵循这个一般流程（Hochreiter
    et al. 2001; Olah 2015）。
- en: The LSTM has proven to be such a good way to implement a recurrent cell that
    when people speak of “an RNN” they often mean a network that uses the LSTM in
    particular. A popular variation of the LSTM is the *gated recurrent unit*, or
    *GRU*. It’s not uncommon to try out both the LSTM and GRU in a network to see
    which performs better on a specific task.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM已被证明是一种非常好的递归单元实现方式，因此当人们提到“RNN”时，通常指的是特别使用LSTM的网络。LSTM的一个常见变体是*门控递归单元*（GRU）。在网络中尝试LSTM和GRU，看看哪个在特定任务上表现更好，这并不罕见。
- en: Using Recurrent Neural Networks
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用递归神经网络
- en: It’s easy to build a network with a recurrent cell (whether it’s an LSTM, a
    GRU, or something else). We just place a recurrent layer in our network and train
    as usual.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个包含递归单元的网络（无论是LSTM、GRU还是其他类型）很容易。我们只需在网络中添加一个递归层并像往常一样进行训练。
- en: Working with Sunspot Data
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用太阳黑子数据
- en: Let’s demonstrate this with our sunspot data. We’ll train a network with a single
    recurrent layer holding a tiny LSTM with just three values in its hidden state,
    as shown in [Figure 19-15](#figure19-15) (our convention in this book is that
    a recurrent cell is an LSTM unless stated otherwise). Let’s compare this to the
    output of our old fully connected network of five neurons in [Figure 19-1](#figure19-1).
    We have to be careful about comparing apples and oranges, because these approaches
    are so different, but both networks are about as small as they can be and still
    do something useful.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用太阳黑子数据来演示这个。我们将训练一个网络，该网络具有一个包含单个LSTM的递归层，LSTM的隐藏状态中只有三个值，如[图19-15](#figure19-15)所示（本书中的约定是，除非另有说明，否则递归单元是LSTM）。让我们将其与我们旧的全连接网络的输出进行比较，该网络有五个神经元，显示在[图19-1](#figure19-1)中。我们必须小心比较苹果和橙子，因为这些方法如此不同，但两个网络都尽可能小，并且仍能做一些有用的事情。
- en: '![F19015](Images/F19015.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![F19015](Images/F19015.png)'
- en: 'Figure 19-15: A tiny RNN consisting of a single LSTM with three values in its
    hidden state'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图19-15：由单个LSTM组成的微型RNN，其隐藏状态中有三个值
- en: Like before, let’s train using five sequential values taken from the training
    data. In contrast to the fully connected layer, which received all five values
    at once, the RNN gets the values one at a time in five successive steps. The results
    are shown in [Figure 19-16](#figure19-16). Keeping in mind our warning about apples
    and oranges, the results for this little RNN look very much like the results from
    our fully connected network, shown in [Figure 19-5](#figure19-5) (the loss values
    and overall error measured during training were also roughly the same).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们将使用从训练数据中提取的五个连续值进行训练。与一次性接收所有五个值的全连接层不同，RNN在五个连续步骤中每次接收一个值。结果如[图19-16](#figure19-16)所示。记住我们关于苹果和橙子的警告，这个小型RNN的结果看起来与我们全连接网络的结果非常相似，后者的结果显示在[图19-5](#figure19-5)中（在训练过程中测量的损失值和总体误差也大致相同）。
- en: '![F19016](Images/F19016.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![F19016](Images/F19016.png)'
- en: 'Figure 19-16: Predicting sunspot data with the tiny RNN of [Figure 19-15](#figure19-15)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图19-16：使用[图19-15](#figure19-15)中的微型RNN预测太阳黑子数据
- en: Generating Text
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成文本
- en: The last results were encouraging, so let’s try the next challenge and use an
    RNN to generate text. Rather than predict the next word, as we did earlier, for
    this example let’s give our system a sequence of letters and ask it to predict
    the next letter. As we saw earlier, this is a much easier task, because there
    are far fewer letters than words. We’ll use 89 symbols from the standard English
    keyboard as our character set. With luck, using characters will let us get away
    with a smaller network than a word-based approach would require.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 上一次的结果令人鼓舞，所以让我们尝试下一个挑战，使用RNN生成文本。与之前预测下一个单词不同，这次我们将给系统一个字母序列，并要求它预测下一个字母。如前所述，这是一个更简单的任务，因为字母比单词少得多。我们将使用标准英文键盘上的89个符号作为我们的字符集。幸运的话，使用字符可能让我们避免使用比基于单词的方法更大的网络。
- en: Let’s train our RNN on sequences of characters taken from the collected short
    stories of Sherlock Holmes, and ask it to predict the next character (Doyle 1892).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在从《福尔摩斯短篇故事集》收集的字符序列上训练我们的RNN，并要求它预测下一个字符（Doyle 1892）。
- en: Training an RNN requires tradeoffs. We can use more cells, or more state in
    each cell, but these all cost time or memory. Larger networks let us work with
    longer windows, which will probably lead to better predictions. On the other hand,
    using fewer, smaller units and smaller windows makes the system faster, so we
    can run through more training samples in any given span of time. As usual, the
    best choice for any given system and data requires some experimentation.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 训练RNN需要权衡。我们可以使用更多的单元，或在每个单元中使用更多的状态，但这些都需要时间或内存。较大的网络可以让我们处理更长的窗口，这可能会导致更好的预测。另一方面，使用更少、更小的单元和较小的窗口使系统更快，这样我们就可以在给定时间内运行更多的训练样本。像往常一样，任何系统和数据的最佳选择都需要一些实验。
- en: After some trial and error, we settled on the network of [Figure 19-17](#figure19-17).
    This can surely be improved, but it’s small and works well enough for this discussion.
    Our input window is 40 characters long. Each LSTM cell contains 128 elements of
    state memory. The final fully connected layer has 89 outputs, one for each possible
    symbol. The small box after the last fully connected layer is our shorthand in
    this chapter (and Chapter 20) for a softmax activation function. Thus, the output
    of this network is a list of 89 probabilities, one for each possible character.
    We’ll choose the most probable character every time.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![F19017](Images/F19017.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19-17: A small RNN for processing text one character at a time'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: To create the training set, we chopped up the original source material into
    about a half-million overlapping strings of 40 characters, starting every third
    character.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Once training is done, we can generate new text by autoregression, creating
    each new 40-character input by adding the last output to the end of the previous
    input and dropping that previous input’s first entry (Chen et al. 2017). We can
    repeat this as many times as we desire. [Figure 19-18](#figure19-18) illustrates
    autoregression for a window of four characters.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![F19018](Images/F19018.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19-18: Generating text one character at a time with autoregression'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'To watch the progress of the network, after each epoch of training, we generated
    some text using the network so far. We started with a seed of 40 sequential characters
    starting from a random location in the source material. A nice thing about autoregression
    is that we can run it as long as we like and generated unlimited amounts of output.
    Here is the beginning of one run after the first epoch of training (the seed is
    shown in red):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: er price.” “If he waits a little longer wew fet ius ofuthe henss lollinod fo
    snof thasle, anwt wh alm mo gparg lests and and metd tingen, at uf tor alkibto-Panurs
    the titningly ad saind soot on ourne” Fy til, Min, bals’ thid the
  id: totrans-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In a sense, that’s remarkably good. The “words” are about English-sized, and
    although they’re not real words, they could be. That is, they’re not strings of
    random characters. Many of them can even be easily pronounced. And this was after
    just a single epoch. After 50 epochs, things improved a lot. Here’s some output
    in response to a new random seed.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: nt blood to the face, and no man could hardly question off his pockets of trainer,
    that name to say, yisligman, and to say I am two out of them, with a second. “I
    conturred these cause they not you means to know hurried at your little platter.’
    “‘Why shoubing, you shout it of them,” Treating, I found this step-was another
    write so put.” “Excellent!” Holmes to be so lad, reached.
  id: totrans-155
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Wow. Things are much better. Most of these words are real. The punctuation is
    great. And even some of the words that aren’t in the dictionary, like conturred
    and shoubing, seem like they could be.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the system has no knowledge of words at all. It only knows the
    probabilities of letters following sequences of other letters. For such a simple
    network, this is remarkable. By letting this run, we can generate as much of this
    text as we like. It doesn’t get any more coherent, but it doesn’t get any more
    incoherent, either.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，系统完全不知道单词是什么。它只知道字母跟随其他字母序列的概率。对于这样一个简单的网络来说，这非常了不起。通过让它运行，我们可以生成任意数量的文本。它不会变得更加连贯，但也不会变得更加不连贯。
- en: A larger model with bigger LSTMs, more of them, or both, will give us increasingly
    credible results at the cost of more training time (Karpathy 2015).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更大的模型，包含更大的 LSTM、更高的数量，或者两者兼有，将以更长的训练时间为代价，给我们带来更可信的结果（Karpathy 2015）。
- en: Different Architectures
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不同的架构
- en: We can incorporate recurrent cells into other types of networks, extending the
    capabilities of some types of networks we’ve already seen. We can also combine
    multiple recurrent cells to perform sequence operations beyond what any one cell
    can do. Let’s look at a few examples.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将递归单元（recurrent cells）整合到其他类型的网络中，从而扩展我们已经见过的一些网络类型的功能。我们还可以将多个递归单元组合起来，执行超出任何单个单元能完成的序列操作。让我们来看几个例子。
- en: CNN-LSTM Networks
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CNN-LSTM 网络
- en: We can mix our LSTM cells with a CNN to create a hybrid called a *CNN-LSTM network*.
    This is great for jobs like classifying video frames. The convolutional layers
    are responsible for finding and identifying objects, while the recurrent layers
    that come after are responsible for tracking how the objects move from one frame
    to the next.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 LSTM 单元与 CNN 混合，创建一种混合网络，叫做 *CNN-LSTM 网络*。这对于分类视频帧等任务非常有效。卷积层负责查找和识别物体，而紧随其后的递归层则负责跟踪物体如何从一帧移动到另一帧。
- en: Deep RNNs
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 深度 RNN
- en: Another way to use recurrent cells is to stack up many of them in a row. We
    call the result a *deep RNN*. We just take the outputs from the cells on one layer
    and use them as the inputs to the cells on the next layer. [Figure 19-19](#figure19-19)
    shows one way to connect things up for three layers, drawn in both rolled-up and
    unrolled forms. As usual, the RNN units on each layer have their own internal
    weights and hidden state.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用递归单元的另一种方法是将它们按顺序堆叠起来。我们称结果为 *深度 RNN*。我们只是将一层中单元的输出作为下一层单元的输入。[图 19-19](#figure19-19)
    展示了三层的连接方式，分别以卷起和展开的形式绘制。像往常一样，每一层的 RNN 单元都有自己的内部权重和隐藏状态。
- en: '![F19019](Images/F19019.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![F19019](Images/F19019.png)'
- en: 'Figure 19-19: A deep RNN. Left: The network using our icons. Right: The layers
    in unrolled form.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19-19：深度 RNN。左：使用我们图标的网络。右：未展开形式的各层。
- en: The appeal of this architecture is that each RNN can be specialized for a particular
    task. For example, in [Figure 19-19](#figure19-19) the first layer might translate
    an input sentence into an abstract, common language, the second might rephrase
    it to change the mood, and then the third could translate that into a different
    target language. By training each LSTM individually, we gain the advantages of
    specialization, such as the freedom to update or improve each layer independently
    of the others. If we replace one LSTM layer with another, we will need to do some
    extra training on the whole network to make sure the layers work together smoothly.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构的吸引力在于，每个 RNN 都可以针对特定任务进行专业化。例如，在[图 19-19](#figure19-19)中，第一层可能将输入句子翻译成一个抽象的、通用的语言，第二层可能重新表述它以改变语气，第三层则可能将其翻译成另一种目标语言。通过单独训练每个
    LSTM，我们可以获得专业化的优势，比如能够独立更新或改进每一层。如果我们用另一个 LSTM 层替换其中一个，我们需要对整个网络进行一些额外的训练，以确保各层协同工作顺畅。
- en: Bidirectional RNNs
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 双向 RNN
- en: Let’s return to translation and consider just how hard the problem is. Take
    the sentence, “I saw the hot dog train.” We can find at least six different ways
    to interpret this (witnessing an exercise routine by a warm dog, an attractive
    dog, or a frankfurter, and witnessing a locomotive pulling a chain of each of
    these three kinds of things). Some interpretations are goofier than the others,
    but they’re all valid. Which one do we choose when we translate?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到翻译问题，并考虑这个问题有多么困难。拿句子“我看到了热狗火车”来说。我们至少可以找到六种不同的解释方式（目睹一只温暖的狗做锻炼、一只迷人的狗、一根香肠，或目睹一辆火车拉着这些三种东西中的每一种）。有些解释比其他的更荒谬，但它们都是有效的。当我们翻译时，应该选择哪一种呢？
- en: 'Another famous sentence is, “I saw the man on the hill in Texas with the telescope
    at noon on Monday,” which has 132 interpretations (Mooney 2019). Aside from the
    words themselves, delivery also makes a huge difference in meaning. By stressing
    each word of, “I didn’t say he stole the money,” we can produce seven completely
    distinct meanings (Bryant 2019). Linguistic ambiguity is at the heart of a classic
    line from Groucho Marx in the film *Animal Crackers*: “One morning I shot an elephant
    in my pajamas. How he got into my pajamas, I’ll never know” (Heerman 1930).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个著名的句子是：“I saw the man on the hill in Texas with the telescope at noon on
    Monday”（我在星期一中午在德克萨斯州用望远镜看到山丘上的那个男人），它有132种不同的解释（Mooney 2019）。除了单词本身外，语调的变化也会对意思产生巨大影响。通过强调“I
    didn’t say he stole the money”（我没说他偷了钱）中的每个词，我们可以产生七种完全不同的意思（Bryant 2019）。语言的歧义性在Groucho
    Marx在电影*Animal Crackers*中的经典台词中得到了体现：“One morning I shot an elephant in my pajamas.
    How he got into my pajamas, I’ll never know”（有一天早上，我穿着睡衣射杀了一只大象。它是怎么进我的睡衣的，我永远也不知道）(Heerman
    1930)。
- en: 'One way to get a handle on all this complexity is to consider multiple words
    in a sentence as we translate it, rather than each word one at a time. For example,
    consider these sentences: I cast my fate to the wind, The cast on my arm is heavy,
    and The cast of the play is all here. These sentences illustrate that the English
    word cast is a homonym, or a word that can have different meanings. Linguists
    call this *polysemy*, and it’s a feature in many languages (Vicente and Falkum
    2017). Our three sentences involving cast translate into Portuguese as, respectively,
    Eu lancei meu destino ao vento, O gesso no meu braço é pesado, and O elenco da
    peça está todo aqui. The word cast translates, respectively, to lancei, gesso,
    and elenco (Google 2020). In these examples, the only way to choose the proper
    word in Portuguese is to know the words that follow cast in the original sentence,
    in addition to those that come before.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 处理所有这些复杂性的一个方法是，在翻译句子时考虑句子中的多个词，而不是一个一个词地翻译。例如，考虑这些句子：I cast my fate to the
    wind（我把命运交给了风），The cast on my arm is heavy（我手臂上的石膏很重），The cast of the play is
    all here（话剧的演员阵容都在这里）。这些句子说明了英语单词“cast”是一个同形异义词，或者说是一个具有不同含义的词。语言学家称之为*多义性*，这是许多语言的一个特征（Vicente
    和 Falkum 2017）。我们的三个含有“cast”的句子分别翻译成葡萄牙语是：Eu lancei meu destino ao vento（我把命运交给了风），O
    gesso no meu braço é pesado（我手臂上的石膏很重），和 O elenco da peça está todo aqui（话剧的演员阵容都在这里）。其中，“cast”分别翻译成了lancei、gesso和elenco（Google
    2020）。在这些例子中，选择合适的葡萄牙语单词的方法是，除了了解“cast”前面的词外，还需要知道它后面的词。
- en: If we’re translating in real time, then we may not know which translation to
    use based only on the words we’ve heard so far. In such situations, all we can
    do is guess, or wait for more words to arrive, and then try to catch up. But if
    we’re working with the whole sentence, such as when we’re translating a written
    book or story, we have all the words already available.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在实时翻译，那么仅凭目前听到的词，我们可能无法确定使用哪个翻译。在这种情况下，我们能做的就是猜测，或者等待更多的词汇到来，然后试着赶上。但如果我们在翻译整句，比如在翻译一本书或故事时，我们已经可以访问所有的词汇。
- en: One way to use the later words in the sentence is to feed the words into our
    RNN backward, such as wind the to fate my cast I. But this doesn’t solve the problem
    in general because sometimes we might need the earlier words, too. What we really
    want is to have both the preceding and following words available.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中后面的词语的一种使用方式是将这些词倒序输入到我们的RNN中，例如：wind the to fate my cast I。但这通常不能解决问题，因为有时我们也可能需要前面的词。我们真正需要的是能够同时访问前后的词语。
- en: We can do this with our existing tools and a bit of cleverness, by creating
    two independent RNNs. The first gets the words in their forward, or natural order.
    The second gets the words in their backward order, as shown in [Figure 19-20](#figure19-20).
    We call this a *bidirectional RNN*, or a *bi-RNN* (Schuster and Paliwal 1997).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过现有的工具和一点巧妙的方法来实现这一点，即创建两个独立的RNN。第一个按自然顺序接收词语，第二个按倒序接收词语，如[图 19-20](#figure19-20)所示。我们称之为*双向RNN*，或*双RNN*（Schuster
    和 Paliwal 1997）。
- en: '![F19020](Images/F19020.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![F19020](Images/F19020.png)'
- en: 'Figure 19-20: A bidirectional RNN, or bi-RNN. Left: Our icon for this layer.
    Right: An unrolled bi-RNN diagram.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19-20：双向RNN，或双RNN。左：我们为这一层设计的图标。右：展开的双RNN图示。
- en: In [Figure 19-20](#figure19-20) we feed the sentence simultaneously to the lower
    recurrent cell in forward order and the upper recurrent cell in backward order.
    That is, we give input 0 to the lower cell at the same time we give input 4 to
    the upper cell. Then we give input 1 to the lower cell while we give input 3 to
    the upper cell, and so on. Once all the words have been processed, each recurrent
    cell will have produced an output for each word. We simply concatenate those outputs
    and that’s the output of the bi-RNN.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图19-20](#figure19-20)中，我们将句子同时输入到前向顺序的下部递归单元和反向顺序的上部递归单元。也就是说，我们在将输入0给下部单元的同时，将输入4给上部单元。接着我们将输入1给下部单元，同时将输入3给上部单元，依此类推。所有单词处理完成后，每个递归单元将为每个单词生成一个输出。我们只需将这些输出连接起来，这就是双向RNN的输出。
- en: We can stack up lots of bi-RNNs to make a *deep bi-RNN*. [Figure 19-21](#figure19-21)
    shows such a network with three bi-RNN layers. On the left is our schematic for
    this layer, and on the right, we draw each layer in its unrolled form. In this
    diagram, we have three layers, each containing two independent recurrent cells.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以堆叠多个双向RNN，形成*深度双向RNN*。[图19-21](#figure19-21)展示了一个包含三层双向RNN的网络。左侧是我们为该层绘制的示意图，右侧是每一层展开后的形式。在这个图中，我们有三层，每层包含两个独立的递归单元。
- en: '![F19021](Images/F19021.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![F19021](Images/F19021.png)'
- en: 'Figure 19-21: A deep bi-RNN. Left: A block diagram using our captions. Right:
    An unrolled deep bi-RNN.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图19-21：一个深度双向RNN。左侧：使用我们标注的框图。右侧：一个展开的深度双向RNN。
- en: As before, part of the value here is that each bi-RNN can be independently trained
    for a different task, and a new bi-RNN can be swapped in if we find (or train)
    another one that performs better.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，这里的一部分价值在于，每个双向RNN可以为不同的任务独立训练，如果我们找到（或训练）另一个表现更好的双向RNN，可以将其替换进来。
- en: Seq2Seq
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Seq2Seq
- en: A challenge for any translation system is that different languages use different
    word orders. A classic version of this is that in English, adjectives usually
    precede nouns, while it’s not so simple in French. For example, I love my big
    friendly dog translates to J’adore mon gros chien amical, where chien corresponds
    to dog, but the adjectives gros and amical, corresponding to big and friendly,
    surround the noun.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 任何翻译系统面临的挑战之一是不同语言使用不同的词序。一个经典的例子是，在英语中，形容词通常位于名词之前，而在法语中情况则不那么简单。例如，“I love
    my big friendly dog”翻译成法语为“J’adore mon gros chien amical”，其中“chien”对应“dog”，但形容词“gros”和“amical”分别对应“big”和“friendly”，却位于名词的两侧。
- en: 'This suggests that instead of translating one word at a time, we should translate
    entire sentences. This makes even more sense when the input and output sentences
    have different lengths. Take the five-word English sentence My dog is eating dinner.
    In Portuguese, this takes only four words: Meu cachorro está jantando, while in
    Scottish Gaelic it takes six: Tha mo chù ag ithe dinnear (Google 2020).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，与其一次翻译一个单词，不如翻译整句话。当输入和输出句子的长度不同的时候，这种方法显得更为合理。例如，五个单词的英文句子“My dog is eating
    dinner”在葡萄牙语中只需四个单词：“Meu cachorro está jantando”，而在苏格兰盖尔语中则需要六个单词：“Tha mo chù
    ag ithe dinnear”（Google 2020）。
- en: So rather than work word by word, let’s turn a complete sequence into another
    complete sequence, possibly of a different length. A popular algorithm for converting
    one entire sequence into another sequence is called *seq2seq* (for “sequence to
    sequence”) (Sutskever, Vinyals, and Le 2014).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，与其逐个词地翻译，不如将完整的序列转化为另一个完整的序列，可能长度不同。一个常用的算法，用于将一个完整序列转换为另一个序列，称为*seq2seq*（“sequence
    to sequence”的缩写）（Sutskever, Vinyals, 和 Le 2014）。
- en: The key idea of seq2seq is to use two RNNs, which we treat as an *encoder* and
    a *decoder*. Let’s see how the system works after training is done. We feed our
    input to the encoder, one word at a time, as usual, but we ignore its outputs.
    When the whole input has been processed, we take the encoder’s final hidden state
    and hand that to the decoder. The decoder uses the encoder’s final hidden state
    as its own initial hidden state and produces the output sequence using autoregression.
    [Figure 19-22](#figure19-22) shows the idea.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq的关键思想是使用两个RNN，我们将其视为*编码器*和*解码器*。让我们看看训练完成后系统是如何工作的。我们按常规将输入数据喂给编码器，一次输入一个单词，但我们忽略其输出。当整个输入处理完毕后，我们将编码器的最终隐藏状态交给解码器。解码器使用编码器的最终隐藏状态作为自己的初始隐藏状态，并通过自回归生成输出序列。[图19-22](#figure19-22)展示了这一思想。
- en: '![f19022](Images/f19022.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![f19022](Images/f19022.png)'
- en: 'Figure 19-22: The architecture of seq2seq. The encoder, left, processes the
    input and sends its hidden state to the decoder, right, which produces the output.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19-22：seq2seq 的架构。左侧是编码器，处理输入并将其隐藏状态发送给右侧的解码器，解码器生成输出。
- en: In [Figure 19-22](#figure19-22) we’re explicitly showing the autoregression
    step by feeding the output of each decoder step to the input of the next. If the
    encoder-decoder architecture looks familiar, it’s because it’s the same basic
    structure as the autoencoders we saw in Chapter 18\. In this use, what we previously
    called the latent vector is now called the *context vector*.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 19-22](#figure19-22)中，我们通过将每个解码器步骤的输出作为输入喂给下一个解码器，明确展示了自回归步骤。如果编码器-解码器结构看起来很熟悉，那是因为它与我们在第
    18 章看到的自编码器的基本结构相同。在这种用法中，我们之前称为潜在向量的部分现在被称为 *上下文向量*。
- en: Let’s look a little more closely at each of these two RNNs and how they translate
    a sentence.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这两个 RNN 以及它们如何翻译一个句子。
- en: The encoder starts with its hidden state set to some initial value, such as
    all zeros. It consumes the first word, updates its hidden state, and computes
    an output value. We simply ignore the output value. The only thing we care about
    is the evolving hidden state inside the encoder.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器从其隐藏状态设置为某个初始值开始，比如全零。它处理第一个词，更新其隐藏状态，并计算一个输出值。我们简单地忽略输出值。我们关心的唯一事情是编码器内隐藏状态的变化。
- en: When the last word has been processed, the hidden state of the encoder is used
    to initialize the hidden state of the decoder.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当最后一个词被处理完后，编码器的隐藏状态将用于初始化解码器的隐藏状态。
- en: Like any RNN, the decoder needs an input. By convention, we give the decoder
    a special start token. This can be written any way we like so long as it’s obviously
    special and not part of the normal vocabulary of our inputs or outputs. A common
    convention writes it in all capitals between square or angle brackets, such as
    `[START]`. Like all the words in our vocabulary, this special token gets its own
    unique number.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 像任何 RNN 一样，解码器需要输入。按照惯例，我们给解码器一个特殊的开始标记。这个标记可以用任何我们喜欢的方式书写，只要它明显是特殊的，并且不属于输入或输出的正常词汇。常见的惯例是将其大写并放在方括号或尖括号中，例如
    `[START]`。像我们词汇表中的所有词一样，这个特殊标记也有一个独特的编号。
- en: Now that the decoder has an input, it updates its hidden state (initially, the
    final hidden state from the encoder) and produces an output value. We do pay attention
    to this output, because it’s the first word of our translation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在解码器有了输入，它更新其隐藏状态（最初是编码器的最终隐藏状态），并生成一个输出值。我们确实关注这个输出，因为它是我们翻译的第一个词。
- en: Now we use autoregression to make the rest of the translation. The decoder takes
    in the previous output word as input, updates its hidden state, and produces a
    new output. This continues until the decoder decides that there are no more words
    to produce. It marks this by producing another special token, such as `[END]`,
    and stops.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用自回归来完成其余的翻译。解码器将前一个输出词作为输入，更新其隐藏状态，并生成新的输出。这一过程会持续，直到解码器决定没有更多的词需要生成。它通过生成另一个特殊标记（例如
    `[END]`）来标记这一点，然后停止。
- en: We trained a seq2seq model to translate from English to Dutch (Hughes 2020).
    Both RNNs had 1,024 elements in their state. The training data consisted of about
    50,000 sentences in Dutch, along with their English translations (Kelly 2020).
    We used about 40,000 sentences for training, and the rest for testing. We trained
    for ten epochs. In the following two examples, we provide an English sentence,
    the Dutch translation provided by seq2seq, and the translation of the Dutch back
    to English provided by Google Translate.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了一个 seq2seq 模型，将英文翻译成荷兰语（Hughes 2020）。这两个 RNN 的状态中各有 1,024 个元素。训练数据包含大约
    50,000 个荷兰语句子，以及它们的英文翻译（Kelly 2020）。我们使用了大约 40,000 个句子进行训练，其余的用于测试。我们训练了十个周期。在接下来的两个例子中，我们提供了一个英文句子，seq2seq
    提供的荷兰语翻译，以及谷歌翻译将荷兰语翻译回英文的结果。
- en: do you know what time it is
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你知道现在几点了吗
- en: weet u hoe laat het is
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您知道现在几点了吗
- en: Do you know what time it is
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你知道现在几点了吗
- en: i like playing the piano
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我喜欢弹钢琴
- en: ik speel graag piano
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我喜欢弹钢琴
- en: i like to play the piano
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我喜欢弹钢琴
- en: 'Those are pretty great results for such a small network and training set! On
    the other hand, our small model doesn’t degrade too gracefully when the inputs
    get more complex, as this set of inputs and outputs shows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于如此小的网络和训练集，这些结果相当不错！另一方面，当输入变得更加复杂时，我们的小模型表现得不太好，正如这个输入输出集合所示：
- en: John told Sam that his bosses said that if he worked late, they would give him
    a bonus
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 约翰告诉萨姆，他的老板说如果他加班，他们会给他奖金
- en: hij nodig had hij een nieuw hij te helpen
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: hij nodig had hij een nieuw hij te helpen
- en: he needed a new he help
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他需要一个新的他帮助
- en: The seq2seq method has much to recommend it. It’s conceptually simple, it works
    well in many situations, and it’s easy to implement in modern libraries (Chollet
    2017; Robertson 2017). But seq2seq has a built-in limitation in the form of the
    context vector. This is just the hidden state of the encoder after the last word,
    so it’s of a fixed, finite size. This one vector has to hold everything about
    the sentence, since it’s the only information that the decoder gets.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq 方法有很多优点。它在概念上很简单，在许多情况下都能很好地工作，并且在现代库中实现起来很容易（Chollet 2017；Robertson
    2017）。但是，seq2seq 存在一个内在的限制，即上下文向量。这只是编码器在处理完最后一个单词后的隐藏状态，因此它是固定的、有限的大小。这个向量必须包含关于句子的一切，因为它是解码器所获得的唯一信息。
- en: If we give the encoder a sentence that begins The table has four sturdy, then
    we can imagine a reasonable amount of memory could retain enough information about
    each word in the sequence that it could remember we’re talking about a table,
    and the next word should be legs. But no matter how much memory we give to our
    encoder’s hidden state, we can always make a sentence longer than it can remember.
    For example, suppose our sentence was The table, despite all the long-distance
    moves, the books dropped onto it, the kids running full-speed into it, serving
    variously as a fort, a stepladder, and a doorstop, still had four sturdy. The
    next word should still be legs, but our hidden state would have to become a lot
    bigger to remember enough information to work that out.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们给编码器一个句子，开始是“桌子有四条坚固的腿”，那么我们可以想象，合理的记忆量能够保留足够的关于序列中每个单词的信息，记得我们在谈论一张桌子，且下一个单词应该是“腿”。但是，无论我们给编码器的隐藏状态多么大，我们总是可以构造一个比它能记住的内容更长的句子。例如，假设我们的句子是：“桌子，尽管经历了所有长距离的移动，书本掉到上面，孩子们全速撞向它，作为堡垒、阶梯和门挡等多重功能，它仍然有四条坚固的腿。”下一个单词应该仍然是“腿”，但我们的隐藏状态需要变得更大，才能记住足够的信息来理解这一点。
- en: No matter how big our hidden state is, a bigger sentence can always come along
    and require more memory than we have. This is called the *long-term dependency
    problem* (Hochreiter et al. 2001; Olah 2015). [Figure 19-23](#figure19-23) shows
    an unrolled seq2seq diagram where the input has many words (Karim 2019). A context
    vector that could remember all of that information would need to be large, with
    correspondingly large neural networks inside each RNN to manage and control it.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们的隐藏状态有多大，一个更长的句子总是会出现，并且需要比我们拥有的更多的记忆。这就是所谓的*长期依赖问题*（Hochreiter et al. 2001；Olah
    2015）。[图 19-23](#figure19-23) 展示了一个展开的 seq2seq 图，其中输入包含许多单词（Karim 2019）。一个能够记住所有这些信息的上下文向量需要很大，且每个
    RNN 中必须有相应的大型神经网络来管理和控制它。
- en: '![f19023](Images/f19023.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![f19023](Images/f19023.png)'
- en: 'Figure 19-23: Encoding a very long input sentence before sending it a decoder'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19-23：在将非常长的输入句子发送到解码器之前进行编码
- en: Maybe depending on a single context vector to represent every useful piece of
    information in the input isn’t the best way to do things. The seq2seq architecture
    ignores all of the encoder’s hidden states except the last. For a long input,
    those intermediate hidden states can hold information that gets forgotten by the
    time we reached the end of the sentence.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 或许依赖于单一的上下文向量来表示输入中每个有用的信息并不是最好的做法。seq2seq 架构忽略了编码器的所有隐藏状态，除了最后一个。对于长输入，这些中间的隐藏状态可能包含的信息会在我们到达句子结尾时被遗忘。
- en: The dependence on a single context vector plus the need to train one word at
    a time are big problems for RNN architectures. Though they’re useful in many applications,
    these are serious drawbacks.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖单一的上下文向量并且需要一次训练一个单词，对于 RNN 架构来说是一个很大的问题。尽管它们在许多应用中非常有用，但这些都是严重的缺点。
- en: Despite these problems, RNNs are a popular way to handle sequences, particularly
    if they’re not too large.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些问题，RNN 仍然是处理序列的一种流行方式，尤其是当序列不太长时。
- en: Summary
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: We’ve covered a lot in this chapter about processing language and sequences.
    We saw that we can predict the next element of a sequence with fully connected
    layers, but they have problems because there’s no memory of the inputs. We saw
    how to use recurrent cells with local, or hidden, memory to maintain a record
    of everything they’ve seen in a single context vector that is modified with each
    input.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们已经讨论了很多关于处理语言和序列的内容。我们看到我们可以用全连接层预测序列中的下一个元素，但它们存在问题，因为没有输入的记忆。我们还看到了如何使用带有局部或隐藏记忆的递归单元来保持它们在一个上下文向量中看到的一切，并随着每个输入的变化进行修改。
- en: We saw some examples of using RNNs, and then how to use two RNNs to build a
    translator called seq2seq. Though seq2seq is simple and can do a good job, it
    has two drawbacks that are common to most RNN systems. For example, the system
    relies on one context vector to carry all the information about the sentence.
    Second, the network needs to be trained one word at a time.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看了一些使用 RNN 的例子，然后学习了如何使用两个 RNN 构建一个名为 seq2seq 的翻译器。虽然 seq2seq 简单且能做得很好，但它有两个常见于大多数
    RNN 系统的缺点。例如，该系统依赖于一个上下文向量来携带关于句子的所有信息。第二，网络需要逐个单词进行训练。
- en: Despite these issues, RNNs are a popular and powerful tool for processing sequential
    data of any kind, from language to seismic data, song lyrics, and medical histories.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些问题，RNN 仍然是处理任何类型序列数据的流行且强大的工具，从语言到地震数据、歌曲歌词和病历等。
- en: In the next chapter, we’ll look at another way to handle sequences that avoids
    the limitations of RNNs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨另一种处理序列的方法，它避免了 RNN 的局限性。
