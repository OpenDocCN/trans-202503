<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
<head>
<title>Chapter 1: The Geometric Structure of Data</title>
<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:ea5eeba6-9dea-4463-bfe4-b91d6c6b5861" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter chapter">
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_1" title="1"/><a class="XrefDestination" id="1"/><span class="XrefDestination" id="xref-503083c01-001"/>1</span><br/>
<span class="ChapterTitle"><a class="XrefDestination" id="WhyGeometry?"/><span class="XrefDestination" id="xref-503083c01-002"/>The Geometric Structure of Data</span></h1>
</header>
<figure class="opener">
<img alt="" src="image_fi/book_art/chapterart.png"/>
</figure>
<p class="ChapterIntro">You might wonder why you need data science methods rooted in topology and geometry when traditional data science methods are already so popular and powerful. The answer to this has two parts. First, data today comes in a variety of formats far more exotic than the usual spreadsheet, such as a social network or a text document. While such exotic data used to be referred to as <em>unstructured</em>, we now recognize that it often is structured, but with a more sophisticated geometric structure than a series of spreadsheets in a relational database. Topological and geometric data science allow us to work directly in these exotic realms and translate them into the more familiar realm of spreadsheets. Second, a relatively recent discovery suggests that geometry even lurks behind <span epub:type="pagebreak" id="Page_2" title="2"/>spreadsheet-structured data. With topological and geometric data science, we can harness the power of this hidden geometry.</p>
<p>We’ll start this chapter with a quick refresher of the main concepts in traditional machine learning, discussing what it means for data to be structured and how this is typically used by machine learning algorithms. We’ll then review supervised learning, overfitting, and the curse of dimensionality from a geometric perspective. Next, we’ll preview a few other common types of data—network data, image data, and text data—and hint at how we can use their geometry in machine learning. If you’re already familiar with traditional machine learning and the challenges of applying it to modern forms of data, you’re welcome to skip to <span class="xref" itemid="xref_target_Chapter 2"><a href="c02.xhtml">Chapter 2</a></span>, where the technical content officially begins, although you may find the geometric perspectives of traditional machine learning topics offered in this chapter interesting regardless.</p>
<h2 id="h1-503083c01-0001"><a class="XrefDestination" id="MachineLearningRoadMap"/><span class="XrefDestination" id="xref-503083c01-003"/>Machine Learning Categories</h2>
<p class="BodyFirst">Many types of machine learning algorithms exist, and more are invented every day. It can be hard to keep track of all the latest developments, but it helps to think of machine learning algorithms as falling into a few basic categories.</p>
<h3 id="h2-503083c01-0001"><a class="XrefDestination" id="SupervisedLearning"/><span class="XrefDestination" id="xref-503083c01-004"/>Supervised Learning</h3>
<p class="BodyFirst"><em>Supervised learning</em> algorithms generally aim to predict something, perhaps a treatment outcome under a new hospital protocol or the probability of a client leaving in the next six months. The variable we predict is called the <em>dependent variable</em> or <em>target</em>, and the variables used to predict it are called <em>independent variables</em> or <em>predictors</em>. When we’re predicting a numerical variable (such as a symptom severity scale), this is called <em>regression</em>; when we’re predicting a categorical variable (such as survived or died classes), this is called <em>classification</em>.</p>
<p>Some of the most popular supervised learning algorithms are <em>k</em>-nearest neighbors (<em>k</em>-NN), naive Bayes classifiers, support vector machines, random forests, gradient boosting, and neural networks. You don’t need to know any of these topics to read this book, but it will help to be familiar with at least one regression method and one classification method, such as linear regression and logistic regression. That said, don’t fret if you’re not sure about them—this chapter will cover the concepts you need.</p>
<p>Each supervised learning algorithm is a specific type of function that has as many input variables as there are independent variables in the data. We think of this function as predicting the value of the dependent variable for any choice of values of the independent variables (see <a href="#figure1-1" id="figureanchor1-1">Figure 1-1</a>). For linear regression with independent variables <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, . . . , <em>x</em><sub><em>n</em></sub>, this is a linear function: <em>f</em>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, . . . , <em>x</em><sub><em>n</em></sub>) =<em> a</em><sub>1</sub><em>x</em><sub>1 </sub>+<em> a</em><sub>2</sub> <em>x</em><sub>2 </sub>+ . . . + <em/><sub><em/></sub>. For other methods, it is a <span epub:type="pagebreak" id="Page_3" title="3"/>complicated nonlinear function. For many methods, this function is nonparametric, meaning we can compute it algorithmically but can’t write it down with a formula.</p>
<figure>
<img alt="" class="" src="image_fi/503083c01/f01001.png"/>
<figcaption><p><a id="figure1-1">Figure 1-1</a>: The flow of independent variables, supervised machine learning algorithm (viewed as a function), and prediction of a dependent variable</p></figcaption>
</figure>
<p>Using a supervised learning algorithm usually involves splitting your data into two sets. There’s <em>training</em> data, where the algorithm tries to adjust the parameters in the function so that the predicted values are as close as possible to the actual values. In the previous linear regression example, the parameters are the coefficients <em>a</em><sub><em>i</em></sub>. There’s also <em>testing</em> data, where we measure how good a job the algorithm is doing. A <em>hyperparameter</em> is any parameter that the user must specify (as opposed to the parameters that are learned directly from the data during the training process). The <em>k</em> specifying number of nearest neighbors in <em>k</em>-NN is an example of a hyperparameter.</p>
<p>After a supervised learning algorithm has been trained, we can use it to make new predictions and to estimate the impact of each independent variable on the dependent variable (called <em>feature importance</em>). Feature importance is helpful for making intervention decisions. For instance, knowing which factors most influence patient death from an infectious disease can inform vaccination strategies when vaccine supplies are limited.</p>
<h3 id="h2-503083c01-0002"><a class="XrefDestination" id="UnsupervisedLearning"/><span class="XrefDestination" id="xref-503083c01-005"/>Unsupervised Learning</h3>
<p class="BodyFirst"><em>Unsupervised learning</em> algorithms tend to focus on data exploration—for example, reducing the dimensionality of a dataset to better visualize it, finding how the data points are related to each other, or detecting anomalous data points. In unsupervised learning, there is no dependent variable—just independent variables. Accordingly, we don’t split the data into training and testing sets; we just apply unsupervised methods to all the data. Applications of unsupervised learning include market segmentation and ancestry group visualization based on millions of genetic markers. Examples of unsupervised learning algorithms include k-means clustering, hierarchical clustering, and principal component analysis (PCA)—but again, you don’t need to know any of these topics to read this book.</p>
<p>Unsupervised and supervised learning can be productively combined. For example, you might use unsupervised learning to find new questions about the data and use supervised learning to answer them. You might use unsupervised learning for dimension reduction as a preprocessing step to improve the performance of a supervised learning algorithm.</p>
<h3 id="h2-503083c01-0003"><span epub:type="pagebreak" id="Page_4" title="4"/><a class="XrefDestination" id="OtherTypesofMachineLearning"/><span class="XrefDestination" id="xref-503083c01-006"/>Matching Algorithms and Other Machine Learning</h3>
<p class="BodyFirst">Another common application of machine learning involves <em>matching algorithms</em>, which compute the distance between points to find similar individuals within a dataset. These algorithms are commonly used to recommend a product to a user; they’re also used in data integrity checks to make sure that sufficient data exists on certain populations before allowing a machine learning algorithm to create a model.</p>
<p>Data integrity is increasingly important with the rise of machine learning and artificial intelligence. If important subgroups within a population aren’t captured well in the data, the algorithm will bias itself toward the majority groups. For example, if language recognition systems don’t have sufficient data from African or Asian language groups, it’s difficult for the system to learn human speech sounds unique to those language groups, such as Khoisan clicks or Mandarin tones. Matching algorithms are also used to try to tease out cause and effect from empirical evidence when a randomized controlled trial is not possible because they allow us to pair up similar participants as though they had been assigned to a treatment group and a placebo group.</p>
<p>We could mention many other types of algorithms, and, in practice, there tends to be overlap between algorithm categories. For instance, YouTube’s recommendation algorithm uses <em>deep learning</em> (which involves machine learning based on neural networks with multiple “hidden layers”) in both a supervised and an unsupervised way, as well as matching algorithms and another pillar of machine learning called <em>reinforcement learning</em> (where algorithms develop strategies on their own by exploring a real or simulated environment—beyond the reach of this book). However, the basic road map to machine learning provided earlier will guide us throughout this book.</p>
<p>Next, let’s take a closer look at the format of the data these algorithms are expecting.</p>
<h2 id="h1-503083c01-0002"><a class="XrefDestination" id="StructuredData"/><span class="XrefDestination" id="xref-503083c01-007"/>Structured Data</h2>
<p class="BodyFirst">Machine learning algorithms, and data science and statistical methods more generally, typically operate on <em>structured data</em> (also called <em>tabular data</em>), which means a spreadsheet-type object (a data frame or matrix) in which the columns are the variables and the rows are the <em>data points</em> (also called <em>instances</em> or <em>observations</em>). These are usually stored in a relational database along with other structured data. The tabular structure is what allows us to talk about independent variables, dependent variables, and data points. A big focus of this book is how to deal with data that doesn’t come presented in this nice format. But even with tabular data, a geometric perspective can be quite useful.</p>
<p>To start, let’s dive into an example that will show how geometry can help us better understand and work with categorical variables in structured data.</p>
<h3 id="h2-503083c01-0004"><span epub:type="pagebreak" id="Page_5" title="5"/><a class="XrefDestination" id="TheGeometryofDummyVariables"/><span class="XrefDestination" id="xref-503083c01-008"/>The Geometry of Dummy Variables</h3>
<p class="BodyFirst"><a href="#figure1-2" id="figureanchor1-2">Figure 1-2</a> shows part of a spreadsheet stored as a Microsoft Excel workbook. The last column here is Outcome, so we view that as the dependent variable; the other three columns are the independent variables. If we used this data for a supervised learning algorithm, it would be a regression task (since the dependent variable is numerical). The first independent variable is binary numerical (taking values 0 and 1), the second independent variable is discrete numerical (taking whole-number values), and the third independent variable is categorical (with three categories of gender). Some algorithms accept categorical variables, whereas others require all variables to be numerical. Unless the categories are ordered (such as survey data with values such as “very dissatisfied,” “dissatisfied,” “satisfied,” and “very satisfied”), the way to convert a categorical variable to numerical is to replace it with a collection of binary <em>dummy variables</em>, which encode each category in a yes/no format represented by the values 1 and 0. In <a href="#figure1-3" id="figureanchor1-3">Figure 1-3</a>, we’ve replaced the gender variable column with two dummy variable columns.</p>
<figure>
<img alt="" class="keyline" src="image_fi/503083c01/f01002.png"/>
<figcaption><p><a id="figure1-2">Figure 1-2</a>: An example of structured data in a Microsoft Excel workbook</p></figcaption>
</figure>
<p>Even for such a common and simple process as this, geometric considerations help illuminate what is going on. If the values of a categorical variable are ordered, then we can convert them to a single numerical variable by placing the values along a one-dimensional axis in a way that reflects the order of these values. For example, the survey values “satisfied,” “very satisfied,” and “extremely satisfied” could be coded as 1, 2, and 3, or if you wanted the difference between “satisfied” and “very satisfied” to be smaller than the difference between “very satisfied” and “extremely satisfied,” then you could code these as, say, 1, 2, and 4.</p>
<span epub:type="pagebreak" id="Page_6" title="6"/><figure>
<img alt="" class="keyline" src="image_fi/503083c01/f01003.png"/>
<figcaption><p><a id="figure1-3">Figure 1-3</a>: A transformed, structured dataset in which the categorical variable has been replaced with two binary dummy variables</p></figcaption>
</figure>
<p>If the categories are not ordered—such as Male, Female, and Non-Binary—we wouldn’t want to force them all into one dimension because that would artificially impose an order on them and would make some of them closer than others (see <a href="#figure1-4" id="figureanchor1-4">Figure 1-4</a>).</p>
<figure>
<img alt="" class="" src="image_fi/503083c01/f01004.png"/>
<figcaption><p><a id="figure1-4">Figure 1-4</a>: Placing the values of a categorical variable in a single dimension makes some of them closer than others.</p></figcaption>
</figure>
<p>Geometrically, we are creating new axes for the different categories when we create dummy variables. There are two ways of doing this. Sometimes, you’ll see people use one dummy variable for each value of the categorical variable, whereas at other times you’ll see people use dummy variables for all but one of the values (as we did in <a href="#figure1-3">Figure 1-3</a>). To understand the difference, let’s take a look at our three-category gender variable.</p>
<p>Using three dummy variables places the categories as the vertices of an equilateral triangle: Male has coordinates (1,0,0), Female has coordinates (0,1,0), and Non-Binary has coordinates (0,0,1). This ensures the categories are all at the same distance from each other. Using only two dummy variables means Male has coordinates (1,0), Female has coordinates (0,1), and <span epub:type="pagebreak" id="Page_7" title="7"/>Non-Binary has coordinates (0,0). This projects our equilateral triangle in three-dimensional space down to a right triangle in the two-dimensional plane, and in doing so it distorts the distances. Male and Female are now closer to Non-Binary than they are to each other, because they are separated by the length √2 <span class="MathInline">≈</span> 1.4 hypotenuse in this isosceles right triangle with side lengths 1 (see <a href="#figure1-5" id="figureanchor1-5">Figure 1-5</a>). Consequently, some machine learning algorithms would mistakenly believe the categories Male and Female are more similar to the category Non-Binary than they are to each other.</p>
<figure>
<img alt="" class="" src="image_fi/503083c01/f01005.png"/>
<figcaption><p><a id="figure1-5">Figure 1-5</a>: Two approaches to creating gender dummy variables. On the left, we have one axis for each category, which ensures the categories are equidistant. On the right, we use only two axes, which causes some categories to be closer than others.</p></figcaption>
</figure>
<p>So why are both dummy variable methods used? Using <em>n</em> dummy variables for an <em>n</em>-value categorical variable rather than <em>n</em> – 1 leads to <em>multicollinearity</em>, which in statistical language is a correlation among the independent variables. The correlation here is that each of the dummy variables is completely and linearly determined by the others. Algebraically, this is a <em>linear dependence</em>, which means one column is a linear combination of the other columns. This linear dependence can be seen geometrically: when placing the <em>n</em> categories in <em>n</em>-dimensional space, they span an (<em>n</em> – 1)-dimensional plane only. In <a href="#figure1-5">Figure 1-5</a>, linear combinations of the three vectors on the left span only the plane containing the triangle, whereas on the right the linear combinations span the full two-dimensional coordinate system.</p>
<p>Multicollinearity causes computational problems for linear and logistic regression, so for those algorithms, we should use <em>n</em> – 1 dummy variables rather than all <em>n</em>. Even for methods that don’t run into this specific computational issue, using fewer independent variables when possible is generally better because this helps reduce the curse of dimensionality—a fundamental topic in data science that we’ll visit from a geometric perspective shortly.</p>
<p>On the other hand, for algorithms like <em>k</em>-NN, where distances between data points are crucial, we don’t want to drop one of the dummy variables, as that would skew the distances (as we saw in <a href="#figure1-5">Figure 1-5</a>) and lead to subpar performance. There is a time and a place for both dummy variable methods, and thinking geometrically can help us decide which to use when.</p>
<p>After using dummy variables to convert all categorical variables to numerical variables, we are ready to consider the geometry of the spreadsheet.</p>
<h3 id="h2-503083c01-0005"><span epub:type="pagebreak" id="Page_8" title="8"/><a class="XrefDestination" id="TheGeometryofNumericalSpreadsheets"/><span class="XrefDestination" id="xref-503083c01-009"/>The Geometry of Numerical Spreadsheets</h3>
<p class="BodyFirst">We can think of a numerical spreadsheet as describing a collection of points (one for each row) in a <em>Euclidean vector space</em>, <b>R</b><sup><em>d</em></sup>, which is a geometric space that looks like a flat piece of paper in two dimensions and a solid brick in three dimensions but extends infinitely in all directions and can be any dimension. Here, <em>d</em> is the number of columns, which is also the dimension of the space. Each column in the numerical dataset represents an axis in this space. Concretely, the <em>d</em>-dimensional coordinates of each data point are simply the values in that row.</p>
<p>When <em>d</em> = 1, this Euclidean vector space <b>R</b><sup><em>d</em></sup> is a line. When <em>d</em> = 2, it is a plane. When <em>d</em> = 3, it is the usual three-dimensional space we are used to thinking in. While humans can’t really visualize more than three perpendicular axes, higher-dimensional geometry can be analyzed with mathematical and computational tools regardless. It is important to recognize here that just as there are many two-dimensional shapes beyond a flat plane (for instance, the surface of a sphere or of a donut, or even stranger ones like a Möbius strip), there are many three-dimensional geometric spaces beyond the familiar Euclidean space (such as the inside of a sphere in three dimensions or the surface of a sphere in four dimensions). This holds for higher-dimensional spaces as well. Working with structured data has traditionally meant viewing the data from the perspective of <b>R</b><sup><em>d</em></sup> rather than any of these other kinds of geometric spaces.</p>
<p>The Euclidean vector space structure of <b>R</b><sup><em>d</em></sup> is powerful; it allows us to compute all kinds of useful things. We can compute the distance between any pair of data points, which is necessary for a wide range of machine learning algorithms. We can compute the line segment connecting any two points, which is used by the Synthetic Minority Oversampling Technique (SMOTE) to adjust training samples with imbalanced classes. We can compute the mean of each coordinate in the data, which is helpful for <em>imputing</em> missing values (that is, filling in missing values with best guesses for the true value based on known data).</p>
<p>However, this nice Euclidean vector space structure is also specific and rigid. Thankfully, we can compute distances between points, shortest paths connecting points, and various forms of interpolation in much more general settings where we don’t have global Euclidean coordinates, including <em>manifolds</em> (geometric objects like spheres that when zoomed in look like the usual Euclidean space but globally can have much more interesting shape and structure—to come in <span class="xref" itemid="xref_target_Chapter 5"><a href="c05.xhtml">Chapter 5</a></span>) and <em>networks</em> (relational structures formally introduced in <span class="xref" itemid="xref_target_Chapter 2"><a href="c02.xhtml">Chapter 2</a></span>).</p>
<p>As a concrete example, suppose you are working with large-scale geospatial data, such as ZIP code–based crime statistics. How do you cluster data points or make any kind of predictive model? The most straightforward approach is to use latitude and longitude as variables to convey the geospatial aspect of the data. But problems quickly arise because this approach projects the round Earth down to a flat plane in a way that <span epub:type="pagebreak" id="Page_9" title="9"/>distorts the distances quite significantly. For instance, longitude ranges from –180° to +180°, so two points on opposite sides of the prime meridian could be very close to each other in terms of miles but extremely far from each other in terms of longitudes (see <a href="#figure1-6" id="figureanchor1-6">Figure 1-6</a>). It’s helpful to have machine learning algorithms that can work on spherical data without the need to map data onto a flat surface.</p>
<figure>
<img alt="" class="" src="image_fi/503083c01/f01006.png"/>
<figcaption><p><a id="figure1-6">Figure 1-6</a>: Using latitude and longitude (left) as variables for geospatial data distorts distances between data points. Shown here, very near points on opposite sides of the prime meridian are represented by very far points in the longitude-latitude plane (right).</p></figcaption>
</figure>
<p>Even when you are working with data that is already structured as a tabular spreadsheet, there might be hidden geometry that is relevant. For example, imagine you have three numerical variables (so that your data lives in <b>R</b><sup>3</sup>) but all your data points live on or near the surface of a sphere in this three-dimensional space. Would you want to consider distances between points as the path lengths along the sphere’s surface (which is what is done in spherical geometry) or straight-line distances that cut through the sphere (which is what is done with traditional Euclidean machine learning)? The answer depends on the context and is something that data scientists must decide based on domain knowledge—it is not typically something that an algorithm should decide on its own. For example, if your data points represent locations in a room that an aerial drone could visit, then Euclidean distance is better; if your data points represent airports across the globe that an international airline services, then spherical geometry is better.</p>
<p>One of the main tasks of topological and geometric data science is discovering the geometric objects on which your data points naturally live (like the sphere in the airport example, but perhaps very complex shapes in high dimensions). The other main task is exploiting these geometric objects, which usually involves one or more of the following:</p>
<ul class="disc">
<li>Applying versions of the usual machine learning algorithms that have been adapted to more general geometric settings</li>
<li>Applying new geometrically powered algorithms that are based on the shape of data</li>
<li>Providing meaningful global coordinates to transform your data into a structured spreadsheet in a way that traditional statistical and machine learning tools can be successfully applied</li>
</ul>
<p><span epub:type="pagebreak" id="Page_10" title="10"/>The main goal of this book is to carefully go through all these ideas and show you how to implement them easily and effectively. But first, in the remainder of this introductory chapter, we’ll explain some of the geometry involved in a few traditional data science topics (like we did earlier with dummy variables). We’ll also hint at the geometry involved in a few different types of “unstructured” data, as a preview of what’s to come later in the book.</p>
<h3 id="h2-503083c01-0006"><a class="XrefDestination" id="TheGeometryofSupervisedLearning"/><span class="XrefDestination" id="xref-503083c01-010"/>The Geometry of Supervised Learning</h3>
<p class="BodyFirst">In this section, we’ll provide a geometric view of a few standard machine learning topics: classification, regression, overfitting, and the curse of dimensionality.</p>
<h4 id="h3-503083c01-0001"><a class="XrefDestination" id="Classification"/><span class="XrefDestination" id="xref-503083c01-011"/>Classification</h4>
<p class="BodyFirst">Once a dataset has been converted to a numerical spreadsheet (with <em>d</em> columns, let’s say), the job of a supervised classifier is to label the predicted class of each new input data point. This can be viewed in terms of <em>decision boundaries</em>, which means we carve up the space <b>R</b><sup><em>d</em></sup> into nonoverlapping regions and assign a class to each region, indicating the label that the classifier will predict for all points in the region. (Note that the same class can be assigned to multiple regions.) The type of geometric shapes that are allowed for the regions is determined by the choice of supervised classifier method, while the particular details of these shapes are learned from the data in the training process. This provides an illuminating geometric window into the classification process.</p>
<p>In <a href="#figure1-7" id="figureanchor1-7">Figure 1-7</a>, we see decision boundaries for a few standard classification algorithms in a simple binary classification example when <em>d</em> = 2.</p>
<figure>
<img alt="" class="" src="image_fi/503083c01/f01007.png"/>
<figcaption><p><a id="figure1-7">Figure 1-7</a>: The decision boundaries in two dimensions for a few classification algorithms</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_11" title="11"/>Logistic regression produces linear decision boundaries. (Though, by adding higher-order terms to the model, you can achieve nonlinear decision boundaries.) Decision trees are built by splitting the independent variables with individual inequalities, which results in decision boundaries made up of horizontal and vertical line segments. In higher dimensions, instead of horizontal and vertical lines, we have planes that are aligned with the coordinate axes. Random forests, which are ensembles of decision trees, still produce decision boundaries of this form, but they tend to involve many more pieces, producing curved-looking shapes that are really made out of lots of small horizontal and vertical segments. The <em>k</em>-NN classifiers produce polygonal decision boundaries since they carve up the space based on which of the finitely many training data points are closest. Neural networks can produce complex, curving decision boundaries; this high level of flexibility is both a blessing and a curse because it can lead to overfitting if you’re not careful (we’ll discuss overfitting shortly).</p>
<p>Studying the decision boundaries produced by different classifier algorithms can help you better understand how each algorithm works; it can also help you choose which algorithm to use based on how your data looks (for higher-dimensional data, where <em>d</em> &gt; 2, you can get two-dimensional snapshots of the data by plotting different pairs of variables). Just remember that there are many choices involved—which variables to use, whether to include higher-order terms, what values to set for the hyperparameters, and so on—and all of these choices influence the types of decision boundaries that are possible. Whenever you encounter a classification algorithm that you aren’t familiar with yet, one of the best ways to develop an intuition for it is to plot the decision boundaries it produces and see how they vary as you adjust the hyperparameters.</p>
<h4 id="h3-503083c01-0002"><a class="XrefDestination" id="Regression"/><span class="XrefDestination" id="xref-503083c01-012"/>Regression</h4>
<p class="BodyFirst">Supervised regression can also be viewed geometrically, though it is a little harder to visualize. Rather than carving up space into finitely many regions based on the class predictions, regression algorithms assign a numerical value to each point in the space; when <em>d</em> = 2, this can be plotted as a heatmap or a three-dimensional surface. <a href="#figure1-8" id="figureanchor1-8">Figure 1-8</a> shows an example of this, where we first create 10 random points with random dependent variable values (shown in the top plot with the circle size indicating the value), then we 3D scatterplot the predicted values for a dense grid of points, and finally we shade according to height when using <em>k</em>-NN with <em>k</em> = 3 (bottom left) and a random forest (bottom right).</p>
<span epub:type="pagebreak" id="Page_12" title="12"/>
<figure>
<img alt="" class="" src="image_fi/503083c01/f01008_m.png"/>
<figcaption><p><a id="figure1-8">Figure 1-8</a>: The training data with dependent variable values indicated by circle size (top plot), and the three-dimensional prediction surfaces for two nonlinear regression algorithms: 3-NN (bottom left) and random forest (bottom right)</p></figcaption>
</figure>
<p>The prediction surface for linear regression (not shown here) is one large, slanted plane, whereas for the two methods illustrated here the surface is a collection of finitely many flat regions for which the prediction value remains constant. Notice that these regions are polygons for <em>k</em>-NN and rectangles for the random forest; this will always be the case. Also, for the particular choice of hyperparameters used in this example, the regions here are smaller for the random forest than for <em>k</em>-NN. Put another way, the random forest here slices up the data space with surgical precision compared to the <em>k</em>-NN algorithm; the latter is more like carving a pumpkin with a butcher’s knife. But this will not always be the case—this comparison of coarseness depends on the number of trees used in the random forest and the number of neighbors used in <em>k</em>-NN. Importantly, a finer partition for regression is like a more flexible decision boundary for classification: it often looks good on the training data but then generalizes poorly to new data. This brings us to our next topic.</p>
<h4 id="h3-503083c01-0003"><span epub:type="pagebreak" id="Page_13" title="13"/><a class="XrefDestination" id="Overfitting"/><span class="XrefDestination" id="xref-503083c01-013"/>Overfitting</h4>
<p class="BodyFirst">Let’s return to the decision boundary plots in <a href="#figure1-7">Figure 1-7</a>. At first glance, it would seem that the more flexible the boundaries are, the better the algorithm performs. This is true when considering the training data, but what really matters is how well algorithms perform on test data. A well-known issue in predictive analytics is <em>overfitting</em>, which is when a predictive algorithm is so flexible that it learns the particular details of the training data and, in doing so, will actually end up performing worse on new unseen data.</p>
<p>In <a href="#figure1-7">Figure 1-7</a>, the logistic regression algorithm misclassifies the leftmost circle, whereas the decision tree creates a long sliver-shaped region to correctly classify that single point. If circles tend to fall on the left and pluses on the right, then it’s quite likely that creating this sliver region will hurt the classification performance overall when it is used on new data—and if so, this is an example of the decision tree overfitting the training data.</p>
<p>In general, as a predictive algorithm’s flexibility increases, the training error tends to keep decreasing until it eventually stabilizes, whereas the test error tends to decrease at first and then reaches a minimum and then increases (see <a href="#figure1-9" id="figureanchor1-9">Figure 1-9</a>). We want the bottom of the test error curve: that’s the best predictive performance we’re able to achieve. It occurs when the algorithm is flexible enough to fit the true shape of the data distribution but rigid enough that it doesn’t learn spurious details specific to the training data.</p>
<figure>
<img alt="" class="" src="image_fi/503083c01/f01009.png"/>
<figcaption><p><a id="figure1-9">Figure 1-9</a>: A plot of training error versus test error as a function of a predictive algorithm’s flexibility, illustrating the concept of overfitting</p></figcaption>
</figure>
<p>The general behavior illustrated in <a href="#figure1-9">Figure 1-9</a> often occurs when varying hyperparameters: the classifier decision boundaries become more flexible as the number of neurons in a neural network increases, as the number of neighbors <em>k</em> in <em>k</em>-NN decreases, as the number of branches in a decision tree increases, and so on.</p>
<h4 id="h3-503083c01-0004"><a class="XrefDestination" id="TheCurseofDimensionality"/><span class="XrefDestination" id="xref-503083c01-014"/>The Curse of Dimensionality</h4>
<p class="BodyFirst">Sometimes, it helps to think of the x-axis in <a href="#figure1-9">Figure 1-9</a> as indicating complexity rather than flexibility. More flexible algorithms tend to be more <span epub:type="pagebreak" id="Page_14" title="14"/>complex, and vice versa. One of the simplest yet most important measures of the complexity of an algorithm is the number of independent variables it uses. This is also called the <em>dimensionality</em> of the data. If the number of independent variables is <em>d</em>, then we can think of the algorithm as inputting points in a <em>d</em>-dimensional Euclidean vector space <b>R</b><sup><em>d</em></sup>. For a fixed number of data points, using too few independent variables tends to cause underfitting, whereas using too many tends to cause overfitting. Thus, <a href="#figure1-9">Figure 1-9</a> can also be interpreted as showing what happens to a predictive algorithm’s error scores as the dimensionality of the data increases without increasing the size of the dataset.</p>
<p>This eventual increase in test error as dimensionality increases is an instance of a general phenomenon known as the <em>curse of dimensionality</em>. When dealing with structured data where the number of columns is large relative to the number of rows (a common situation in genomics, among other areas), overfitting is likely for predictive algorithms, and the numerical linear algebra driving many machine learning methods breaks down. This is an enormous problem, and many techniques have been developed to help counteract it—some of which will come up later in this book. For now, let’s see how geometry can shed some light on the curse of dimensionality.</p>
<p>One way to understand the curse of dimensionality is to think of Euclidean distances, meaning straight-line distance as the bird flies in however many dimensions exist. Imagine two pairs of points drawn on a square sheet of paper, where the points in one pair are near each other and the points in the other pair are far from each other, as in <a href="#figure1-10" id="figureanchor1-10">Figure 1-10</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c01/f01010r.png"/>
<figcaption><p><a id="figure1-10">Figure 1-10</a>: A plot of two pairs of points in a two-dimensional space</p></figcaption>
</figure>
<p>Let’s perturb these points a bit by adding some Gaussian noise; that is, we’ll draw four vectors from a bivariate normal distribution and add these to the coordinates of the four points. Doing this moves the points slightly in random directions. Let’s do this many times, and each time we’ll record the Euclidean distance between the left pair after perturbation and also between the right pair. If the perturbation is large enough, we might occasionally end up with the points on the left farther from each other than the points on the right, but, overall, the Euclidean distances for the left perturbations will be smaller than those for the right perturbations, as we see in the histogram in <a href="#figure1-11" id="figureanchor1-11">Figure 1-11</a>.</p>
<span epub:type="pagebreak" id="Page_15" title="15"/><figure>
<img alt="" class="" src="image_fi/503083c01/f01011.png"/>
<figcaption><p><a id="figure1-11">Figure 1-11</a>: A histogram of the Euclidean distances after random small perturbations for the nearby points on the left side of <a href="#figure1-10">Figure 1-10</a> (shown in light gray) and the faraway points on the right side of that figure (shown in dark gray)</p></figcaption>
</figure>
<p>Next, let’s embed our square sheet of paper as a two-dimensional plane inside a higher-dimensional Euclidean space <b>R</b><sup><em>d</em></sup> and then repeat this experiment of perturbing the points and computing Euclidean distances. In higher dimensions, these perturbations take place in more directions. Concretely, you can think of this as padding the <em>x</em> and <em>y</em> coordinates for our points with <em>d</em>-2 zeros and then adding a small amount of noise to each of the <em>d</em> coordinates. <a href="#figure1-12" id="figureanchor1-12">Figure 1-12</a> shows the resulting histograms of Euclidean distances when doing this process for <em>d</em> = 10 and <em>d</em> = 100.</p>
 <figure>
<img alt="" class="" src="image_fi/503083c01/f01012_m.png"/>
<figcaption><p><a id="figure1-12">Figure 1-12</a>: Histograms of Euclidean distances as in <a href="#figure1-11">Figure 1-11</a>, except after embedding in <em>d</em> = 10 dimensions (left) and <em>d</em> = 100 dimensions (right)</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_16" title="16"/>We see that as the dimension <em>d</em> increases, the two distributions come together and overlap more and more. Consequently, when there is noise involved (as there always is in the real world), adding extra dimensions destroys our ability to discern between the close pair of points and the far pair. Put another way, the signal in your data will become increasingly lost to the noise as the dimensionality of your data increases, unless you are certain that these additional dimensions contain additional signal. This is a pretty remarkable insight that we see revealed here through relatively simple Euclidean geometry!</p>
<p>We can also use perturbations to see why large dimensionality can lead to overfitting. In <a href="#figure1-13" id="figureanchor1-13">Figure 1-13</a>, on the left, we see four points in the plane <b>R</b><sup>2</sup> labeled by two classes in a configuration that is not linearly separable (meaning a logistic regression classifier without higher-order terms won’t be able to correctly class all these points).</p>
<figure>
<img alt="" class="" src="image_fi/503083c01/f01013.png"/>
<figcaption><p><a id="figure1-13">Figure 1-13</a>: Two classes of data points that are not linearly separable in two dimensions (left) but are linearly separable after placing them in three dimensions (right) and perturbing them there</p></figcaption>
</figure>
<p>Even if we perturb the points with a small amount of noise, there will be no line separating the two classes. On the right side of this figure, we have embedded these points in <b>R</b><sup>3</sup> simply by adding a third coordinate to each point that is equal to a constant. (Geometrically, this means we lay the original <b>R</b><sup>2</sup> down flat above the <em>xy</em>-plane in this three-dimensional space.) We then perturb the points a small amount. After this particular perturbation in three dimensions, we see that the two classes do become linearly separable, meaning a logistic regression classifier will be able to achieve 100 percent accuracy here. (In the figure, we have sketched a slanted plane that separates the two classes.)</p>
<p>At first glance, this additional flexibility seems like a good thing (and sometimes it can be!) since it allowed us to increase our training accuracy. But notice that our classifier in three dimensions didn’t learn a meaningful way to separate the classes in a way likely to generalize on new, unseen data. It really just learned the vertical noise from one particular perturbation. <span epub:type="pagebreak" id="Page_17" title="17"/>In other words, increasing the dimensionality of data tends to increase the likelihood that classifiers will fit noise in the training data, and this is a recipe for overfitting.</p>
<p>There is also a geometric perspective of the computational challenges caused by the curse of dimensionality. Imagine a square with a length of 10 units on both sides, giving an area of 100 units. If we add another axis, we’ll get a volume of 1,000 units. Add another, and we’ll have a four-dimensional cube with a four-dimensional volume of 10,000 units. This means data becomes more spread out—sparser—as the dimensionality increases. If we take a relatively dense dataset with 100 points in a low-dimensional space and place it in a space with 1,000 dimensions, then there will be a lot of the space that isn’t near any of those 100 points. Someone wandering about in that space looking for points may not find one without a lot of effort. If there’s a finite time frame for the person to look, they might not find a point within that time frame. Simply put, computations are harder in higher dimensions because there are more coordinates to keep track of and data points are harder to find.</p>
<p>In the following chapters, we’ll look at a few different ways to wrangle high-dimensional data through geometry, including ways to reduce the data’s dimensionality, create algorithms that model the data geometry explicitly to fit models, and calculate distances in ways that work better than Euclidean distance in high-dimensional spaces. The branch of machine learning algorithms designed to handle high-dimensional data is still growing thanks to subjects like genomics and proteomics, where datasets typically have millions or billions of independent variables. It is said that necessity is the mother of invention, and indeed many machine learning methods have been invented out of the necessity of dealing with high-dimensional real-world datasets.</p>
<h2 id="h1-503083c01-0003"><a class="XrefDestination" id="BeyondStructuredData"/><span class="XrefDestination" id="xref-503083c01-015"/>Unstructured Data</h2>
<p class="BodyFirst">Most of the data that exists today does not naturally live in a spreadsheet format. Examples include text data, network data, image data, and even video or sound clip data. Each of these formats comes with its own geometry and analytic challenges. Let’s start exploring some of these types of unstructured data and see how geometry can help us understand and model the data.</p>
<h3 id="h2-503083c01-0007"><a class="XrefDestination" id="NetworkData"/><span class="XrefDestination" id="xref-503083c01-016"/>Network Data</h3>
<p class="BodyFirst">In the next chapter, you’ll get the official definitions related to networks, but you might already have a sense of networks from dealing with social media. Facebook friendships form an undirected network (nodes as Facebook users and edges as friendships among them), and Twitter accounts form a directed network (directional edges because you have both followers and accounts <span epub:type="pagebreak" id="Page_18" title="18"/>you follow). There is nothing inherently Euclidean or spreadsheet structured about network data. In recent years, deep learning has been extended from the usual Euclidean spreadsheet setting to something much more general called <em>Riemannian manifolds</em> (which we’ll get to in <span class="xref" itemid="xref_target_Chapter 5"><a href="c05.xhtml">Chapter 5</a></span>); the main application of this generalization (called <em>geometric deep learning</em>) has been network data, especially for social media analytics.</p>
<p>For instance, Facebook uses geometric deep learning algorithms to automatically detect fake “bot” accounts. In addition to looking at traditional structured data associated with each account such as demographics and number of friends, these detection algorithms use the rich non-Euclidean geometry of each account’s network of friends. Intuitively speaking, it’s easy to create fake accounts that have realistic-looking interests and numbers of friends, but it’s hard to do this in a way such that these accounts’ friendship networks are structured similarly to the organic friendship networks formed by real people. Network geometry provides ways of measuring this notion of “similarity.”</p>
<p>Geometric deep learning has also been used to detect fake news on Twitter by transforming the detailed propagation patterns of stories through the network into independent variables for a supervised learning algorithm. We won’t get to geometric deep learning in this book, but there is still plenty to do and say when it comes to working with network data. For example, we can use geometric properties of a network to extract numerical variables that bring network data back into the familiar territory of structured data.</p>
<h3 id="h2-503083c01-0008"><a class="XrefDestination" id="ImageData"/><span class="XrefDestination" id="xref-503083c01-017"/>Image Data</h3>
<p class="BodyFirst">Another form of “unstructured” data that actually has a rich geometric structure is image data. You can think of each pixel in an image as a numerical variable if the image is grayscale or as three variables if it is color (red, green, and blue values). We can then try to use these variables to cluster images with an unsupervised algorithm or classify them with a supervised algorithm. But the problem when doing this is that there is no spatial awareness. A pair of adjacent pixels is treated the same as a pair of pixels on opposite sides of the image. A large branch of deep learning, called <em>convolutional neural networks</em> (CNNs), has been developed to bring spatial awareness into the picture. CNNs create new variables from the pixel values by sliding small windows around the image. Success in this realm is largely what brought widespread public acclaim to deep learning, as CNNs smashed all the previous records in image recognition and classification tasks.</p>
<p>Let’s consider a simple case of two images that could be included in a larger animal classification dataset used in conservation efforts (see <a href="#figure1-14" id="figureanchor1-14">Figure 1-14</a>).</p>
<span epub:type="pagebreak" id="Page_19" title="19"/>
<figure>
<img alt="" class="" src="image_fi/503083c01/f01014_m.png"/>
<figcaption><p><a id="figure1-14">Figure 1-14</a>: An elephant (left) and a lioness (right) at Kruger National Park</p></figcaption>
</figure>
<p>The animals are shown in natural environments where leaves, branches, and lighting vary. They have different resolutions. The colors of each animal vary. The extant shapes related to both the animals and the other stuff near the animals differ. Manually deriving meaningful independent variables to classify these images would be difficult. Thankfully, CNNs are built to handle such image data and to automatically create useful independent variables.</p>
<p>The basic idea is to consider each image as a mathematical surface (see <a href="#figure1-15" id="figureanchor1-15">Figure 1-15</a>) and then walk across this surface creating a map of its salient features—peaks, valleys, and other relevant geometric occurrences. The next layer in the CNN walks across this map and creates a map of its salient features, which is then fed to the next layer, and so on. In the end, the CNN converts each image to a sequence of maps that hierarchically encode the image’s content, with the final layer being the map that is actually used for classification. For these animal images, the first map might identify high-contrast regions in the image. The next map might assemble these regions into outlines of shapes. The following map might indicate which of these shapes are animals. Another layer might locate specific anatomical features within the animals—and these anatomical features could then form the basis for the final species classification.</p>
<p>The precise way the CNN builds these maps is learned internally through the supervised training process: as the algorithm is fed labeled data, connections between neurons in each layer forge, break, and forge again until the final layer is as helpful as possible for the classification task. We’ll further explore CNNs and their quantum versions in <span class="xref" itemid="xref_target_Chapter 10"><a href="c10.xhtml">Chapter 10</a></span>.</p>
<span epub:type="pagebreak" id="Page_20" title="20"/><figure>
<img alt="" class="" src="image_fi/503083c01/f01015.png"/>
<figcaption><p><a id="figure1-15">Figure 1-15</a>: The head of the lioness in <a href="#figure1-14">Figure 1-14</a>, viewed geometrically as a 3D mathematical surface</p></figcaption>
</figure>
<p>Using methods from computational geometry to quantify peaks and valleys has applications beyond image recognition and classification. A scientist might want to understand the dynamic process or structure of a scientific phenomenon, such as the flow of water or light on an object. The peaks, valleys, and contours of the object impact how light will scatter when it hits the object, and they’ll also determine how liquids would flow down the object. We’ll cover how to mine data for relevant peaks, valleys, and contours later in this book.</p>
<h3 id="h2-503083c01-0009"><a class="XrefDestination" id="TextData"/><span class="XrefDestination" id="xref-503083c01-018"/>Text Data</h3>
<p class="BodyFirst">Another form of “unstructured” data that has risen to prominence in recent years is text data. Here, the structure that comes with the data is not spatial like it is for images; it’s linguistic. State-of-the-art text processing (for instance, used by Google to process search phrases or by Facebook and Twitter to detect posts that violate platform policies) harnesses deep learning to create something called <em>vector embeddings</em>, which translate text into <b/><span epub:type="pagebreak" id="Page_21" title="21"/>Rd, where each word or sentence is represented as a point in a Euclidean vector space. The coordinates of each word or sentence are learned from data by reading vast amounts of text, and the deep learning algorithm chooses them in a way that in essence translates linguistic meaning into geometric meaning. We’ll explore deep learning text embeddings in <span class="xref" itemid="xref_target_Chapter 9"><a href="c09.xhtml">Chapter 9</a></span>.</p>
<p>For example, we might want to visualize different sets of variables concerning text documents. Because the variables form a high-dimensional space, we can’t plot them in a way that humans can visualize. In later chapters, we’ll learn about geometric ways to map high-dimensional data into lower-dimensional spaces such that the data can be visualized easily in a plot. We can decorate these plots with colors or different shapes based on the document type or other relevant document properties. If similar documents cluster together in these plots, it’s likely that some of the variables involved will help us distinguish between documents. New documents with unknown properties but measured values for these variables can then be grouped by a classification algorithm. We’ll explore this further in <span class="xref" itemid="xref_target_Chapter 9"><a href="c09.xhtml">Chapter 9</a></span>.</p>
<h2 id="h1-503083c01-0004"><a class="XrefDestination" id="Summary"/><span class="XrefDestination" id="xref-503083c01-019"/>Summary</h2>
<p class="BodyFirst">This chapter provided a brief review of the main concepts of traditional machine learning, but it put a geometric spin on these concepts that will likely be new for most readers. Woven into this review was a discussion of what it means for data to be structured. The main takeaway is that essentially all data has meaningful structure, but this structure is often of a geometric nature, and geometric tools are needed to put the data into a more traditional spreadsheet format. This is a theme we’ll develop in much more depth throughout the book. This chapter also hinted at some of the important geometry hidden in spreadsheet data. One of the main goals of the book is to show how to use this hidden geometry to improve the performance of machine learning algorithms.</p>
<p>We’ll start in <span class="xref" itemid="xref_target_Chapters 2">Chapters <a href="c02.xhtml">2</a></span> and <span class="xref" itemid="xref_target_3"><a href="c03.xhtml">3</a></span> by diving into algorithms designed for analyzing network data, including social and geographic networks. This includes local and global metrics to understand network structure and the role of individuals in the network, clustering methods developed for use on network data, link prediction algorithms to suggest new edges in a network, and tools for understanding how processes or epidemics spread through networks.</p>
</section>
</body>
</html>