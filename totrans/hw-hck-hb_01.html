<html><head></head><body>
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="1" id="Page_1"/>1</span><br/>
<span class="ChapterTitle">Dental Hygiene: Introduction to Embedded Security</span></h1>
</header>
<figure class="opener">
<img src="image_fi/book_art/chapterart.png" alt=""/>
</figure>
<p class="ChapterIntro">The sheer variety of embedded devices makes studying them fascinating, but that same variety can also leave you scratching your head over yet another shape, package, or weird integrated circuit (IC) and what it means in relation to its security. This chapter begins with a look at various hardware components and the types of software running on them. We then discuss attackers, various attacks, assets and security objectives, and countermeasures to provide an overview of how security threats are modeled. We describe the basics of creating an attack tree you can use both for defensive purposes (to find opportunities for countermeasures) and offensive purposes (to reason about the easiest possible attack). Finally, we conclude with thoughts on coordinated disclosure in the hardware world.</p>
<h2 id="h1-278748c01-0001"><span epub:type="pagebreak" title="2" id="Page_2"/>Hardware Components</h2>
<p class="BodyFirst">Let’s start by looking at the relevant parts of the physical implementation of an embedded device that you’re likely to encounter. We’ll touch on the main bits you’ll observe when first opening a device.</p>
<p>Inside an embedded device is a <em>printed circuit board (PCB)</em> that generally includes the following hardware components: processor, volatile memory, nonvolatile memory, analog components, and external interfaces (see <a href="#figure1-1" id="figureanchor1-1">Figure 1-1</a>).</p>
<figure>
<img src="image_fi/278748c01/f01001.png" alt="f01001"/>
<figcaption><p><a id="figure1-1">Figure 1-1</a>: Typical PCB of an embedded device</p></figcaption>
</figure>
<p>The magic of computation happens in a <em>processor (central processing unit</em>, or <em>CPU)</em>. In <a href="#figure1-1">Figure 1-1</a>, the processor is embedded inside the <em>System-on-Chip (SoC)</em> in the center <span class="CodeAnnotation" aria-label="annotation1">1</span>. Generally, the processor executes the main software and operating system (OS), and the SoC contains additional hardware peripherals.</p>
<p>Usually implemented in dynamic RAM (DRAM) chips in discrete packages,<em> volatile memory</em> <span class="CodeAnnotation" aria-label="annotation2">2</span> is memory that the processor uses while it’s in action; its contents are lost when the device powers down. DRAM memory operates at frequencies close to the processor frequency, and it needs wide buses in order to keep up with the processor.</p>
<p><span epub:type="pagebreak" title="3" id="Page_3"/>In <a href="#figure1-1">Figure 1-1</a>, <em>nonvolatile memory</em> <span class="CodeAnnotation" aria-label="annotation3">3</span> is where the embedded device stores data that needs to persist after power to the device is removed. This memory storage can be in the form of EEPROMs, flash memory, or even SD cards and hard drives. Nonvolatile memory usually contains code for booting as well as stored applications and saved data.</p>
<p>Although not very interesting for security in their own right, the <em>analog components</em>, such as resistors, capacitors, and inductors, are the starting point for <em>side-channel analysis</em> and <em>fault-injection attacks</em>, which we’ll discuss at length in this book. On a typical PCB the analog components are all the little black, brown, and blue parts that don’t look like a chip and may have labels starting with “C,” “R,” or “L.”</p>
<p><em>External interfaces</em> provide the means for the SoC to make connections to the outside world. The interfaces can be connected to other commercial off-the-shelf (COTS) chips as part of the PCB system interconnect. This includes, for example, a high-speed bus interface to DRAM or to flash chips as well as low-speed interfaces, such as I2C and SPI to a sensor. The external interfaces can also be exposed as connectors and pin headers on the PCB; for example, USB and PCI Express (PCIe) are examples of high-speed interfaces that connect devices externally. This is where all communication happens; for example, with the internet, local debugging interfaces, or sensors and actuators. (See <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span> for more details on interfacing with devices.)</p>
<p>Miniaturization allows an SoC to have more <em>intellectual property (IP) blocks</em>. <a href="#figure1-2" id="figureanchor1-2">Figure 1-2</a> shows an example of an Intel Skylake SoC.</p>
<figure>
<img src="image_fi/278748c01/f01002.png" alt="f01002"/>
<figcaption><p><a id="figure1-2">Figure 1-2</a>: Intel Skylake SoC (public domain by Fritzchens Fritz)</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="4" id="Page_4"/>This die contains multiple cores, including the main central processing unit (CPU) cores, the Intel Converged Security and Management Engine (CSME), the graphics processing unit (GPU), and much more. Internal buses in an SoC are harder to access than external buses, making SoCs an inconvenient starting point for hacking. SoCs can contain the following IP blocks:</p>
<p class="ListHead"><b>Several (micro)processors and peripherals</b></p>
<p class="ListBody">For instance, an application processor, a crypto engine, a video accelerator, and the I2C interface driver.</p>
<p class="ListHead"><b>Volatile memory</b></p>
<p class="ListBody">In the form of DRAM ICs stacked on top of the SoC, SRAMs, or register banks.</p>
<p class="ListHead"><b>Nonvolatile memory</b></p>
<p class="ListBody">In the form of on-die read-only memory (ROM), one-time-programmable (OTP) fuses, EEPROM, and flash memory. OTP fuses typically encode critical chip configuration data, such as identity information, lifecycle stage, and anti-rollback versioning information.</p>
<p class="ListHead"><b>Internal buses</b></p>
<p class="ListBody">Though technically just a bunch of microscopic wires, the interconnect between the different components in the SoC is, in fact, a major security consideration. Think of this interconnect as the network between two nodes in an SoC. Being a network, the internal buses could be susceptible to spoofing, sniffing, injection, and all other forms of man-in-the-middle attacks. Advanced SoCs include access control at various levels to ensure that components in the SoC are “firewalled” from each other.</p>
<p>Each of these components is part of the <em>attack surface</em>, the starting point for an attacker, and is therefore of interest. In<span class="xref" itemid="xref_target_ Chapter 2"> Chapter 2</span>, we’ll study these external interfaces more in depth, and in <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span>, we’ll look at ways to find information on the various chips and components.</p>
<h2 id="h1-278748c01-0002">Software Components</h2>
<p class="BodyFirst">Software is a structured collection of CPU instructions and data that a processor executes. For our purposes, it doesn’t matter whether that software is stored in ROM, flash, or on an SD card—although it may come as a disappointment to our elder readers that we will not cover punch cards. Embedded devices can contain some (or none) of the following types of software.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	Although this book focuses on hardware attacks, often a hardware attack is used to compromise software. Via hardware vulnerabilities, attackers can gain access to parts of the software that are normally hard to access or that shouldn’t be accessible at all.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-278748c01-0001"><span epub:type="pagebreak" title="5" id="Page_5"/>Initial Boot Code</h3>
<p class="BodyFirst">The initial boot code is the set of instructions a processor executes when it’s first powered on. The initial boot code is generated by the processor manufacturer and stored in ROM. The main function of <em>boot ROM code</em> is to prepare the main processor to run the code that follows. Normally, it allows a bootloader to execute in the field, including routines for authenticating a bootloader or for supporting alternate bootloader sources (such as through USB). It’s also used for support during manufacturing for personalization, failure analysis, debugging, and self-tests. Often the features available in the boot ROM are configured via <em>fuses</em>, which are one-time programmable bits integrated into the silicon that provide the option to disable some of the boot ROM functionality permanently when the processor leaves the manufacturing facility.</p>
<p>Boot ROM has properties differentiating it from regular code: it is immutable, it is the first code to run on a system, and it must have access to the complete CPU/SoC to support manufacturing, debugging, and chip failure analysis. Developing ROM code requires a lot of care. Because it’s immutable, it’s usually not possible to patch a vulnerability in ROM that is detected post-manufacture (although some chips support <em>ROM</em> <em>patching</em> via fuses). Boot ROM executes before any network functionality is active, so physical access is required to exploit any vulnerabilities. A vulnerability exploited during this phase of boot likely results in direct access to the entire system.</p>
<p>Considering the high stakes for manufacturers in terms of reliability and reputation, in general, boot ROM code is usually small, clean, and well verified (at least it should be).</p>
<h3 id="h2-278748c01-0002">Bootloader</h3>
<p class="BodyFirst">The <em>bootloader</em> initializes the system after the boot ROM executes. It is typically stored on nonvolatile but mutable storage, so it can be updated in the field. The PCB’s original equipment manufacturer (OEM) generates the bootloader, allowing it to initialize PCB-level components. It may also optionally lock down some security features in addition to its primary task of loading and authenticating an operating system or <em>trusted execution environment (TEE)</em>. In addition, the bootloader may provide functionality for provisioning a device or debugging. Being the earliest mutable code to run on a device, the bootloader is an attractive target to attack. Less-secure devices may have a boot ROM that doesn’t authenticate the bootloader, allowing attackers to replace the bootloader code easily.</p>
<p>Bootloaders are authenticated with digital signatures, which are typically verified by embedding a public key (or the hash of a public key) in the boot ROM or fuses. Because this public key is hard to modify, it’s considered the<em> root of trust</em>. The manufacturer signs the bootloader using the private key associated with the public key, so the boot ROM code can verify and trust that the manufacturer produced it. Once the bootloader is <span epub:type="pagebreak" title="6" id="Page_6"/>trusted, it can, in turn, embed a public key for the next stage of code and provide trust that the next stage is authentic. This <em>chain of trust</em><em> </em>can extend all the way down to applications running on an OS (see <a href="#figure1-3" id="figureanchor1-3">Figure 1-3</a>).</p>
<figure>
<img src="image_fi/278748c01/f01003.png" alt="f01003"/>
<figcaption><p><a id="figure1-3">Figure 1-3</a>: Chain of trust—bootloader stages and verification</p></figcaption>
</figure>
<p>Theoretically, creating this chain of trust seems pretty secure, but the scheme is vulnerable to a number of attacks, ranging from exploiting verification weaknesses to fault injection, timing attacks, and more. See Jasper’s talk at Hardwear.io USA 2019 “Top 10 Secure Boot Mistakes” on YouTube (<a href="https://www.youtube.com/watch?v=B9J8qjuxysQ/" class="LinkURL">https://www.youtube.com/watch?v=B9J8qjuxysQ/</a>) for an overview of the top 10 mistakes.</p>
<h3 id="h2-278748c01-0003">Trusted Execution Environment OS and Trusted Applications</h3>
<p class="BodyFirst">At the time of writing, the TEE is a rare feature in smaller embedded devices, but it’s very common in phones and tablets based on systems such as Android. The idea is to create a “virtual” secure SoC by partitioning an entire SoC into “secure” and “nonsecure” worlds. This means that every component on the SoC is either exclusively active in the secure world, exclusively active in the nonsecure world, or is able to switch between the two dynamically. For instance, an SoC developer may choose to put a crypto engine in the secure world, networking hardware in the nonsecure world, and allow the main processor to switch between the two worlds. This could allow the system to encrypt network packets in the secure world and then transmit them via the nonsecure world—that is, the “normal world”—ensuring that the encryption key never reaches the main OS or a user application on the processor.</p>
<p>On mobile phones and tablets, the TEE includes its own operating system, with access to all secure world components. The <em>rich execution environment (REE)</em> includes the “normal world” operating system, such as a Linux or iOS kernel and user applications.</p>
<p>The goal is to keep all nonsecure and complex operations, such as user applications, in the nonsecure world, and all secure operations, such as banking applications, in the secure world. These secure applications are called <em>trusted applications (TAs)</em> . The TEE kernel is an attack target that, once compromised, typically provides complete access to both the secure and nonsecure worlds.</p>
<h3 id="h2-278748c01-0004"><span epub:type="pagebreak" title="7" id="Page_7"/>Firmware Images</h3>
<p class="BodyFirst">Firmware is the low-level software that runs on CPUs or peripherals. Simple peripherals in a device are often fully hardware based, but more complex peripherals can contain a microcontroller that runs firmware. For instance, most Wi-Fi chips require a firmware “blob” to be loaded after power-up. For those running Linux, a look at <em>/lib/firmware</em> shows how much firmware is involved in running PC peripherals. As with any piece of software, firmware can be complex and therefore sensitive to attacks.</p>
<h3 id="h2-278748c01-0005">Main Operating System Kernel and Applications</h3>
<p class="BodyFirst">The main OS in an embedded system can be a general-purpose OS, like Linux, or a real-time OS, like VxWorks or FreeRTOS. Smart cards may contain proprietary OSs that run applications written in Java Card. These OSs can offer security functionality (for example, cryptographic services) and implement <em>process isolation</em>, which means if one process is compromised, another process may still be secure.</p>
<p>An OS makes life easier for software developers who can rely on a broad range of existing functionality, but that may not be a viable option for smaller devices. Very small devices may have no OS kernel but run only one <em>bare-metal</em> program to manage them. This usually implies no process isolation, so compromising one function leads to compromising the entire device.</p>
<h2 id="h1-278748c01-0003">Hardware Threat Modeling</h2>
<p class="BodyFirst">Threat modeling is one of the more important necessities in the defense of any system. Resources for defending a system are not unlimited, so analyzing how those resources are best spent to minimize attack opportunities is essential. This is the road to “good enough” security.</p>
<p>When performing threat modeling, we roughly do the following: take a defensive view to identify the system’s important assets and ask ourselves how those assets should be secured. On the flip side, from an offensive viewpoint, we could identify who the attackers might be, what their goals might be, and what attacks they could choose to attempt. These considerations provide insights into what to protect and how to protect the most valuable assets.</p>
<p>The standard reference work for threat modeling is Adam Shostack’s book <em>Threat Modeling: Designing for Security</em> (Wiley, 2014). The broad field of threat modeling is fascinating, as it includes security of the development environment through to manufacturing, supply chain, shipping, and the operational lifetime. We’ll address the basic aspects of threat modeling here and apply them to embedded device security, focusing on the device itself.</p>
<h3 id="h2-278748c01-0006">What Is Security?</h3>
<p class="BodyFirst">The <em>Oxford English Dictionary</em> defines security as “the state of being free from danger or threat.” This rather binary definition implies that the only secure system is either one that no one would bother to attack or one that can <span epub:type="pagebreak" title="8" id="Page_8"/>defend every threat. The former, we call a <em>brick</em>, because it no longer can boot; the latter, we call a <em>unicorn</em>, because unicorns don’t exist. There is no perfect security, so you could argue that any defense is not worth the effort. This attitude is known as <em>security nihilism</em>. However, that attitude disregards the important fact that a <em>cost-benefit</em> trade-off is associated with each and every attack.</p>
<p>We all understand cost and benefit in terms of money. For an attacker, costs are usually related to buying or renting equipment needed for carrying out attacks. Benefits come in the form of fraudulent purchases, stolen cars, ransomware payouts, and slot machine cash-outs, just to name a few.</p>
<p>The costs and benefits of performing attacks are not exclusively monetary, however. An obvious non-monetary cost is time; a less obvious cost is attacker frustration. For example, an attacker who is hacking for fun may simply move on to another target in the face of frustration. There is surely a defense lesson here. See Chris Domas’s talk at DEF CON 23 for more on this idea: “Repsych: Psychological Warfare in Reverse Engineering.” Nonmonetary benefits include gathering personally identifiable information and fame derived from conference publications or successful sabotage (although those benefits may also be monetized).</p>
<p>In this book, we consider a system “secure enough” if the cost of an attack is higher than the benefit. A system design may not be impenetrable, but it should be hard enough that no one will see an entire attack through to success. In summary, threat modeling is the process of determining how to reach a secure-enough state in a particular device or system. Next, let’s look at several aspects that affect the benefits and costs of an attack.</p>
<h4 id="h3-278748c01-0001">Attacks Through Time</h4>
<p class="BodyFirst">The US National Security Agency (NSA) has a saying: “Attacks always get better; they never get worse.” In other words, attacks get cheaper and stronger over time. This tenet particularly holds at larger timescales, because of increased public knowledge of a target, decreased cost of computing power, and the ready availability of hacking hardware. The time from a chip’s initial design to final production can span several years, followed by at least a year to implement the chip in a device, resulting in three to five years before it’s operational in a commercial environment. This chip may need to remain operational for a few years (in the case of Internet of Things [IoT] products), or 10 years (for an electronic passport), or even for 20 years (in automotive and medical environments). Thus, designers need to take into account whatever attacks might be happening 5 to 25 years hence. This is clearly impossible, so often software fixes have to be pushed out to mitigate unpatchable hardware problems. To put it in perspective, 25 years ago a smart card may have been very hard to break, but after working your way through this book, a 25-year-old smart card should pose little resistance in extracting its keys.</p>
<p>Cost differences also appear on smaller timescales when going from an initial attack to repeating that attack. The <em>identification phase</em> involves identifying vulnerabilities. The <em>exploitation phase</em> follows, which involves using the identified vulnerabilities to exploit a target. In the case of (scalable) <span epub:type="pagebreak" title="9" id="Page_9"/>software vulnerabilities, the identification cost may be significant, but the exploitation cost is almost zero, as the attack can be automated. For hardware attacks, the exploitation cost may still be significant.</p>
<p>On the benefits side, attacks typically have a limited window within which they have value. Cracking Commodore 64 copy protection today provides little monetary advantage. A video stream of your favorite sportsball game has high value only while the game is in progress and before the result is known. The day afterward, its value is significantly lower.</p>
<h4 id="h3-278748c01-0002">Scalability of Attacks</h4>
<p class="BodyFirst">The identification and exploitation phases of software and hardware attacks differ significantly from each other in terms of cost and benefit. The cost of the hardware exploitation phase may be comparable to that of the identification phase, which is uncommon for software. For instance, a securely designed smart card payment system makes use of diversified keys so that finding the key on one card means you learn nothing about the key of another card. If card security is sufficiently strong, attackers need weeks or months and expensive equipment to make a few thousand dollars’ worth of fraudulent purchases on each card. They must repeat the process for every new card to gain the next few thousand dollars. If the cards are that strong, obviously no business case exists for financially motivated attackers; such an attack scales poorly.</p>
<p>On the other hand, consider the Xbox 360 modchips. <a href="#figure1-4" id="figureanchor1-4">Figure 1-4</a> shows the Xenium ICE modchip as the white PCB to the left.</p>
<figure>
<img src="image_fi/278748c01/f01004.png" alt="f01004"/>
<figcaption><p><a id="figure1-4">Figure 1-4</a>: Xenium ICE modchip in an Xbox, used to bypass code verification (photo by Helohe, CC BY 2.5 license)</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="10" id="Page_10"/>A Xenium ICE modchip on the left in <a href="#figure1-4">Figure 1-4</a> is soldered to the main Xbox PCB in order to perform its attack. The board automates a fault injection attack to load arbitrary firmware. This hardware attack is so easily performed, selling modchips could be turned into a business; therefore, we say it “scales well” (<span class="xref" itemid="xref_target_Chapter 13">Chapter 13</span> provides a more detailed description of this attack).</p>
<p>Hardware attackers benefit from economies of scale, but only if the exploitation cost is very low. One example of this is hardware attacks to extract secrets that can then be used at scale, such as recovery of a master firmware update key hidden in hardware facilitating access to a multitude of firmware. Another example is the once-off operation of extracting boot ROM or firmware code, which can expose system vulnerabilities that can be exploited many times over.</p>
<p>Finally, scale is not important for some hardware attacks. For example, hacking once would be sufficient for obtaining an unencrypted copy of a video from a digital rights management (DRM) system that is then pirated, as is the case with launching a single nuclear missile or decrypting a president’s tax returns.</p>
<h3 id="h2-278748c01-0007">The Attack Tree</h3>
<p class="BodyFirst">An <em>attack tree</em> visualizes the steps an attacker takes when going from the attack surface to the ability to compromise an asset, allowing us to analyze an attack strategy systematically. The four ingredients we consider in an attack tree are attackers, attacks, assets (security objectives), and countermeasures (see <a href="#figure1-5" id="figureanchor1-5">Figure 1-5</a>).</p>
<figure>
<img src="image_fi/278748c01/f01005.png" alt="f01005"/>
<figcaption><p><a id="figure1-5">Figure 1-5</a>: Relationship between elements of threat modeling</p></figcaption>
</figure>
<h2 id="h1-278748c01-0004">Profiling the Attackers</h2>
<p class="BodyFirst">Profiling the attackers is important because attackers have motives, resources, and limitations. You could claim that botnets or worms are nonhuman players lacking motivation, but a worm is initially launched by a person pressing <span class="KeyCaps">enter</span> with glee, anger, or greedy anticipation.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	Throughout this book, we use the word <em>device</em> for targets of an attack and <em>equipment</em> for the tools an attacker uses to perform the attack.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Profiling the attacker hinges significantly on the nature of the attack required for a particular type of device. The attack itself determines the necessary equipment and expense required; both factors help profile the <span epub:type="pagebreak" title="11" id="Page_11"/>attacker to some extent. The government wanting to unlock a mobile phone is an example of a costly attack that has a high incentive, such as espionage and state security.</p>
<p>The following are some common attack scenarios and associated motives, characters, and capabilities of the corresponding attackers:</p>
<p class="ListHead"><b>Criminal enterprise</b></p>
<p class="ListBody">Financial gain primarily motivates criminal enterprise attacks. Maximizing profit requires scaling. As discussed before, a hardware attack may be at the root of a scalable attack, which necessitates a well-provisioned hardware attack laboratory. As an example, consider attacks on the pay-TV industry, where pirates have solid business cases that justify millions of dollars’ worth of equipment.</p>
<p class="ListHead"><b>Industry competition</b></p>
<p class="ListBody">An attacker’s motivation in this security scenario ranges from <em>competitive analysis</em> (an innocent euphemism for reverse engineering to see what the competition is doing), to sleuthing IP infringement, to gathering ideas and inspiration for improving one’s own related product. Indirect sabotage by damaging a competitor’s brand image is a similar tactic. This type of attacker is not necessarily an individual but may be part of a team employed (perhaps underground) or externally hired by a company that has all the needed hardware tools.</p>
<p class="ListHead"><b>Nation-states</b></p>
<p class="ListBody">Sabotage, espionage, and counterterrorism are common motivators. Nation-states likely have all the tools, knowledge, and time at their disposal. By the infamous words of James Mickens, if the Mossad (national intelligence agency of Israel) targets you, whatever you do in terms of countermeasures, “you’re still gonna be Mossad’ed upon.”</p>
<p class="ListHead"><b>Ethical hackers</b></p>
<p class="ListBody">Ethical hackers may be a threat, but with a different risk. They may have hardware skills and access to basic tools at home or expensive tools at a local university, making them as well-equipped as malicious attackers. Ethical hackers are drawn to problems where they feel they can make a difference. They can be hobbyists driven to understand how things work, or people who strive to be the best or well known for their abilities. They also can be researchers who trade on their skills for a primary or secondary income, or patriots or protestors who strongly support or oppose causes. An ethical hacker doesn’t necessarily present no risk. One smart lock manufacturer once lamented to us that a big concern of the company was ending up on stage as an example at an ethical hacking event; they perceived this as impacting the trust in their brand. In reality, most criminals will use a brick to “hack” the lock, so lock customers have little risk of a hack, but the slogan “Don’t worry, they’ll use a brick and not a computer,” doesn’t work so well in a public relations campaign.</p>
<p class="ListHead"><b><span epub:type="pagebreak" title="12" id="Page_12"/>Layperson attackers</b></p>
<p class="ListBody">This last type of attacker is typically an individual or small group of people with an axe to grind by way of hurting another individual, company, or infrastructure. They might, however, not always have the technical acumen. Their aim could be financial gain via blackmail or selling trade secrets, or simply to hurt another party. Successful hardware attacks from such attackers are generally unlikely due to limited knowledge and budget. (For all laypersons out there, please don’t DM us on how to break into your ex’s Facebook account.)</p>
<p>Identifying potential attackers is not necessarily clear-cut and depends on the device. In general, it is easier to profile attackers when considering a concrete product versus a product’s component. For instance, the threat of hacking a brand of IoT coffee makers over the internet to produce a weak brew could be linked to the various attacker types just listed. Profiling becomes more complex higher up the supply chain of a device. A component in IoT devices may be an <em>advanced encryption standard (AES)</em> accelerator provided by an IP vendor. This accelerator is integrated in an SoC, which is integrated onto a PCB, from which a final device is made. How would the IP vendor of the AES accelerator identify the threats on the 1,001 different devices using that AES accelerator? The vendor would need to concentrate more on the type of attack than on the attackers (for instance, by implementing a degree of resistance against side-channel attacks).</p>
<p>When you design a device, we strongly advise you to ascertain from your component suppliers what attack types have been guarded against. Threat modeling without that knowledge cannot be thorough, and perhaps more important, if suppliers aren’t queried on this, they won’t be motivated to improve their security measures.</p>
<h2 id="h1-278748c01-0005">Types of Attacks</h2>
<p class="BodyFirst">Hardware attacks obviously target hardware, such as opening up a <em>Joint Test Action Group (JTAG)</em> debugging port, but they may also target software, such as bypassing password verification. This book does not address software attacks on software, but it does address using software to attack hardware.</p>
<p>As mentioned previously, the attack surface is the starting point for an attacker—the directly accessible bits of hardware and software. When considering the attack surface, we usually assume full physical access to the device. However, being within Wi-Fi range (<em>proximate range</em>) or being connected through any network (<em>remote</em>) can also be a starting point for an attack.</p>
<p>The attack surface may start with the PCB, whereas a more skilled attacker may extend the attack surface to the chip using decapping and microprobing techniques, as described later in this chapter.</p>
<h3 id="h2-278748c01-0008">Software Attacks on Hardware</h3>
<p class="BodyFirst">Software attacks on hardware use various software controls over hardware or the monitoring of hardware. There are two subclasses of software attacks on hardware: fault injection and side-channel attacks.</p>
<h4 id="h3-278748c01-0003"><span epub:type="pagebreak" title="13" id="Page_13"/>Fault Injection</h4>
<p class="BodyFirst"><em>Fault injection</em> is the practice of pushing hardware to a point that induces processing errors. A fault injection by itself is not an attack; it’s what you do with the effect of the fault that turns it into an attack. Attackers attempt to exploit these artificially produced errors. For example, they can obtain privileged access by bypassing security checks. The practice of injecting a fault and then exploiting the effect of that fault is called a <em>fault attack</em>.</p>
<p><em>DRAM hammering</em> is a well-known fault injection technique in which the DRAM memory chip is bombarded with an unnatural access pattern in three adjacent rows. By repeatedly activating the outer two rows, bit flips occur in the center <em>victim row</em>. The <em>Rowhammer attack</em> exploits DRAM bit flips by causing victim rows to be page tables. <em>Page tables</em> are structures maintained by an operating system that limit the memory access of applications. By changing access control bits or physical memory addresses in those page tables, an application can access memory it normally would not be able to access, which easily leads to privilege escalation. The trick is to massage the memory layout such that the victim row with page tables is in between attacker-controlled rows and then activate these rows from high-level software. This method has been proven possible on the x86 and ARM processors, from low-level software all the way up to JavaScript. See the article “Drammer: Deterministic Rowhammer Attacks on Mobile Platforms” by Victor van der Veen et al. for more information.</p>
<p><em>CPU overclocking</em> is another fault-injection technique. Overclocking the CPU causes a temporary fault called a <em>timing fault</em> to occur. Such a fault can manifest itself as a bit error in a CPU register. <em>CLKSCREW</em>  is an example of a CPU overclocking attack. Because software on mobile phones can control the CPU frequency as well as the core voltage, by lowering the voltage and momentarily increasing the CPU frequency, an attacker can induce the CPU to make faults. By timing this correctly, attackers can generate a fault in the RSA signature verification, which allows them to load improperly signed arbitrary code. For more information, see “CLKSCREW: Exposing the Perils of Security-Oblivious Energy Management” by Adrian Tang et al.</p>
<p>You can find these kinds of vulnerabilities anywhere software can force hardware to run outside normal operating parameters. We expect further variants will continue to emerge.</p>
<h4 id="h3-278748c01-0004">Side-Channel Attacks</h4>
<p class="BodyFirst">Software timing relates to the amount of wall-clock time required for a processor to complete a software task. In general, more complex tasks need more time. For example, sorting a list of 1,000 numbers takes longer than sorting a list of 100 numbers. It should be no surprise that an attacker can use software execution time as a handle for an attack. In modern embedded systems, it is easy for an attacker to measure the execution time, often down to the resolution of a single clock cycle! This leads to <em>timing attacks</em>, in which an attacker tries to relate software execution time to the value of internal secret information.</p>
<p><span epub:type="pagebreak" title="14" id="Page_14"/>For instance, the <code>strcmp</code> function in C determines whether two strings are the same. It compares characters one by one, starting at the front, and when it encounters a differing character, it terminates. When using <code>strcmp</code> to compare an entered password to a stored password, the duration of <code>strcmp</code>’s execution leaks information about the password, as it terminates upon finding the first nonmatching character between the attacker’s candidate password and the password protecting the device. The <code>strcmp</code> execution time therefore leaks the number of initial characters in the password that are correct. (We detail this attack in <span class="xref" itemid="xref_target_Chapter 8">Chapter 8</span> and describe the proper way of implementing this comparison in <span class="xref" itemid="xref_target_Chapter 14">Chapter 14</span>.)</p>
<p><em>RAMBleed</em> is another side-channel attack that can be launched from software, as demonstrated by Kwong et al. in “RAMBleed: Reading Bits in Memory Without Accessing Them.” It uses the Rowhammer-style weaknesses to read bits from DRAM. In a RAMBleed attack, the flips happen in an attacker’s row based on the data in victim rows. This way, an attacker can observe the memory contents of another process.</p>
<h4 id="h3-278748c01-0005">Microarchitectural Attacks</h4>
<p class="BodyFirst">Now that you understand the principle of timing attacks, consider the following. Modern-day CPUs are fast because of the huge number of optimizations that have been identified and implemented over the years. A cache, for instance, is built on the premise that recently accessed memory locations are soon likely to be accessed again. Therefore, the data at those memory locations is stored physically closer to the CPU for faster access. Another example of an optimization arose from the insight that the result of multiplying a number <em>N</em> by 0 or 1 is trivial, so performing the full multiplication calculation isn’t needed, as the answer is always simply 0 or <em>N</em>. Such optimizations are part of the <em>microarchitecture</em>, which is the hardware implementation of an instruction set.</p>
<p>However, this is where optimizations for speed and security are at odds. If the optimization is activated related to some secret value, that optimization may hint at values in the data. For instance, if a multiplication of <em>N</em> times <em>K</em> for an unknown <em>K</em> is sometimes faster than other times, the value of <em>K</em> could be 0 or 1 in the fast cases. Or, if a memory region is cached, it can be accessed faster, so a fast access means a particular region has been accessed recently.</p>
<p>The notorious <em>Spectre</em> attack from 2018 exploits a neat optimization called <em>speculative execution</em>. Computing whether a conditional branch should be taken or not takes time. Instead of waiting for the branch condition to be computed, speculative execution guesses the branch condition and executes the next instructions as if the guess is correct. If the guess is correct, the execution simply continues, and if the guess is incorrect, the execution will be rolled back. This speculative execution, however, still affects the state of the CPU caches. Spectre forces a CPU to perform a speculative operation that affects the cache in a way that depends on some secret value, and then it uses a cache timing attack to recover the secret. As shown in “Spectre Attacks: Exploiting Speculative Execution,” by Paul Kocher et al., <span epub:type="pagebreak" title="15" id="Page_15"/>we can use this trick in some existing or crafted programs to dump the entire process memory of a victim process. The larger issue at hand is that processors have been optimized for speed in this way for decades, and there are many optimizations that may be exploited similarly.</p>
<h3 id="h2-278748c01-0009">PCB-Level Attacks</h3>
<p class="BodyFirst">The PCB is often the initial attack surface for devices, so it’s crucial for attackers to learn as much as possible from the PCB design. The design provides clues as to where exactly to hook into the PCB or reveals where better attack points are located. For example, to reprogram a device’s firmware (potentially enabling full control over a device), the attacker first needs to identify the firmware programming port on the PCB.</p>
<p>For PCB-level attacks, all that’s needed to access many devices is a screwdriver. Some devices implement physical tamper resistance and tamper response, such as FIPS (Federal Information Processing Standard) 140 level 3 or 4 validated devices or payment terminals. Although it’s an interesting sport in itself, bypassing tamper-proofing and getting to the electronics is beyond the scope of this book.</p>
<p>One example of a PCB-level attack is taking advantage of SoC options that are configured by pulling certain pins high or low using <em>straps</em>. The straps are visible on the PCB as 0 Ω (zero-ohm) resistors (see <a href="#figure1-6" id="figureanchor1-6">Figure 1-6</a>). These SoC options may well include debug enablement, booting without signature checking, or other security-related settings.</p>
<figure>
<img src="image_fi/278748c01/f01006.png" alt="f01006"/>
<figcaption><p><a id="figure1-6">Figure 1-6</a>: Zero-ohm resistors (R29 and R31)</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="16" id="Page_16"/>Adding or removing the straps to change configuration is trivial. Although modern multilayer PCBs and surface-mount devices complicate modifications, all you need are a steady hand, a microscope, tweezers, a heat gun, and, above all, patience to complete the task.</p>
<p>Another useful attack at the PCB level is to read the flash chip on a PCB, which typically contains most of the software that runs in the device, revealing a treasure trove of information. Although some flash devices are read-only, most allow you to write critical changes back to them in a way that removes or limits security functions. The flash chip likely enforces read-only permissions via some access control mechanism, which may be susceptible to fault injection.</p>
<p>For systems designed with security in mind, changes to flash should result in a non-bootable system because the flash image needs to include a valid digital signature. Sometimes the flash image is scrambled or encrypted; the former can be reversed (we’ve seen simple XORs), and the latter requires acquiring the key.</p>
<p>We’ll discuss PCB reverse engineering in more detail in <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span>, and we’ll discuss controlling the clock and power when we look at interfacing with real targets.</p>
<h3 id="h2-278748c01-0010">Logical Attacks</h3>
<p class="BodyFirst"><em>Logical attacks</em> work at the level of logical interfaces (for instance, by communicating through existing I/O ports). Unlike a PCB-level attack, a logical attack does not work at the physical level. A logical attack is aimed at the embedded device’s software or firmware and tries to breach the security without physical hacking. You could compare it to breaking into a house (device) by realizing that the owner (software) has a habit of leaving the back door (interface) unlocked; hence, no lockpicking is needed.</p>
<p>Famous logical attacks revolve around memory corruption and code injection, but logical attacks have a much wider scope. For example, if the debugging console is still available on a hidden serial port of an electronic lock, sending the “unlock” command may trigger the lock to open. Or, if a device powers down some countermeasures in low-power conditions, injecting low-battery signals can disable those security measures. Logical attacks target design errors, configuration errors, implementation errors, or features that can be abused to break the security of a system.</p>
<h4 id="h3-278748c01-0006">Debugging and Tracing</h4>
<p class="BodyFirst">Among the most powerful control mechanisms built into a CPU during design and manufacture are the hardware debugging and tracing functions. This is often implemented on top of a <em>Joint Test Action Group (JTAG)</em><em> </em>or<em> Serial Wire Debug (SWD)</em> interface. <a href="#figure1-7" id="figureanchor1-7">Figure 1-7</a> shows an exposed JTAG header.</p>
<p>Be aware that on secure devices, fuses, a PCB strap, or some proprietary secret code or challenge/response mechanism can turn off debugging and tracing. Perhaps only the JTAG header is removed on less-secure devices (more on JTAG in the following chapters).</p>
<span epub:type="pagebreak" title="17" id="Page_17"/><figure>
<img src="image_fi/278748c01/f01007.png" alt="f01007"/>
<figcaption><p><a id="figure1-7">Figure 1-7</a>: PCB with exposed JTAG header. Normally, it’s not labeled as nicely as in this example!</p></figcaption>
</figure>
<h4 id="h3-278748c01-0007">Fuzzing Devices</h4>
<p class="BodyFirst"><em>Fuzzing</em> is a technique borrowed from software security that aims at identifying security problems in code specifically. Fuzzing’s typical goal is to find crashes to exploit for code injection. <em>Dumb fuzzing</em> amounts to sending random data to a target and observing its behavior. Robust and secure targets remain stable under such an attack, but less-robust or less-secure targets may show abnormal behavior or crash. Crash dumps or a debugger inspection can pinpoint the source of a crash and its exploitability. <em>Smart fuzzing</em> focuses on protocols, data structures, typical crash-causing values, or code structure and is more effective at generating <em>corner cases</em> (situations that should not normally be expected) that will crash a target. <em>Generation-based fuzzing </em>creates inputs from scratch, whereas <em>mutation-based fuzzing </em>takes existing inputs and modifies them.<em> Coverage-guided fuzzing</em> uses additional data (for instance, coverage information about which parts of the program are exercised with a particular input) to allow you to find deeper bugs.</p>
<p>You also can apply fuzzing to devices, though under much more challenging circumstances as compared to fuzzing software. With device fuzzing, it is typically much harder to obtain coverage information about the software running on it, because you may have much less control over that software. Fuzzing over an external interface without further control over the device disallows obtaining coverage information, and in some cases, doing so makes establishing whether a corruption occurred difficult. Finally, fuzzing is effective when it can be done at high speed. In software fuzzing, this can be thousands to millions of cases per second. Achieving this performance is nontrivial on embedded devices. <em>Firmware re-hosting</em><em> </em>is a technique that takes a device’s firmware and puts it in an emulation environment that can be run on PCs. It resolves most of the issues with on-device fuzzing, at the cost of having to create a working emulation environment.</p>
<h4 id="h3-278748c01-0008"><span epub:type="pagebreak" title="18" id="Page_18"/>Flash Image Analysis</h4>
<p class="BodyFirst">Most devices include flash chips that are external to the main CPU. If a device is software-upgradeable, you can often find firmware images on the internet. Once you’ve obtained an image, you can use various flash image analysis tools, such as <em>binwalk</em>, to help identify the various parts of the image, including code sections, data sections, filesystem(s), and digital signatures.</p>
<p>Finally, disassembly and decompiling of the various software images is very important in determining possible vulnerabilities. There is also some initial interesting work regarding static analysis (such as concolic execution) of device firmware. See “BootStomp: On the Security of Bootloaders in Mobile Devices” by Nilo Redini et al.</p>
<h3 id="h2-278748c01-0011">Noninvasive Attacks</h3>
<p class="BodyFirst">Noninvasive attacks don’t physically modify a chip. Side-channel attacks use some measurable behavior of a system to disclose secrets (for example, measuring a device’s power consumption to extract an AES key). A fault attack uses fault injection into the hardware to circumvent a security mechanism; for example, a large electromagnetic (EM) pulse can disable a password verification test so that it accepts any password. (Chapters <span class="xref" itemid="xref_target_4">4</span> and <span class="xref" itemid="xref_target_5">5</span> of this book are devoted to these topics.)</p>
<h3 id="h2-278748c01-0012">Chip-Invasive Attacks</h3>
<p class="BodyFirst">This class of attack targets the package or silicon inside a package and, therefore, operates at a miniature scale—that of the wires and gates. Doing this requires much more sophisticated, advanced, and expensive techniques and equipment than we’ve discussed so far. Such attacks are beyond the scope of this book, but here’s a brief look at what advanced attackers can do.</p>
<h4 id="h3-278748c01-0009">Decapsulation, Depackaging, and Rebonding</h4>
<p class="BodyFirst"><em>Decapsulation</em> is the process of removing some of the IC packaging material using chemical warfare, usually by dripping fuming nitric or sulfuric acid onto the chip package until it dissolves. The result is a hole in the package through which you can examine the microchip itself, and if you do it properly, the chip still works.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	You can do decapsulation at home as long as a chemical hood and other safety features are in place. For the brave, the gospel of <em>PoC||GTFO</em> from No Starch Press contains details on how to perform decapsulation domestically.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>When <em>depackaging</em>, you dunk the whole package in acid, after which the entire chip is laid bare. You need to rebond the chip to restore its functionality, which means reattaching the tiny wires that normally connect the chip to the pins of a package (see <a href="#figure1-8" id="figureanchor1-8">Figure 1-8</a>).</p>
<span epub:type="pagebreak" title="19" id="Page_19"/><figure>
<img src="image_fi/278748c01/f01008.png" alt="f01008"/>
<figcaption><p><a id="figure1-8">Figure 1-8</a>: A decapped chip that shows exposed bonding wires (Travis Goodspeed, CC BY 2.0 license)</p></figcaption>
</figure>
<p>Even though they may die in the process, dead chips are fine for imaging and optical reverse engineering. However, for most attacks, chips must be alive.</p>
<h4 id="h3-278748c01-0010">Microscopic Imaging and Reverse Engineering</h4>
<p class="BodyFirst">Once the chip is exposed, the first step is to identify the larger functional blocks of the chip and, specifically, find the blocks of interest. <a href="#figure1-2">Figure 1-2</a> shows some of these structures. The largest blocks on the die will be memory, like static RAM (SRAM) for CPU caches or tightly coupled memory, and ROM for boot code. Any long, mostly straight bunches of lines are buses interconnecting CPUs and peripherals. Simply knowing the relative sizes and what the various structures look like allows you to begin reverse engineering chips.</p>
<p>When a chip is decapped, as in <a href="#figure1-8">Figure 1-8</a>, you can see only the top metal layer. To reverse engineer the entire chip, you also need to <em>delayer</em> it, which means polishing off the chip’s individual metal layers to expose the one below it.</p>
<p><a href="#figure1-9" id="figureanchor1-9">Figure 1-9</a> shows a cross section of a <em>complementary metal oxide semiconductor (CMOS) </em>chip, which is how most modern chips are built. As you can see, a number of layers and vias of copper metals eventually connect the transistors (polysilicon/substrate). The lowest-level metal is used for creating <em><span epub:type="pagebreak" title="20" id="Page_20"/>standard cells</em>, which are the elements that create logical gates (AND, XOR, and so on) from a number of transistors. Top-level metals are usually used for power and clock routing.</p>
<figure>
<img src="image_fi/278748c01/f01009.png" alt="f01009"/>
<figcaption><p><a id="figure1-9">Figure 1-9</a>: Cross section of CMOS</p></figcaption>
</figure>
<p><a href="#figure1-10" id="figureanchor1-10">Figure 1-10</a> shows photographs of the different layers inside a typical chip.</p>
<figure>
<img src="image_fi/278748c01/f01010.png" alt="f01010"/>
<figcaption><p><a id="figure1-10">Figure 1-10</a>: Different layers inside a CMOS chip (image courtesy of Christopher Tarnovsky, <a href="http://mailto:semiconductor.guru@gmail.com">semiconductor.guru@gmail.com</a>)</p></figcaption>
</figure>
<p>Good chip imaging allows you to rebuild a netlist from the images or a binary dump of the boot ROM. A <em>netlist</em> is essentially a description of how all gates are connected, which encompasses all the digital logic in a design. Both a netlist and a boot ROM dump allow attackers to find weaknesses in the code or chip design. Chris Gerlinsky’s “Bits from the Matrix: Optical ROM Extraction” and Olivier Thomas’s “Integrated Circuit Offensive Security,” presented at the Hardwear.io 2019 conference, provide good introductions to the topic.</p>
<h4 id="h3-278748c01-0011">Scanning Electron Microscope Imaging</h4>
<p class="BodyFirst">A <em>scanning electron microscope (SEM) </em>performs a raster scan of a target using an electron beam and takes measurements from an electron detector to form an image of the scanned target with a resolution of better than 1 nm, allowing you to image individual transistors and wires. As with microscope imaging, you can create netlists from the images.</p>
<h4 id="h3-278748c01-0012"><span epub:type="pagebreak" title="21" id="Page_21"/>Optical Fault Injection and Optical Emission Analysis</h4>
<p class="BodyFirst">Once a chip surface is visible, it’s possible to have “phun with photons.” Due to an effect called <em>hot carrier luminescence</em>, switching transistors occasionally emit photons. With an IR-sensitive <em>charge-coupled device (CCD)</em> sensor like those used in hobbyist astronomy, or an <em>avalanche photodiode (APD)</em> if you want to get fancy, you can detect active photon areas, which contributes to the reverse engineering process (or more specifically to side-channel analysis), as in correlating secret keys with photon measurements. See “Simple Photonic Emission Analysis of AES: Photonic Side Channel Analysis for the Rest of Us” by Alexander Schlösser et al.</p>
<p>In addition to using photons to observe processes, you can also use them to inject faults by changing the gates’ conductivity, which is called <em>optical fault injection</em> (see <span class="xref" itemid="xref_target_Chapter 5">Chapter 5</span> and <span class="xref" itemid="xref_target_Appendix A">Appendix A</span> for more details).</p>
<h4 id="h3-278748c01-0013">Focused Ion Beam Editing and Microprobing</h4>
<p class="BodyFirst">A <em>focused ion beam (FIB)</em> , pronounced “fib,” uses a beam of ions either to mill away parts of a chip or deposit material onto a chip at a nanometer scale, allowing attackers to cut chip wires, re-route chip wires, or create probe pads for microprobing. FIB edits take time and skill (and an expensive FIB), but as you can imagine, such edits can circumvent many hardware security mechanisms if an attacker is able to locate them. The numbers in <a href="#figure1-11" id="figureanchor1-11">Figure 1-11</a> show holes a FIB created in order to access lower metal layers. The “hat” structures around the holes are created to bypass an active shield countermeasure.</p>
<p><em>Microprobing</em> is a technique used to measure or inject current into a chip wire, which may not require a FIB probe pad for larger feature sizes. Skill is a prerequisite for performing any of these attacks, although once an attacker has the resources to perform attacks at this level, it is extraordinarily difficult to maintain security.</p>
<figure>
<img src="image_fi/278748c01/f01011.png" alt="f01011"/>
<figcaption><p><a id="figure1-11">Figure 1-11</a>: A number of FIB edits to facilitate microprobing (image courtesy of Christopher Tarnovsky, <a href="http://mailto:semiconductor.guru@gmail.com">semiconductor.guru@gmail.com</a>)</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="22" id="Page_22"/>We’ve covered a number of different attacks relative to embedded systems here. Remember that any single attack is enough to compromise a system. The cost and skills vary drastically, however, so be sure to understand what sort of security objective you require. Resisting an attack from someone with a million-dollar budget and resisting an attack from someone with $25 and a copy of this book are very different endeavors.</p>
<h2 id="h1-278748c01-0006">Assets and Security Objectives</h2>
<p class="BodyFirst">The question to ask when considering the assets being designed into the product is, “What assets do I really care about?” An attacker will ask the same question. The assets’ defender might arrive at a wide range of answers to this seemingly simple question. The CEO of a company may focus on brand image and financial health. The chief privacy officer cares about the confidentiality of consumers’ private information, and the cryptographer in residence is paranoid about secret key material. All of those responses to the question are interrelated. If keys are exposed, customer privacy may be impacted, which in turn negatively impacts brand image and consequently threatens the financial health of the entire company. However, at each level, the protection mechanisms differ.</p>
<p>An asset also represents a value to an attacker. What exactly is valuable depends on the attacker’s motivation. It might be a vulnerability that allows the attacker to sell a code execution exploit to other attackers. The desired asset could be credit-card details or victims’ payment keys. Corporate-world intentions might be to target a competitor’s brand maliciously.</p>
<p>When threat modeling, analyze both attacker and defender perspectives. For the purposes of this book, we limit ourselves to technical assets on a device, so we assume that our assets are represented as some sequence of bits on a target device that are to remain confidential and integrity-protected. <em>Confidentiality</em> is the property of keeping an asset hidden from attackers, and <em>integrity</em> is the property of not allowing an attacker to modify it.</p>
<p>As a security enthusiast, you may be wondering why we didn’t mention availability. <em>Availability</em> is the property of maintaining a responsive and functional system, and it’s particularly important for data centers and systems that deal with safety, such as industrial control systems and autonomous vehicles, where interruption in system functionality can’t happen.</p>
<p>It makes sense to defend asset availability only in situations when a device cannot be physically accessed, such as when access is via the network and internet. Making such services unavailable is the purpose of denial-of-service attacks that take down websites. For embedded devices, compromising availability is trivial: just switch it off, hit it with a hammer, or blow it up.</p>
<p>A <em>security objective</em> is how well you want to protect the assets you define, against what types of attacks and attackers, and for how long. Defining security objectives helps focus the design arguments on the strategies to counter the expected threats. Inevitably trade-offs will occur due to many possible scenarios, and although we realize there are no one-size-fits-all solutions, we give some common examples next.</p>
<p><span epub:type="pagebreak" title="23" id="Page_23"/>Though not very common, specification of a device’s strengths and weaknesses is a sure sign of security maturity of a vendor.</p>
<h3 id="h2-278748c01-0013">Confidentiality and Integrity of Binary Code</h3>
<p class="BodyFirst">Typically, for binary code, the main objective is integrity protection or making sure the code that runs on the device is the code the author intended. Integrity protection restricts code modification but presents a double-edged sword. Strong integrity protection can lock down a device from its owner, limiting the code available to run on it. A whole community of hackers tries to circumvent these mechanisms on gaming consoles in order to run their own code. On the other hand, integrity protection certainly has the unintended benefit of protecting against malware infecting the boot chain, game piracy, or governments installing a backdoor.</p>
<p>The goal for confidentiality as a security objective is to make it more difficult to copy intellectual property, such as digital content, or to find vulnerabilities in firmware. The latter also makes it harder for bona fide security researchers to find and report vulnerabilities, as well as for attackers to exploit those vulnerabilities. (See the “Disclosing Security Issues” section on page <span class="xref" itemid="xref_target_33">33</span> for more on this complex dilemma.)</p>
<h3 id="h2-278748c01-0014">Confidentiality and Integrity of Keys</h3>
<p class="BodyFirst">Cryptography turns data protection problems into key protection problems. In practice, keys are typically easier to protect than full data blobs. For threat modeling, note that there are now two assets: the plaintext data and the key itself. Confidentiality of keys as an objective, therefore, usually links to confidentiality of the data that is being protected.</p>
<p>For example, integrity is important when public keys are stored on a device for authenticity checks: if attackers can substitute the original public keys with their own, they can sign arbitrary data that passes signature verification on the device. However, integrity is not always an objective for keys; for instance, if the purpose of a key is to decrypt a stored data blob, modifying the key simply results in the inability to perform the decryption.</p>
<p>Another interesting aspect is how keys are securely injected into a device or generated at the manufacturing stage. An option is to encrypt or sign the keys themselves, but that involves yet another key. It’s turtles all the way down. Somewhere in the system exists a <em>root of trust</em>, a key or mechanism that we simply have to trust.</p>
<p>A typical solution is to trust the manufacturing process during initial key generation or during key injection. For instance, <em>Trusted Platform Module (TPM) </em>specification v2.0 calls for an <em>endorsement primary seed (EPS)</em>. This EPS is a unique identifier for each TPM, and it is used to derive some primary key material. As per the specification, this EPS must be injected into the TPM or created on the TPM during manufacturing.</p>
<p>This practice does limit exposure of key material, but it creates a critical central collection point for key material at the manufacturing facility. Key injection systems especially must be well protected to avoid compromising the injected keys for <em>all</em> parts being configured by this system. Best <span epub:type="pagebreak" title="24" id="Page_24"/>practices involve key generation on-device, such that the manufacturing facility doesn’t have access to all keys, as well as secret splitting, making sure that different stages in manufacturing inject or generate different parts of the key material.</p>
<h3 id="h2-278748c01-0015">Remote Boot Attestation</h3>
<p class="BodyFirst"><em>Boot attestation</em> is the ability to verify cryptographically that a system did in fact boot from authentic firmware images. <em>Remote boot attestation</em><em> </em>is the ability to do so remotely. Two parties are involved in attestation: the <em>prover </em>intends to prove to the <em>verifier</em> that some <em>measurements </em>of the system have not been tampered with. For instance, you can use remote boot attestation to allow or deny a device access to an enterprise network or to decide to provide an online service to a device. In the latter case, the device is the prover, the online service is the verifier, and the measurements are hashes of configuration data and (firmware) images used during boot. To prove the measurements are not tampered with, they are digitally signed using a private key during the boot stages. The verifier can check the signatures against an allow or block list and should have a means of verifying the private key used for creating the signatures. The verifier detects tampering and ensures that the remote device is not running old and perhaps vulnerable boot images.</p>
<p>As always, this presents a few practical issues. First, the verifier must somehow be able to trust the prover’s signing key—for instance, by trusting a certificate containing the prover’s public key, which is signed by some trustworthy authority. In the best case, this authority has been able to establish trust during the manufacturing process, as described previously. Second, the more comprehensive the coverage of the boot images and data, the more there will be different configurations in the field. This means that it becomes infeasible to allow all <em>known-good</em> configurations, so one has to revert to blocking <em>known-bad</em> configurations. However, determining a <em>known-bad</em> configuration is not a trivial exercise and can usually be done only after a modification has been detected and analyzed.</p>
<p>Note that boot attestation guards the boot-time components that are hashed for authenticity. It does not guard against runtime attacks, such as code injection.</p>
<h3 id="h2-278748c01-0016">Confidentiality and Integrity of Personally Identifiable Information</h3>
<p class="BodyFirst"><em>Personally identifiable information (PII)</em> is data that can identify an individual. The obvious data includes names, cell phone numbers, addresses, and credit card numbers, but the less obvious data could be accelerometer data recorded in a wearable device. PII confidentiality becomes an issue when applications installed on a device exfiltrate this information. For example, accelerometer data that characterizes a person’s walking gait can be used to identify that person: see “Gait Identification Using Accelerometer on Mobile Phone” by Hoang Minh Thang et al. Mobile phone power consumption data can pinpoint a person’s location from the way the radio in the phone consumes power, depending on the distance to cell towers, as described in “PowerSpy: Location Tracking Using Mobile Device Power Analysis” by Yan Michalevsky et al.</p>
<p><span epub:type="pagebreak" title="25" id="Page_25"/>The medical field has regulation around PII as well. The <em>Health Insurance Portability and Accountability Act (HIPAA)</em> of 1996 is a law in the United States with a strong focus on privacy for medical information and applies to any system processing patient PII. HIPAA has rather nonspecific requirements for technical security.</p>
<p>Integrity of PII data is essential to avoid impersonation. In banking smart cards, the key material is tied to an account and, therefore, to an identity. EMVCo, a credit-card consortium, has very explicit technical requirements in contrast to HIPAA. For instance, key material must be protected against logical, side-channel, and fault attacks, and this protection needs to be proven by actual attacks performed by an accredited lab.</p>
<h3 id="h2-278748c01-0017">Sensor Data Integrity and Confidentiality</h3>
<p class="BodyFirst">You have just learned how sensor data is related to PII. Integrity has to be important, because the device needs to sense and record its environment accurately. This is even more crucial when the system is using sensor input to control actuators. A great (though disputed) example is that of a US RQ-170 drone being forced to land in Iran, allegedly after its GPS signal was spoofed to make it believe it was landing at a US base in Afghanistan.</p>
<p>When a device is using some form of artificial intelligence for decision making, the integrity of the decisions is challenged by a field of research called <em>adversarial machine learning</em>. One example is exploiting weaknesses in neural net classifiers by artificially modifying pictures of a stop sign. To humans, the modification is not detectable, but the picture can be completely unrecognizable using standard image recognition algorithms when it should in fact have been recognizable. Although the recognition of the neural net may be foiled, modern self-driving cars have a database of the locations of signs that they can fall back to, so in this particular instance, it shouldn’t be a safety issue. “Practical Black-Box Attacks Against Machine Learning” by Nicolas Papernot et al. has more details.</p>
<h3 id="h2-278748c01-0018">Content Confidentiality Protection</h3>
<p class="BodyFirst">Content protection boils down to trying to make sure people pay for the media content they consume and that they stay within some license restrictions, such as date and geographic location, using <em>digital rights/restrictions management (DRM)</em>. DRM mostly depends on encryption of the data stream for transport of content in/out of a device and on access control logic inside a device to deny software access to plaintext content. For mobile devices, most of the protection requirements are aimed at software-only attacks, but for set-top boxes, protection requirements include side-channel and fault attacks. Thus, set-top boxes are considered harder to break and are used for higher-value content.</p>
<h3 id="h2-278748c01-0019">Safety and Resilience</h3>
<p class="BodyFirst"><em>Safety</em> is the property of not causing harm (to people, for example), and <em>resilience</em> is the ability to remain operational in case of (non-malicious) failures. <span epub:type="pagebreak" title="26" id="Page_26"/>For instance, a microcontroller in a satellite will be subject to intensive radiation that causes so-called <em>single event upsets (SEUs)</em>. SEUs flip bits in the state of the chip, which could lead to errors in its decision making. The resilient solution is to detect this and correct the error or detect and reset to a known-good state. Such resilience may not necessarily be secure; it gives someone attempting fault injection unlimited tries as the system keeps accepting abuse.</p>
<p>Similarly, it isn’t safe to shut down an autonomous vehicle’s control unit at highway speeds as soon as a sensor indicates malicious activity. First, any detector can generate false positives, and, second, this potentially allows an attacker to use the sensor to harm all passengers. As with all objectives, this one presents the product’s developer with trade-offs between security and safety/resilience. Resilience and safety are not the same as security; sometimes they are at odds with security. For an attacker, this means opportunities exist to break a device because of good intentions to make it safe or resilient.</p>
<h2 id="h1-278748c01-0007">Countermeasures</h2>
<p class="BodyFirst">We define <em>countermeasures</em> as any (technical) means to reduce the probability of success or impact of an attack. Countermeasures have three functions: protect, detect, and respond. (We discuss some of these countermeasures further in <span class="xref" itemid="xref_target_Chapter 14">Chapter 14</span>.)</p>
<h3 id="h2-278748c01-0020">Protect</h3>
<p class="BodyFirst">This category of countermeasures attempts to avoid or mitigate attacks. An example is encrypting the contents of flash memory against prying eyes. If the key is hidden well, it provides almost unbreakable protection. Other protection measures offer only partial protection. If a single CPU instruction corruption can cause an exploitable fault, randomizing the critical instruction’s timing over five clock cycles still gives an attacker a 20 percent probability of hitting it. Bypassing some protection measures completely is possible because they protect only against a specific class of attacks (for instance, a side-channel countermeasure does not protect against code injection).</p>
<h3 id="h2-278748c01-0021">Detect</h3>
<p class="BodyFirst">This category of countermeasure requires either some kind of hardware detection circuitry or detection logic in software. For instance, you can monitor a chip’s power supply for voltage peaks or dips that are indicative of a voltage fault attack. You can also use software to detect anomalous states. For example, systems that constantly analyze network traffic or application logs can detect attacks. Other common anomaly detection techniques are the verification of so-called stack canaries, detecting guard pages that have been accessed, finding switch statements with no matching case, and cyclic redundancy check (CRC) errors on internal variables, among many others.</p>
<h3 id="h2-278748c01-0022"><span epub:type="pagebreak" title="27" id="Page_27"/>Respond</h3>
<p class="BodyFirst">Detection has little purpose without a response. The type of response depends on the device’s use case. For highly secure devices, like payment smart cards, wiping all device secrets (effectively self-inflicting a denial-of-service attack) would be wise when detecting an attack. Doing this would not be a good idea in safety-critical systems that must continue to operate. In those cases, phoning home or falling back to a crippled-but-safe mode are more appropriate responses. Another undervalued but effective response for human attackers is to bore the will to live out of them (for instance, by resetting a device and increasingly lengthening the boot time).</p>
<p>Countermeasures are critical to building a secure system. Especially in hardware, where physical attacks may be impossible to protect against fully, adding detection and response often raises the bar beyond what an attacker is willing to do or is even capable of doing.</p>
<h2 id="h1-278748c01-0008">An Attack Tree Example</h2>
<p class="BodyFirst">Now that we’ve described the four ingredients needed for effective threat modeling, let’s start with an example where we, as attackers, want to hack our way into an IoT toothbrush with the purpose of extracting confidential information and (just for fun) increasing the brushing speed to something that 9 out of 10 dentists would disapprove of (but that last one loves a good challenge).</p>
<p>In our sample attack tree, shown in <a href="#figure1-12" id="figureanchor1-12">Figure 1-12</a>, we have the following:</p>
<ul>
<li>Rounded boxes indicate the states an attacker is in or assets an attacker has compromised (“nouns”).</li>
<li>Square boxes indicate successful attacks the attacker has performed (“verbs”).</li>
<li>A solid arrow shows the consequential flow between the preceding states and attacks.</li>
<li>A dotted arrow indicates attacks that are mitigated by some countermeasure.</li>
<li>Several incoming arrows indicate that “any single one of the arrows can lead to this.”</li>
<li>The “and” triangle means all the incoming arrows must be satisfied.</li>
</ul>
<p>The numbers in the attack tree mark the stages of the toothbrush attack. As attackers, we have physical access to an IoT toothbrush (1). Our mission is to install a telnet backdoor on the toothbrush to determine what PII is present on the device (8) and also to run the toothbrush at ludicrous speed (11).</p>
<span epub:type="pagebreak" title="28" id="Page_28"/><figure>
<img src="image_fi/278748c01/f01012.png" alt="f01012"/>
<figcaption><p><a id="figure1-12">Figure 1-12</a>: IoT toothbrush attack tree</p></figcaption>
</figure>
<p>Lowercase letters indicate the attacks, and Roman numerals indicate the mitigations. One of the first things we do is desolder the flash and read out the contents—all 16MB of it (a). We see, however, that the image has no readable strings. After some entropy analysis, the content appears to be either encrypted or compressed, but since there is no header identifying the compression format, we assume this content is encrypted as shown in attack (2) and attack mitigation (i). To decrypt it, we need the encryption key. It doesn’t seem to be stored in flash, a mitigation shown at (ii), so it’s likely stored somewhere in ROM or fuses. Without access to a scanning electron microscope, we are unable to “read them out” from silicon.</p>
<p>Instead, we decide to poke around with power analysis. We hook up a power probe and oscilloscope and take a power trace of the system while it <span epub:type="pagebreak" title="29" id="Page_29"/>is booting. The trace shows about one million little peaks. Knowing from our flash readout that the image is 16MB, we deduce that each peak corresponds to 16 bytes of encrypted data. We’ll just assume this is an AES-128 encryption in either of the common <em>electronic code block (ECB) </em>or<em> cipher block chaining (CBC)</em> modes. <em>ECB</em> is a mode where each block is decrypted independently of other blocks, and <em>CBC</em> is a mode where the decryption of the latter blocks depends on the earlier blocks. Since we know the firmware image’s ciphertext, we can try a power analysis attack based on the peaks we measure. After much preprocessing of the traces and doing a differential power analysis (DPA) attack (b), we’re able to identify a likely key candidate. (Don’t worry; you will learn what DPA is as you progress further through this book.) Decryption with ECB produces garbage, but CBC gives us several readable strings in attack (c); it seems we have found the right key at stage (3) and have successfully decrypted the image at stage (4)!</p>
<p>From the decrypted image, we can use traditional software reverse engineering (g) techniques to identify which code blocks do what, where data is stored, how actuators are driven, and, important from a security point of view, we can now look for vulnerabilities in the code (9). Further, we modify the decrypted image at stage (d) to include a backdoor that will allow us to telnet into the toothbrush remotely (5).</p>
<p>We re-encrypt the image and flash it in attack (d), only to discover that the toothbrush doesn’t boot. We have run into what is most likely firmware signature verification. Without the private key used to sign the images, we cannot run a modified image due to mitigation (iii). One common attack on this countermeasure is that of voltage fault injection. With fault injection, we’ll aim to corrupt the one instruction responsible for deciding whether to accept or reject a firmware image. This is usually a comparison that uses the Boolean result returned from an <code>rsa_signature_verify()</code> function. Since this code is implemented in ROM, we cannot really get information about the implementation from reverse engineering. So, we try an old trick—take a side-channel trace when the unmodified image boots and compare it to a side-channel trace of booting a modified image in attack (e). The point where the traces differ is likely to be the moment where the boot code decides whether to accept the firmware image in stage (6). We generate a fault at that instant to attempt to modify the decision.</p>
<p>We load the malicious image and drop the voltage for a few hundred nanoseconds at a random point in a 5-microsecond window in attack (f), at around the instant when we determined the decision is made. After a few hours of repeating this attack, we’re in luck; the toothbrush boots our malicious image in stage (7). Now that the modified code allows us to telnet in, we reach the stage (8), where we can remotely control the brushing and spy on any usage of the brush. And now in the final and fun stage (11), we turn up the speed to ludicrous!</p>
<p>This is obviously a silly example, since the obtained information and access are likely not valuable enough for a serious attacker; physical access is needed to perform the side-channel and fault attacks, and a reset of <span epub:type="pagebreak" title="30" id="Page_30"/>the device by the proper owner causes a denial of service. However, it’s an enlightening exercise, and it’s always worthwhile to play with these toy scenarios.</p>
<p>When drawing attack trees, it’s easy to get carried away and make the tree huge. Remember that attackers probably will try only a few of the easiest attacks (this tool helps identify what those are). Focus on the relevant attacks, which you can determine by profiling the attacker and attack capabilities earlier in the threat modeling.</p>
<h3 id="h2-278748c01-0023">Identification vs. Exploitation</h3>
<p class="BodyFirst">The toothbrush attack path concentrates on the <em>identification</em> <em>phase</em> of an attack by finding a key, reverse engineering the firmware, modifying the image, and discovering the fault-injection instant. Remember that exploitation is the endeavor to scale up the hack by accessing multiple devices. When repeating the attack on another device, you can reuse much of the information gained during identification. Subsequent attack results require only flashing an image in attack (d) at stage (5), knowing the fault injection point in stage (6), and generating the fault by attack (f). Exploitation effort is always lower than identification effort. In some formalisms of creating attack trees, each arrow is annotated with the attack cost and effort, but here we avoid getting too much into quantitative risk modeling.</p>
<h3 id="h2-278748c01-0024">Scalability</h3>
<p class="BodyFirst">The toothbrush attack is not scalable, because the exploitation phase requires physical access. For PII or remote actuation, it’s usually of interest for an attacker only if this can be done at scale.</p>
<p>However, let’s say that in our reverse engineering attack (g), stage (9) manages to identify a vulnerability for which we create an exploit (h) in stage (10). We find that the vulnerability is accessible through an open TCP port, so attack (j) can exploit this vulnerability remotely. This instantly changes the attack’s entire scale. Having used hardware attacks in the identification phase, we can rely solely on remote software attacks in the exploitation phase (12). Now, we can attack any toothbrush, gain access to anyone’s brushing habits, and irritate gums on a global scale. What a time to be alive.</p>
<h3 id="h2-278748c01-0025">Analyzing the Attack Tree</h3>
<p class="BodyFirst">The attack tree helps visualize attack paths in order to discuss them as a team, identify points where additional countermeasures can be built, and analyze the efficacy of existing countermeasures. For instance, it is easy to see that mitigation by firmware image encryption (i) has forced the attacker to use a side-channel attack (b), which is more difficult than simply reading out a memory. Similarly, mitigation by firmware image signing (iii) forced an attacker into a fault-injection attack (f).</p>
<p>However, the main risk is still the scalable attack path through exploitation (j), which is currently unmitigated. Obviously, the vulnerability should <span epub:type="pagebreak" title="31" id="Page_31"/>be patched, anti-exploitation countermeasures should be introduced, and network restrictions should be put in place to disallow anybody from directly connecting to the toothbrush remotely.</p>
<h3 id="h2-278748c01-0026">Scoring Hardware Attack Paths</h3>
<p class="BodyFirst">Apart from visualizing attack paths for analysis, we can also add some quantification to figure out which attacks are easier or cheaper for an attacker. In this section, we introduce several industry-standard rating systems.</p>
<p>The<em> Common Vulnerability Scoring System (CVSS)</em> attempts to score vulnerabilities for severity, typically in the context of networked computers in an organization. It assumes the vulnerability is known and tries to score how bad it would be if it were to be exploited. The <em>Common Weakness Scoring System (CWSS)</em><em> </em>quantifies weaknesses in systems, but those weaknesses are not necessarily vulnerabilities and not necessarily in the context of networked computers. Finally, the <em>Joint Interpretation Library (JIL)</em> is used to score (hardware) attack paths in the Common Criteria (CC) certification scheme.</p>
<p>All of these scoring methods have various parameters and scores for each parameter, which together create a final tally to help compare various vulnerabilities or attack paths. These scoring methods also share the advantage of replacing indefinite arguments about parameters with scores that make sense only in the target context of the scoring method. <a href="#table1-1" id="tableanchor1-1">Table 1-1</a> provides an overview of the three ratings and where they are applicable.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table1-1">Table 1-1</a>: Overview of Attack Rating Systems</p></figcaption>
<table id="table-278748c01-0001" border="1">
<thead>
<tr>
<td/>
<td><b>Common Vulnerability Scoring System </b></td>
<td><b>Common Weakness Scoring System</b></td>
<td><b>Common Criteria Joint Interpretation Library</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><b>Purpose </b></td>
<td>Helps organizations with their vulnerability management processes</td>
<td>Prioritizes software weakness addressing the needs of government, academia, and industry</td>
<td>Rates attacks in order to pass/fail CC evaluation</td>
</tr>
<tr>
<td><b>Impact </b></td>
<td>Distinguishes confidentiality/integrity/availability </td>
<td>Technical impact 0.0–1.0, acquired privilege (layer)</td>
<td>N/A</td>
</tr>
<tr>
<td><b>Asset </b><b>v</b><b>alue </b></td>
<td>N/A</td>
<td>Business impact 0.0–1.0</td>
<td>N/A</td>
</tr>
<tr>
<td><b>Identification </b><b>c</b><b>ost </b></td>
<td>Assumes identification already happened</td>
<td>Likelihood of discovery</td>
<td>Identification phase rating for elapsed time, expertise, knowledge, access, equipment, and open samples</td>
</tr>
<tr>
<td><b>Exploitation </b><b>c</b><b>ost </b></td>
<td>Various factors; no hardware aspects</td>
<td>Various factors; no hardware aspects</td>
<td>Exploitation phase rating</td>
</tr>
<tr>
<td><b><span epub:type="pagebreak" title="32" id="Page_32"/>Attack </b><b>v</b><b>ector </b></td>
<td>Four levels, from physical to remote</td>
<td>Level 0.0–1.0, from physical to internet</td>
<td>Assumes physically present attacker</td>
</tr>
<tr>
<td><b>External </b><b>m</b><b>itigations </b></td>
<td>“Modified” category includes mitigations</td>
<td>External control effectiveness</td>
<td>No external mitigations</td>
</tr>
<tr>
<td><b>Scalability</b></td>
<td>Not really, some related aspects </td>
<td>Not really, some related aspects</td>
<td>Low exploitation cost may imply scalability</td>
</tr>
</tbody>
</table>
</figure>
<p>In a defensive context, you can use ratings to judge the impact of an attack after it occurs as a means to decide how to respond to an attack. For instance, if a vulnerability is detected in a piece of software, CVSS scoring can help decide whether to roll out an emergency patch (with all its associated costs) or push out the fix in the next major version if the vulnerability is minor.</p>
<p>You can also use scoring in a defensive context to judge which countermeasures are needed. In the context of Common Criteria Smart Card certification, the JIL scoring actually becomes a critical part of the security objective—the chip must resist attacks rated at up to 30 points to be considered resistant to attackers of high attack potential. The SOG-IS document “Application of Attack Potential to Smartcards” explains the scoring, and it touches upon a number of hardware attacks. To give you an idea of the rating, if it takes a few weeks to pull out a secret key using a two-laser beam system for laser fault injection, this attack rates 30 or below. If it takes six months to pull out a key using a side-channel attack, implementing a countermeasure is not necessary, as this attack rates 31 or higher.</p>
<p>The CWSS is aimed at rating weaknesses in systems before they are exploited. It is a useful scoring method during development, as it helps assign priorities to the weaknesses’ remedies. Everyone knows that each fix comes at a cost and that attempting to fix all bugs isn’t practical, so rating weaknesses allows developers to concentrate on the most significant ones.</p>
<p>In reality, most attackers do some sort of scoring as well to minimize the cost and maximize the impact of the attack. Although attackers do not publish much on these topics, Dino Dai Zovi had a cool talk called “Attacker Math 101” at SOURCE Boston 2011 that attempted to put some bounds on attacker costing.</p>
<p>These ratings are limited, ambiguous, imprecise, subjective, and not market specific, but they form a good starting point for discussing an attack or vulnerability. If you’re doing threat modeling for embedded systems, we recommend starting with JIL, which is primarily focused on hardware attacks. When concerned with software attacks, use CWSS, as those are the contexts for the scoring methods. With CWSS, you can drop irrelevant aspects and tune others, such as business impacts, to assess asset values or scalability. Also, make sure you score the entire attack path, from the attacker’s starting point all the way through to the impact on the asset, so <span epub:type="pagebreak" title="33" id="Page_33"/>you have a consistent comparison between scores. None of the three ratings deal well with scalability: an attack on a million systems may produce only a marginally worse score than on a single system. Other limitations undoubtedly exist, but currently no better known industry standards exist.</p>
<p>In various security certification schemes, an implicit or explicit security objective is present. For example, as mentioned earlier, for smart cards, attacks of only 30 JIL points or lower are considered relevant. An attack like in Tarnovsky’s 2010 Black Hat DC presentation “Deconstructing a ‘Secure’ Processor” is more than 30 points and, therefore, not considered part of the security objective. For FIPS 140-2, no attacks outside the specific list of attacks are considered relevant. For example, a side-channel attack can compromise a FIPS 140-2 validated crypto engine in a day, and the FIPS 140-2 security objective will still consider it to be secure. Any time you use a device that has a security certificate, check that the certificate’s security objectives are in line with yours.</p>
<h2 id="h1-278748c01-0009">Disclosing Security Issues</h2>
<p class="BodyFirst">Disclosure of security issues is a hotly debated topic, and we do not purport to be solving that in a few paragraphs. We do want to add some color to the debate when it comes to hardware security issues. Hardware and software will always have security issues. With software, you may be able to distribute new versions or patches. Fixing hardware is expensive for many reasons. </p>
<p>We believe the goal of disclosure is public security and safety, not manufacturer business cases or researcher fame and fortune. This means disclosure must serve the public in the long run. Disclosure is a tool to force manufacturers to fix a vulnerability and also to inform the public about a certain product’s risks. An unwelcome side effect of full disclosure is that a large group of attackers will be able to exploit the vulnerability until a fix is widely available.</p>
<p>For hardware vulnerabilities, the bug is often not patchable after manufacturing, though issuing a software patch can mitigate it. In that case, a similar convention to software disclosure of 90 days until disclosure may work fine. For pure hardware fixes, we are not aware of such conventions (though we’ve seen the application of software conventions).</p>
<p>In hardware, it’s common that a software update cannot work around a bug, and patches are practically impossible to distribute and install. A well-intentioned manufacturer can fix a bug in the next release, but products in the field will remain vulnerable. In this situation, the only advantage to disclosure is an informed public; the disadvantage is a long time span until the vulnerable products are replaced or discontinued. An alternative is partial disclosure. For instance, a manufacturer may name the risk and the product but not disclose the details of how to exploit the vulnerability. (This strategy hasn’t worked well in the software world, where the vulnerabilities are often found quickly even after an unspecific disclosure.)</p>
<p>Complications increase when the vulnerability is not patchable and can directly affect health and safety. Consider an attack that can shut down <span epub:type="pagebreak" title="34" id="Page_34"/>every single pacemaker remotely. Disclosure of the latter situation will surely spook patients away from having pacemakers fitted, causing more people to die from heart attacks. On the other hand, it would encourage the supplier to increase the security in the next version, reducing the risk of an attack with lethal consequences. Unique trade-offs will occur for self-driving cars, IoT toothbrushes, SCADA systems and every other application and device. Even more challenges arise when vulnerabilities exist in one type of chip used in a variety of products.</p>
<p>We’re not claiming to have the magic answer to all situations here, but we encourage everyone to consider carefully the kind of disclosure to pursue. Manufacturers should design systems around the premise that they will be broken and plan safe scenarios around that premise. Unfortunately, this practice is not prevalent, especially in situations where time to market and low cost rule.</p>
<h2 id="h1-278748c01-0010">Summary</h2>
<p class="BodyFirst">This chapter outlined some embedded security basics. We described the software and hardware components you’ll undoubtedly stumble upon when analyzing a device, and we discussed what “security” means philosophically. To analyze security properly, we introduced the four components of a threat model: the attackers, various (hardware) attacks, a system’s assets and security objectives, and, finally, the types of countermeasures you can implement. We also described tools to create, analyze, and rate attacks using an attack tree and industry-standard rating systems. Finally, we explored the tricky topic of disclosure in the context of hardware vulnerabilities.</p>
<p>Laden with all this knowledge, our next step will be to start poking at devices, which is what we’ll do in the next chapter.</p>
</section>
</body></html>