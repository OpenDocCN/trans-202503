<html><head></head><body>
<h2 class="h2" id="ch11"><span epub:type="pagebreak" id="page_187"/><span class="big">11</span><br/>CONTROL PLANE AND ACCESS CONTROL</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">The control plane manages the Kubernetes cluster, storing the desired state of applications, monitoring the current state to detect and recover from any issues, scheduling new containers, and configuring network routing. In this chapter, we’ll look closely at the API server, the primary interface for the control plane and the entry point for any status retrieval and changes made to the entire cluster.</p>&#13;
<p class="indent">Although we will focus on the API server, the control plane includes multiple other services, each with a role to play. The other control plane services act as clients to the API server, watching cluster changes and taking appropriate action to update the state of the cluster. The following list describes the other control plane components:</p>&#13;
<div class="bqparan">&#13;
<p class="noindent5"><strong>Scheduler</strong> Assigns each new Pod to a node.</p>&#13;
<p class="noindent5"><strong>Controller manager</strong> Has multiple responsibilities, including creating Pods for Deployments, monitoring nodes, and reacting to outages.</p>&#13;
<p class="noindent5"><span epub:type="pagebreak" id="page_188"/><strong>Cloud controller manager</strong> This optional component interfaces with an underlying cloud provider to check on nodes and configure network traffic routing.</p>&#13;
</div>&#13;
<p class="indent">As we demonstrate the workings of the API server, we’ll also see how Kubernetes manages security to ensure that only authorized users and services can query the cluster and make changes. The purpose of a container orchestration environment like Kubernetes is to provide a platform for any kind of containerized application we might need to run, so this security is critically important to ensure that the cluster is used only as intended.</p>&#13;
<h3 class="h3" id="ch00lev1sec47">API Server</h3>&#13;
<p class="noindent">Despite its centrality to the Kubernetes architecture, the API server’s purpose is simple. It exposes an interface using HTTP and representational state transfer (REST) to perform basic creation, retrieval, update, and deletion of resources in the cluster. It performs authentication to identify clients, authorization to ensure that clients have permission for the specific request, and validation to ensure that any created or updated resources match the corresponding specification. It also reads from and writes to a data store based on the commands it receives from clients.</p>&#13;
<p class="indent">However, the API server is not responsible for actually updating the current state of the cluster to match the desired state. That is the responsibility of other control plane and node components. For example, if a client creates a new Kubernetes Deployment, the API server’s job is solely to update the data store with the resource information. It is then the job of the scheduler to decide where the Pods will run, and the job of the <span class="literal">kubelet</span> service on the assigned nodes to create and monitor the containers and to configure networking to route traffic to the containers.</p>&#13;
<p class="indent">For this chapter, we have a three-node Kubernetes cluster configured by our automation scripts. Each of the three nodes acts as a control plane node, so three copies of the API server are running. We can communicate with any of these three because they all share the same backend database. The API server is listening for secure HTTP connections on port 6443, the default port.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">We’ve been using <span class="literal">kubectl</span> to communicate with the API server to create and delete resources and retrieve status, and <span class="literal">kubectl</span> has been using secure HTTP on port 6443 to talk to the cluster. It knows to do this because of a Kubernetes configuration file that was installed into <em>/etc/kubernetes</em> by <span class="literal">kubeadm</span> when the cluster was initialized. This configuration file also contains authentication information that gives us permission to read cluster status and make changes.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_189"/>Because the API server is expecting secure HTTP, we can use <span class="literal">curl</span> to communicate directly with the Kubernetes API. This will give us a better feel for how the communication actually works. Let’s begin with a simple <span class="literal">curl</span> command:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">curl https://192.168.61.11:6443/</span>&#13;
curl: (60) SSL certificate problem: unable to get local issuer certificate&#13;
More details here: https://curl.se/docs/sslcerts.html&#13;
...</pre>&#13;
<p class="indent">This error message shows that <span class="literal">curl</span> does not trust the certificate that the API server is offering. We can use <span class="literal">curl</span> to see this certificate:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">curl -kv https://192.168.61.11:6443/</span>&#13;
...&#13;
* Server certificate:&#13;
*  subject: CN=kube-apiserver&#13;
...&#13;
*  issuer: CN=kubernetes&#13;
...</pre>&#13;
<p class="indent">The <span class="literal">-k</span> option tells <span class="literal">curl</span> to ignore any certificate issues, whereas <span class="literal">-v</span> tells <span class="literal">curl</span> to provide us with extra logging information about the connection.</p>&#13;
<p class="indent">For <span class="literal">curl</span> to trust this certificate, it will need to trust the <span class="literal">issuer</span>, as the issuer is the signer of the certificate. Let’s fetch the certificate from our Kubernetes installation so that we can point <span class="literal">curl</span> to it:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cp /etc/kubernetes/pki/ca.crt .</span></pre>&#13;
<p class="indent">Be sure to add the <span class="literal">.</span> at the end to copy this file to the current directory. We’re doing this solely to make the following commands easier to type.</p>&#13;
<p class="indent">Let’s examine this certificate before we use it:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">openssl x509 -in ca.crt -text</span>&#13;
Certificate:&#13;
...&#13;
        Issuer: CN = kubernetes&#13;
...&#13;
        Subject: CN = kubernetes</pre>&#13;
<p class="indent">The <span class="literal">Issuer</span> and the <span class="literal">Subject</span> are the same, so this is a <em>self-signed</em> certificate. It was created by <span class="literal">kubeadm</span> when we initialized this cluster. Using a generated certificate allows <span class="literal">kubeadm</span> to adapt to our particular cluster networking configuration and allows our cluster to have a unique certificate and key without requiring an external certificate authority (CA). However, it does mean that we need to configure <span class="literal">kubectl</span> to trust this certificate on any system for which we need to communicate with this API server.</p>&#13;
<p class="indent">We can now tell <span class="literal">curl</span> to use this certificate to verify the API server:</p>&#13;
<pre><span epub:type="pagebreak" id="page_190"/>root@host01:~# <span class="codestrong1">curl --cacert ca.crt https://192.168.61.11:6443/</span>&#13;
{&#13;
...&#13;
  "status": "Failure",&#13;
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",&#13;
...&#13;
  "code": 403&#13;
}</pre>&#13;
<p class="indent">Now that we’re providing <span class="literal">curl</span> with the correct root certificate, <span class="literal">curl</span> can validate the API server certificate and we can successfully connect to the API server. However, the API server responds with a 403 error, indicating that we are not authorized. This is because at the moment we are not providing any authentication information for <span class="literal">curl</span> to pass to the API server, so the API server sees us as an anonymous user.</p>&#13;
<p class="indent">One final note: for this <span class="literal">curl</span> command to work, we need to be selective in the hostname or IP address we use. The API server is listening on all network interfaces, so we could connect to it using <span class="literal">localhost</span> or <span class="literal">127.0.0.1</span>. However, those are not listed in the <span class="literal">kube-apiserver</span> certificate and cannot be used for secure HTTP because <span class="literal">curl</span> will not trust the connection.</p>&#13;
<h3 class="h3" id="ch00lev1sec48">API Server Authentication</h3>&#13;
<p class="noindent">We need to provide authentication information before the API server will accept our requests, so let’s understand the API server’s process for authentication. Authentication is handled through a set of plug-ins, each of which looks at the request to determine whether it can identify the client. The first plug-in that successfully identifies the client provides identity information to the API server. This identity is then used with authorization to determine what the client is allowed to do.</p>&#13;
<p class="indent">Because authentication is based on plug-ins, it’s possible to have as many different ways of authenticating clients as needed. It’s even possible to add a proxy in front of the API server that performs custom authentication logic and passes the user’s identity to the API server in an HTTP header.</p>&#13;
<p class="indent">For our purposes, we’ll focus on three authentication primary plug-ins that are used within the cluster itself or as part of the cluster setup process: <em>client certificates</em>, <em>bootstrap tokens</em>, and <em>service accounts</em>.</p>&#13;
<h4 class="h4" id="ch00lev2sec74">Client Certificates</h4>&#13;
<p class="noindent">As mentioned previously, an HTTP client like <span class="literal">curl</span> validates the server’s identity by comparing the server’s hostname to its certificate and also by checking the certificate’s signature against a list of trusted CAs. In addition to checking the server identity, secure HTTP also allows a client to submit a certificate to the server. The server checks the signature against its list of trusted authorities and then uses the subject of the certificate as the client’s identity.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_191"/>Kubernetes uses HTTP client certificate authentication extensively to enable cluster services to authenticate with the API server. This includes control plane components as well as the <span class="literal">kubelet</span> service running on each node. We can use <span class="literal">kubeadm</span> to list the certificates used by the control plane:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubeadm certs check-expiration</span>&#13;
...&#13;
CERTIFICATE                ...  RESIDUAL TIME   CERTIFICATE AUTHORITY ...&#13;
admin.conf                 ...  363d                                  ...&#13;
apiserver                  ...  363d            ca                    ...&#13;
apiserver-etcd-client      ...  363d            etcd-ca               ...&#13;
apiserver-kubelet-client   ...  363d            ca                    ...&#13;
controller-manager.conf    ...  363d                                  ...&#13;
etcd-healthcheck-client    ...  363d            etcd-ca               ...&#13;
etcd-peer                  ...  363d            etcd-ca               ...&#13;
etcd-server                ...  363d            etcd-ca               ...&#13;
front-proxy-client         ...  363d            front-proxy-ca        ...&#13;
scheduler.conf             ...  363d                                  ...&#13;
...</pre>&#13;
<p class="indent">The <span class="literal">RESIDUAL TIME</span> column shows how much time is left before these certificates expire; by default, they expire after one year. Use <span class="literal">kubeadm certs renew</span> to renew them, passing the name of the certificate as a parameter.</p>&#13;
<p class="indent">The first item in the list, <span class="literal">admin.conf</span>, is how we’ve been authenticating ourselves to the cluster in the past few chapters. During initialization, <span class="literal">kubeadm</span> created this certificate and stored its information in the <em>/etc/kubernetes/admin.conf</em> file. Every <span class="literal">kubectl</span> command we’ve run has been using this file because our automation scripts are setting the <span class="literal">KUBECONFIG</span> environment variable:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">echo $KUBECONFIG</span>&#13;
/etc/kubernetes/admin.conf</pre>&#13;
<p class="indent">If we had not set <span class="literal">KUBECONFIG</span>, <span class="literal">kubectl</span> would be using the default, which is a file called <em>.kube/config</em> in the user’s home directory.</p>&#13;
<p class="indent">The <em>admin.conf</em> credentials are designed to provide emergency access to the cluster, bypassing authorization. In a production cluster, we would avoid using these credentials directly for everyday operations. Instead, the best practice for a production cluster is to integrate a separate identity manager for administrators and normal users. For our example, because we don’t have a separate identity manager, we’ll instead create an additional certificate for a regular user. This kind of certificate may be useful for an automated process that runs outside the cluster, but it can’t integrate with the identity manager.</p>&#13;
<p class="indent">We can create a new client certificate using <span class="literal">kubeadm</span>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubeadm kubeconfig user --client-name=me \</span>&#13;
  <span class="codestrong1">--config /etc/kubernetes/kubeadm-init.yaml &gt; kubeconfig</span></pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_192"/>The <span class="literal">kubeadm kubeconfig user</span> command asks the API server to generate a new client certificate. Because this certificate is signed by the cluster’s CA, it is valid for authentication. The certificate is saved into the <em>kubeconfig</em> file along with the necessary configuration to connect to the API server:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cat kubeconfig</span>&#13;
apiVersion: v1&#13;
clusters:&#13;
- cluster:&#13;
    certificate-authority-data: ...&#13;
    server: https://192.168.61.10:6443&#13;
  name: kubernetes&#13;
contexts:&#13;
- context:&#13;
    cluster: kubernetes&#13;
    user: me&#13;
  name: me@kubernetes&#13;
current-context: me@kubernetes&#13;
kind: Config&#13;
preferences: {}&#13;
users:&#13;
- name: me&#13;
  user:&#13;
    client-certificate-data: ...&#13;
    client-key-data: ...</pre>&#13;
<p class="indent">The <span class="literal">clusters</span> section defines the information needed to connect to the API server, including the load-balanced address shared by all three API servers in our highly available configuration. The <span class="literal">users</span> section defines the new user we created along with its client certificate.</p>&#13;
<p class="indent">Thus far, we’ve successfully created a new user, but we haven’t given that user any permissions yet, so we won’t be very successful using these credentials:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">KUBECONFIG=kubeconfig kubectl get pods</span>&#13;
Error from server (Forbidden): pods is forbidden: User "me" cannot list &#13;
  resource "pods" in API group "" in the namespace "default"</pre>&#13;
<p class="indent">Later in the chapter, we’ll see how to give permissions to this user.</p>&#13;
<h4 class="h4" id="ch00lev2sec75">Bootstrap Tokens</h4>&#13;
<p class="noindent">Initializing a distributed system like a Kubernetes cluster is challenging. The <span class="literal">kubelet</span> service running on each node must be added to the cluster. To do this, <span class="literal">kubelet</span> must connect to the API server and obtain a client certificate signed by the cluster’s CA. The <span class="literal">kubelet</span> service then uses this client certificate to authenticate to the cluster.</p>&#13;
<p class="indent">This certificate generation must be done securely so that we eliminate <span epub:type="pagebreak" id="page_193"/>the possibility of adding rogue nodes to the cluster and eliminate the possibility of a rogue process being able to impersonate a real node. For this reason, the API server cannot provide a certificate for just any node that asks to be added to the cluster. Instead, the node must generate its own private key, submit a certificate signing request (CSR) to the API server, and receive a signed certificate.</p>&#13;
<p class="indent">To keep this process secure, we need to ensure that a node is authorized to submit a certificate signing request. But this submission must happen before the node has the client certificate that it uses for more permanent authentication—we have a chicken-or-egg problem! Kubernetes solves this via time-limited tokens, known as <em>Bootstrap Tokens</em>. The bootstrap token becomes a preshared secret that is known to the API server and the new nodes. Making this token time limited reduces the risk to the cluster if it is exposed. The Kubernetes controller manager has the task of automatically cleaning up bootstrap tokens when they expire.</p>&#13;
<p class="indent">When we initialized our cluster, <span class="literal">kubeadm</span> created a bootstrap token, but it was configured to expire after two hours. If we need to join additional nodes to the cluster after that, we can use <span class="literal">kubeadm</span> to generate a new bootstrap token:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">TOKEN=$(kubeadm token create)</span>&#13;
root@host01:~# <span class="codestrong1">echo $TOKEN</span>&#13;
pqcnd6.4wawyqgkfaet06zm</pre>&#13;
<p class="indent">This token is added as a Kubernetes <em>Secret</em> in the <span class="literal">kube-system</span> Namespace. We look at secrets in more detail in <a href="ch16.xhtml#ch16">Chapter 16</a>. For now, let’s just verify that it exists:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n kube-system get secret</span>&#13;
NAME                    TYPE                           DATA   AGE&#13;
...&#13;
bootstrap-token-pqcnd6  bootstrap.kubernetes.io/token  6      64s&#13;
...</pre>&#13;
<p class="indent">We can use this token to make requests of the API server by using HTTP Bearer authentication. This means that we provide the token in an HTTP header called <span class="literal">Authorization</span>, prefaced with the word <span class="literal">Bearer</span>. When the bootstrap token authentication plug-in sees that header and matches the provided token against the corresponding secret, it authenticates us to the API server and allows us access to the API.</p>&#13;
<p class="indent">For security reasons, bootstrap tokens have access only to the certificate signing request functionality of the API server, so that’s all our token will be allowed to do.</p>&#13;
<p class="indent">Let’s use our bootstrap token to list all of the certificate signing requests:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">curl --cacert ca.crt \</span>&#13;
  <span class="codestrong1">-H "Authorization: Bearer $TOKEN" \</span>&#13;
  <span class="codestrong1">https://192.168.61.11:6443/apis/certificates.k8s.io/v1/certificatesigningrequests</span>&#13;
{&#13;
  "kind": "CertificateSigningRequestList",&#13;
<span epub:type="pagebreak" id="page_194"/>  "apiVersion": "certificates.k8s.io/v1",&#13;
  "metadata": {&#13;
    "resourceVersion": "21241"&#13;
  },&#13;
  "items": [&#13;
...&#13;
  ]&#13;
}</pre>&#13;
<p class="indent">It’s important to know how bootstrap tokens work, given that they’re essential to adding nodes to the cluster. However, as the name implies, that’s really the only purpose for a bootstrap token; it’s not typical to use them for normal API server access. For normal API server access, especially from inside the cluster, we need a <em>ServiceAccount</em>.</p>&#13;
<h4 class="h4" id="ch00lev2sec76">Service Accounts</h4>&#13;
<p class="noindent">Containers running in the Kubernetes cluster often need to communicate with the API server. For example, all of the various components we deployed on top of our cluster in <a href="ch06.xhtml#ch06">Chapter 6</a>, including the Calico network plug-in, the Longhorn storage driver, and the metrics server, communicate with the API server to watch and modify the cluster state. To support this, Kubernetes automatically injects credentials into every running container.</p>&#13;
<p class="indent">Of course, for security reasons, giving each container only the API server permissions it requires is important, so we should create a separate ServiceAccount for each application or cluster component to do that. The information for these ServiceAccounts is then added to the Deployment or other controller so that Kubernetes will inject the correct credentials. In some cases, we may use multiple ServiceAccount with a single application, restricting each application component to only the access it needs.</p>&#13;
<p class="indent">In addition to using a separate ServiceAccount per application or component, it’s also good practice to use a separate Namespace per application. As we’ll see in a moment, permissions can be limited to a single Namespace. Let’s start by creating the Namespace:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl create namespace sample</span>&#13;
namespace/sample created</pre>&#13;
<p class="indent">A ServiceAccount uses a bearer token, which is stored in a secret automatically generated by Kubernetes when the ServiceAccount is created. Let’s make a ServiceAccount for a Deployment that we’ll create in this chapter:</p>&#13;
<p class="noindent6"><em>read-pods-sa.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: ServiceAccount&#13;
metadata:&#13;
  name: read-pods&#13;
  namespace: sample</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_195"/>Note that we use the metadata to place this ServiceAccount in the <span class="literal">sample</span> Namespace we just created. We could also use the <span class="literal">-n</span> flag with <span class="literal">kubectl</span> to specify the Namespace. We’ll use the usual <span class="literal">kubectl apply</span> to create this ServiceAccount:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/read-pods-sa.yaml</span>&#13;
serviceaccount/read-pods created</pre>&#13;
<p class="indent">When the ServiceAccount is created, the controller manager detects this and automatically creates a Secret with the credentials:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n sample get serviceaccounts</span>&#13;
NAME        SECRETS   AGE&#13;
default     1         27s&#13;
read-pods   1         8s&#13;
root@host01:~# <span class="codestrong1">kubectl -n sample get secrets</span>&#13;
NAME                    TYPE                                  DATA   AGE&#13;
default-token-mzwpt     kubernetes.io/service-account-token   3      43s&#13;
read-pods-token-m4scq   kubernetes.io/service-account-token   3      25s</pre>&#13;
<p class="indent">Note that in addition to the <span class="literal">read-pods</span> ServiceAccount we just created, there is already a <span class="literal">default</span> ServiceAccount. This account was created automatically when the Namespace was created; it will be used if we don’t specify to Kubernetes which ServiceAccount to use for a Pod.</p>&#13;
<p class="indent">The newly created ServiceAccount does not have any permissions yet. To start adding permissions, we need to take a look at <em>role-based access control</em> (RBAC).</p>&#13;
<h3 class="h3" id="ch00lev1sec49">Role-Based Access Controls</h3>&#13;
<p class="noindent">After the API server has found an authentication plug-in that can identify the client, it uses the identity to determine whether the client has permissions to perform the desired action, which is done by assembling a list of roles that belong to the user. Roles can be associated directly with a user or with a group in which the user is a member. Group membership is part of the identity. For example, client certificates can specify a user’s groups by including organization fields as part of the certificate’s subject.</p>&#13;
<h4 class="h4" id="ch00lev2sec77">Roles and Cluster Roles</h4>&#13;
<p class="noindent">Each role has a set of permissions. A permission allows a client to perform one or more actions on one or more types of resources.</p>&#13;
<p class="indent">As an example, let’s define a role that will give a client permission to read Pod status. We have two choices: we can create a <em>Role</em> or a <em>ClusterRole</em>. A Role is visible and usable within a single Namespace, whereas a ClusterRole is visible and usable across all Namespaces. This difference allows administrators to define common roles across the cluster that are immediately available when new Namespaces are created, while also allowing the delegation of access control for a specific Namespace.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_196"/>Here’s an example definition of a ClusterRole. This role only has the ability to read data about Pods; it cannot change Pods or access any other cluster information:</p>&#13;
<p class="noindent6"><em>pod-reader.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: rbac.authorization.k8s.io/v1&#13;
kind: ClusterRole&#13;
metadata:&#13;
  name: pod-reader&#13;
rules:&#13;
- apiGroups: [""]&#13;
  resources: ["pods"]&#13;
  verbs: ["get", "watch", "list"]</pre>&#13;
<p class="indent">Because this is a cluster-wide role, it doesn’t make sense to assign it to a Namespace, so we don’t specify one.</p>&#13;
<p class="indent">The critical part of this definition is the list of rules. Each ClusterRole or Role can have as many rules as necessary. Each rule has a list of <span class="literal">verbs</span> that define what actions are allowed. In this case, we identified <span class="literal">get</span>, <span class="literal">watch</span>, and <span class="literal">list</span> as the verbs, with the effect that the role allows reading Pods but not any actions that would modify them.</p>&#13;
<p class="indent">Each rule applies to one or more resource types, based on the combination of <span class="literal">apiGroups</span> and <span class="literal">resources</span> identified. Each rule gives permissions for the actions listed as <span class="literal">verbs</span>. In this case, the empty string <span class="literal">""</span> is used to refer to the default API group, which is where Pods are located. If we wanted to also include Deployments and StatefulSets, we would need to define our rule as follows:</p>&#13;
<pre>- apiGroups: ["", "apps"]&#13;
  resources: ["pods", "deployments", "statefulsets"]&#13;
  verbs: ["get", "watch", "list"]</pre>&#13;
<p class="indent">We need to add <span class="literal">"apps"</span> to the <span class="literal">apiGroups</span> field because Deployment and StatefulSet are part of that group (as identified in the <span class="literal">apiVersion</span> when we declare the resource). When we declare a Role or ClusterRole, the API server will accept any strings in the <span class="literal">apiGroups</span> and <span class="literal">resources</span> fields, regardless of whether the combination actually identifies any resource types, so it’s important to pay attention to which group a resource is in.</p>&#13;
<p class="indent">Let’s define our <span class="literal">pod-reader</span> ClusterRole:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pod-reader.yaml</span>&#13;
clusterrole.rbac.authorization.k8s.io/pod-reader created</pre>&#13;
<p class="indent">Now that the ClusterRole exists, we can apply it. To do that, we need to create a role binding.</p>&#13;
<h4 class="h4" id="ch00lev2sec78"><span epub:type="pagebreak" id="page_197"/>Role Bindings and Cluster Role Bindings</h4>&#13;
<p class="noindent">Let’s apply this <span class="literal">pod-reader</span> ClusterRole to the <span class="literal">read-pods</span> ServiceAccount we created earlier. We have two options: we can create a <em>RoleBinding</em>, which will assign the permissions in a specific Namespace, or a <em>ClusterRoleBinding</em>, which will assign the permissions across all Namespaces. This feature is beneficial because it means we can create a ClusterRole such as <span class="literal">pod-reader</span> once and have it visible across the cluster, but create the binding in an individual Namespace so that users and ServiceAccount are restricted to only the Namespaces they should be allowed to access. This helps us apply the pattern we saw earlier of having a Namespace per application, while at the same time it keeps non-administrators away from key infrastructure components such as the components running in the <span class="literal">kube-system</span> Namespace.</p>&#13;
<p class="indent">In keeping with this practice, we’ll create a RoleBinding so that our ServiceAccount has permissions to read Pods only in the <span class="literal">sample</span> Namespace:</p>&#13;
<p class="noindent6"><em>read-pods-bind.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: rbac.authorization.k8s.io/v1&#13;
kind: RoleBinding&#13;
metadata:&#13;
  name: read-pods&#13;
  namespace: sample&#13;
subjects:&#13;
- kind: ServiceAccount&#13;
  name: read-pods&#13;
  namespace: sample&#13;
roleRef:&#13;
  kind: ClusterRole&#13;
  name: pod-reader&#13;
  apiGroup: rbac.authorization.k8s.io</pre>&#13;
<p class="indent">Not surprisingly, a RoleBinding ties together a Role or a ClusterRole and a subject. The RoleBinding can contain multiple subjects, so we can bind the same role to multiple users or groups with a single binding.</p>&#13;
<p class="indent">We define a Namespace in both the metadata and where we identify the subject. In this case, these are both <span class="literal">sample</span>, as we want to grant the ServiceAccount the ability to read Pod status in its own Namespace. However, these could be different to allow a ServiceAccount in one Namespace to have specific permissions in another Namespace. And of course we could also use a ClusterRoleBinding to give out permissions across all Namespaces.</p>&#13;
<p class="indent">We can now create the RoleBinding:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/read-pods-bind.yaml</span>&#13;
rolebinding.rbac.authorization.k8s.io/read-pods created</pre>&#13;
<p class="indent">We’ve now given permission for the <span class="literal">read-pods</span> ServiceAccount to read Pods in the <span class="literal">sample</span> Namespace. To demonstrate how it works, we need to create a Pod that is assigned to the <span class="literal">read-pods</span> ServiceAccount.</p>&#13;
<h4 class="h4" id="ch00lev2sec79"><span epub:type="pagebreak" id="page_198"/>Assigning a Service Account to Pods</h4>&#13;
<p class="noindent">To assign a ServiceAccount to a Pod, just add the <span class="literal">serviceAccountName</span> field to the Pod spec:</p>&#13;
<p class="noindent6"><em>read-pods-deploy.yaml</em></p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: read-pods&#13;
  namespace: sample&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: read-pods&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: read-pods&#13;
    spec:&#13;
      containers:&#13;
      - name: read-pods&#13;
        image: alpine&#13;
        command: ["/bin/sleep", "infinity"]&#13;
      serviceAccountName: read-pods</pre>&#13;
<p class="indent">The ServiceAccount identified must exist in the Namespace that the Pod is created in. Kubernetes will inject the Pod’s containers with the Service-Account token so that the containers can authenticate to the API server.</p>&#13;
<p class="indent">Let’s walk through an example to show how this works and how the authorization is applied. Start by creating this Deployment:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/read-pods-deploy.yaml</span>&#13;
deployment.apps/read-pods created</pre>&#13;
<p class="indent">This creates an Alpine container running <span class="literal">sleep</span> that we can use as a base for shell commands.</p>&#13;
<p class="indent">To get to a shell prompt, we’ll first get the generated name of the Pod and then use <span class="literal">kubectl exec</span> to create the shell:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n sample get pods</span>&#13;
NAME                        READY   STATUS    RESTARTS   AGE&#13;
read-pods-9d5565548-fbwjb   1/1     Running   0          6s&#13;
root@host01:~# <span class="codestrong1">kubectl -n sample exec -ti read-pods-<span class="codeitalic1">9d5565548-fbwjb</span> -- /bin/sh</span>&#13;
/ #</pre>&#13;
<p class="indent">The ServiceAccount token is mounted in the directory <em>/run/secrets/kubernetes.io/serviceaccount</em>, so change to that directory and list its contents:</p>&#13;
<pre><span epub:type="pagebreak" id="page_199"/>/ # <span class="codestrong1">cd /run/secrets/kubernetes.io/serviceaccount</span>&#13;
/run/secrets/kubernetes.io/serviceaccount # <span class="codestrong1">ls -l</span>&#13;
total 0&#13;
lrwxrwxrwx    1 root     root  ...  ca.crt -&gt; ..data/ca.crt&#13;
lrwxrwxrwx    1 root     root  ...  namespace -&gt; ..data/namespace&#13;
lrwxrwxrwx    1 root     root  ...  token -&gt; ..data/token</pre>&#13;
<p class="indent">These files show up as odd looking symbolic links, but the contents are there as expected. The <em>ca.crt</em> file is the root certificate for the cluster, which is needed to trust the connection to the API server.</p>&#13;
<p class="indent">Let’s save the token in a variable so that we can use it:</p>&#13;
<pre>/run/secrets/kubernetes.io/serviceaccount # <span class="codestrong1">TOKEN=$(cat token)</span></pre>&#13;
<p class="indent">We can now use this token with <span class="literal">curl</span> to connect to the API server. First, though, we need to install <span class="literal">curl</span> into our Alpine container:</p>&#13;
<pre>default/run/secrets/kubernetes.io/serviceaccount # <span class="codestrong1">apk add curl</span>&#13;
...&#13;
OK: 8 MiB in 19 packages</pre>&#13;
<p class="indent">Our ServiceAccount is allowed to perform <span class="literal">get</span>, <span class="literal">list</span>, and <span class="literal">watch</span> operations on Pods. Let’s list all Pods in the <span class="literal">sample</span> Namespace:</p>&#13;
<pre>/run/secrets/kubernetes.io/serviceaccount # <span class="codestrong1">curl --cacert ca.crt \</span>&#13;
  <span class="codestrong1">-H "Authorization: Bearer $TOKEN" \</span>&#13;
  <span class="codestrong1">https://kubernetes.default.svc/api/v1/namespaces/sample/pods</span>&#13;
  "kind": "PodList",&#13;
  "apiVersion": "v1",&#13;
  "metadata": {&#13;
    "resourceVersion": "566610"&#13;
  },&#13;
  "items": [&#13;
    {&#13;
      "metadata": {&#13;
        "name": "read-pods-9d5565548-fbwjb",&#13;
...&#13;
  ]&#13;
}</pre>&#13;
<p class="indent">As with the bootstrap token, we use HTTP Bearer authentication to pass the ServiceAccount token to the API server. Because we’re operating from inside a container, we can use the standard address <span class="literal">kubernetes.default.svc</span> to find the API server. This works because a Kubernetes cluster always has a service in the <span class="literal">default</span> Namespace that routes traffic to API server instances using the Service networking we saw in <a href="ch09.xhtml#ch09">Chapter 9</a>.</p>&#13;
<p class="indent">The <span class="literal">curl</span> command is successful because our ServiceAccount is bound to the <span class="literal">pod-reader</span> Role we created. However, the RoleBinding is limited to the <span epub:type="pagebreak" id="page_200"/><span class="literal">sample</span> Namespace, and as a result, we aren’t allowed to list Pods in a different Namespace:</p>&#13;
<pre>/run/secrets/kubernetes.io/serviceaccount # <span class="codestrong1">curl --cacert ca.crt \</span>&#13;
  <span class="codestrong1">-H "Authorization: Bearer $TOKEN" \</span>&#13;
  <span class="codestrong1">https://kubernetes.default.svc/api/v1/namespaces/kube-system/pods</span>&#13;
{&#13;
  "kind": "Status",&#13;
  "apiVersion": "v1",&#13;
  "metadata": {&#13;
  },&#13;
  "status": "Failure",&#13;
  "message": "pods is forbidden: User &#13;
    \"system:serviceaccount:default:read-pods\" cannot list resource &#13;
    \"pods\" in API group \"\" in the namespace \"kube-system\"",&#13;
  "reason": "Forbidden",&#13;
  "details": {&#13;
    "kind": "pods"&#13;
  },&#13;
  "code": 403&#13;
}</pre>&#13;
<p class="indent">We can use the error message to be certain that our ServiceAccount assignment and authentication worked as expected because the API server recognizes us as the <span class="literal">read-pods</span> ServiceAccount. However, we don’t have a RoleBinding with the right permissions to read Pods in the <span class="literal">kube-system</span> Namespace, so the request is rejected.</p>&#13;
<p class="indent">Similarly, because we have permission only for Pods, we can’t list our Deployment, even though it is also in the <span class="literal">sample</span> Namespace:</p>&#13;
<pre>/run/secrets/kubernetes.io/serviceaccount # <span class="codestrong1">curl --cacert ca.crt \</span>&#13;
  <span class="codestrong1">-H "Authorization: Bearer $TOKEN" \</span>&#13;
  <span class="codestrong1">https://kubernetes.default.svc/apis/apps/v1/namespaces/sample/deploy</span>&#13;
ments&#13;
{&#13;
  "kind": "Status",&#13;
  "apiVersion": "v1",&#13;
  "metadata": {&#13;
  },&#13;
  "status": "Failure",&#13;
  "message": "deploy.apps is forbidden: User &#13;
    \"system:serviceaccount:default:read-pods\" cannot list resource &#13;
    \"deploy\" in API group \"apps\" in the namespace \"sample\"",&#13;
  "reason": "Forbidden",&#13;
  "details": {&#13;
    "group": "apps",&#13;
    "kind": "deploy"&#13;
  },&#13;
<span epub:type="pagebreak" id="page_201"/>  "code": 403&#13;
}</pre>&#13;
<p class="indent">The slightly different path scheme for the URL, starting with <em>/apis/apps/v1</em> instead of <em>/api/v1</em>, is needed because Deployments are in the <span class="literal">apps</span> API group rather than the default group. This command fails in a similar way because we don’t have the necessary permissions to list Deployments.</p>&#13;
<p class="indent">We’re finished with this shell session, so let’s exit it:</p>&#13;
<pre>/run/secrets/kubernetes.io/serviceaccount # <span class="codestrong1">exit</span></pre>&#13;
<p class="indent">Before we leave the RBAC topic, though, let’s illustrate an easy way to grant normal user permissions for a Namespace without allowing any administrator functions.</p>&#13;
<h4 class="h4" id="ch00lev2sec80">Binding Roles to Users</h4>&#13;
<p class="noindent">To grant normal user permissions, we’ll leverage an existing ClusterRole called <span class="literal">edit</span> that’s already set up to grant view and edit permissions for most of the resource types users need.</p>&#13;
<p class="indent">Let’s take a quick look at the <span class="literal">edit</span> ClusterRole to see what permissions it has:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get clusterrole edit -o yaml</span>&#13;
...&#13;
apiVersion: rbac.authorization.k8s.io/v1&#13;
kind: ClusterRole&#13;
...&#13;
rules:&#13;
...&#13;
- apiGroups:&#13;
  - ""&#13;
  resources:&#13;
  - pods&#13;
  - pods/attach&#13;
  - pods/exec&#13;
  - pods/portforward&#13;
  - pods/proxy&#13;
  verbs:&#13;
  - create&#13;
  - delete&#13;
  - deletecollection&#13;
  - patch&#13;
  - update&#13;
...</pre>&#13;
<p class="indent">The full list has a large number of different rules, each with its own set of permissions. The subset in this example shows just one rule, used to provide edit permission for Pods.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_202"/>Some commands related to Pods, such as <span class="literal">exec</span>, are listed separately to allow for more granular control. For example, for a production system, it can be useful to allow some individuals the ability to create and delete Pods and see logs, but not provide the ability to use <span class="literal">exec</span>, because that might be used to access sensitive production data.</p>&#13;
<p class="indent">Previously, we created a user called <em>me</em> and saved the client certificate to a file called <em>kubeconfig</em>. However, we didn’t bind any roles to that user yet, so the user has only the very limited permissions that come with automatic membership in the <em>system:authenticated</em> group.</p>&#13;
<p class="indent">As a result, as we saw earlier, our normal user can’t even list Pods in the <span class="literal">default</span> Namespace. Let’s bind this user to the edit role. As before, we’ll use a regular RoleBinding, scoped to the <span class="literal">sample</span> Namespace, so this user won’t be able to access our cluster infrastructure components in the <span class="literal">kube-system</span> Namespace.</p>&#13;
<p class="indent"><a href="ch11.xhtml#ch11list1">Listing 11-1</a> presents the RoleBinding we need.</p>&#13;
<p class="noindent6"><em>edit-bind.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: rbac.authorization.k8s.io/v1&#13;
kind: RoleBinding&#13;
metadata:&#13;
  name: editor&#13;
  namespace: sample&#13;
subjects:&#13;
- kind: User&#13;
  name: me&#13;
  apiGroup: rbac.authorization.k8s.io&#13;
roleRef:&#13;
  kind: ClusterRole &#13;
  name: edit&#13;
  apiGroup: rbac.authorization.k8s.io</pre>&#13;
<p class="caption" id="ch11list1"><em>Listing 11-1: Bind the edit role to a user</em></p>&#13;
<p class="indent">Now we apply this RoleBinding to add permissions to our user:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/edit-bind.yaml</span>&#13;
rolebinding.rbac.authorization.k8s.io/editor created</pre>&#13;
<p class="indent">We’re now able to use this user to view and modify Pods, Deployments, and many other resources:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">KUBECONFIG=kubeconfig kubectl -n sample get pods</span>&#13;
NAME                        READY   STATUS    RESTARTS   AGE&#13;
read-pods-9d5565548-fbwjb   1/1     Running   0          54m&#13;
root@host01:~# <span class="codestrong1">KUBECONFIG=kubeconfig kubectl delete -f /opt/read-pods-deploy.yaml</span>&#13;
deployment.apps "read-pods" deleted</pre>&#13;
<p class="indent">However, because we used a RoleBinding and not a ClusterRoleBinding, this user has no visibility into other Namespaces:</p>&#13;
<pre><span epub:type="pagebreak" id="page_203"/>root@host01:~# <span class="codestrong1">KUBECONFIG=kubeconfig kubectl get -n kube-system pods</span>&#13;
Error from server (Forbidden): pods is forbidden: User "me" cannot list &#13;
  resource "pods" in API group "" in the namespace "kube-system"</pre>&#13;
<p class="indent">The error message displayed by <span class="literal">kubectl</span> is identical in form to the <span class="literal">message</span> field that is part of the API server’s JSON response. This is not a coincidence; <span class="literal">kubectl</span> is a friendly command line interface in front of the API server’s REST API.</p>&#13;
<h3 class="h3" id="ch00lev1sec50">Final Thoughts</h3>&#13;
<p class="noindent">The API server is an essential component in the Kubernetes control plane. Every other service in the cluster is continuously connected to the API server, watching the cluster for changes, so it can take appropriate action. Users also use the API server to deploy and configure applications and to monitor state. In this chapter, we saw the underlying REST API that the API server provides to create, retrieve, update, and delete resources. We also saw the extensive authentication and authorization capabilities built in to the API server to ensure that only authorized users and services can access and modify the cluster state.</p>&#13;
<p class="indent">In the next chapter, we’ll examine the other side of our cluster’s infrastructure: the node components. We’ll see how the <span class="literal">kubelet</span> Service hides any differences between container engines and how it uses the container capabilities we saw in <a href="part01.xhtml#part01">Part I</a> to create, start, and configure containers in the cluster.<span epub:type="pagebreak" id="page_204"/></p>&#13;
</body></html>