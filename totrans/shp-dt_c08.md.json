["```\n#load plot library and create the function\nlibrary(scatterplot3d)\nx<-seq(-10,10,0.01)\ny<-seq(-10,10,0.01)\nz<-2*sin(y)-cos(x)\nwhich(z==min(z))\nwhich(z==max(z))\nscatterplot3d(x,y,z,main=\"Scatterplot of 3-Dimensional Data\")\n```", "```\n#partition into training and test samples\nmydata<-as.data.frame(cbind(x,y,z))\nset.seed(10)\ns<-sample(1:2001,0.7*2001)\ntrain<-mydata[s,]\ntest<-mydata[-s,]\n```", "```\n#load package and create model\nlibrary(lasso2)\netastart<-NULL\nlas1<-gl1ce(z~.,train,family=gaussian(),bound=0.5,standardize=F)\nlpred1<-predict(las1,test)\nsum((lpred1-test$z)^2)/601\n```", "```\n> **las1**\nCall:\ngl1ce(formula=z ~ .,data=train,family=gaussian(),standardize=F,\n    bound=0.5)\n\nCoefficients:\n(Intercept)           x           y\n 0.05068903  0.04355682  0.00000000\n\nFamily:\n\nFamily: gaussian\nLink function: identity\n\nThe absolute L1 bound was       :  0.5\nThe Lagrangian for the bound is :  1.305622e-13\n```", "```\n#create linear model\nl<-lm(z~.,train)\nsummary(l)\nlpred<-predict(l,test)\nsum((lpred1-test$z)^2)/601\n```", "```\n> **summary(l)**\n Call:\nlm(formula=z ~ .,data=train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max\n-2.51261 -1.48663  0.07368  1.48680  2.37086\n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) 0.050689   0.041267   1.228     0.22\nx           0.043557   0.007112   6.124 1.18e-09 ***\ny                 NA                    NA\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.544 on 1398 degrees of freedom\nMultiple R-squared:  0.02613,   Adjusted R-squared:  0.02543\nF-statistic: 37.51 on 1 and 1398 DF,  p-value: 1.183e-09\n```", "```\n#load data and set seed\nmydata<-read.csv(\"QuoraSample.csv\")\nset.seed(1)\n\n#sample data to split into two datasets with stratified sampling\n#to ensure more balance in the training set with respect to depression\nm1<-mydata[mydata$Depression==1,]\nm2<-mydata[mydata$Depression==0,]\ns1<-sample(1:4,3)\ns2<-sample(1:18,6)\ntrain<-rbind(m1[s1,],m2[s2,])\ntest<-rbind(m1[-s1,],m2[-s2,])\n```", "```\n#run the homotopy-based Lasso model\nlas1<-gl1ce(factor(Depression)~.,train,family=binomial(),bound=2,standardize=F)\nlpred1<-round(predict(las1,test,type=\"response\"))\nlength(which(lpred1==test$Depression))/length(test$Depression)\n\n#run the logistic regression model\ngl<-glm(factor(Depression)~.,train,family=binomial(link=\"logit\"))\nglpred<-round(predict(gl,test,type=\"response\"))\nlength(which(glpred==test$Depression))/length(test$Depression)\n```", "```\n> **las1**\nCall:\ngl1ce(formula=factor(Depression) ~ .,data=train,family=binomial(),\n    standardize=F,bound=2)\n\nCoefficients:\n           (Intercept)                     IQ               Bullying\n           -8.31322182             0.04551602             0.00000000\n     Teacher.Hostility                Boredom     Lack.of.Motivation\n            0.00000000             0.51213722             0.00000000\n      Outside.Learning Put.in.Remedial.Course\n           -1.01281345             0.42953330\n\nFamily:\n\nFamily: binomial\nLink function: logit\n\nThe absolute L1 bound was       :  2\nThe Lagrangian for the bound is :  0.4815216\n```", "```\n> **summary(gl)**\nCALL:\nglm(formula=factor(Depression) ~ .,family=binomial(link=\"logit\"),\n    data=train)\n\nDeviance Residuals:\n        20          21           6          16          18           4\n 3.971e-06   1.060e-05    -3.971e-06  -3.971e-06  -2.110e-08\n        11           2          19\n-8.521e-06  -1.060e-05  -1.060e-05\n\nCoefficients:\n                         Estimate Std. Error z value Pr(>|z|)\n(Intercept)            -7.297e+02  1.229e+06  -0.001        1\nIQ                      3.934e+00  6.742e+03   0.001        1\nBullying               -1.421e+01  5.193e+05   0.000        1\nTeacher.Hostility       2.798e+01  3.424e+05   0.000        1\nBoredom                -1.967e+01  8.765e+04   0.000        1\nLack.of.Motivation      4.174e+01  2.496e+05   0.000        1\nOutside.Learning       -6.535e+01  2.765e+05   0.000        1\nPut.in.Remedial.Course  1.121e+02  2.712e+05   0.000        1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1.1457e+01  on 8  degrees of freedom\nResidual deviance: 5.6954e-10  on 1  degrees of freedom\nAIC: 16\nNumber of Fisher Scoring iterations: 24\n```"]