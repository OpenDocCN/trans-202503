- en: '**13'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HANDLING TIME SERIES AND TEXT DATA**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A *time series* is a dataset indexed by time, usually at regular time intervals.
    Here are some familiar examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Stock market data consisting of the price of a given equity on a daily basis,
    or even hourly, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weather data, daily or in even finer granularity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demographic data, such as the number of births, say, monthly or even yearly,
    to plan for school capacity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Electrocardiogram data measuring electrical activity in the heart at regular
    time intervals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A special type of time series is that of written or spoken speech. Here “time”
    is word positioning. If, say, we are working at the sentence level, and a sentence
    consists of eight words, there would be Word 1, Word 2, and so on through Word
    8, with the index 1 through 8 playing the role of “time.”
  prefs: []
  type: TYPE_NORMAL
- en: The field of time series methodology has been highly developed by statisticians,
    economists, and the like. As usual, ML specialists have developed their own methods,
    mainly as applications of neural networks. The methods known as *recurrent neural
    networks (RNNs)* and *long short-term memories (LSTMs)* are especially notable.
  prefs: []
  type: TYPE_NORMAL
- en: Both the statistical and ML approaches use very subtle and intricate techniques
    whose mathematical content is well above the math level of this book. Nevertheless,
    one can still build some very powerful ML applications while sticking to the basics,
    and this chapter will have this theme. It will present methods to apply the `qe*`-series
    functions to general time series problems, and to a special kind of text recognition
    setting (that does not make use of the time series nature of the text).
  prefs: []
  type: TYPE_NORMAL
- en: 13.1 Converting Time Series Data to Rectangular Form
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One often hears the terms *rectangular data* and *tabular data* in discussions
    of ML, referring to the usual *n* × *p* data frame or matrix of *n* rows, with
    each row representing one data point of *p* features. As a quick non−time series
    example we’ve used several times in this book, say we are trying to predict human
    weight from height and age, with a sample of 1,000 people. Then we would have
    *n* = 1000 and *p* = 2.
  prefs: []
  type: TYPE_NORMAL
- en: It’s clear that the words “rectangular” and “tabular” are allusions to the rectangular
    shape or table of the associated data frame or matrix. But this is rather misleading.
    Image data also has the form, such as *n* = 70000 and *p* = 28² = 784 for the
    MNIST data, yet image data is not referred to as rectangular.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of time series, though, one in fact can convert a time series to
    rectangular form and then apply ML methods, which is what we’ll do here.
  prefs: []
  type: TYPE_NORMAL
- en: '***13.1.1 Toy Example***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Say our training set time series `x` is (5,12,13,8,88,6). For concreteness,
    let’s say this is daily data, so we have six days of data here, which we’ll call
    day 1, day 2, and so on. On each day, we know the series values up to the present
    and wish to predict the next day.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use a *lag* of 2, which means that we predict a given day by the previous
    two. In `x` above, that means we:'
  prefs: []
  type: TYPE_NORMAL
- en: Predict day 3 from the 5 and 12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict day 4 from the 12 and 13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict day 5 from the 13 and 8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict day 6 from the 8 and 88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Think of what the above description (“predicting the 13 . . .”) means in terms
    of our usual “X” (features matrix) and “Y” (outcomes vector) notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/unch13equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that X has only 4 rows, not 6, and Y is of length 4, not 6\. That is due
    to our lag of 2; we need 2 prior data points. So we cannot even start our analysis
    until day 3.
  prefs: []
  type: TYPE_NORMAL
- en: Here we will deal only with *univariate* time series. But we can also handle
    the multivariate case—for example, predicting daily temperature, humidity, and
    wind speed from their previous values.
  prefs: []
  type: TYPE_NORMAL
- en: '***13.1.2 The regtools Function TStoX()***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The function `TStoX()` does what its name implies—converts a time series to
    an “X” matrix. “Y” is created too and returned in the final column. For the previous
    toy example, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Our “X” data are then in the first two columns, and “Y” is the third column.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function returns a matrix, which we can convert to a data frame if we wish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We could then use any of our `qe*`-series functions, such as random forests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In other words, everything was done as before, with one exception: we cannot
    take our holdout set to be a random subset of the data, as the remaining data
    would no longer be for consecutive time periods. We will elaborate on this point
    shortly.'
  prefs: []
  type: TYPE_NORMAL
- en: 13.2 The qeTS() Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'But, instead of calling, say, `qeRF()` “by hand,” as above, we again have a
    convenient wrapper, `qeTS()`, which transforms from time series format to “X,
    Y” form and then applies our favorite ML method to the result. The wrapper’s call
    form is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here `qeName` is the quoted name of a `qe*`-series function—for example, `'qeRF'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The argument `opts` allows us to use nondefault versions of the arguments of
    the quoted-name function. For instance, to use k-NN and *k* = 10, write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A comment should be made regarding `holdout`. While it plays its usual role
    in the `qe*`-series, note that cross-validation is usually difficult in time series
    contexts. We cannot choose for our holdout set some randomly chosen numbers from
    our data, since in time series we predict one datum from its immediately preceding,
    time-contiguous values. But here, we conduct the holdout operation on the output
    of `TStoX()`, whose output *is* rows of sets of contiguous values, so it works.
  prefs: []
  type: TYPE_NORMAL
- en: '13.3 Example: Weather Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here we will use some weather time series data collected by NASA, which is included
    in `regtools`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'That last column is precipitation. Let’s fit a model for it and then predict
    the first day after the end of the data, day 4018, based on day 4016 and day 4017:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: So, we predict a bit more than 1 inch of rain.
  prefs: []
  type: TYPE_NORMAL
- en: We used a lag of 2 days here. How would other lag values fare? We could use
    `qeFT()` here, but things are a bit complicated. For example, there is no `yName`
    argument for `qeTS()`, so instead we use `replicMeans()` (see [Section 3.2.2](ch03.xhtml#ch03lev2sec2)).
  prefs: []
  type: TYPE_NORMAL
- en: How about a lag of 1 instead of 2? We call `replicMeans()`, asking it to execute
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '1,000 times and then report the mean of the resulting 1,000 values of `testAcc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a Mean Squared Prediction Error of 2.12\. Is that good? As usual,
    let’s compare this to how well we can predict from the mean alone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Ah, we’re in business.
  prefs: []
  type: TYPE_NORMAL
- en: What about other lags?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It does seem that the lag makes some difference. A lag of 3 days seems best,
    though as usual, we must keep in mind the effect of sampling variation. (The `replicMeans()`
    function also provides a standard error, which is not shown here.)
  prefs: []
  type: TYPE_NORMAL
- en: 'How about trying some other ML methods? Let’s consider a linear model, since
    most classical time series methods use linear models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As noted, classical time series methods, for example, the *autoregressive* model,
    are linear. We see that a linear model doesn’t work so well on this particular
    dataset. Fitting a polynomial improves things substantially but still doesn’t
    match k-NN.
  prefs: []
  type: TYPE_NORMAL
- en: Maybe random forests?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: It’s still not as good as k-NN. However, with hyperparameter tuning in both
    cases, either method might end up the victor.
  prefs: []
  type: TYPE_NORMAL
- en: 13.4 Bias vs. Variance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The value of the lag impacts bias and variance, though in possibly complex ways.
  prefs: []
  type: TYPE_NORMAL
- en: A larger lag clearly increases bias; time periods in the more distant past are
    likely less relevant. It’s similar to the problem of a large *k* in k-NN.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the variance aspect is tricky. A larger lag smooths out the
    day-to-day (or other temporal) variation—that is, it reduces variance. But a larger
    lag also increases *p*, the number of features, increasing variance. The overall
    effect is thus complex.
  prefs: []
  type: TYPE_NORMAL
- en: 13.5 Text Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The field of text analysis is highly complex, similar to that of the image
    recognition field. As in the latter case, in this book we can only scratch the
    surface, in two senses:'
  prefs: []
  type: TYPE_NORMAL
- en: We will limit ourselves to document classification, as opposed to, say, language
    translation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will limit ourselves to the bag-of-words model (see the next section). This
    approach merely relies on how often various words appear in a document and not
    on the order in which the words appear.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we do not cover advanced methods such as the aforementioned *recurrent neural
    networks (RNNs)*, or even more advanced methods such as *hidden Markov models
    (HMMs)*.
  prefs: []
  type: TYPE_NORMAL
- en: '***13.5.1 The Bag-of-Words Model***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Say we wish to do automatic classification of newspaper articles. Our software
    notices that the words *bond* and *yield* are contained in some document and classifies
    it in the Financial category.
  prefs: []
  type: TYPE_NORMAL
- en: This is the *bag-of-words model*. We decide on a set of words, the “bag,” and
    compute the frequency of appearance of each word in each document class. These
    frequencies are often stored in a *document-term matrix (DTM)*, `d`. The entry
    `d[i,j]` is equal to the number of times word `j` appears in document `i` in our
    training set. Or, `d[i,j]` may simply be 1 or 0, indicating whether word `j` appears
    in document `i` at all.
  prefs: []
  type: TYPE_NORMAL
- en: The matrix `d` then becomes our “X,” with “Y” being the vector of class labels,
    such as Financial, Sports, and so on. Each row of X represents our data on one
    document, with a corresponding label in Y.
  prefs: []
  type: TYPE_NORMAL
- en: Again, this is a simple model. Our guess that the document above is in the Financial
    class may be incorrect if, say, a sentence in the document reads “The bond between
    family members will typically yield a stable family environment.” A more sophisticated
    analysis would account for, say, the words in between *bond* and *yield*. The
    bag-of-words model may, in some cases, be less accurate than a time series−based
    approach. Yet it is easy to implement and performs well in many applications.
  prefs: []
  type: TYPE_NORMAL
- en: '***13.5.2 The qeText() Function***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'And, of course, there is a `qeML` function for this, `qeText()`. It has this
    call form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the `data` argument, there is assumed one row per document, with the column
    indicated by `yName` stating the class of each document, such as Financial; the
    other column (there must be exactly two) stores the document texts. The argument
    `qeName` specifies the ML method to be used, and `opts` specifies optional arguments
    for that method. The term *stop words* refers to rather insignificant words such
    as *the* and *is*, which are ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'The role of the `kTop` argument is as follows: the software does a census of
    all the words in the documents in the training data and selects the `kTop` most
    frequent ones to use as features.'
  prefs: []
  type: TYPE_NORMAL
- en: '***13.5.3 Example: Quiz Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `qeML` package has a built-in dataset named `quizzes`, consisting of the
    text of quizzes I’ve given in various courses. One might ask whether one can predict
    the course from the text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'There were 143 quiz documents. The eighth of these will have the quiz text
    stored in `quizzes[8,1]` as one very long character string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The course number is in `quizzes[8,2]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This was ECS 158, Introduction to Parallel Computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an illustration, let’s pretend we don’t know the class of this document
    and try to predict it using random forests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The predicted course is ECS 158.
  prefs: []
  type: TYPE_NORMAL
- en: '***13.5.4 Example: AG News Dataset***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This dataset consists of short news articles in four categories: World, Sports,
    Business, and Sci/Tech. It is obtainable from the CRAN package `textdata`, which
    provides interfaces for downloading various text data testbeds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look around:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Plenty of data here with 120,000 documents. Well, maybe *too* much, as the
    run time may be long. For a quick example, let’s just take 10,000 rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let’s try fitting a model, say, SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Not too bad. We reduced a base error of 74 percent to 46 percent. The latter
    is still rather high, so we would next try tweaking the SVM hyperparameters. Note
    that `kTop` is also a hyperparameter! We should try different values for it too.
  prefs: []
  type: TYPE_NORMAL
- en: 13.6 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We see here that, even without advanced methods, one may be able to fit good
    prediction models for time series and text data. In both cases, `qe*`-series functions
    `qeTS()` and `qeText()` enable convenient use of our favorite ML methods.
  prefs: []
  type: TYPE_NORMAL
