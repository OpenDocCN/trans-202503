- en: '18'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '18'
- en: AFFINITY AND DEVICES
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和性和设备
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: The ideal application exhibits complete simplicity. It is simple to design.
    It is simple to develop. It is simple to deploy. Its individual components are
    stateless, so it’s easy to scale to serve as many users as needed. The individual
    service endpoints act as pure functions where the output is determined solely
    by the input. The application operates on a reasonable amount of data, with modest
    CPU and memory requirements, and requests and responses easily fit into a JSON
    structure that is at most a couple of kilobytes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 理想化的应用程序展示了完全的简单性。它的设计简单，开发简单，部署简单。它的各个组件都是无状态的，因此很容易扩展以服务尽可能多的用户。每个服务端点都充当纯粹的函数，其输出仅由输入决定。应用程序处理的数据量合理，CPU和内存需求适中，请求和响应容易适配到一个最多只有几千字节的JSON结构中。
- en: Of course, outside of tutorials, ideal applications don’t exist. Real-world
    applications store state, both in long-term persistent storage and in caches that
    can be accessed quickly. Real-world applications have data security and authorization
    concerns, so they need to authenticate users, remember who those users are, and
    limit access accordingly. And many real-world applications need to access specialized
    hardware rather than just using idealized CPU, memory, storage, and network resources.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，除了教程之外，理想化的应用程序是不存在的。现实世界中的应用程序会存储状态，包括长期的持久存储和可以快速访问的缓存。现实世界的应用程序有数据安全和授权方面的考虑，因此它们需要进行用户身份验证，记住用户是谁，并相应地限制访问权限。许多现实世界的应用程序还需要访问专用硬件，而不仅仅是使用理想化的CPU、内存、存储和网络资源。
- en: We want to deploy real-world applications on our Kubernetes cluster, not just
    idealized applications. This means that we need to make smart decisions about
    how to deploy the application components that move us away from an ideal world
    in which the cluster decides how many container instances to run and where to
    schedule them. However, we don’t want to create an application architecture that
    is so rigid that we lose our cluster’s scalability and resiliency. Instead, we
    want to work within the cluster to give it hints about how to deploy our application
    components while still maintaining as much flexibility as possible. In this chapter,
    we’ll explore how our application components can enforce a little bit of coupling
    to other components or to specialized hardware without losing the benefits of
    Kubernetes.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在Kubernetes集群上部署现实世界中的应用程序，而不仅仅是理想化的应用程序。这意味着我们需要做出明智的决策，关于如何部署那些让我们远离理想化世界的应用程序组件——在那个世界中，集群决定运行多少个容器实例以及如何调度它们。然而，我们不想创建一个过于僵化的应用架构，以至于失去集群的可扩展性和弹性。相反，我们希望在集群内工作，给集群一些提示，指导如何部署我们的应用组件，同时尽可能保持灵活性。在本章中，我们将探讨我们的应用组件如何在不失去Kubernetes优势的情况下，强制与其他组件或专用硬件之间形成一定的耦合。
- en: Affinity and Anti-affinity
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 亲和性与反亲和性
- en: We’ll begin by looking at the case in which we want to manage the scheduling
    of Pods so that we can prefer or avoid co-locating multiple containers on the
    same node. For example, if we have two containers that consume significant network
    bandwidth communicating with each other, we might want those two containers to
    run together on a node to reduce latency and avoid slowing down the rest of the
    cluster. Or, if we want to ensure that a highly available component can survive
    the loss of a node in the cluster, we may want to split Pod instances so they
    run on as many different cluster nodes as possible.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先看一下管理Pods调度的情况，这样我们可以优先或避免将多个容器部署在同一个节点上。例如，如果我们有两个消耗大量网络带宽并相互通信的容器，我们可能希望这两个容器一起运行在一个节点上，以减少延迟并避免拖慢集群中的其他部分。或者，如果我们希望确保一个高可用组件能够在集群中的一个节点丢失时依然存活，我们可能希望将Pod实例拆分，使它们尽可能在不同的集群节点上运行。
- en: One way to co-locate containers is to combine multiple separate containers into
    a single Pod specification. That is a great solution for cases in which two processes
    are completely dependent on each other. However, it removes the ability to scale
    the instances separately. For example, in a web application backed by distributed
    storage, we might need many more instances of the web server process than we would
    need of the storage process. We need to place those application components in
    different Pods to be able to scale them separately.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 合并多个独立的容器到一个 Pod 规范中，是共置容器的一种方法。这对于两个进程完全相互依赖的情况是一个很好的解决方案。然而，这也失去了单独扩展实例的能力。例如，在一个由分布式存储支持的
    Web 应用中，我们可能需要比存储进程更多的 Web 服务器进程实例。我们需要将这些应用组件放置在不同的 Pod 中，以便能够单独扩展它们。
- en: In [Chapter 8](ch08.xhtml#ch08), when we wanted to guarantee that a Pod ran
    on a specified node, we added the `nodeName` field to the Pod specification to
    override the scheduler. That was fine for an example, but for a real application
    it would eliminate the scaling and failover that are essential for performance
    and reliability. Instead, we’ll use the Kubernetes concept of *affinity* to give
    the scheduler hints about how to allocate Pods without forcing any Pod to run
    on a specific node.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 8 章](ch08.xhtml#ch08)中，当我们想确保一个 Pod 在指定的节点上运行时，我们在 Pod 规范中添加了 `nodeName`
    字段以覆盖调度器。这个方法对于示例来说是可以的，但对于实际应用，它会消除性能和可靠性所必需的扩展和故障转移功能。相反，我们将使用 Kubernetes 的
    *亲和性* 概念，为调度器提供关于如何分配 Pod 的提示，而不强制任何 Pod 必须在特定节点上运行。
- en: Affinity allows us to restrict where a Pod should be scheduled based on the
    presence of other Pods. Let’s look at an example using the `iperf3` network testing
    application.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和性允许我们根据其他 Pods 的存在来限制 Pod 应该调度到哪里。让我们来看一个使用 `iperf3` 网络测试应用的例子。
- en: '**CLUSTER ZONES**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**集群区域**'
- en: Pod affinity is most valuable for large clusters that span multiple networks.
    For example, we might deploy a Kubernetes cluster to multiple different data centers
    to eliminate single points of failure. In those cases, we would configure affinity
    based on a zone, which might contain many nodes. Here, we have only a small example
    cluster, so we’ll treat each node in our cluster as a separate zone.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 亲和性对于跨多个网络的大型集群最为有用。例如，我们可能会将 Kubernetes 集群部署到多个不同的数据中心，以消除单点故障。在这些情况下，我们会根据一个包含多个节点的区域来配置亲和性。在这里，我们只有一个小型示例集群，所以我们将把集群中的每个节点视为一个独立的区域。
- en: Anti-affinity
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 反亲和性
- en: 'Let’s start with the opposite of affinity: *anti-affinity*. Anti-affinity causes
    the Kubernetes scheduler to avoid co-locating Pods. In this case, we’ll create
    a Deployment with three separate `iperf3` server Pods, but we’ll use anti-affinity
    to distribute those three Pods across our nodes so that each node gets a Pod.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从亲和性的反面开始：*反亲和性*。反亲和性会导致 Kubernetes 调度器避免将 Pods 共置在一起。在这种情况下，我们将创建一个有三个独立
    `iperf3` 服务器 Pod 的 Deployment，但我们将使用反亲和性规则将这三个 Pod 分布到不同的节点上，使每个节点都有一个 Pod。
- en: '**NOTE**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例代码库位于* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。
    *有关如何设置的详细信息，请参见 [第 xx 页](ch00.xhtml#ch00lev1sec2)中的“运行示例”。*'
- en: 'Here’s the YAML definition we need:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们需要的 YAML 定义：
- en: '*ipf-server.yaml*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*ipf-server.yaml*'
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This Deployment resource is typical except for the new `affinity` section ➊.
    We specify an anti-affinity rule that is based on the same label that the Deployment
    uses to manage its Pods. With this rule, we specify that we don’t want a Pod to
    be scheduled into a zone that already has a Pod with the `app=iperf-server` label.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Deployment 资源是典型的，除了新的 `affinity` 部分 ➊。我们指定了一个基于 Deployment 用来管理其 Pods 的相同标签的反亲和性规则。通过这个规则，我们指定不希望将
    Pod 调度到已经有 `app=iperf-server` 标签的区域。
- en: The `topologyKey` ➌ specifies the size of the zone. In this case, each node
    in the cluster has a different `hostname` label, so each node is considered to
    be a different zone. The anti-affinity rule therefore prevents `kube-scheduler`
    from placing a second Pod onto a node after the first Pod has already been scheduled
    there.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`topologyKey` ➌ 指定了区域的大小。在这种情况下，集群中的每个节点都有不同的 `hostname` 标签，因此每个节点都被视为一个不同的区域。因此，反亲和性规则会阻止
    `kube-scheduler` 在第一个 Pod 已经调度到某个节点后，再将第二个 Pod 调度到该节点。'
- en: Finally, because we specified the rule using `requiredDuringScheduling` ➋, it’s
    a *hard* anti-affinity rule, which means that the scheduler won’t schedule the
    Pod unless it can satisfy the rule. It is also possible to use `preferredDuringScheduling`
    and assign a weight to give the scheduler a hint without preventing Pod scheduling
    if the rule can’t be satisfied.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，因为我们使用 `requiredDuringScheduling` ➋ 指定了规则，所以这是一个 *硬* 反亲和性规则，这意味着调度器不会调度 Pod，除非它能满足这个规则。如果规则不能满足，也可以使用
    `preferredDuringScheduling` 并分配一个权重，给调度器提供提示，但不会阻止 Pod 调度。
- en: '**NOTE**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The topologyKey can be based on any label that’s applied on the node. Cloud-based
    Kubernetes distributions typically automatically apply labels to each node based
    on the availability zone for that node, making it easy to use anti-affinity to
    spread Pods across availability zones for redundancy.*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*topologyKey 可以基于应用于节点的任何标签。基于云的 Kubernetes 分发通常会根据节点的可用区自动为每个节点应用标签，这使得使用反亲和性在可用区之间分布
    Pods 以实现冗余变得容易。*'
- en: 'Let’s apply this Deployment and see the result:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用这个 Deployment 并查看结果：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As soon as our Pods are running, we see that a Pod has been allocated to each
    node in the cluster:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的 Pod 启动运行，我们会看到每个节点都被分配了一个 Pod：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Because we have three nodes and three instances, it’s essentially identical
    to using a DaemonSet, but this approach is more flexible because it doesn’t require
    an instance on every node. In a large cluster, we still might need only a few
    Pod instances to meet demand for this service. Using anti-affinity with zones
    based on hostnames allows us to specify the correct scale for our Deployment while
    still distributing each Pod to a distinct node for higher availability. And anti-affinity
    can be used to distribute Pods across other types of zones as well.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们有三个节点和三个实例，这与使用 DaemonSet 本质上是相同的，但这种方法更加灵活，因为它不需要每个节点上都有实例。在大型集群中，我们可能只需要少量的
    Pod 实例来满足服务需求。使用基于主机名的反亲和性与区域相结合，可以让我们在仍然将每个 Pod 分配到不同节点以提高可用性的同时，指定部署的正确规模。而且反亲和性也可以用于将
    Pods 分布到其他类型的区域。
- en: 'Before we continue, let’s create a Service with which our `iperf3` clients
    will be able to find a server instance. Here’s the YAML:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们创建一个 Service，供我们的 `iperf3` 客户端找到一个服务器实例。以下是 YAML 文件：
- en: '*ipf-svc.yaml*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*ipf-svc.yaml*'
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s apply this to the cluster:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将此应用于集群：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The Service picks up all three Pods:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 服务会启动所有三个 Pod：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `ep` is short for `endpoints`. Each Service has an associated Endpoint object
    that records the current Pods that are receiving traffic for the Service.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`ep` 是 `endpoints` 的缩写。每个 Service 都有一个相关联的 Endpoint 对象，用来记录当前接收流量的 Pods。'
- en: Affinity
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲和性
- en: 'We’re now ready to deploy our `iperf3` client to use these server instances.
    We would like to distribute the clients to each node in the same way, but we want
    to make sure that each client is deployed to a node that has a server instance.
    To do this, we’ll use both an affinity and an anti-affinity rule:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备将 `iperf3` 客户端部署到这些服务器实例上。我们希望以相同的方式将客户端分配到每个节点，但我们需要确保每个客户端都部署到一个有服务器实例的节点上。为此，我们将使用亲和性和反亲和性规则：
- en: '*ipf-client.yaml*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*ipf-client.yaml*'
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The additional `podAffinity` rule ➊ ensures that each client instance is deployed
    to a node only if a server instance is already present. The fields in an affinity
    rule work the same way as an anti-affinity rule.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的 `podAffinity` 规则 ➊ 确保每个客户端实例只有在服务器实例已经存在的情况下才会部署到节点。亲和性规则中的字段与反亲和性规则相同。
- en: 'Let’s deploy the client instances:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们部署客户端实例：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After these Pods are running, we can see that they have also been distributed
    across all three nodes in the cluster:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些 Pods 运行后，我们可以看到它们已经分布到集群中的所有三个节点：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It may seem like we’ve deployed our `iperf3` client and server in a way that
    enables each client to talk to its local server instance, maximizing the bandwidth
    between client and server. However, that’s not actually the case. Because the
    `iperf-server` Service is configured with all three Pods, each client Pod is connecting
    to a random server. As a result, our clients may not behave correctly. You might
    see logs indicating that a client is able to connect to a server, but you might
    also see client Pods in the `Error` or `CrashLoopBackOff` state, with log output
    like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们已将`iperf3`客户端和服务器部署得能够使每个客户端连接到其本地的服务器实例，从而最大化客户端和服务器之间的带宽。然而，实际上并非如此。因为`iperf-server`服务配置了所有三个
    Pods，每个客户端 Pod 都连接到一个随机的服务器。因此，我们的客户端可能无法正常工作。你可能会看到日志显示某个客户端能够连接到服务器，但也可能会看到客户端
    Pods 处于`Error`或`CrashLoopBackOff`状态，并且有类似如下的日志输出：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This indicates that a client is connecting to a server that already has a client
    connected, which means that we must have at least two clients using the same server.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示某个客户端正在连接到已经有客户端连接的服务器，这意味着至少有两个客户端在使用同一个服务器。
- en: Service Traffic Routing
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务流量路由
- en: 'We would like to configure our client Pods with the ability to access the local
    server Pod we deployed rather than a server Pod on a different node. Let’s start
    by confirming that traffic is being routed randomly across all three server Pods.
    We can examine the `iptables` rules created by `kube-proxy` for this Service:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望配置我们的客户端 Pods，使其能够访问我们部署的本地服务器 Pod，而不是不同节点上的服务器 Pod。让我们首先确认流量是否在所有三个服务器
    Pods 之间随机路由。我们可以查看`kube-proxy`为该服务创建的`iptables`规则：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We’re running this command on *host01*, and we see that there are three separate
    `iptables` rules, with a random selection of the destination. This means that
    the `iperf3` client on *host01* could potentially be routed to any server Pod.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*host01*上运行这个命令，看到有三条独立的`iptables`规则，并且目标是随机选择的。这意味着，*host01*上的`iperf3`客户端可能会被路由到任何一个服务器
    Pod。
- en: To fix that, we need to change the internal traffic policy configuration of
    our Service. By default, the policy is `Cluster`, indicating that all Pods in
    the cluster are valid destinations. We can change the policy to `Local`, which
    restricts the Service to route only to Pods on the same node.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要更改我们服务的内部流量策略配置。默认情况下，策略是`Cluster`，表示集群中的所有 Pods 都是有效的目标。我们可以将策略更改为`Local`，这样就会限制服务仅路由到同一节点上的
    Pods。
- en: 'Let’s patch the Service to change this policy:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修补服务来更改这个策略：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The change takes effect immediately, as we can see by looking at the `iptables`
    rules again:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 更改立即生效，我们可以通过再次查看`iptables`规则来验证：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This time, only one possible destination is configured on *host01*, as there
    is only one local Pod instance for this Service.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，只有一个可能的目标被配置在*host01*上，因为该服务只有一个本地 Pod 实例。
- en: 'After a few minutes, the `iperf3` clients now show the kind of output we expect
    to see:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，`iperf3`客户端现在显示出我们预期看到的输出：
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Not only are all of the clients able to connect to a unique server, but the
    performance is consistently high as the network connection is local to each node.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅所有客户端都能够连接到独特的服务器，而且由于网络连接是本地到每个节点的，性能始终很高。
- en: 'Before we go further, let’s clean up these resources:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们清理这些资源：
- en: '[PRE14]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Although the `Local` internal traffic policy is useful for maximizing bandwidth
    between client and server, it has a major limitation. If a node does not contain
    a healthy Pod instance, clients on that node will not be able to access the Service
    at all, even if there are healthy instances on other nodes. It is critical when
    using this design pattern to also configure a readiness probe, as described in
    [Chapter 13](ch13.xhtml#ch13), that checks not only the Pod itself but also its
    Service dependencies. This way, if a Service is inaccessible on a particular node,
    the client on that node will also report itself to be unhealthy so that no traffic
    will be routed to it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`Local`内部流量策略有助于最大化客户端和服务器之间的带宽，但它也有一个主要的限制。如果某个节点没有健康的 Pod 实例，那么该节点上的客户端将根本无法访问服务，即使其他节点上有健康的实例。在使用这种设计模式时，至关重要的是还要配置一个就绪探针，如[第13章](ch13.xhtml#ch13)中所述，它不仅检查
    Pod 本身，还检查其服务依赖性。这样，如果某个节点上的服务无法访问，该节点上的客户端也会报告自己为不健康，从而不会有流量路由到它。
- en: The affinity and anti-affinity capabilities we’ve seen allows us to give hints
    to the scheduler without losing the scalability and resilience we want for our
    application components. However, even though it might be tempting to use these
    features whenever we have closely connected components in our application architecture,
    it’s probably best to allow the scheduler to work unhindered and add affinity
    only for cases in which real performance testing shows that it makes a significant
    difference.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所看到的亲和性和反亲和性功能使我们能够在不牺牲应用组件的可扩展性和弹性的前提下，向调度器提供提示。然而，尽管在应用架构中有紧密连接的组件时，使用这些功能可能很有诱惑力，但最好是让调度器无阻碍地工作，仅在实际的性能测试表明它能够带来显著差异时，才添加亲和性。
- en: Service routing for improved performance is an active area of development in
    Kubernetes. For clusters running across multiple zones, a new feature called Topology
    Aware Hints can enable Kubernetes to route connections to Services to the closest
    instances wherever possible, improving network performance while still allowing
    cross-zone traffic where necessary.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能，服务路由是 Kubernetes 中的一个活跃开发领域。对于跨多个区域运行的集群，一种名为拓扑感知提示（Topology Aware Hints）的新功能，可以使
    Kubernetes 将连接路由到离服务实例最近的地方，从而提高网络性能，同时在必要时允许跨区域流量。
- en: Hardware Resources
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件资源
- en: Affinity and anti-affinity allow us to control where Pods are scheduled but
    should be used only if necessary. But what about cases for which a Pod needs access
    to some specialized hardware that is available only on some nodes? For example,
    we might have processing that would benefit from a graphics processing unit (GPU),
    but we might limit the number of GPU nodes in the cluster to reduce cost. In that
    case, it is absolutely necessary to ensure that the Pod is scheduled in the right
    place.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和性和反亲和性允许我们控制 Pods 的调度位置，但应该仅在必要时使用。那么，对于某些 Pod 需要访问仅在某些节点上可用的专用硬件的情况该怎么办呢？例如，我们可能有需要图形处理单元（GPU）加速的处理任务，但为了降低成本，我们可能会限制集群中的
    GPU 节点数量。在这种情况下，确保 Pod 被调度到正确的地方是绝对必要的。
- en: As before, we could tie our Pod directly to a node using `nodeName`. But we
    might have many nodes in our cluster with the right hardware, so what we really
    want is to be able to tell Kubernetes about the requirement and then let the scheduler
    decide how to satisfy it.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们可以通过 `nodeName` 将 Pod 直接绑定到某个节点。但集群中可能有多个节点具备所需的硬件，因此我们真正需要的是能够向 Kubernetes
    说明需求，然后让调度器决定如何满足这个需求。
- en: 'Kubernetes provides two related methods to address this need: device plug-ins
    and extended resources. A device plug-in provides the most complete functionality,
    but the plug-in itself must exist for the hardware device. Meanwhile, extended
    resources can be used for any hardware device, but the Kubernetes cluster only
    tracks allocation of the resource; it doesn’t actually manage its availability
    in the container.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了两种相关的方法来解决这一需求：设备插件和扩展资源。设备插件提供了最完整的功能，但插件本身必须存在于硬件设备上。同时，扩展资源可以用于任何硬件设备，但
    Kubernetes 集群只会跟踪该资源的分配，而不实际管理其在容器中的可用性。
- en: Implementing a device plug-in requires close collaboration with `kubelet`. Similar
    to the storage plug-in architecture we saw in [Chapter 15](ch15.xhtml#ch15), a
    device plug-in registers itself with the `kubelet` instance running on a node,
    identifying any devices it manages. Pods identify any devices they require, and
    the device manager tells `kubelet` how to make the device available inside the
    container (typically by mounting the device from the host into the container’s
    filesystem).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 实现设备插件需要与 `kubelet` 紧密协作。类似于我们在[第 15 章](ch15.xhtml#ch15)中看到的存储插件架构，设备插件会向运行在节点上的
    `kubelet` 实例注册自己，标识它管理的任何设备。Pod 标识它们所需的设备，设备管理器告诉 `kubelet` 如何在容器内使设备可用（通常是通过将设备从主机挂载到容器的文件系统中）。
- en: Because we’re operating in a virtualized example cluster, we don’t have any
    specialized hardware to demonstrate a device plug-in, but an extended resource
    works identically from an allocation standpoint, so we can still get a feel for
    the overall approach.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们是在一个虚拟化的示例集群中操作，因此没有专用硬件来演示设备插件，但扩展资源从分配的角度来看是相同的，因此我们仍然可以对整体方法有所了解。
- en: Let’s begin by updating the cluster to indicate that one of the nodes has an
    example extended resource. We do this by patching the `status` for the node. Ideally,
    we could do this with `kubectl patch`, but unfortunately it’s not possible to
    update the `status` of a resource with that command, so we’re reduced to using
    `curl` to call the Kubernetes API directly. The */opt* directory has a script
    to make this easy. [Listing 18-1](ch18.xhtml#ch18list1) presents the relevant
    part.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过更新集群，指示某个节点具有示例扩展资源。我们通过修补节点的 `status` 来实现这一点。理想情况下，我们可以使用 `kubectl patch`
    来执行此操作，但不幸的是，无法通过该命令更新资源的 `status`，因此我们只能使用 `curl` 直接调用 Kubernetes API。 */opt*
    目录下有一个脚本可以简化此过程。[清单 18-1](ch18.xhtml#ch18list1)展示了相关部分。
- en: '*add-hw.sh*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*add-hw.sh*'
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*Listing 18-1: Special hardware script*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 18-1：特殊硬件脚本*'
- en: This `curl` command sends a JSON patch object to update the `status` field for
    the node, adding an entry called `bookofkubernetes.com/special-hw` under `capacity`.
    The `~1` acts as a slash character.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该 `curl` 命令发送一个 JSON 补丁对象来更新节点的 `status` 字段，在 `capacity` 下添加一个名为 `bookofkubernetes.com/special-hw`
    的条目。`~1` 起到斜杠字符的作用。
- en: 'Run the script to update the node:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本以更新节点：
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The response from the API server includes the entire Node resource. Let’s double-check
    just the field we care about to make sure it applied:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从 API 服务器返回的响应包括整个节点的资源。让我们再次确认我们关心的字段，以确保它已经应用：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The extended resource shows up alongside the standard resources for the node.
    We can now request this resource similar to how we request standard resources,
    as we saw in [Chapter 14](ch14.xhtml#ch14).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展资源与节点的标准资源一起显示。现在，我们可以像请求标准资源一样请求该资源，正如我们在[第14章](ch14.xhtml#ch14)中看到的那样。
- en: 'Here’s a Pod that requests the special hardware:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个请求特殊硬件的 Pod：
- en: '*hw.yaml*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*hw.yaml*'
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We specify the requirement for the special hardware using the `resources` field.
    The resource is either allocated or not allocated; thus, there’s no distinction
    between requests and limits, so Kubernetes expects us to specify it using `limits`.
    When we apply this to the cluster, the Kubernetes scheduler will ensure that this
    Pod runs on a node that can meet this requirement:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `resources` 字段来指定对特殊硬件的需求。资源要么被分配，要么不分配；因此，`requests` 和 `limits` 之间没有区别，所以
    Kubernetes 希望我们使用 `limits` 来指定。当我们将此应用到集群时，Kubernetes 调度器会确保该 Pod 运行在能够满足此要求的节点上：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As a result, the Pod ends up on `host02`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Pod 最终被调度到 `host02`：
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Additionally, the node status now reflects an allocation for this extended
    resource:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，节点状态现在反映了该扩展资源的分配：
- en: '[PRE21]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Both the available quantity of three `special-hw` that we specified when we
    added the extended resource in [Listing 18-1](ch18.xhtml#ch18list1) and the allocation
    of that resource to our Pod are arbitrary. The extended resource acts like a semaphore
    in preventing too many users from using the same resource, but we would need to
    add additional processing to deconflict multiple users if we really had three
    separate special hardware devices on the same node.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[清单 18-1](ch18.xhtml#ch18list1)中添加扩展资源时，所指定的三台 `special-hw` 的可用数量，以及该资源分配给
    Pod 的方式，都是任意的。扩展资源就像一个信号量，防止过多的用户同时使用同一资源，但如果我们真的有三个单独的特殊硬件设备在同一节点上运行，我们需要增加额外的处理来避免多个用户冲突。
- en: 'If we do try to over-allocate based on what we specified is available, the
    Pod won’t be scheduled. We can confirm this if we try to add another Pod that
    needs all three of our special hardware devices:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们根据指定的可用资源尝试过度分配，Pod 将无法调度。如果我们尝试添加另一个需要所有三个特殊硬件设备的 Pod，我们可以确认这一点：
- en: '*hw3.yaml*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*hw3.yaml*'
- en: '[PRE22]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s try to add this Pod to the cluster:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试将这个 Pod 添加到集群中：
- en: '[PRE23]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Because there aren’t enough special hardware devices available, this Pod stays
    in the Pending state:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有足够的特殊硬件设备可用，因此这个 Pod 保持在 Pending 状态：
- en: '[PRE24]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The Pod will wait for the hardware to be available. Let’s delete our original
    Pod to free up room:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 将等待硬件可用。让我们删除原始的 Pod 以释放空间：
- en: '[PRE25]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Our new Pod will now start running:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新 Pod 现在将开始运行：
- en: '[PRE26]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As before, the Pod was scheduled onto `host02` because of the special hardware
    requirement.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，Pod 被调度到 `host02`，这是由于特殊硬件的需求。
- en: Device drivers work identically from an allocation standpoint. In both cases,
    we use the `limits` field to identify our hardware requirements. The only difference
    is that we don’t need to patch the node manually to record the resource, because
    `kubelet` updates the node’s status automatically when the device driver registers.
    Additionally, `kubelet` invokes the device driver to perform any necessary allocation
    and configuration of the hardware when a container is created.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 设备驱动程序从资源分配的角度来看是相同的。在这两种情况下，我们都使用`limits`字段来确定硬件要求。唯一的不同之处在于，我们不需要手动修补节点来记录资源，因为当设备驱动程序注册时，`kubelet`会自动更新节点的状态。此外，当容器创建时，`kubelet`会调用设备驱动程序来执行任何必要的硬件分配和配置。
- en: Final Thoughts
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最终思考
- en: Unlike ideal applications, in the real world we often must deal with closely
    coupled application components and the need for specialized hardware. It’s critical
    that we account for those application requirements without losing the flexibility
    and resiliency that we gain from deploying our application to a Kubernetes cluster.
    In this chapter, we’ve seen how affinity and device drivers allow us to provide
    hints and resource requirements to the scheduler while still allowing it the flexibility
    to manage the application at scale dynamically.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 与理想应用程序不同，在现实世界中，我们通常需要处理紧密耦合的应用组件和对专用硬件的需求。至关重要的是，我们必须在不失去从将应用程序部署到Kubernetes集群中获得的灵活性和弹性的前提下，考虑这些应用程序的需求。在本章中，我们看到亲和性和设备驱动程序如何使我们能够向调度程序提供提示和资源要求，同时仍然允许它具有动态管理应用程序规模的灵活性。
- en: Scheduling is not the only concern we might have as we consider how to obtain
    the desired behavior and performance from real-world applications. In the next
    chapter, we’ll see how we can shape the processing and memory allocation for our
    Pods through the use of quality-of-service classes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 调度并不是我们在考虑如何从现实世界应用程序中获得所需行为和性能时唯一需要关注的问题。在下一章中，我们将看到如何通过使用服务质量类来塑造我们Pod的处理和内存分配。
