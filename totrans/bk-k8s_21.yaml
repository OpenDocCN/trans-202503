- en: '18'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AFFINITY AND DEVICES
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The ideal application exhibits complete simplicity. It is simple to design.
    It is simple to develop. It is simple to deploy. Its individual components are
    stateless, so it’s easy to scale to serve as many users as needed. The individual
    service endpoints act as pure functions where the output is determined solely
    by the input. The application operates on a reasonable amount of data, with modest
    CPU and memory requirements, and requests and responses easily fit into a JSON
    structure that is at most a couple of kilobytes.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, outside of tutorials, ideal applications don’t exist. Real-world
    applications store state, both in long-term persistent storage and in caches that
    can be accessed quickly. Real-world applications have data security and authorization
    concerns, so they need to authenticate users, remember who those users are, and
    limit access accordingly. And many real-world applications need to access specialized
    hardware rather than just using idealized CPU, memory, storage, and network resources.
  prefs: []
  type: TYPE_NORMAL
- en: We want to deploy real-world applications on our Kubernetes cluster, not just
    idealized applications. This means that we need to make smart decisions about
    how to deploy the application components that move us away from an ideal world
    in which the cluster decides how many container instances to run and where to
    schedule them. However, we don’t want to create an application architecture that
    is so rigid that we lose our cluster’s scalability and resiliency. Instead, we
    want to work within the cluster to give it hints about how to deploy our application
    components while still maintaining as much flexibility as possible. In this chapter,
    we’ll explore how our application components can enforce a little bit of coupling
    to other components or to specialized hardware without losing the benefits of
    Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Affinity and Anti-affinity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll begin by looking at the case in which we want to manage the scheduling
    of Pods so that we can prefer or avoid co-locating multiple containers on the
    same node. For example, if we have two containers that consume significant network
    bandwidth communicating with each other, we might want those two containers to
    run together on a node to reduce latency and avoid slowing down the rest of the
    cluster. Or, if we want to ensure that a highly available component can survive
    the loss of a node in the cluster, we may want to split Pod instances so they
    run on as many different cluster nodes as possible.
  prefs: []
  type: TYPE_NORMAL
- en: One way to co-locate containers is to combine multiple separate containers into
    a single Pod specification. That is a great solution for cases in which two processes
    are completely dependent on each other. However, it removes the ability to scale
    the instances separately. For example, in a web application backed by distributed
    storage, we might need many more instances of the web server process than we would
    need of the storage process. We need to place those application components in
    different Pods to be able to scale them separately.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 8](ch08.xhtml#ch08), when we wanted to guarantee that a Pod ran
    on a specified node, we added the `nodeName` field to the Pod specification to
    override the scheduler. That was fine for an example, but for a real application
    it would eliminate the scaling and failover that are essential for performance
    and reliability. Instead, we’ll use the Kubernetes concept of *affinity* to give
    the scheduler hints about how to allocate Pods without forcing any Pod to run
    on a specific node.
  prefs: []
  type: TYPE_NORMAL
- en: Affinity allows us to restrict where a Pod should be scheduled based on the
    presence of other Pods. Let’s look at an example using the `iperf3` network testing
    application.
  prefs: []
  type: TYPE_NORMAL
- en: '**CLUSTER ZONES**'
  prefs: []
  type: TYPE_NORMAL
- en: Pod affinity is most valuable for large clusters that span multiple networks.
    For example, we might deploy a Kubernetes cluster to multiple different data centers
    to eliminate single points of failure. In those cases, we would configure affinity
    based on a zone, which might contain many nodes. Here, we have only a small example
    cluster, so we’ll treat each node in our cluster as a separate zone.
  prefs: []
  type: TYPE_NORMAL
- en: Anti-affinity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s start with the opposite of affinity: *anti-affinity*. Anti-affinity causes
    the Kubernetes scheduler to avoid co-locating Pods. In this case, we’ll create
    a Deployment with three separate `iperf3` server Pods, but we’ll use anti-affinity
    to distribute those three Pods across our nodes so that each node gets a Pod.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the YAML definition we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ipf-server.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This Deployment resource is typical except for the new `affinity` section ➊.
    We specify an anti-affinity rule that is based on the same label that the Deployment
    uses to manage its Pods. With this rule, we specify that we don’t want a Pod to
    be scheduled into a zone that already has a Pod with the `app=iperf-server` label.
  prefs: []
  type: TYPE_NORMAL
- en: The `topologyKey` ➌ specifies the size of the zone. In this case, each node
    in the cluster has a different `hostname` label, so each node is considered to
    be a different zone. The anti-affinity rule therefore prevents `kube-scheduler`
    from placing a second Pod onto a node after the first Pod has already been scheduled
    there.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, because we specified the rule using `requiredDuringScheduling` ➋, it’s
    a *hard* anti-affinity rule, which means that the scheduler won’t schedule the
    Pod unless it can satisfy the rule. It is also possible to use `preferredDuringScheduling`
    and assign a weight to give the scheduler a hint without preventing Pod scheduling
    if the rule can’t be satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The topologyKey can be based on any label that’s applied on the node. Cloud-based
    Kubernetes distributions typically automatically apply labels to each node based
    on the availability zone for that node, making it easy to use anti-affinity to
    spread Pods across availability zones for redundancy.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply this Deployment and see the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As soon as our Pods are running, we see that a Pod has been allocated to each
    node in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Because we have three nodes and three instances, it’s essentially identical
    to using a DaemonSet, but this approach is more flexible because it doesn’t require
    an instance on every node. In a large cluster, we still might need only a few
    Pod instances to meet demand for this service. Using anti-affinity with zones
    based on hostnames allows us to specify the correct scale for our Deployment while
    still distributing each Pod to a distinct node for higher availability. And anti-affinity
    can be used to distribute Pods across other types of zones as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we continue, let’s create a Service with which our `iperf3` clients
    will be able to find a server instance. Here’s the YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ipf-svc.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s apply this to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The Service picks up all three Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `ep` is short for `endpoints`. Each Service has an associated Endpoint object
    that records the current Pods that are receiving traffic for the Service.
  prefs: []
  type: TYPE_NORMAL
- en: Affinity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We’re now ready to deploy our `iperf3` client to use these server instances.
    We would like to distribute the clients to each node in the same way, but we want
    to make sure that each client is deployed to a node that has a server instance.
    To do this, we’ll use both an affinity and an anti-affinity rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ipf-client.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The additional `podAffinity` rule ➊ ensures that each client instance is deployed
    to a node only if a server instance is already present. The fields in an affinity
    rule work the same way as an anti-affinity rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s deploy the client instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After these Pods are running, we can see that they have also been distributed
    across all three nodes in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'It may seem like we’ve deployed our `iperf3` client and server in a way that
    enables each client to talk to its local server instance, maximizing the bandwidth
    between client and server. However, that’s not actually the case. Because the
    `iperf-server` Service is configured with all three Pods, each client Pod is connecting
    to a random server. As a result, our clients may not behave correctly. You might
    see logs indicating that a client is able to connect to a server, but you might
    also see client Pods in the `Error` or `CrashLoopBackOff` state, with log output
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that a client is connecting to a server that already has a client
    connected, which means that we must have at least two clients using the same server.
  prefs: []
  type: TYPE_NORMAL
- en: Service Traffic Routing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We would like to configure our client Pods with the ability to access the local
    server Pod we deployed rather than a server Pod on a different node. Let’s start
    by confirming that traffic is being routed randomly across all three server Pods.
    We can examine the `iptables` rules created by `kube-proxy` for this Service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We’re running this command on *host01*, and we see that there are three separate
    `iptables` rules, with a random selection of the destination. This means that
    the `iperf3` client on *host01* could potentially be routed to any server Pod.
  prefs: []
  type: TYPE_NORMAL
- en: To fix that, we need to change the internal traffic policy configuration of
    our Service. By default, the policy is `Cluster`, indicating that all Pods in
    the cluster are valid destinations. We can change the policy to `Local`, which
    restricts the Service to route only to Pods on the same node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s patch the Service to change this policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The change takes effect immediately, as we can see by looking at the `iptables`
    rules again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This time, only one possible destination is configured on *host01*, as there
    is only one local Pod instance for this Service.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few minutes, the `iperf3` clients now show the kind of output we expect
    to see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Not only are all of the clients able to connect to a unique server, but the
    performance is consistently high as the network connection is local to each node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we go further, let’s clean up these resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Although the `Local` internal traffic policy is useful for maximizing bandwidth
    between client and server, it has a major limitation. If a node does not contain
    a healthy Pod instance, clients on that node will not be able to access the Service
    at all, even if there are healthy instances on other nodes. It is critical when
    using this design pattern to also configure a readiness probe, as described in
    [Chapter 13](ch13.xhtml#ch13), that checks not only the Pod itself but also its
    Service dependencies. This way, if a Service is inaccessible on a particular node,
    the client on that node will also report itself to be unhealthy so that no traffic
    will be routed to it.
  prefs: []
  type: TYPE_NORMAL
- en: The affinity and anti-affinity capabilities we’ve seen allows us to give hints
    to the scheduler without losing the scalability and resilience we want for our
    application components. However, even though it might be tempting to use these
    features whenever we have closely connected components in our application architecture,
    it’s probably best to allow the scheduler to work unhindered and add affinity
    only for cases in which real performance testing shows that it makes a significant
    difference.
  prefs: []
  type: TYPE_NORMAL
- en: Service routing for improved performance is an active area of development in
    Kubernetes. For clusters running across multiple zones, a new feature called Topology
    Aware Hints can enable Kubernetes to route connections to Services to the closest
    instances wherever possible, improving network performance while still allowing
    cross-zone traffic where necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware Resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Affinity and anti-affinity allow us to control where Pods are scheduled but
    should be used only if necessary. But what about cases for which a Pod needs access
    to some specialized hardware that is available only on some nodes? For example,
    we might have processing that would benefit from a graphics processing unit (GPU),
    but we might limit the number of GPU nodes in the cluster to reduce cost. In that
    case, it is absolutely necessary to ensure that the Pod is scheduled in the right
    place.
  prefs: []
  type: TYPE_NORMAL
- en: As before, we could tie our Pod directly to a node using `nodeName`. But we
    might have many nodes in our cluster with the right hardware, so what we really
    want is to be able to tell Kubernetes about the requirement and then let the scheduler
    decide how to satisfy it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes provides two related methods to address this need: device plug-ins
    and extended resources. A device plug-in provides the most complete functionality,
    but the plug-in itself must exist for the hardware device. Meanwhile, extended
    resources can be used for any hardware device, but the Kubernetes cluster only
    tracks allocation of the resource; it doesn’t actually manage its availability
    in the container.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a device plug-in requires close collaboration with `kubelet`. Similar
    to the storage plug-in architecture we saw in [Chapter 15](ch15.xhtml#ch15), a
    device plug-in registers itself with the `kubelet` instance running on a node,
    identifying any devices it manages. Pods identify any devices they require, and
    the device manager tells `kubelet` how to make the device available inside the
    container (typically by mounting the device from the host into the container’s
    filesystem).
  prefs: []
  type: TYPE_NORMAL
- en: Because we’re operating in a virtualized example cluster, we don’t have any
    specialized hardware to demonstrate a device plug-in, but an extended resource
    works identically from an allocation standpoint, so we can still get a feel for
    the overall approach.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by updating the cluster to indicate that one of the nodes has an
    example extended resource. We do this by patching the `status` for the node. Ideally,
    we could do this with `kubectl patch`, but unfortunately it’s not possible to
    update the `status` of a resource with that command, so we’re reduced to using
    `curl` to call the Kubernetes API directly. The */opt* directory has a script
    to make this easy. [Listing 18-1](ch18.xhtml#ch18list1) presents the relevant
    part.
  prefs: []
  type: TYPE_NORMAL
- en: '*add-hw.sh*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 18-1: Special hardware script*'
  prefs: []
  type: TYPE_NORMAL
- en: This `curl` command sends a JSON patch object to update the `status` field for
    the node, adding an entry called `bookofkubernetes.com/special-hw` under `capacity`.
    The `~1` acts as a slash character.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the script to update the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The response from the API server includes the entire Node resource. Let’s double-check
    just the field we care about to make sure it applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The extended resource shows up alongside the standard resources for the node.
    We can now request this resource similar to how we request standard resources,
    as we saw in [Chapter 14](ch14.xhtml#ch14).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a Pod that requests the special hardware:'
  prefs: []
  type: TYPE_NORMAL
- en: '*hw.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the requirement for the special hardware using the `resources` field.
    The resource is either allocated or not allocated; thus, there’s no distinction
    between requests and limits, so Kubernetes expects us to specify it using `limits`.
    When we apply this to the cluster, the Kubernetes scheduler will ensure that this
    Pod runs on a node that can meet this requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, the Pod ends up on `host02`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, the node status now reflects an allocation for this extended
    resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Both the available quantity of three `special-hw` that we specified when we
    added the extended resource in [Listing 18-1](ch18.xhtml#ch18list1) and the allocation
    of that resource to our Pod are arbitrary. The extended resource acts like a semaphore
    in preventing too many users from using the same resource, but we would need to
    add additional processing to deconflict multiple users if we really had three
    separate special hardware devices on the same node.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we do try to over-allocate based on what we specified is available, the
    Pod won’t be scheduled. We can confirm this if we try to add another Pod that
    needs all three of our special hardware devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '*hw3.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s try to add this Pod to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Because there aren’t enough special hardware devices available, this Pod stays
    in the Pending state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The Pod will wait for the hardware to be available. Let’s delete our original
    Pod to free up room:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Our new Pod will now start running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As before, the Pod was scheduled onto `host02` because of the special hardware
    requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Device drivers work identically from an allocation standpoint. In both cases,
    we use the `limits` field to identify our hardware requirements. The only difference
    is that we don’t need to patch the node manually to record the resource, because
    `kubelet` updates the node’s status automatically when the device driver registers.
    Additionally, `kubelet` invokes the device driver to perform any necessary allocation
    and configuration of the hardware when a container is created.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike ideal applications, in the real world we often must deal with closely
    coupled application components and the need for specialized hardware. It’s critical
    that we account for those application requirements without losing the flexibility
    and resiliency that we gain from deploying our application to a Kubernetes cluster.
    In this chapter, we’ve seen how affinity and device drivers allow us to provide
    hints and resource requirements to the scheduler while still allowing it the flexibility
    to manage the application at scale dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling is not the only concern we might have as we consider how to obtain
    the desired behavior and performance from real-world applications. In the next
    chapter, we’ll see how we can shape the processing and memory allocation for our
    Pods through the use of quality-of-service classes.
  prefs: []
  type: TYPE_NORMAL
