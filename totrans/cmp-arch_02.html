<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="chn"><span epub:type="pagebreak" id="page_3"/><strong>1</strong></h2>&#13;
<h2 class="cht"><strong>HISTORICAL ARCHITECTURES</strong></h2>&#13;
<div class="image1"><img src="../images/f0003-01.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="chq">Computer science is a much older subject than many people think. This chapter will begin 40,000 years ago and progress to the present day. A modern microchip may seem impenetrable and alien at first sight, but if you know the history, you can understand its smaller components in terms of structures that have developed gradually over thousands of years.</p>&#13;
<p class="indent">There are a number of other reasons to study the history of the field. Seeing the deep history of computer science gives us more credibility and authority as a field distinct from, say, mathematics or engineering. Seeing how ideas have evolved gradually by building on one another can also protect us from myths of the “lone genius” and reveal how such people were perhaps just like ourselves. Finally, following the general trends through “the arc of history” not only explains how we got to where we are but can also suggest where we’re headed next, to help us predict, or create, the future.</p>&#13;
<h3 class="h3" id="lev13"><span epub:type="pagebreak" id="page_4"/>What Is a Computer?</h3>&#13;
<p class="noindent">When we imagine “a computer” today, we probably think of a device such as a desktop PC, game console, or smartphone. But those aren’t the only machines humans have used for calculating and computing. To trace the history of computers, we first need to decide what counts as a computer and how computers are different from mere calculators or calculating machines. This is a surprisingly difficult question, one that is still argued over. My own rule of thumb for deciding if something is a computer is, Can you program <em>Space Invaders</em> on it? A simple calculator can’t do this, so it isn’t a computer; a programmable calculator usually can, so it is a computer.</p>&#13;
<p class="indent">Let’s look at some further concepts that are often suggested for defining computers. Some sources—including the <em>Oxford English Dictionary</em>—require computers to be electronic. But similar machines can be made out of other substrates, such as water. Consider <em>MONIAC</em>, which stands for Monetary National Income Analogue Computer, a pun on the earlier ENIAC computer that we’ll examine later in the chapter. Built in 1949, and shown in <a href="ch01.xhtml#ch01fig1">Figure 1-1</a>, MONIAC was an analog water computer used to simulate the flow of money through the economy and to illustrate the effects of economic interventions on an economic model.</p>&#13;
<div class="image"><img id="ch01fig1" src="../images/f0004-01.jpg" alt="Image" width="473" height="642"/></div>&#13;
<p class="figcap"><em>Figure 1-1: The MONIAC water computer and its creator, Bill Phillips</em></p>&#13;
<p class="indent">MONIAC allowed you to increase the interest rate and observe the effects on unemployment. Tanks of water showed the positions of money in sectors of the economy such as the central bank, savings, and investment, according to the theory of economics built into the machine.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_5"/>Some people argue computers must be <em>digital</em>, as opposed to <em>analog</em>. A digital machine is one that represents data using <em>digits</em>, discrete sets of symbols such as the binary digits 0 and 1. In contrast, an analog machine has an infinite, continuous set of possible states, such as the amounts of water in MONIAC’s tanks, making MONIAC an analog machine.</p>&#13;
<p class="indent">Where does MONIAC stand regarding my original <em>Space Invaders</em> test? It only computes results for a single economic model, although it might be able to run other economic models if we were able to reconfigure some of the tubes and reservoirs to have different sizes and connections. By extension, perhaps MONIAC could implement <em>any</em> computation, such as running <em>Space Invaders</em>, through more severe reconfigurations of this nature. But would we then have the same computer in a new configuration, or would we have a new, different machine that still only computes one other, different, thing? In other words, is MONIAC <em>reprogrammable</em>?</p>&#13;
<p class="indent">I’ve been using <em>Space Invaders</em> as a test program, but it’s tempting to say that for something to be a computer, you must be able to reprogram it to do <em>anything</em>. However, computation theory shows that this can’t be used as a definition. Given any candidate computer, it’s always possible to find problems it can’t solve. These are usually problems about predicting the candidate computer’s own future behavior, which can lead it into an infinite loop.</p>&#13;
<p class="indent">Diving a little deeper into computation theory, we get <em>Church’s thesis</em>, a more rigorous definition of a computer that most modern computer scientists agree with. It can be paraphrased as:</p>&#13;
<p class="block">A computer is a machine that can simulate any other machine, given as much memory as it asks for.</p>&#13;
<p class="noindent">We’ll call machines that satisfy Church’s thesis <em>Church computers</em>. In particular, machines clearly exist that can do the following, so a Church computer must also be able to perform these tasks:</p>&#13;
<ul class="bullet">&#13;
<li class="tm">Read, write, and process data</li>&#13;
<li class="tm">Read, write, and execute programs</li>&#13;
<li class="tm">Add (and hence do arithmetic)</li>&#13;
<li class="tm">Jump (<span class="literal">goto</span> statements)</li>&#13;
<li class="tm">Branch (<span class="literal">if</span> statements)</li>&#13;
</ul>&#13;
<p class="noindent">We can now see that the <em>Space Invaders</em> definition is a reasonable approximation of Church’s thesis in many cases: while <em>Space Invaders</em> is a simplistic video game, it happens to require all of the above tasks, which are also the basic ingredients of many other computational tasks and machines. Hence, a machine that can be <em>reprogrammed</em> (rather than hardwired) to play <em>Space Invaders</em> is usually powerful enough to simulate any other machine, too (as long as we provide as much extra memory as it asks for).</p>&#13;
<p class="indent">The rest of this chapter traces the history of computers and computerlike devices in chronological order, starting in the Stone Age. As you read, ask yourself who invented the first computer, and note the point where you think the computer was invented. People often argue for drawing this line <span epub:type="pagebreak" id="page_6"/>in different places, based on their own definitions of what counts as a computer. Where will <em>you</em> draw the line, and why?</p>&#13;
<h3 class="h3" id="lev14">Before the Industrial Revolution</h3>&#13;
<p class="noindent">In this section, we’ll take a look at the various preindustrial machines we may or may not consider to be computers. In doing so, we’ll see that humans have been using mechanisms resembling computers for longer than we might have thought.</p>&#13;
<h4 class="h4" id="lev15"><em>The Stone Age</em></h4>&#13;
<p class="noindent">Our anatomical species, <em>Homo sapiens</em>, is around 200,000 years old, but it’s widely believed that we lacked modern intelligence until the cognitive revolution of around 40,000 BCE. We don’t know exactly how this happened. One current theory is that a single genetic mutation in the FOXP2 gene occurred and was selected by the extreme evolutionary pressures of the Ice Age. This suddenly enabled the brain to form arbitrary new hierarchical concepts, in turn giving rise to language and technology. According to this theory, from then on humans were as intelligent as we are now. They would have been capable of learning, say, quantum computing, had they been given access to modern facilities and information.</p>&#13;
<p class="indent">One marker of this shift may be the <em>Lebombo bone</em>, shown in <a href="ch01.xhtml#ch01fig2">Figure 1-2</a>—a bone with carved notches that may have been used as a tally stick around 40,000 BCE. In a tally, one mark represents one physical thing. Perhaps these notches signified animals, items of food, favors owed by one person to another, or days to time some hunting or social project.</p>&#13;
<div class="image"><img id="ch01fig2" src="../images/f0006-01.jpg" alt="Image" width="630" height="61"/></div>&#13;
<p class="figcap"><em>Figure 1-2: The Lebombo bone</em></p>&#13;
<p class="indent">The <em>Ishango bone</em>, shown in <a href="ch01.xhtml#ch01fig3">Figure 1-3</a>, is another bone containing human-made tally-like marks, dating to later in the Ice Age, around 20,000 BCE. Unlike the Lebombo bone, the Ishango bone marks appear to be grouped into tally-like clusters of mostly prime numbers between 3 and 19, and these clusters are grouped into three lines that sum to 60 or 48.</p>&#13;
<p class="indent">As with the Lebombo bone, it’s possible that the marks in the Ishango bone are at purely random locations and were made for some physical purpose, such as to improve hand grip. But several authors have studied the Ishango bone’s patterns and argued that they functioned as a tally, an aid for calculation, a lunar agricultural calendar or menstrual cycle calendar, or most speculatively, a table of prime numbers. The totals of 60 and 48 are multiples of 12, and 12 is known to have been the original base for arithmetic in later civilizations, before we shifted to base 10.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_7"/><img id="ch01fig3" src="../images/f0007-01.jpg" alt="Image" width="351" height="700"/></div>&#13;
<p class="figcap"><em>Figure 1-3: The Ishango bone, argued by some to extend from tallying to calculation</em></p>&#13;
<p class="indent">The Lebombo bone appears to be an example of data representation. Arguably, it may have been used for a simple form of calculation such as adding one to its total each time a new mark was made. Some interpretations of the Ishango bone suggest its use in more advanced calculations, perhaps interactively, like using a pen and paper to perform and keep track of multiple steps of a math problem.</p>&#13;
<p class="indent">Could you program a bone to play <em>Space Invaders</em>? You could devise a set of rules for a human to follow, telling them to make scratches to update representations of the game characters. Gameplay would be quite slow, and the human would have to be there to perform the updates. There’s no evidence that humans ever used bones in this programmable way—though maybe one day another bone could be found and its scratches decoded as instructions for a human operator to follow.</p>&#13;
<h4 class="h4" id="lev16"><em>The Bronze Age</em></h4>&#13;
<p class="noindent">The ice melted around 4000 BCE, enabling the first cities to grow. Cities required new and larger forms of organization, such as keeping track of trading and taxes. To enable this, by 3000 BCE the Sumerian city culture in Mesopotamia (modern-day Iraq) developed the first writing system, and by 2500 BCE it possessed the first indisputable calculating machine, the abacus (<a href="ch01.xhtml#ch01fig4">Figure 1-4</a>). The word <em>abacus</em> means “sand box,” which suggests that before this date the same machinery was implemented using simple rocks in the <span epub:type="pagebreak" id="page_8"/>sand. The oldest abaci we find in archaeology are the more advanced ones made from wood and beads.</p>&#13;
<div class="image"><img id="ch01fig4" src="../images/f0008-01.jpg" alt="Image" width="473" height="311"/></div>&#13;
<p class="figcap"><em>Figure 1-4: An abacus</em></p>&#13;
<p class="indent">In its usual usage, the state of the abacus in <a href="ch01.xhtml#ch01fig4">Figure 1-4</a> represents the (decimal, natural) number 070710678. There are nine columns, each representing one of the digits in this number. Each column is split into a lower box containing five beads and an upper box containing two beads. The default position for the beads in the lower box is down, and the default position for beads in the upper box is up. In this state, a column represents the digit 0. Each bead pushed up from the bottom to the top of the lower box is worth 1. Each bead pushed down from the top to the bottom of the upper box is worth 5.</p>&#13;
<p class="indent">To add 1 to a number on the abacus (that is, <em>increment</em> it), you raise one bead from the lower box of the rightmost column. If all five beads in a column’s lower box are raised, you push them all back down and replace them by lowering one of the beads in the upper box in the same column. If both upper beads are lowered, you push them back up and replace them by raising one bead from the lower box in the column on its left. Moving data from a column to the one on its left is known as a <em>carry</em> operation.</p>&#13;
<p class="indent">To add two numbers, <em>a</em> + <em>b</em>, you first set up the abacus to represent the digits of <em>a</em>. You then perform <em>b</em> increments as above. The state of the abacus then represents the result.</p>&#13;
<p class="indent">This style of calculation—where the first number is “loaded onto” the device and the second is “added into” it, leaving only the final result as the state of the system—is known as an <em>accumulator architecture</em>, and it’s still in common use today. It “accumulates” the result of a series of calculations; for example, we can add a list of many numbers together by adding each of them in turn into the state and seeing the latest accumulated total after each addition.</p>&#13;
<p class="notes"><strong><span class="nt">NOTE</span></strong></p>&#13;
<p class="noindent"><em>The abacus in this example uses decimal digits for familiarity. The original Sumerian version used base 12.</em></p>&#13;
<p class="indenta">The concept of the algorithm dates from this time. Calculations written on clay tablets, such as those in <a href="ch01.xhtml#ch01fig5">Figure 1-5</a>, show that number-literate people at this time thought in terms of computation rather than mathematics, <span epub:type="pagebreak" id="page_9"/>being taught to perform algorithms for arithmetic operations and carrying them out for practice, as opposed to doing proofs.</p>&#13;
<div class="image"><img id="ch01fig5" src="../images/f0009-01.jpg" alt="Image" width="625" height="560"/></div>&#13;
<p class="figcap"><em>Figure 1-5: A tablet showing the steps of a long division algorithm</em></p>&#13;
<p class="indent">The clay tablets show lines of step-by-step arithmetic that may have been performed using the tablets themselves as data storage. Or the tablets may have been used to notate the states of an abacus for teaching purposes.</p>&#13;
<p class="indent">The abacus was—and in a few places still is—most often used for adding numbers, such as summing the prices of items in a shopping basket, but other ancient abacus arithmetic algorithms are also known, including for subtraction, multiplication, and long division. These were performed similarly to their modern pen-and-paper equivalents. Modern enthusiasts (you can search for them on YouTube) have also shown how to use the abacus for more advanced algorithms such as finding square roots and computing the digits of <em>π</em>. As these algorithms get more complex, the memory of the abacus often needs to be extended with extra columns. Like the Stone Age bones, the abacus could be used as the data store for <em>any</em> algorithm if a human is instructed what actions to perform on it. If you want to argue that it’s a computer, you may again need to consider the role of the human.</p>&#13;
<h4 class="h4" id="lev17"><em>The Iron Age</em></h4>&#13;
<p class="noindent">The Bronze Age city civilizations of Mesopotamia and its neighbors collapsed, mysteriously, around 1200 BCE. They were followed by a “dark age” period, until classical ancient Greece arose around 500 BCE to 300 BCE: the time of Pythagoras, Plato, and Aristotle. Greek power was gradually replaced by the Roman Republic and Roman Empire from around 300 BCE to 400 CE.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_10"/>The Antikythera mechanism (<a href="ch01.xhtml#ch01fig6">Figure 1-6</a>) dates from this period, around 100 BCE. It was found in 1901 in a Mediterranean shipwreck; the sunken ship appeared to be on its way from Greece to Rome, with the mechanism for sale or as tribute. The mechanism was only understood and reverse engineered in 2008. We now know that it was a mechanical, clockwork analog machine used to predict astronomical (and likely astrological) events, including five planet positions, moon phases, and the timings of eclipses and the Olympic Games. It consisted of 37 bronze gears, and the user turned a handle to simulate the future course of their states. The results were displayed on clock faces, computed by the ratios of mechanical gears. Enthusiasts recently rebuilt a functioning version using LEGO (<a href="ch01.xhtml#ch01fig6">Figure 1-6</a>).</p>&#13;
<div class="image"><img id="ch01fig6" src="../images/f0010-01.jpg" alt="Image" width="881" height="408"/></div>&#13;
<p class="figcap"><em>Figure 1-6: The Antikythera mechanism remains, as found in a Mediterranean shipwreck (left), and a reconstructed Antikythera mechanism using LEGO (right)</em></p>&#13;
<p class="indent"><em>Odometers</em> were long-range distance-measuring machines that the Greeks and Romans used to survey and map their empires. There is indirect evidence of their use from around 300 BCE due to the existence of very accurate distance measurements that would have been hard to obtain any other way. The reconstruction in <a href="ch01.xhtml#ch01fig7">Figure 1-7</a> is based on direct archaeological remains from around 50 CE.</p>&#13;
<p class="indent">This type of odometer worked similarly to the measuring wheels you might have used in elementary school that clicked each time they were pushed a certain distance, typically 1 yard or 1 meter. It is also related to modern odometers used in cars and robotics.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_11"/><img id="ch01fig7" src="../images/f0011-01.jpg" alt="Image" width="552" height="414"/></div>&#13;
<p class="figcap"><em>Figure 1-7: A Roman odometry cart</em></p>&#13;
<p class="indent">The odometer is pulled by a horse, like a cart. There are a number of metal balls stored in cavities in a circular wooden gear. One of the wheels has a peg attached so that once per rotation it taps and rotates the gear by a small fixed angle. A ball-sized hole under one position of the gear allows a ball above it to fall out of its cavity and into a collecting box below. The total distance traveled is thus logged by the number of balls in the counting box at the end of the trip.</p>&#13;
<p class="indent">Are these machines computers? There are clearly notions of data being used to represent objects in the world, as well as forms of automation and calculation. But like MONIAC, each machine does only one thing: predict eclipses or measure distance. You couldn’t easily reprogram either to play <em>Space Invaders</em>.</p>&#13;
<p class="indent">Like MONIAC, the Antikythera mechanism is an analog machine: its gears rotate continuously and can be in any position. The odometer, in contrast, is digital, like the abacus. Its gear advances only by a discrete amount with each “click” as the peg passes it, and the collecting box always holds a discrete number of balls. Unlike the abacus, however, the odometer is automatic; it doesn’t require a human operator, only a horse as a source of power.</p>&#13;
<p class="indent">You might be able to reprogram the Antikythera mechanism—and with some creativity, the odometer—if you were allowed to completely reconfigure all the gears, including adding and removing gears of arbitrary sizes in arbitrary locations. Then you could try to represent and simulate other physical systems or perform other calculations. As with MONIAC, some consider physically reconfiguring the hardware in this way to be cheating. They would argue that this creates a new, different machine, rather than a different program running on the original machine.</p>&#13;
<h4 class="h4" id="lev18"><span epub:type="pagebreak" id="page_12"/><em>The Islamic Golden Age</em></h4>&#13;
<p class="noindent">After the fall of Rome in 476 CE, western Europe entered the so-called Dark Ages for a thousand years, and the history of computing in western Europe records basically no progress during this time.</p>&#13;
<p class="indent">However, the Roman Empire continued to operate from its new eastern capital, Byzantium (now Istanbul, Turkey). There was a flow of ideas between Byzantium, Greece, and the Islamic world, the latter becoming the new intellectual center of the time. A particular musical idea from this culture introduces the important concept of programming.</p>&#13;
<p class="indent">The ancient Greeks previously had a portable <em>hydraulis</em> instrument, related to modern church organs. It was composed of a set of pipes, played by a keyboard and powered from an air reservoir pumped by a servant. The Greeks clearly possessed the technology needed to make self-playing versions of the hydraulis, but there’s no evidence of them doing so.</p>&#13;
<p class="indent">It was Islamic scholars, the Banu Musa brothers, who built the first known automated musical instrument: the automated flute player of Baghdad, around 900 CE, shown in <a href="ch01.xhtml#ch01fig8">Figure 1-8</a>.</p>&#13;
<div class="image"><img id="ch01fig8" src="../images/f0012-01.jpg" alt="Image" width="812" height="431"/></div>&#13;
<p class="figcap"><em>Figure 1-8: A Greek hydraulis (left) and a sketch of the Baghdad automated flute player (right)</em></p>&#13;
<p class="indent">The innovation was to use a slowly rotating barrel with movable pins around its edge to indicate the positions of musical notes. As the barrel rotates, the pins make contact with levers that allow air to flow into the instrument to sound a note. The movable nature of the pins allows different compositions to be programmed into the device, making it the first known <em>programmable</em> automatic machine. The pins may be viewed today as a binary code: at each time and pitch, there is either a note (1) or no note (0).</p>&#13;
<p class="indent">Is this a computer? Unlike the Iron Age machines, it can clearly run multiple programs. However, there’s no notion of calculation or of decisionmaking: once a program begins, it will play through and can’t change its behavior in response to any input or even to its own state.</p>&#13;
<h4 class="h4" id="lev19"><span epub:type="pagebreak" id="page_13"/><em>The Renaissance and Enlightenment</em></h4>&#13;
<p class="noindent">Byzantium fell in 1453, sending many scholars and their books back to western Europe and helping it wake from the Dark Ages. Leonardo da Vinci was the definitive “renaissance man” of this time: a prolific scientist, artist, and engineer. He possessed many of these old books and looked to them for inspiration. He was probably familiar with Antikythera-like systems thanks to these books. One of his manuscripts from around 1502, the <em>Codex Madrid</em>, contains an unbuilt design (<a href="ch01.xhtml#ch01fig9">Figure 1-9</a>) for a mechanical analog calculator based on Antikythera-like principles.</p>&#13;
<div class="image"><img id="ch01fig9" src="../images/f0013-01.jpg" alt="Image" width="621" height="256"/></div>&#13;
<p class="figcap"><em>Figure 1-9: The da Vinci calculator’s original manuscript</em></p>&#13;
<p class="indent">The design was rediscovered and successfully constructed in 1968. There are 13 wheels, each representing the columns of a decimal number. Their possible positions are <em>continuous</em>: rather than switching abruptly from one decimal digit to another, they move smoothly by means of gearing. The gear ratio is 1:10 between each pair of columns, so each column’s wheel rotates at one-tenth the speed of the column on its right.</p>&#13;
<p class="indent">Like the abacus, the calculator is an accumulator whose state at any point in time represents a single number, again as digits in columns. One number <em>a</em> can be added to another <em>b</em>. The first number <em>a</em> could be loaded onto the machine by advancing the mechanism to represent its digits. Then it would be turned an additional amount <em>b</em> to advance the total to <em>a</em> + <em>b</em>.</p>&#13;
<p class="indent">For example, to calculate 2,130 + 1,234, we first load 2,130 onto the device, then advance by 1,234 to get 3,364. The numbers wouldn’t be precisely aligned at the end of the computation due to the continuous rotation of the wheels. For example, the 6 in the tens place would be almost halfway between showing 6 and 7 because the digit after it is a 4, which is almost halfway to the next carry. In a sense it is a “weaker” machine than the Roman odometer, because the odometer has a notion of converting from continuous wheel positions to discrete symbols using its pin-and-ball mechanism.</p>&#13;
<p class="indent">Da Vinci’s concept was extended by Blaise Pascal in 1642. <a href="ch01.xhtml#ch01fig10">Figure 1-10</a> shows Pascal’s calculator design and a modern build of it. (It has recently been argued that Pascal’s calculator was invented earlier, in 1623, by Wilhelm Schickard.)</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_14"/><img id="ch01fig10" src="../images/f0014-01.jpg" alt="Image" width="1119" height="453"/></div>&#13;
<p class="figcap"><em>Figure 1-10: Pascal’s calculator: the original design and a 2016 LEGO rebuild</em></p>&#13;
<p class="indent">Pascal’s calculator includes a digital mechanism similar to the odometer (rather than da Vinci’s analog gearing) to implement its carry mechanism. When a column reaches the number 9 and another unit is added to it, it triggers a unit turn of the next column as it returns itself to the number 0.</p>&#13;
<p class="indent">Unlike the Antikythera mechanism, which represented the states of physical (astronomical) objects, da Vinci’s and Pascal’s machines operate on pure numbers. You could argue this gives them more general-purpose roles than the Antikythera mechanism. That said, the range of their calculations is limited to addition, which in a sense makes them less powerful than the abacus, which had algorithms for other arithmetic operations. On the other hand, like the Antikythera mechanism, these calculators require less human work than an abacus.</p>&#13;
<p class="indent">Some see the move from da Vinci’s analog to Pascal’s digital operation as very important. Digital operation appears to involve a simple concept of the machine making a “decision”: a carry is either made or not made at each step. Decision-making is certainly important for some tasks, but clearly not so much for addition because both calculators can do it equally well.</p>&#13;
<h3 class="h3" id="lev20">The Steam Age</h3>&#13;
<p class="noindent">Steam power had been known to the Greeks and Romans as a curiosity, and anyone who has ever boiled water with a lid will have noticed that steam can move the lid around. But it was only from around 1700 in Britain that steam was harnessed in earnest, to power the industrial revolution. Seeded by Enlightenment ideas, especially Newton’s physics, this was a positive feedback cycle in which machines and coal were used to produce more machines and extract more coal. Coal was burned to heat water into steam, and steam was first used to pump water from coal mines. In time, steam came to power many other machines, some with computer-like characteristics.</p>&#13;
<h4 class="h4" id="lev21"><span epub:type="pagebreak" id="page_15"/><em>The Jacquard Loom</em></h4>&#13;
<p class="noindent">The production of textiles was a major application of new machines during the Steam Age. But unlike plain cotton clothes, traditional weaving patterns were highly complex. Thus they were considered to be more valuable because they were rarer and more expensive.</p>&#13;
<p class="indent">In 1804, Joseph Jacquard created a variant of the weaving machines of the time that employed replaceable punched cards to guide the positions of the hooks and needles used in the weave (<a href="ch01.xhtml#ch01fig11">Figure 1-11</a>).</p>&#13;
<div class="image"><img id="ch01fig11" src="../images/f0015-01.jpg" alt="Image" width="473" height="630"/></div>&#13;
<p class="figcap"><em>Figure 1-11: A Jacquard loom</em></p>&#13;
<p class="indent">The punched cards could be “chained” together into long tapes to make complex, reusable patterns at a lower price.</p>&#13;
<p class="notes"><strong><span class="nt">NOTE</span></strong></p>&#13;
<p class="noindent"><em>“Chain” became the standard command to load the next program from magnetic tapes in later electronic devices, used until the 1990s. Weaving concepts like “thread” and “warp” are also used as metaphors in modern multithreaded programming and in parallel GPUs.</em></p>&#13;
<h4 class="h4" id="lev22"><em>Victorian Barrel Organs and Music Boxes</em></h4>&#13;
<p class="noindent">Barrel-based musical instruments, similar in technology to the Baghdad automatic flute player and shown in <a href="ch01.xhtml#ch01fig12">Figure 1-12</a>, were popular during the 19th century. The job of an “organ grinder” was to push a portable barrel organ onto a main street, then manually turn its handle to provide power. A rotating barrel with pins marking the positions of notes would then allow air into the organ pipes, as in the Baghdad version.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_16"/><img id="ch01fig12" src="../images/f0016-01.jpg" alt="Image" width="1126" height="350"/></div>&#13;
<p class="figcap"><em>Figure 1-12: Two Victorian-style barrel organs (left and center) and a music box (right)</em></p>&#13;
<p class="indent">The same mechanism was (and still is) used in the music box from this period, in which a spring is wound up to store energy and then released to power a smaller pinned barrel, whose pins strike small xylophone-like metal bars directly to play a few bars of music such as a famous theme from a ballet. The rotating barrel is often topped with a small sculpture, such as a ballerina, that rotates along with the music.</p>&#13;
<p class="indent">Charles Babbage hated organ grinders playing outside his house and led a public campaign to rid them from the streets of London. But their barrel organs were to form a fundamental influence on his work.</p>&#13;
<h4 class="h4" id="lev23"><em>Babbage’s Difference Engine</em></h4>&#13;
<p class="noindent">Babbage designed two different machines, the Difference Engine and the Analytical Engine. The former (<a href="ch01.xhtml#ch01fig13">Figure 1-13</a>) was first; it was successfully built and commercialized by Georg Scheutz and others from 1855 and widely used in industry until the 1930s. Recent LEGO rebuilds also exist.</p>&#13;
<p class="indent">The Difference Engine was designed to produce tables of values of arbitrary polynomial functions. Most mathematical functions can be well approximated by polynomials via Taylor series expansion, so the machine could be used to make tables of values for any such function. You may have used similar tables in modern exams to look up values of trigonometric or statistical functions when a calculator isn’t allowed. In Babbage’s time, the killer application of these tables was in shipping, for navigation purposes. Tables had previously been computed by hand and contained many expensive errors, so there was a large economic demand to perfect them by machine.</p>&#13;
<p class="indent">The machine can be powered either by steam or by a human cranking the handle. Like Pascal’s calculator, the Difference Engine represents decimal digits by discretized rotations of gears. Numbers are represented by a vertical column of such digits (like Pascal’s calculator turned on its side). The Difference Engine then extends this to a 2D parallel architecture, with multiple vertical columns arranged horizontally. Each of these columns represents a different number.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_17"/><img id="ch01fig13" src="../images/f0017-01.jpg" alt="Image" width="788" height="651"/></div>&#13;
<p class="figcap"><em>Figure 1-13: A metal rebuild of Babbage’s Difference Engine</em></p>&#13;
<p class="indent">There are two dimensions of parallelization in the Difference Engine: digit-wise and term-wise. Digit-wise addition, for example, is a different algorithm from the sequential high school method of addition. Instead of starting from the rightmost column and moving left and applying carries, it adds each pair of digits at the same time, then handles any carrying afterward. For example, to add 364 + 152, the three additions 3 + 1, 6 + 5, and 4 + 2 are all performed at the same time to give 416. The carry from 6 + 5 = 11 is then added to give 516. Carrying is a difficult operation to get right in this context, and Babbage devoted most of his engineering time to it. The visual effect of carries can be seen on YouTube videos of the Difference Engine as a visible ripple of information propagating across the 2D surface of the machine. Such ripples are also seen in computations on modern parallel GPUs.</p>&#13;
<p class="indent">Is the Difference Engine a computer? It can run different “programs” to calculate different equations, but these equations have no obvious concept of changing their behavior during a calculation; there’s nothing like an if statement to test intermediate results and do something different based on them. It’s more like a modern media streaming device in which numbers flow smoothly through a processing pipeline.</p>&#13;
<h4 class="h4" id="lev24"><em>Babbage’s Analytical Engine</em></h4>&#13;
<p class="noindent">The Difference Engine was limited to computing tables of polynomial functions, but Babbage’s second project, the Analytical Engine (<a href="ch01.xhtml#ch01fig14">Figure 1-14</a>), was designed as a completely general-purpose, programmable machine.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_18"/><img id="ch01fig14" src="../images/f0018-01.jpg" alt="Image" width="763" height="716"/></div>&#13;
<p class="figcap"><em>Figure 1-14: A modern partial rebuild of Babbage’s Analytical Engine</em></p>&#13;
<p class="indent">To obtain this generality, the Analytical Engine provides a range of arithmetic and other operations as simple machines, together with a memory for storing data and the ability to read in programs from punch cards. The programs dictate a sequence of memory reads and writes and arithmetic operations, and allow branching depending on the state of the calculation—an if statement.</p>&#13;
<p class="indent">Babbage went through many variations of the Analytical Engine’s design on paper, but physically built only a tiny part of it just before he died. He got very sidetracked with the fine details of the carry mechanism and was obsessed with constantly redesigning components rather than sticking with one version and getting them integrated to actually work. (Today this style of project management would be known as <em>yak shaving</em>.) This annoyed the research funding agencies of the time, making it hard for Babbage to get money to build anything. Thus, unlike with the Difference Engine, we don’t have a working version or even a single final design document of the Analytical Engine. However, components have recently been reconstructed from Babbage’s plans using modern manufacturing technologies.</p>&#13;
<p class="indent">With more moving parts than the Difference Engine, the Analytical Engine would have required more power; this would have had to come from a steam engine rather than a manual crank. It would have also required more precisely machined gears, as computations would need to work their way through a longer series of gears. Like the factory machines and steam locomotives of the period, it would have smelled of oil, smoke, and steam, and gleamed in polished brass: Babbage was the original steampunk.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_19"/>The core of the Analytical Engine contained many independent simple machines that each performed some function, such as adding numbers and testing if one number equaled another. The adding machine was roughly a copy of Pascal’s calculator, and the other simple machines were variations of it.</p>&#13;
<p class="indent">The Analytical Engine introduced the modern concept of computer memory. Its “store” would have consisted of a large number of copies of a simple machine, again similar to a Pascal calculator, each of which could retain a different number. Each machine would be given a numerical identifier or “address” to specify the correct one to read from or write to.</p>&#13;
<p class="indent">A sequence of <em>instructions</em> would have been coded in binary and punched onto paper tape, using a mechanism taken from the Jacquard loom. Each instruction would tell the engine to activate one of the simple machines. Usually, after each instruction, the machine would have line-fed the punched paper along to load the next one (a bit like a typewriter). However, the machine also would have had the ability to check the result of the latest simple machine and, depending on its value, could jump to a different line in the paper. This would give programs the ability to alter their behavior in response to intermediate results.</p>&#13;
<p class="indent">A program could also be made to run forever by gluing the bottom of the punched paper to its top, making a physical loop, as in the (later) paper tape machine shown in <a href="ch01.xhtml#ch01fig15">Figure 1-15</a>.</p>&#13;
<div class="image"><img id="ch01fig15" src="../images/f0019-01.jpg" alt="Image" width="473" height="316"/></div>&#13;
<p class="figcap"><em>Figure 1-15: A punch tape program loop</em></p>&#13;
<p class="indent">We don’t have any examples of actual programs written for the Analytical Engine. Rather, Babbage and his collaborator Ada Lovelace wrote down example <em>states</em> and <em>outputs</em> from imaginary runs as long tables, showing them at each step of program execution. This is similar to the notations on the Babylonians’ clay tablets, which illustrate algorithms by showing the effects rather than the instructions used to generate them. From these execution traces, modern readers can infer roughly what the programs and the machine’s instruction set used to build them would have been.</p>&#13;
<p class="indent">Babbage wrote the first of these example traces for small, almost trivial mathematical functions, which illustrate roughly the full set of instructions in use. But Babbage was the hardware person, more concerned with designing the machine itself, and never wrote anything longer, thinking that <span epub:type="pagebreak" id="page_20"/>programming would be relatively trivial compared to designing the architecture. Lovelace was the software person, and she wrote much longer traces for complex functions. She also wrote speculations about what larger programs could achieve, including ideas about AI. If Babbage is claimed as “the first programmer,” then Lovelace might be “the first software engineer” for thinking about programming more seriously and at scale.</p>&#13;
<p class="indent">Was the Analytical Engine a Church computer? Its design contains all the basic features of a modern computer: CPU, memory, a bus, registers, a control unit, and an arithmetic unit. It can read, write, and process data. It can do arithmetic. Unlike the purely calculating machines before it, it can jump (goto) and branch (if), moving to different instructions in the program according to the state of its calculations.</p>&#13;
<p class="indent">However, to be able to simulate any other machine, it would need to be able to read, write, and execute programs as well as read, write, and process data. But its programs were fixed on the punched paper, rather than held in memory like in a modern PC. This kind of architecture, where the data and program are stored separately, often with the program fixed as firmware, is called a <em>Harvard architecture</em>, as opposed to a <em>von Neumann architecture</em>, where the program and data are stored together.</p>&#13;
<p class="indent">Today, Harvard architectures are used in embedded systems, especially in digital signal processing chips. It’s possible to set up a Harvard architecture that can simulate other computers, including those that modify their own programs. This can be done by writing a single <em>virtual machine (VM)</em> program on the fixed program punch cards (or modern firmware). The VM reads, executes, and writes further programs in memory.</p>&#13;
<p class="indent">Lovelace or Babbage could have written a VM program for the Analytical Engine, but they didn’t consider it. The same could be said about many other machines, however. For example, a VM could be written for and executed on a Sumerian abacus if a programmer chose to do so. Church’s thesis is about the <em>potential</em> for a machine to simulate any other machine, not the actualization of it doing so. But it depends on what “level” of machine we consider: the underlying hardware or virtual machines running at higher software levels.</p>&#13;
<p class="indent">And, of course, the Analytical Engine was never built or tested in full—does this need to be done to justify “being a computer,” or is the basic design sufficient by itself?</p>&#13;
<h4 class="h4" id="lev25"><em>Mechanical Differential Analyzers</em></h4>&#13;
<p class="noindent">The industrial revolution largely progressed through practical hackers building machines based on their intuitions, then testing whether they worked. But over time, mathematical theories were adapted or invented to describe and predict the behavior of many engineering systems, giving rise to academic engineering. Most of these theories made use of calculus. Developed earlier by Gottfried Wilhelm Leibniz and (independently) Sir Isaac Newton <span epub:type="pagebreak" id="page_21"/>for different purposes, calculus quickly took off as a general tool for modeling how all kinds of systems, including industrial machinery, change over continuous time, through equations such as</p>&#13;
<div class="imagec"><img src="../images/f0021-01.jpg" alt="Image" width="100" height="58"/></div>&#13;
<p class="noindent">where <em>x</em> is part of the state of the world being modeled, <em>f</em> is some function of it, and <em>dx</em>/<em>dt</em> is the rate of change of <em>x</em>. This type of equation can numerically simulate the state of the world over time by iteratively computing <em>dx</em>/<em>dt</em> and using it to update <em>x</em>. Like making the Difference Engine’s tables of polynomials, this is a highly repetitive and error-prone process ripe for mechanical automation.</p>&#13;
<p class="indent">In 1836, the same year that the Analytical Engine was developed, Gaspard-Gustave de Coriolis realized that since the behavior of a mechanical device could be <em>described</em> by a differential equation, the same device could be viewed as computing the solution to that equation. So, to solve a new equation, a physical device could be designed that matched it, and that device could then be run for a period of time to give the required answer.</p>&#13;
<p class="indent">More general differential equations can involve acceleration and higher derivatives, and multiple variables. Coriolis’s idea was extended by others, including Lord Kelvin in 1872 and James Thomson in 1876, to solve these systems, again by constructing analog mechanical devices to match them. The key component of these machines was the ball and disc integrator (<a href="ch01.xhtml#ch01fig16">Figure 1-16</a>), in which a movable ball transfers motion from a spinning disc to an output shaft.</p>&#13;
<div class="image"><img id="ch01fig16" src="../images/f0021-02.jpg" alt="Image" width="473" height="355"/></div>&#13;
<p class="figcap"><em>Figure 1-16: A ball and disc integrator from Kelvin’s differential analyzer</em></p>&#13;
<p class="indent">Like the Difference Engine, these machines were built only to solve a single class of problems: differential equations. But much, or perhaps all, of the world and its problems can be modeled by differential equations. As inherently analog machines, they can be viewed as continuing the tradition of da Vinci’s analog calculator, while Babbage’s machine built on Pascal’s digital calculator.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_22"/>The concept of using the physical properties of the world to model itself has recently been revived in quantum computing, where simulating physical and chemical quantum systems appears to be a major application with a particularly good fit to the way quantum machines compute.</p>&#13;
<h3 class="h3" id="lev26">The Diesel Age</h3>&#13;
<p class="noindent">Between the purely mechanical machines of the industrial revolution and later electronic machines, there was a hybrid period in which electricity was combined with mechanical motion to build electromechanical machines.</p>&#13;
<p class="indent">The key electromechanical technology is the <em>relay</em>, a mechanical switch in an electrical circuit whose physical position is controlled using a magnet, which, in turn, is controlled by another electrical signal. Relays are a special type of <em>solenoid</em>, a coil of wire that generates a linear magnetic field when a current flows through it. This magnetic field can be used to physically move a magnet (called the <em>armature</em>) inside the coil, and that motion can be used, for example, to open and close a valve in a water pipe or to start a car engine. Replace the water pipe with a second electrical circuit, and the valve with an electrical switch, and you have yourself a relay.</p>&#13;
<p class="indent">Relays are still used today (<a href="ch01.xhtml#ch01fig17">Figure 1-17</a>). For example, in robotics safety systems, we often need to physically connect and disconnect the main battery to and from the robot’s motors. A safety monitor checks if everything is okay and makes the physical relay connection if so, but disconnects it if anything seems wrong. You can hear these relays click when the current changes and the armature physically moves.</p>&#13;
<div class="image"><img id="ch01fig17" src="../images/f0022-01.jpg" alt="Image" width="321" height="351"/></div>&#13;
<p class="figcap"><em>Figure 1-17: A relay showing a wire coil</em></p>&#13;
<p class="indent">Electromechanical machines were more efficient than purely mechanical ones, and found widespread commercial and military use in the period around the turn of the 20th century and the two World Wars. Some of the machines you’ll see in the next section were still in use in the 1980s. Others have uncertain fates due to ongoing government secrecy, as this period includes the cryptology machines of World War II.</p>&#13;
<h4 class="h4" id="lev27"><em>The IBM Hollerith Tabulating Machine</em></h4>&#13;
<p class="noindent">The US Constitution requires that a census be taken and processed every 10 years, and by 1890 the population had grown to a size where human processing of its statistics was impossible. This created an embarrassing backlog of work for the government and a strong demand for an automated solution.</p>&#13;
<p class="indent">Herman Hollerith designed a machine to automate data processing and used it successfully in the 1890 census to do big data analytics on information from 62 million citizens. Each citizen’s data was transferred from a <span epub:type="pagebreak" id="page_23"/>written census form to a punch card by a human clerk. This seems to have been inspired not by Jacquard’s and Babbage’s machines, but independently by inspectors punching holes in train tickets to represent different journeys or times. Each question on the census was multiple choice, and was encoded on the punch card by punching out one of the several options. <a href="ch01.xhtml#ch01fig18">Figure 1-18</a> shows an example of this.</p>&#13;
<div class="image"><img id="ch01fig18" src="../images/f0023-01.jpg" alt="Image" width="857" height="331"/></div>&#13;
<p class="figcap"><em>Figure 1-18: A replica of the IBM Hollerith machine (left) and a punched card (right)</em></p>&#13;
<p class="indent">Stacks of cards could be read into the machine, which would check for the presence or absence of certain features or combinations of features, then use an electrical analog of a Pascal calculator to accumulate the total count of cards having these features. As Hollerith (1894) explained:</p>&#13;
<p class="block1">It is not sufficient to know simply the number of males and females, but we must know, for example, how many males there are at each age-period, as well as how many females at each age-period; or, in other words, we must count age and sex in combination. By a simple use of the well-known electrical relay we can secure this or any other possible combination. It must not be understood that only two items can be combined; in this way any number of items can be combined. We are only limited by the number of counters and relays.</p>&#13;
<p class="noindent">This means that the machine is roughly capable of modern SQL queries, including <span class="literal">SELECT, WHERE, GROUP BY</span>, and <span class="literal">ORDER BY</span>.</p>&#13;
<p class="indent">Following the machine’s widely reported success in the 1890 census, Hollerith incorporated the Tabulating Machine Company in 1896. It became the Computing-Tabulating-Recording Company in 1911, then International Business Machines (IBM) in 1924. IBM was described as doing “super-computing” by the <em>New York World</em> newspaper in 1931 and performed similar commercial big data analytics for many governments and companies before 1936. It continues to do so today.</p>&#13;
<h4 class="h4" id="lev28"><em>Electromechanical Differential Analyzers</em></h4>&#13;
<p class="noindent">Analog mechanical differential analyzers reached widespread practical use when it became possible to power them using electricity. Electrical circuits also provided a major new application for differential analyzers, as they are <span epub:type="pagebreak" id="page_24"/>often described using the same kinds of differential equations as used in mechanics. Hazen and Bush’s 1928 system, built at MIT, is often credited for the mass popularization of electromechanical differential analyzers, and its concept quickly spread to research teams at the universities of Manchester and Cambridge (<a href="ch01.xhtml#ch01fig19">Figure 1-19</a>) in the UK. Some of these British research machines were built using Meccano (similar to an Erector Set) on smaller budgets than the American versions.</p>&#13;
<div class="image"><img id="ch01fig19" src="../images/f0024-01.jpg" alt="Image" width="646" height="474"/></div>&#13;
<p class="figcap"><em>Figure 1-19: Maurice Wilkes (right) with the mechanical Cambridge Differential Analyzer, 1937</em></p>&#13;
<p class="indent">Similar machines were used heavily throughout World War II to solve differential equations, such as when calculating projectile trajectories. By attaching pens to the machines’ moving parts, some teams added analog plotters to draw graphs on paper. Versions of these machines were still used in the 1970s as onboard missile guidance systems.</p>&#13;
<h4 class="h4" id="lev29"><em>Electromechanical Machines of World War II</em></h4>&#13;
<p class="noindent">Many popular histories focus on machines used during World War II for <em>cryptography</em>, the enciphering and deciphering of messages by a transmitter and receiver, and <em>cryptanalysis</em>, the cracking of ciphers. Together, these fields are known as <em>cryptology</em>. Cracking ciphers is harder than encrypting and decrypting them. Thus, cryptanalysis machines are the larger, more interesting ones. Should any of the machines from either or both categories qualify as “computers”? Their history has been concealed by government secrecy, and we’re still learning more as documents are made public. This uncertainty has been useful for some biased historians and filmmakers who want their own country or community to have invented the computer.</p>&#13;
<p class="indent">The original Enigma (<a href="ch01.xhtml#ch01fig20">Figure 1-20</a>) was a 1923 electromechanical German commercial cryptography product sold to banks and governments in many countries, including America and Britain.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_25"/><img id="ch01fig20" src="../images/f0025-01.jpg" alt="Image" width="559" height="797"/></div>&#13;
<p class="figcap"><em>Figure 1-20: The German Enigma wiring, showing four input keys (2), four output lamps (9), three rotors (5, 5, 5), a plugboard (8), and a reflector (6)</em></p>&#13;
<p class="indent">The Enigma consists of a typewriter keyboard, output letter lamps, three rotors, and electric wiring. Each rotor acts to substitute a letter for each other letter. The input letter <em>a</em> is passed through the three rotors in sequence, then “reflected” (substituted for 26 – <em>a</em>) and passed backward through the three rotors again. Each time this is done, the end rotor advances by 1, with carries between rotors, as in Pascal’s calculator. Each configuration of a rotor produces a particular set of substitutions. All Enigma operations were symmetric: the same machine state would perform decryption on its own encrypted text. Several versions of the machine were used in the war.</p>&#13;
<p class="indent">The German military M3 Enigma added a stage swapping pairs of letters using a plugboard. Seven years before the war, the Polish, led by Marian Rejewski, broke its encryption by designing and using a singlepurpose electromechanical machine, the <em>Bomba</em>. This incorporated physical Enigma rotors to brute-force all possible encodings of known message headers in advance. The daily keys were then looked up in a reverse-index filecard database. The Polish gave this system to the British at Bletchley Park (which later became GCHQ).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_26"/>In 1938, the Germans changed protocol—not hardware—to remove the known message headers. The Polish mathematician and cryptologist Henryk Zygalski then broke the Enigma again, using optical computing. Information was transferred to punch cards, and the cards were stacked and held up to a light to very quickly find the locations where the light passes through the whole stack.</p>&#13;
<p class="indent">In 1939, the Germans increased the number of possible rotors to insert into the three slots from three to five. This increased the complexity beyond what Zygalski’s method could break. To break this version, the British switched to IBM Hollerith machines to perform similar computations at higher speeds.</p>&#13;
<p class="indent"><em>Dolphin</em> was a stronger M3 protocol used by U-boats, including more swappable rotors and different headers. The <em>British Bombe</em> was designed based on the Polish Bomba and updated for the new task. The additional cryptology was done by Alan Turing, Gordon Welchman, and others, then the machine was designed and manufactured by Harold Keen of IBM.</p>&#13;
<p class="indent"><em>Typex</em> was the British version of Enigma. Like the Germans, they made their own modifications to the commercial Enigma for their military communications. Typex was broken frequently by the B-Dienst—the German equivalent of Bletchley Park—using IBM Hollerith machines.</p>&#13;
<p class="indent">In 1937, IBM president Thomas Watson met Hitler and received an award for the Hollerith machines’ “services to the Reich.” Hollerith machines were later leased from IBM by German concentration camps to enable the Holocaust’s precision—“timing so precise the victims were able to walk right out of the boxcar and into a waiting gas chamber.” They were used to merge big data sources such as census and medical records to produce lists of names and statuses of victims. IBM provided IT consultants to help with the software design, and to make monthly visits to service the machines on site.</p>&#13;
<h4 class="h4" id="lev30"><em>The Zuse Z3</em></h4>&#13;
<p class="noindent">Konrad Zuse was a German engineer who collaborated with the Nazi Party to build the Z3 machine for its military in 1941. The <em>Z3</em> was an electromechanical machine using 2,000 electromechanical relay switches and a mechanical binary memory with 64 addresses of 22 bits. It could run up to 10 instructions per second.</p>&#13;
<p class="indent">In 1998, the Z3 was shown to be theoretically a Church computer, but only via a very obscure and impractical technicality. It could also potentially have very slowly simulated a von Neumann machine, but it was not used to do this.</p>&#13;
<h3 class="h3" id="lev31"><span epub:type="pagebreak" id="page_27"/>The Electrical Age</h3>&#13;
<p class="noindent"><em>Vacuum tubes</em> (aka <em>valves</em>) were invented in 1904 by John Fleming as an efficient replacement for relays. Unlike relays, they have no moving parts; they’re purely electrical, meaning they can switch faster than their electromechanical counterparts. They’re still used today in analog audio amplification, such as in tube or valve guitar amplifiers (<a href="ch01.xhtml#ch01fig21">Figure 1-21</a>).</p>&#13;
<div class="image"><img id="ch01fig21" src="../images/f0027-01.jpg" alt="Image" width="552" height="353"/></div>&#13;
<p class="figcap"><em>Figure 1-21: A guitar amplifier made with vacuum tubes</em></p>&#13;
<p class="indent">A vacuum tube looks and works like an Edison light bulb. A vacuum is created in a sealed glass tube. Inside the tube are three components: an anode, a cathode, and a heater. The anode and cathode are the terminals of the electrical circuit that is being switched on and off, so they have positive and negative voltages, respectively. The heater is the switch. When the heater is turned on, the heat allows electrons to escape from the cathode and travel through the vacuum to the anode, enabling current to flow and switching on the circuit. When the heater is turned off, electrons no longer have enough energy to do this, so the circuit is switched off.</p>&#13;
<p class="indent">When we restrict the heater to being either on or off, we have a digital switch that functions like a relay, forming a basic unit of purely electrical computation. (Alternatively, for audio and other signals amplification, we may allow the heater to have a continuum of heat levels, which cause a continuum of current sizes to flow in the main circuit, creating an analog amplification effect: the small heater control current turns a much larger main circuit current up and down.)</p>&#13;
<h4 class="h4" id="lev32"><em>Pure Electronic Cryptology of World War II</em></h4>&#13;
<p class="noindent">Pure electronic machines appeared later in World War II than the more famous electromechanical ones. They have also been shrouded in secrecy but are sometimes argued to be the “first computers.”</p>&#13;
<p class="indent">In 1942, the German naval Enigma was upgraded to use four instead of three rotor slots (called the “M4 model” by the Germans; its traffic was called “Shark” by the Allies). Brute-force cracking this level of cryptographic <span epub:type="pagebreak" id="page_28"/>complexity required the American approach of throwing money at computing power by paying IBM to produce hundreds of new, fast, fully electronic and vacuum tube-based <em>American Bombes</em>.</p>&#13;
<p class="indent"><em>Fish</em> was a cipher produced by a different German cryptography machine, the Lorenz SZ42; this was not an Enigma, but it used similar rotors. It was discovered by the Allies later in the war than Enigma because its traffic was initially sent only over landline telegraph wires rather than radio, making it harder to intercept. It was broken by a Bletchley team led by Max Newman, using the <em>Colossus</em> machine designed and built by Tommy Flowers and his team in 1944, shown in <a href="ch01.xhtml#ch01fig22">Figure 1-22</a>.</p>&#13;
<div class="image"><img id="ch01fig22" src="../images/f0028-01.jpg" alt="Image" width="709" height="474"/></div>&#13;
<p class="figcap"><em>Figure 1-22: Colossus, Bletchley Park, 1943, with operators Dorothy Du Boisson and Elsie Booker</em></p>&#13;
<p class="indent">Colossus was a fully electronic, vacuum tube-based machine, like the American Bombes, but it was also able to perform different functionalities if physically rewired for them. The British continued to use Colossus to break Russian codes up to the 1960s. Like the Z3, Colossus was only recently shown to be theoretically a Church computer, but only in a convoluted, speculative configuration requiring 10 machines wired together and programmed with a novel virtual machine (VM), which was not done at the time.</p>&#13;
<h4 class="h4" id="lev33"><em>ENIAC</em></h4>&#13;
<p class="noindent"><em>ENIAC (Electronic Numerical Integrator and Computer)</em> was an American vacuum tube machine developed by John Mauchly and J. Presper Eckert in the final years of World War II. It was completed in 1945 and used by the US military for ballistics calculations. It remained in service after the war, doing hydrogen bomb calculations.</p>&#13;
<p class="indent">Mauchly and Eckert were explicit in basing their design on Babbage’s Analytical Engine, translating each of its mechanical components into equivalent vacuum tubes. Like the Analytical Engine, this gives a fully <span epub:type="pagebreak" id="page_29"/>general-purpose machine that can be programmed to execute arbitrary programs of instructions.</p>&#13;
<p class="indent">ENIAC was programmed by physically patching cables into sockets on its panels, as is sometimes still done today to “program” electronic synthesizer “patches.” Original photographs of its programmers writing programs in this way (<a href="ch01.xhtml#ch01fig23">Figure 1-23</a>) were sometimes mistaken for technicians simply maintaining the machine or setting it up to run programs written by other people. We now understand that this is how the actual programming itself was done and that these pictures show the actual programmers at work. As in Lovelace and Babbage’s time, and Bletchley’s, it was assumed that programming was “women’s work” and hardware was “men’s work.”</p>&#13;
<div class="image"><img id="ch01fig23" src="../images/f0029-01.jpg" alt="Image" width="709" height="480"/></div>&#13;
<p class="figcap"><em>Figure 1-23: ENIAC and programmers Betty Jean Jennings and Frances Bilas at work in the 1940s</em></p>&#13;
<p class="indent">ENIAC can run any program (given enough memory), but like the Analytical Engine, it has a Harvard architecture; some might argue that the need to physically patch programs limits its claim to being the first computer. As with many other machines, we could reply that, in theory, someone could have programmed a VM to work around this problem. It was only recently that computer historians rediscovered that someone actually did this for ENIAC!</p>&#13;
<h4 class="h4" id="lev34"><em>Virtual Machine ENIAC</em></h4>&#13;
<p class="noindent">The ENIAC programmers Betty Jean Jennings, Marlyn Wescoff, Ruth Lichterman, Betty Snyder, Frances Bilas, and Kay McNulty eventually got tired of programming ENIAC by physically rewiring cables for each new program. So, as a quick hack, they designed a program with these wires that allowed the client program to be read from a panel of switches instead. This created a virtual machine in which a single fixed hardware program emulated a computer that could read higher-level programs from the switches.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_30"/>Some people argue that “the first computer” was created at this moment, as a software rather than a hardware creation. This would be a beautiful story, but there’s still a problem: the architecture is still a Harvard architecture because the user program is stored in the physical switches and not in the computer’s main memory. This means that a program couldn’t modify its own code, which some people see as a requirement for the “first computer.”</p>&#13;
<p class="indent">The ability for a program to modify its own code is a fairly obscure requirement, rarely necessary outside of a few unsavory security applications and obfuscated coding contests. In theory, the ENIAC programmers <em>could</em> have continued to create a second layer of VM, which could have represented higher-level programs in the data rather than program memory. That would have created a von Neumann architecture, with programs capable of modifying their own code using the same VM idea the programmers had already invented. But they never felt the need to do this. Detractors argue that the <em>potential</em> for the ENIAC programmers to have done this is no more of a claim of “first computer” status than the potential for a Z3 programmer to have built VMs, and so they assert the virtual ENIAC missed being the first computer by a gnat’s whisker.</p>&#13;
<p class="notes"><strong><span class="nt">NOTE</span></strong></p>&#13;
<p class="noindent"><em>Speaking of gnats, the world’s first computer “bug”—and the origin of the modern use of the word—was caught and logged in 1947 by the programmers of another machine, the Harvard Mark II. It was a moth that had gotten stuck inside the machine, causing it to malfunction.</em></p>&#13;
<h4 class="h4" id="lev35"><em>The Manchester Baby</em></h4>&#13;
<p class="noindent">In 1948, Frederic Williams, Tom Kilburn, and Geoff Tootill demonstrated the first “electronic stored-program computer” at what is now the University of Manchester. <em>Stored program</em> means what we now call a von Neumann architecture. The machine was officially named the Small-Scale Experimental Machine and nicknamed “the Baby” (<a href="ch01.xhtml#ch01fig24">Figure 1-24</a>).</p>&#13;
<p class="indent">The Baby’s CPU used around 500 vacuum tubes, together with diodes and other components. It implemented an instruction set of seven instructions. In modern terms, the Baby was a 32-bit machine, with 32 addresses each storing one 32-bit word.</p>&#13;
<p class="indent">The Baby was built from parts including the then broken-up Bletchley Colossus machines; it was quickly scrapped and cannibalized itself to provide parts for the later Manchester Mark I machine. A replica of the Baby can be seen today in Manchester’s Science and Industry Museum. This museum is especially interesting, as it also contains textile processing machines from the industrial revolution, which began in Manchester. These machines form a cultural connection between the Jacquard loom and the Manchester computers.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_31"/><img id="ch01fig24" src="../images/f0031-01.jpg" alt="Image" width="710" height="473"/></div>&#13;
<p class="figcap"><em>Figure 1-24: The Manchester Baby rebuilt in Manchester’s Science and Industry Museum, UK. Note the CRT memory in the center, also used as a display.</em></p>&#13;
<p class="indent">The Baby can probably be programmed to play <em>Space Invaders</em> on its green CRT screen: since the modern rebuild, similar games have been demonstrated for it both in emulation and on the real machine in what is perhaps the most extreme example of retro gaming.</p>&#13;
<p class="indent">Having a von Neumann architecture, the Baby is also able to run programs that modify their own code. Thus, by the time we reach the Baby, we appear to have an indisputable Church computer, as long as we’re happy that it could be “given as much memory as it asks for.” It’s not trivial to wonder how that could be done, though, as the Baby’s architecture is so specific to the 32×32-bit memory design. You <em>could</em> redesign it with a larger memory, but would that really be the same Baby, or a different machine?</p>&#13;
<h4 class="h4" id="lev36"><em>The 1950s and Commercial Computing</em></h4>&#13;
<p class="noindent">UNIVAC (Universal Automatic Computer; <a href="ch01.xhtml#ch01fig25">Figure 1-25</a>) was delivered to its first customer in March 1951. It was Mauchly and Eckert’s commercialized version of their previous ENIAC, making it the first <em>commercial</em> general-purpose stored-program computer. Like ENIAC, UNIVAC was vacuum tube-based. CBS used one to make a successful statistical prediction of the US presidential election of 1952, which brought fame and sales. Mauchly and Eckert’s company still exists as the modern Unisys Corporation.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_32"/><img id="ch01fig25" src="../images/f0032-01.jpg" alt="Image" width="616" height="473"/></div>&#13;
<p class="figcap"><em>Figure 1-25: UNIVAC</em></p>&#13;
<p class="indent">IBM was slow to understand that UNIVAC and other electronic computers would destroy their still-profitable tabulating machines business, with CEO Thomas Watson making the worst futurology prediction in human history in 1948: “I think there is a world market for about five computers.” After waking up to the new technology, IBM produced its own first commercial electronic computer in 1952, the IBM 701.</p>&#13;
<h3 class="h3" id="lev37">The Transistor Age</h3>&#13;
<p class="noindent">A <em>transistor</em> performs the same function as a vacuum tube, but it’s smaller, faster, and cheaper, and it consumes less power and is more reliable. Like tubes, transistors can be used for both analog and digital tasks (they’re found in analog audio amplifiers such as transistor radios and guitar amps), but for computing, they’re used only for their digital properties.</p>&#13;
<p class="indent">William Shockley, John Bardeen, and Walter Brattain discovered the transistor effect in 1947 and were awarded the Nobel Prize in Physics for it in 1956. Work to commercialize transistors began in the 1950s in what is now Silicon Valley, and the technology became mainstream in the 1960s. Transistors remain the basic technology of computers today.</p>&#13;
<div class="image"><img id="ch01fig26" src="../images/f0032-02.jpg" alt="Image" width="236" height="329"/></div>&#13;
<p class="figcap"><em>Figure 1-26: A big transistor</em></p>&#13;
<h4 class="h4" id="lev38"><em>The 1960s and Big Transistors</em></h4>&#13;
<p class="noindent">The transistor “minicomputers” of the 1960s didn’t use microchips, but instead were made from the “big” kinds of transistors, about 1 cm long, that you would put in a breadboard circuit today (<a href="ch01.xhtml#ch01fig26">Figure 1-26</a>). It’s still possible to make a CPU out of such transistors, and a few hobbyists do it for fun (for <span epub:type="pagebreak" id="page_33"/>example, the MOnSter 6502 project by Eric Schlaepfer and Evil Mad Scientist Laboratories).</p>&#13;
<p class="indent">These computers filled a rack and included the classic PDP machines (<a href="ch01.xhtml#ch01fig27">Figure 1-27</a>) used heavily in early AI research. This was also the time when Seymour Cray began building Cray supercomputers, aiming to make the biggest and fastest machines for high-end users.</p>&#13;
<div class="image"><img id="ch01fig27" src="../images/f0033-01.jpg" alt="Image" width="591" height="473"/></div>&#13;
<p class="figcap"><em>Figure 1-27: A transistor-based 1960s PDP-11 mini-computer</em></p>&#13;
<p class="indent">Uses of transistor computers in the 1960s included powering ARPANET, the predecessor of today’s TCP/IP-based internet, and Margaret Hamilton’s 1969 programming of the Apollo moon landing code in assembly language (<a href="ch01.xhtml#ch01fig28">Figure 1-28</a>). The latter was actual rocket science, and required her to create the modern field of software engineering while searching for ways to make this highly critical code more correct.</p>&#13;
<div class="image"><img id="ch01fig28" src="../images/f0033-02.jpg" alt="Image" width="381" height="473"/></div>&#13;
<p class="figcap"><em>Figure 1-28: Hamilton with a printout of her complete assembly program for Apollo 11</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_34"/>In 1965, Gordon Moore, the CEO of Intel, made an observation known since as Moore’s law. As you saw in the introduction, depending on who you ask and how you count, this law says that either the speed of computers or the number of transistors per area doubles every 18 months or every 2 years.</p>&#13;
<h4 class="h4" id="lev39"><em>The 1970s and Integrated Circuits</em></h4>&#13;
<p class="noindent">The 1970s saw the widespread commercialization of <em>integrated circuits</em> (also called <em>ICs</em>, <em>microchips</em>, or <em>chips</em>). ICs had been theorized in 1952 in Britain by Geoffrey Dummer, though the 2000 Nobel Prize in Physics was awarded to Jack Kilby—who had heard Dummer talk about them in 1952—for his invention and patent of a practical version in 1958 at Texas Instruments.</p>&#13;
<p class="indent">IC technology allows electric transistor-based circuits to be miniaturized, so that the same wiring that filled a 1960s rack cabinet can fit on a “chip” of silicon the size of a fingernail. From an architectural view, chips are not very exciting—if you take the wiring diagram from a 1940s vacuum tube machine and just miniaturize it, then you get a chip. If you look at a chip through a microscope, you’ll see similar wiring patterns to, say, the wires on the back of a 1940s, 1950s, or 1960s rack. The silicon chip is then “packaged” inside a larger, usually black lump of plastic, with larger metal pins connecting the fine inputs and outputs of the chip to the outside world, usually a printed circuit board (<a href="ch01.xhtml#ch01fig29">Figure 1-29</a>).</p>&#13;
<div class="image"><img id="ch01fig29" src="../images/f0034-01.jpg" alt="Image" width="234" height="142"/></div>&#13;
<p class="figcap"><em>Figure 1-29: An Intel 4004 chip in its packaging</em></p>&#13;
<p class="indent">The 1970s saw the birth of some of the oldest software that is still in use today. The UNIX operating system was built by Kenneth Thompson and Dennis Ritchie in this time (<a href="ch01.xhtml#ch01fig30">Figure 1-30</a>) and has evolved into current Linux, FreeBSD, and macOS systems.</p>&#13;
<div class="image"><img id="ch01fig30" src="../images/f0034-02.jpg" alt="Image" width="631" height="505"/></div>&#13;
<p class="figcap"><em>Figure 1-30: Thompson and Ritchie creating UNIX on a teletype terminal</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_35"/>UNIX terminals of the time used typewriter-style print heads on paper rolls—like Babbage’s Difference Engine—and programmers would interact with the machine by typing commands on a keyboard; these commands were printed as they typed, along with their resulting outputs. This teletype system is the origin of the x-terminals used today in UNIX-like systems.</p>&#13;
<p class="indent">In contrast to terminal-based interaction, Xerox (the photocopier company) researched graphical user interfaces at its Palo Alto Research Center, Xerox PARC. This included developing the first mouse, as well as the “desktop” metaphor, including files and folders based on physical filing cabinets.</p>&#13;
<p class="indent">This choice to base the interface with a computer on a middle management office, with its desks and filing cabinets—rather than on, say, a school, art gallery, or shop—has been with us, and making computing more boring than it should be, ever since. This may be starting to change, with the rise of handheld interfaces such as Android and TV-based “10-foot” interfaces such as Kodi, which provide feasible alternatives based on “apps.”</p>&#13;
<h4 class="h4" id="lev40"><em>The 1980s Golden Age</em></h4>&#13;
<p class="noindent">Any author covering computer history eventually reaches a point where the story overlaps with their own lifetime, and from then on, they may become somewhat biased. For this author, it occurs here, so you might want to find alternative accounts from others to balance mine out.</p>&#13;
<p class="indent">The 1980s was the golden age of computer architecture: for the first time, electronic computers became cheap and small enough to be mass-produced and bought by normal people to use in their homes. As shown in <a href="ch01.xhtml#ch01fig31">Figure 1-31</a>, this may have been the best time in human history to be a kid interested in computers because you would get a proper computer for Christmas, with direct access to its architecture, at a time before operating systems hid the architecture away from the user.</p>&#13;
<div class="image"><img id="ch01fig31" src="../images/f0035-01.jpg" alt="Image" width="255" height="395"/></div>&#13;
<p class="figcap"><em>Figure 1-31: Home computing in the 1980s: a happy child with their first computer</em></p>&#13;
<p class="indent">These machines were based initially on 8-bit CPUs, such as the 6502 used in the Commodore 64 and Apple II, and then based on 16-bit CPUs, such as the Motorola 68000 used in the Amiga and Atari ST. This period—especially in retro gaming—is known as the 8-bit era and then later the 16-bit era; it’s looked back on with fondness and nostalgia by many who were there, and by many who weren’t.</p>&#13;
<p class="indent">The IBM 5150 PC launched in 1981, based on the Intel 8088 chip. IBM and others sold this and other PCs during the 1980s for use in business offices. The PC concept is the polar opposite of the heterogeneous, architecture-driven home computer market for two reasons. First, it enforces a standardized architecture on the computer components so that multiple manufacturers can produce them to be compatible with one another. Second, it wraps all the hardware under a strict operating system, which controls all access to it via a standardized interface. IBM could use its <span epub:type="pagebreak" id="page_36"/>market clout to enforce standards on components, so it could buy them from the cheapest suppliers and make money by stamping its brand on assembled PCs.</p>&#13;
<p class="indent">In a reaction to the proprietary operating systems being installed on PCs and larger computers, the GNU (recursively standing for “GNU’s Not Unix”) project and Free Software movement were created in this decade by Richard Stallman—this later led to the Linux-based systems and philosophies that we use today.</p>&#13;
<p class="indent">We will study this period in more detail in <a href="ch11.xhtml">Chapter 11</a>.</p>&#13;
<h4 class="h4" id="lev41"><em>The Bland 1990s</em></h4>&#13;
<p class="noindent">The 1990s was a bland, boring, beige decade. It was driven by a commercial focus in the industry that switched from treating users as programmers and community members to users as customers and consumers of software products, such as word processors and spreadsheets. During this time, schools stopped teaching computer science and (under the heavy influence of corporate lobbying by their creators) taught the use of commercial office software.</p>&#13;
<p class="indent">Computer architecture became dominated by the personal computer (PC) standard architecture, which had been used in office computing during the 1980s but was now pushed everywhere by the PC corporations, including on homes and schools. Closed source operating systems were pushed as part of the PC package, making it hard for users to see anything “under the hood” of their machines.</p>&#13;
<p class="indent">Physically, these machines appeared as nearly identical “beige boxes,” as in <a href="ch01.xhtml#ch01fig32">Figure 1-32</a>, and the general drabness of this middle management-style computing culture was later caricatured through Apple’s “I’m a PC” TV commercials, which portrayed the PC as a generic middle manager with a boring beige outfit.</p>&#13;
<p class="indent">As Moore’s law reliably predicted, processor speeds doubled every 18 months; this was the standard measure of how good your computer was, and many would build a new one every couple of years to take advantage of the new speed.</p>&#13;
<div class="image"><img id="ch01fig32" src="../images/f0036-01.jpg" alt="Image" width="391" height="265"/></div>&#13;
<p class="figcap"><em>Figure 1-32: A 1990s desktop</em></p>&#13;
<p class="indent">Related to the move to operating systems was the move from programming in assembly and interpreted languages, such as BASIC, to compiled languages. When languages are compiled, their authors can choose to conceal the source code so that users can no longer see how they work or learn from them by changing them. Compilers had been developed since Grace Hopper’s work in the 1950s, and were used in high-end computing, but this was the first time they and their generated code arrived in homes.</p>&#13;
<p class="indent">The computer games industry similarly became professionalized, separating consumers, who could only buy and play dedicated consoles and <span epub:type="pagebreak" id="page_37"/>games, from commercial developers with the money to access specialist programming tools. Games were sometimes fun to play, but not as much fun as they used to be to write.</p>&#13;
<p class="indent">The World Wide Web went online at CERN in 1990 and grew in popularity, leading to the dot-com investment craze at the end of the decade. As more hackers and eventually consumers joined the web, dedicated rackmounted server computer designs became popular, beginning with the Compaq ProLiant in 1993. Like the Manchester Baby and 1960s minicomputers, these were designed to be stacked in 19-inch rack units, but to be always on with high reliability.</p>&#13;
<p class="indent">For the early modem-connected elite, 1993 also saw the birth of Linux and the beginnings of its GNU-inspired international authors figuring out how to communicate and code with one another at the level of architecture and systems programming.</p>&#13;
<h4 class="h4" id="lev42"><em>The 2000s and Reconnecting the Community</em></h4>&#13;
<p class="noindent">The PC architecture of commodity components plus operating system continued throughout the 2000s. Moore’s law, and the consequent building or buying of a new doubled-speed computer every couple of years, continued. Machines used the same basic PC computer design, with various interfaces and components getting upgraded for speed. Internet speeds also increased, enabling streaming of videos as well as the transfer of text and images. Servers were reduced in size to <em>blades</em>, many of which could be packed together in a single rack unit.</p>&#13;
<p class="indent">Enabled by these advances, Linux matured into a realistic alternative system to the proprietary operating systems previously bundled with PCs. Many of the people involved in older computing communities returned and joined the Linux movement. We realized that things had to go via the operating system route rather than raw architecture; for free software advocates, this was a good thing: it removed any dependency we had on any particular hardware companies. This was now okay because the operating system was free software and thus no one had to be locked in to buying anyone’s specific products. With this hindsight, the 1980s was perhaps not so great because everyone was forced to develop on some non-free architecture platform and was thus utterly dependent on their corporate owners. The 1990s saw a reduction in freedom as a multitude of these corporations and platforms were replaced by a single dominant PC operating system corporation and platform, but since then, Linux life has become even better than the 1980s and 1990s, as we have an open platform and many competing hardware suppliers implementing it.</p>&#13;
<p class="indent">Much of the other open source software we use today developed rapidly alongside Linux, such as Firefox, Python, MySQL, and Apache. In many cases, these tools have older origins, but they only grew to a critical mass of developers and users in the 2000s.</p>&#13;
<p class="indent">The programmers working on the Linux operating system itself got to see and work with the underlying architecture, but for everyone else, architecture was generally still under the hood, as in the 1990s.</p>&#13;
<h4 class="h4" id="lev43"><span epub:type="pagebreak" id="page_38"/><em>The 2010s and the End of Moore’s Law</em></h4>&#13;
<p class="noindent">During the 1990s and 2000s we happily assumed that the clock speeds of our processors would double every couple of years—and they did. Moore’s law became a self-fulfilling prophecy as Silicon Valley chipmakers used it as a target to be reached.</p>&#13;
<p class="indent">However, this all fell apart in the 2010s. Transistor manufacturing technology did continue to double the number of transistors per area, but clock speeds maxed out by 2010, at around 3.5 GHz. Suddenly, processors weren’t getting faster anymore. This is due to the fundamental laws of physics around computation speed and heat. During the Moore’s law period, the temperature of processors had also been rising along with speed; larger and more powerful fans and other cooling systems such as water cooling were needed. The transistors got smaller, but the fans got bigger. If this trend had continued through the 2010s, we would now have processors hotter than the surface of the sun.</p>&#13;
<p class="indent">A closely linked concept is power consumption. As chips give off more heat, they consume more power, and this decade also saw the beginnings of a push toward lower-power, more portable computing, especially in the form of smartphones. This was the decade when we switched from looking up to the sky to looking down at screens in our hands.</p>&#13;
<p class="indent">As mentioned in the introduction, the end of Moore’s law has created what Turing Award winners John Hennessy and David Patterson have described as “a new golden age of architecture.” Where the previous two decades saw computer architecture stagnate as a field, relying on advances in fabrication technologies to create regular gains, the field is now wide open again for radically new ideas. We can’t make computers faster via the speed form of Moore’s law, but we can still fit more and more transistors onto chips with its density form. We can now consider making everything parallel, performing many operations at once, rather than one at a time.</p>&#13;
<p class="indent">As you might expect, the 2010s were characterized by an explosion of new ideas, architectures, hardware, and software, all to enable parallelization. A key computer science question of our time is how much programmers need to worry about this. In one possible future, programmers will continue to write sequential programs, and new kinds of parallel compilers will figure out how to turn step-by-step instructions into parallel executions. In another future, we might find this is not possible, and programmers will have to write explicitly parallel programs themselves. This will completely change the nature of programming and the kinds of skills and thought processes that programmers need.</p>&#13;
<p class="indent">While there remain many parallel architectures still to be explored—and hundreds of university researchers and startup companies now trying to explore and exploit them—the 2010s saw three major new types of parallel architecture succeeding in the real world.</p>&#13;
<p class="indent">First, and most basically, <em>multicore</em> processors are simply chips manufactured to contain more than one copy of a CPU design. The decade began with duo-core systems and progressed through quad, eight, and even more cores. If you were to run just a single program on these machines, then the <span epub:type="pagebreak" id="page_39"/>programmer would have to care about parallelism. But most current computers run an operating system program that in turn enables many programs to run concurrently, sharing the computer’s resources between them. A typical desktop machine might run 10 to 20 processes concurrently during normal operation through this arrangement, so adding <em>N</em> multicores gives a factor <em>N</em> speed up, but only up to this number of processes. Multicores will not scale very well beyond this if they are asked to run ordinary programs.</p>&#13;
<p class="indent">Second, cluster computing, shown in <a href="ch01.xhtml#ch01fig33">Figure 1-33</a>, is another form of parallelism in which many conventional single-core or multicore machines are weakly linked together. Computing work is then split into many independent chunks that can each be assigned to a machine. This requires programs to be written in a specific style, based around the split into independent jobs, and works only for certain types of tasks where such splits are possible.</p>&#13;
<div class="image"><img id="ch01fig33" src="../images/f0039-01.jpg" alt="Image" width="709" height="470"/></div>&#13;
<p class="figcap"><em>Figure 1-33: A 2010s parallel supercomputing cluster</em></p>&#13;
<p class="indent">Cluster computing has been especially useful for “big data” tasks where we usually want to repeat the same processing independently on many data items, and then collate the results (this is known as <em>map-reduce</em>). The Search for Extraterrestrial Intelligence project (SETI@home) pioneered this approach in the 1990s, using compute time on millions of home computers donated by their users to run in the background, analyzing big data from radio telescopes to look for alien messages. The method is also used by search engine companies: for example, a company might assign one commodity Dell PC out of many in a large warehouse to be responsible for storing all the locations on the web containing one particular word, and handling queries about that word. During the 2010s, the underlying map-reduce process was abstracted and open sourced by the Hadoop and Spark projects, which enabled everyone to easily set up and use similar clusters.</p>&#13;
<p class="indent">The third approach, and most interesting architecturally, has been the evolution of graphics cards (also called graphics processing units, or GPUs) into general-purpose parallel computing devices. This presents a completely new silicon-level design concept that also requires a new style of programming, somewhat similar to cluster programming. Now that its graphical <span epub:type="pagebreak" id="page_40"/>roots have been left behind, the concept is continually evolving into many novel architectures, such as recent tensor and neural processing units found on mobile phones.</p>&#13;
<p class="indent">It’s not clear whether the concept of a “programmer” will survive if some of these new parallel architectures become dominant; for example, we might “program” machines by creating specific parallel circuits in hardware, where everything happens at the same time, rather than thinking of a “program” as a set of instructions to be run in series.</p>&#13;
<h4 class="h4" id="lev44"><em>The 2020s, the Cloud, and the Internet of Things</em></h4>&#13;
<p class="noindent">This is the current decade at the time of writing, so any trends identified are somewhat speculative. With that said, the systems that we can see in development labs today suggest that the present decade will see a fundamental split of architectures into two main types.</p>&#13;
<p class="indent">First, increasingly small and cheap devices will be embedded into more and more objects in the real world. This concept, known as the <em>Internet of Things (IoT)</em>, promises to see smart sensors and computers in cities, factories, farms, homes, and pretty much everywhere else.</p>&#13;
<p class="indent">“Smart cities” will be covered in these devices to enable the monitoring of every individual vehicle and pedestrian, to make traffic control and use of city facilities more efficient. “Smart factories” will have tiny devices attached to every item of stock and track them through the manufacturing process. Smart transport, retail, and homes will track the same items right through their supply chains, “from farm to fork” in the case of food. For example, your fridge will sense that you’re running out of cheese, using either the weight of your cheesebox or machine vision looking for cheese, and automatically place an order to your local supermarket to replenish it. The supermarket will aggregate these orders and balance the demand with orders from their distribution centers. Small autonomous robots will then deliver your cheese from the supermarket to your doorstep.</p>&#13;
<p class="indent">The second trend is in the opposite direction. The low-power IoT devices won’t do much computing, but will instead exist primarily to collect and act upon “big data” in the world. This data will then be processed on massive scales in dedicated computing centers: buildings the size of warehouses that are packed with computing power.</p>&#13;
<p class="indent">Computing centers are related to <em>data centers</em>, similar-looking buildings already in existence that exist primarily to <em>store</em> data and make it available over the web, rather than to perform heavy computation on it. This type of computing appeared first at search engine companies, which used many cheap commodity PCs running together to process web crawls and searches. Search companies, and their online shopping peers, discovered they could make a profit by hiring out the spare machines that were sitting idle for general computing use by customers. This style of computing is quite like the big machines of the 1960s and 1970s, whose users would dial in from terminals and share time on them. (Perhaps Thomas Watson’s guess that there is a world market for only five computers will actually turn out to be true if we <span epub:type="pagebreak" id="page_41"/>count each of these cloud computing centers as one computer and ignore the IoT devices.)</p>&#13;
<p class="indent">The IoT devices create a particular interest in low-energy design, but related energy issues also occur in huge cloud computing centers. These buildings can use as much power as factories, give off significant heat, and cost a significant amount to run. Computing centers powered most of the world’s video calls and collaboration tools during the COVID-19 pandemic, enabling many jobs to switch to remote work for the first time. Some computing centers saw shutdowns in 2022 due to an extreme heatwave. Recently, some computing centers have been deliberately located in places such as the Arctic to make use of the natural cooling.</p>&#13;
<p class="indent">So, like Moses, in this decade we will download from the cloud onto our tablets. The two trends of the IoT and the cloud are likely to continue and become more extreme during the 2020s, pulling architecture in two opposite directions. Medium-sized desktop computers seem likely to fall in importance.</p>&#13;
<p class="indent">Already we’re getting used to computing on physically small devices such as tablet computers and the Nintendo Switch, which are starting to make larger desktop machines look a bit silly. “A computer on every desk” was the aim in the 1990s, but these are disappearing and being replaced by a mix of computers in our pockets, streets, and cloud centers. Similar setups have been suggested previously from time to time, including 1950s dial-in mainframes and 1990s “thin clients,” but in the 2020s they seem to be taking off via mobile phones, Amazon Echo, Nest home automation, and Arduinos, as well as cloud providers such as Amazon Web Services, Microsoft Azure, and Google Cloud Platform.</p>&#13;
<h3 class="h3" id="lev45">So Who Invented the Computer?</h3>&#13;
<p class="noindent">The modern concept of computation was defined by Church. Commercial electronic machines of the 1950s, beginning with UNIVAC, through 1960s minicomputers and 1970s microchips up to the present day seem clearly recognizable as computers. But should anything before them be credited as “the first computer”?</p>&#13;
<p class="indent">The Manchester Baby is a Church computer if you are happy that it could be “given as much memory as it asks for,” but it’s not very clear how this would be done. Looking at later commercial machines gives more of a feeling that they could easily be extended with more memory, for example, by plugging in extra circuit boards or hard disks. But in principle they all still have the same problem as the Baby.</p>&#13;
<p class="indent">ENIAC-initial has the potential to be a Church computer if programmed in a certain VM way. ENIAC-VM actually <em>was</em> programmed that way, but was still a Harvard architecture. It needed another layer of unrealized VM to get to RAM programs. Colossus and Zuse Z3 programmers could theoretically have done all of this, too—but didn’t. The same goes for Analytical Engine programmers.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_42"/>IBM has been doing big data analytics on machines described by the media as “supercomputers” since the 1890s, but data analytics isn’t general Church computation unless you can find a way to make any problem look like an SQL query.</p>&#13;
<p class="indent">People have probably been calculating since 40,000 BCE, with abaci, mechanical calculators, paper, pens, clay tablets, bones, rocks, their fingers, and natural numbers in their heads. All the above are theoretically Church computers because they can simulate any machine if programmed in a certain way. So perhaps we have always had computers—and Church was just the first to notice them.</p>&#13;
<h3 class="h3" id="lev46">Summary</h3>&#13;
<p class="noindent">In this chapter, we took a whirlwind tour through the history of computing. Beginning with bones and ending in the cloud, we considered a number of inventions that might, or might not, be called a computer. We also saw a few hypotheses for what makes a computer. Initially we suggested that a computer was anything that could be programmed to play <em>Space Invaders</em>. We then formalized this hypothesis by looking at Church’s thesis, which argues that a computer is a machine that can simulate any other machine, given as much memory as it asks for.</p>&#13;
<p class="indent">Our survey of the history of computing has briefly introduced the big ideas of architecture. In the next chapters, we’ll dive into the details of data representation and CPU computation to see how some of the historical systems work in more detail. This will set us up for <a href="part02.xhtml">Part II</a>’s study of modern electronic hierarchy and the many particular modern architectures of <a href="part03.xhtml">Part III</a>.</p>&#13;
<h3 class="h3" id="lev47">Exercises</h3>&#13;
<h4 class="h4a"><strong>Calculating with an Abacus Simulator</strong></h4>&#13;
<ol class="number">&#13;
<li class="tm">Use an abacus simulator (or a real abacus if you have one) and a tutorial to understand abacus arithmetic. These operations are still the basis for some modern CPU operations, and learning to do them on the abacus will help you understand them in CPUs. A simulator can be found here: <em><a href="https://www.mathematik.uni-marburg.de/~thormae/lectures/ti1/code/abacus/soroban.html">https://www.mathematik.uni-marburg.de/~thormae/lectures/ti1/code/abacus/soroban.html</a></em> and a tutorial for using it at <em><a href="https://www.wikihow.com/Use-an-Abacus">https://www.wikihow.com/Use-an-Abacus</a></em>.</li>&#13;
<li class="tm">Take the last three digits of your phone number as one number and the preceding three digits as a second number, and add them together on the abacus.</li>&#13;
<li class="tm">Take the same pair of numbers and subtract the smaller one from the larger one.</li>&#13;
<li class="tm">Take the last two digits of your phone number as a two-digit number and the preceding two digits as a second two-digit number, and multiply them using the abacus.</li>&#13;
</ol>&#13;
<h4 class="h4a"><span epub:type="pagebreak" id="page_43"/><strong>Speculative History</strong></h4>&#13;
<ol class="number">&#13;
<li class="tm">How do you think world history could have been different if the Antikythera mechanism had arrived safely in Rome and inspired the Roman Empire to use similar machines?</li>&#13;
<li class="tm">How do you think world history could have been different if the Analytical Engine had been fully constructed and commercialized in the British Empire?</li>&#13;
</ol>&#13;
<h4 class="h4a"><strong>Challenging</strong></h4>&#13;
<p class="noindent">Search the internet for examples of advanced operations using an abacus, such as square roots or prime factorization, and try to run them. You may need to use more than one abacus to provide enough columns for some of them.</p>&#13;
<h4 class="h4a"><strong>More Challenging</strong></h4>&#13;
<ol class="number">&#13;
<li class="tm">Write a speculative fiction short story or novel based on one of the premises raised by the “Speculative History” exercises.</li>&#13;
<li class="tm">How could you implement a Church computer using an abacus?</li>&#13;
<li class="tm">Research the SQL-like functions available on the Hollerith machine. Can a Church computer be made from them?</li>&#13;
</ol>&#13;
<h3 class="h3" id="lev48">Further Reading</h3>&#13;
<ul class="bullet">&#13;
<li class="tm">For details of the Hollerith machine, see H. Hollerith, “The Electrical Tabulating Machine,” <em>Journal of the Royal Statistical Society</em> 57, no. 4 (1894): 678–689, <em><a href="https://www.jstor.org/stable/2979610">https://www.jstor.org/stable/2979610</a></em>.</li>&#13;
<li class="tm">For details of Hollerith machines’ role in World War II, see Edwin Black, <em>IBM and the Holocaust: The Strategic Alliance Between Nazi Germany and America’s Most Powerful Corporation</em> (Washington, DC: Dialog Press, 2012).</li>&#13;
<li class="tm">To learn more about 2020s IoT computing, see S. Madakam, R. Ramaswamy, and S. Tripathi, “Internet of Things (IoT): A Literature Review,” <em>Journal of Computer and Communications</em> 3, no. 5 (2015), <em><a href="http://dx.doi.org/10.4236/jcc.2015.35021">http://dx.doi.org/10.4236/jcc.2015.35021</a></em>.</li>&#13;
<li class="tm">To learn more about 2020s cloud computing, see I. Hashem, I. Yaqoob, N.B. Anuar, et al., “The Rise of ‘Big Data’ on Cloud Computing: Review and Open Research Issues,” <em>Information Systems</em> 47 (2015): 98–115.</li>&#13;
<li class="tm">For a dieselpunk novel featuring World War II cryptography, see Neal Stephenson, <em>Cryptonomicon</em> (New York: Avon, 1999).<span epub:type="pagebreak" id="page_44"/></li>&#13;
</ul>&#13;
</div>
</div>
</body></html>