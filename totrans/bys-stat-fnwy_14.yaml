- en: '**11'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MEASURING THE SPREAD OF OUR DATA**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, you’ll learn three different methods—mean absolute deviation,
    variance, and standard deviation—for quantifying the *spread*, or the different
    extremes, of your observations.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, you learned that the mean is the best way to guess
    the value of an unknown measurement, and that the more spread out our observations,
    the more uncertain we are about our estimate of the mean. As an example, if we’re
    trying to figure out the location of a collision between two cars based only on
    the spread of the remaining debris after the cars have been towed away, then the
    more spread out the debris, the less sure we’d be of where precisely the two cars
    collided.
  prefs: []
  type: TYPE_NORMAL
- en: Because the spread of our observations is related to the uncertainty in the
    measurement, we need to be able to quantify it so we can make probabilistic statements
    about our estimates (which you’ll learn how to do in the next chapter).
  prefs: []
  type: TYPE_NORMAL
- en: '**Dropping Coins in a Well**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Say you and a friend are wandering around the woods and stumble across a strange-looking
    old well. You peer inside and see that it seems to have no bottom. To test it,
    you pull a coin from your pocket and drop it in, and sure enough, after a few
    seconds you hear a splash. From this, you conclude that the well is deep, but
    not bottomless.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the supernatural discounted, you and your friend are now equally curious
    as to how deep the well actually is. To gather more data, you grab five more coins
    from your pocket and drop them in, getting the following measurements in seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: 3.02, 2.95, 2.98, 3.08, 2.97
  prefs: []
  type: TYPE_NORMAL
- en: As expected, you find some variation in your results; this is primarily due
    to the challenge of making sure you drop the coin from the same height and time
    then record the splash correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, your friend wants to try his hand at getting some measurements. Rather
    than picking five similarly sized coins, he grabs a wider assortment of objects,
    from small pebbles to twigs. Dropping them in the well, your friend gets the following
    measurements:'
  prefs: []
  type: TYPE_NORMAL
- en: 3.31, 2.16, 3.02, 3.71, 2.80
  prefs: []
  type: TYPE_NORMAL
- en: Both of these samples have a mean (μ) of about 3 seconds, but your measurements
    and your friend’s measurements are spread to different degrees. Our aim in this
    chapter is to come up with a way to quantify the difference between the spread
    of your measurements and the spread of your friend’s. We’ll use this result in
    the next chapter to determine the probability of certain ranges of values for
    our estimate.
  prefs: []
  type: TYPE_NORMAL
- en: For the rest of this chapter we’ll indicate when we’re talking about the first
    group of values (your observations) with the variable *a* and the second group
    (your friend’s observations) with the variable *b*. For each group, each observation
    is denoted with a subscript; for example, *a*[2] is the second observation from
    group *a*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Finding the Mean Absolute Deviation**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll begin by measuring the spread of each observation from the mean (μ). The
    mean for both *a* and *b* is 3\. Since μ is our best estimate for the true value,
    it makes sense to start quantifying the difference in the two spreads by measuring
    the distance between the mean and each of the values. [Table 11-1](ch11.xhtml#ch11tab01)
    displays each observation and its distance from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 11-1:** Your and Your Friend’s Observations and Their Distances from
    the Mean'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Observation** | **Difference from mean** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Group *a*** |  |'
  prefs: []
  type: TYPE_TB
- en: '| 3.02 | 0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.95 | –0.05 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.98 | –0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.08 | 0.08 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.97 | –0.03 |'
  prefs: []
  type: TYPE_TB
- en: '| **Group *b*** |  |'
  prefs: []
  type: TYPE_TB
- en: '| 3.31 | 0.31 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.16 | –0.84 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.02 | 0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.71 | 0.71 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.80 | –0.16 |'
  prefs: []
  type: TYPE_TB
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The distance from the mean is different than the error value, which is the
    distance from the true value and is unknown in this case.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'A first guess at how to quantify the difference between the two spreads might
    be to just sum up their differences from the mean. However, when we try this out,
    we find that the sum of the differences for both sets of observations is exactly
    the same, which is odd given the notable difference in the spread of the two data
    sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0105-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The reason we can’t simply sum the differences from the mean is related to
    why the mean works in the first place: as we know from [Chapter 10](ch10.xhtml#ch10),
    the errors tend to cancel each other out. What we need is a mathematical method
    that makes sure our differences don’t cancel out without affecting the validity
    of our measurements.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason the differences cancel out is that some are negative and some are
    positive. So, if we convert all the differences to positives, we can eliminate
    this problem without invalidating the values.
  prefs: []
  type: TYPE_NORMAL
- en: The most obvious way to do this is to take the *absolute value* of the differences;
    this is the number’s distance from 0, so the absolute value of 4 is 4, and the
    absolute value of –4 is also 4\. This gives us the positive version of our negative
    numbers without actually changing them. To represent an absolute value, we enclose
    the value in vertical lines, as in | –6 | = | 6 | = 6.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take the absolute value of the differences in [Table 11-1](ch11.xhtml#ch11tab01)
    and use those in our calculation instead, we get a result we can work with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0106-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Try working this out by hand, and you should get the same results. This is a
    more useful approach for our particular situation, but it applies only when the
    two sample groups are the same size.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine we had 40 more observations for group *a*—let’s say 20 observations
    of 2.9 and 20 of 3.1\. Even with these additional observations, the data in group
    *a* seems less spread out than the data in group *b*, but the absolute sum of
    group *a* is now 85.19 simply because it has more observations!
  prefs: []
  type: TYPE_NORMAL
- en: 'To correct for this, we can normalize our values by dividing by the total number
    of observations. Rather than dividing, though, we’ll just multiply by 1 over the
    total, which is known as *multiplying the reciprocal* and looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0106-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we have a measurement of the spread that isn’t dependent on the sample
    size! The generalization of this approach is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0106-03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here we’ve calculated the mean of the absolute differences between our observations
    and the mean. This means that for group *a* the average observation is 0.04 from
    the mean, and for group *b* it’s about 0.416 seconds from the mean. We call the
    result of this formula the *mean absolute deviation (MAD)*. The MAD is a very
    useful and intuitive measure of how spread out your observations are. Given that
    group *a* has a MAD of 0.04 and group *b* around 0.4, we can now say that group
    *b* is about 10 times as spread out as group *a*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Finding the Variance**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another way to mathematically make all of our differences positive without
    invalidating the data is to square them: (*x[i]* – μ)². This method has at least
    two benefits over using MAD.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first benefit is a bit academic: squaring values is much easier to work
    with mathematically than taking their absolute value. In this book, we won’t take
    advantage of this directly, but for mathematicians, the absolute value function
    can be a bit annoying in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: The second, and more practical, reason is that squaring results in having an
    *exponential penalty*, meaning measurements very far away from the mean are penalized
    much more. In other words, small differences aren’t nearly as important as big
    ones, as we would feel intuitively. If someone scheduled your meeting in the wrong
    room, for example, you wouldn’t be too upset if you ended up next door to the
    right room, but you’d almost certainly be upset if you were sent to an office
    on the other side of the country.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we substitute the absolute value for the squared difference, we get the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0107-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This formula, which has a very special place in the study of probability, is
    called the *variance*. Notice that the equation for variance is exactly the same
    as MAD except that the absolute value function in MAD has been replaced with squaring.
    Because it has nicer mathematical properties, variance is used much more frequently
    in the study of probability than MAD. We can see how different our results look
    when we calculate their variance:'
  prefs: []
  type: TYPE_NORMAL
- en: Var(group *a*) = 0.002, Var(group *b*) = 0.269
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we’re squaring, however, we no longer have an intuitive understanding
    of what the results of variance mean. MAD gave us an intuitive definition: this
    is the average distance from the mean. Variance, on the other hand, says: this
    is the average squared difference. Recall that when we used MAD, group *b* was
    about 10 times more spread out than group *a*, but in the case of variance, group
    *b* is now 100 times more spread out!'
  prefs: []
  type: TYPE_NORMAL
- en: '**Finding the Standard Deviation**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While in theory variance has many properties that make it useful, in practice
    it can be hard to interpret the results. It’s difficult for humans to think about
    what a difference of 0.002 seconds squared means. As we’ve mentioned, the great
    thing about MAD is that the result maps quite well to our intuition. If the MAD
    of group *b* is 0.4, that means that the average distance between any given observation
    and the mean is literally 0.4 seconds. But averaging over squared differences
    doesn’t allow us to reason about a result as nicely.
  prefs: []
  type: TYPE_NORMAL
- en: 'To fix this, we can take the square root of the variance in order to scale
    it back into a number that works with our intuition a bit better. The square root
    of a variance is called the *standard deviation* and is represented by the lowercase
    Greek letter sigma (σ). It is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0107-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The formula for standard deviation isn’t as scary as it might seem at first.
    Looking at all of the different parts, given that our goal is to numerically represent
    how spread out our data is, we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: We want the difference between our data and the mean, *x[i]* – μ.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to convert negative numbers to positives, so we take the square, (*x[i]*
    – μ)².
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to add up all the differences:![Image](../images/f0108-01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We don’t want the sum to be affected by the number of observations, so we normalize
    it with 1/*n*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we take the square root of everything so that the numbers are closer
    to what they would be if we used the more intuitive absolute distance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we look at the standard deviation for our two groups, we can see that it’s
    very similar to the MAD:'
  prefs: []
  type: TYPE_NORMAL
- en: σ(group *a*) = 0.046, σ(group *b*) = 0.519
  prefs: []
  type: TYPE_NORMAL
- en: The standard deviation is a happy medium between the intuitiveness of MAD and
    the mathematical ease of variance. Notice that, just like with MAD, the difference
    in the spread between *b* and *a* is a factor of 10\. The standard deviation is
    so useful and ubiquitous that, in most of the literature on probability and statistics,
    variance is defined simply as σ², or sigma squared!
  prefs: []
  type: TYPE_NORMAL
- en: So we now have three different ways of measuring the spread of our data. We
    can see the results in [Table 11-2](ch11.xhtml#ch11tab02).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 11-2:** Measurements of Spread by Method'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method of measuring spread** | **Group *a*** | **Group *b*** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Mean absolute deviations | 0.040 | 0.416 |'
  prefs: []
  type: TYPE_TB
- en: '| Variance | 0.002 | 0.269 |'
  prefs: []
  type: TYPE_TB
- en: '| Standard deviation | 0.046 | 0.519 |'
  prefs: []
  type: TYPE_TB
- en: None of these methods for measuring spread is more correct than any other. By
    far the most commonly used value is the standard deviation, because we can use
    it, together with the mean, to define a normal distribution, which in turn allows
    us to define explicit probabilities to possible true values of our measurements.
    In the next chapter, we’ll take a look at the normal distribution and see how
    it can help us understand our level of confidence in our measurements.
  prefs: []
  type: TYPE_NORMAL
- en: '**Wrapping Up**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, you learned three methods for quantifying the spread of a group
    of observations. The most intuitive measurement of the spread of values is the
    mean absolute deviation (MAD), which is the average distance of each observation
    from the mean. While intuitive, MAD isn’t as useful mathematically as the other
    options.
  prefs: []
  type: TYPE_NORMAL
- en: The mathematically preferred method is the variance, which is the squared difference
    of our observations. But when we calculate the variance, we lose the intuitive
    feel for what our calculation means.
  prefs: []
  type: TYPE_NORMAL
- en: Our third option is to use the standard deviation, which is the square root
    of the variance. The standard deviation is mathematically useful and also gives
    us results that are reasonably intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Try answering the following questions to see how well you understand these different
    methods of measuring the spread of data. The solutions can be found at *[https://nostarch.com/learnbayes/](https://nostarch.com/learnbayes/)*.
  prefs: []
  type: TYPE_NORMAL
- en: One of the benefits of variance is that squaring the differences makes the penalties
    exponential. Give some examples of when this would be a useful property.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the mean, variance, and standard deviation for the following values:
    1, 2, 3, 4, 5, 6, 7, 8, 9, 10.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
