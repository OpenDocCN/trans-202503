<html><head></head><body>
<h2 class="h2" id="ch03"><span epub:type="pagebreak" id="page_69"/><strong><span class="big">3</span><br/>SEQUENTIAL LOGIC</strong></h2>
<div class="image1"><img src="../images/common.jpg" alt="Image"/></div>
<p class="noindent">The combinatorial logic you learned about in the last chapter “goes with the flow.” In other words, the outputs change in response to the inputs. But we can’t build computers out of combinatorial logic alone, because it doesn’t give us any way to remove something from the flow and remember it. You can’t add up all the numbers from 1 to 100, for example, unless you can keep track of where you are.</p>
<p class="indent">You’ll learn about <em>sequential logic</em> in this chapter. The term comes from the word <em>sequence</em>, which means “one thing after another in time.” As a human, you have intuitive knowledge about time, just as you do about counting on your fingers, but that doesn’t mean that time is natural for digital circuitry. We have to create it somehow.</p>
<p class="indent">Combinatorial logic deals only with the present state of inputs. Sequential logic, however, deals with both the present and the past. In this chapter, <span epub:type="pagebreak" id="page_70"/>you’ll learn about circuitry both for generating time and for remembering things. We’ll trace some of the various technologies that have been used for these purposes from their early roots through the present day.</p>
<h3 class="h3" id="ch03lev1sec1"><strong>Representing Time</strong></h3>
<p class="noindent">We measure time using some sort of <em>periodic</em> function, such as the rotation of the Earth. We call one full rotation a day, which we subdivide into smaller units such as hours, minutes, and seconds. We could define a second as 1/86,400<sup>th</sup> of an Earth rotation, since there are 86,400 seconds in a day.</p>
<p class="indent">In addition to using an external event like the rotation of the Earth, we can also generate our own periodic functions by applying certain elements of physics, such as the time that it takes for a pendulum to swing. This technique produced the “tick tock” sound in old grandfather clocks. Of course, to be useful, the pendulum has to be calibrated to the measured length of a second.</p>
<p class="indent">With computers, we’re working with electronics, so we need a periodic electrical signal. We could generate one by placing a switch so that it’s whacked by a pendulum. But unless you’re a serious steampunk geek, you probably don’t want a pendulum-powered computer. We’ll learn about more modern approaches in the next section.</p>
<h4 class="h4" id="ch03lev2sec1"><strong><em>Oscillators</em></strong></h4>
<p class="noindent">Let’s look at a trick we can do with an inverter: we can connect the output to the input, as shown in <a href="ch03.xhtml#ch03fig01">Figure 3-1</a>.</p>
<div class="image"><a id="ch03fig01"/><img src="../images/03fig01.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-1: An oscillator</em></p>
<p class="indent">This produces <em>feedback</em>, just like what you get when a microphone is too close to a loudspeaker. The output of the inverter bounces back and forth, or <em>oscillates</em>, between 0 and 1. The speed at which it oscillates is a function of the propagation delay (see “<a href="ch02.xhtml#ch02lev2sec14">Propagation Delay</a>” on <a href="ch02.xhtml#page_57">page 57</a>), and that tends to vary with temperature. It would be useful to have an oscillator with a stable frequency so that we could generate an accurate time reference.</p>
<p class="indent">A cost-effective way to do this is with a crystal. Yes, very new age. Crystals, like magnets, have a relationship with electricity. If you attach <em>electrodes</em> (wires) to a crystal and give it a squeeze, it’ll generate electricity. And if you put some electricity on those wires, the crystal will bend. This is called the <em>piezoelectric</em> effect, and it was discovered by brothers Paul-Jacques (1855–1941) and Pierre (1859–1906) Curie in the late 1800s. The piezoelectric effect has all sorts of applications. A crystal can pick up sound vibrations, making <span epub:type="pagebreak" id="page_71"/>a microphone. Sound vibrations generated by applying electricity to crystals are responsible for the annoying beeps made by many appliances. You can spot a crystal in a circuit diagram by the symbol shown in <a href="ch03.xhtml#ch03fig02">Figure 3-2</a>.</p>
<div class="image"><a id="ch03fig02"/><img src="../images/03fig02.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-2: The crystal schematic symbol</em></p>
<p class="indent">A crystal oscillator alternately applies electricity to a crystal and receives electricity back, using electronic single-pole, double-throw switches. The time it takes a crystal to do this is predictable and very accurate. Quartz is one of the best crystal materials to use. That’s why you see advertisements for accurate quartz timepieces. Keep in mind when you see the price tag on a fancy watch that a really good crystal retails for only about 25 cents.</p>
<h4 class="h4" id="ch03lev2sec2"><strong><em>Clocks</em></strong></h4>
<p class="noindent">Oscillators give us a way to measure time, as you’ve seen. Computers need to keep time for obvious reasons, like being able to play a video at a consistent speed. But there’s another, lower-level reason why time is important. In <a href="ch02.xhtml#ch02">Chapter 2</a>, we discussed how propagation delay affects the time that it takes circuitry to do things. Time gives us a way to wait, for example, for the worst-case delay in an adder before looking at the result so that we know it’s stable and correct.</p>
<p class="indent">Oscillators supply clocks to computers. A computer’s clock is like the drummer in a marching band; it sets the pace for the circuitry. The maximum clock speed or fastest tempo is determined by the propagation delays.</p>
<p class="indent">Component manufacturing involves a lot of statistics because there’s a lot of variance from part to part. The <em>binning</em> process puts components into different bins, or piles, depending on their measured characteristics. The fastest parts that fetch the highest price go into one bin; slower, less expensive parts go into another; and so on. It’s not practical to have an infinite number of bins, so there’s variance within the parts in a bin, although it’s less than the variance for the whole lot of parts. This is one reason why propagation delays are specified as a range; manufacturers provide minimum and maximum values in addition to a typical value. A common logic circuit design error is to use the typical values instead of the minimums and maximums. When you hear about people <em>overclocking</em> their computers, it means they’re gambling that their part was statistically in the middle of its bin and that its clock can be increased by some amount without causing the part to fail.</p>
<h4 class="h4" id="ch03lev2sec3"><strong><em>Latches</em></strong></h4>
<p class="noindent">Now that we have a source of time, let’s try to remember a single bit of information. We can do that with feedback, such as tying the output of an OR gate back to an input, as shown in <a href="ch03.xhtml#ch03fig03">Figure 3-3</a>. This doesn’t create an oscillator such as we saw in <a href="ch03.xhtml#ch03fig01">Figure 3-1</a>, since there’s no inversion. Assume that <em>out</em> <span epub:type="pagebreak" id="page_72"/>starts off at 0 in the circuit in <a href="ch03.xhtml#ch03fig03">Figure 3-3</a>. Now, if <em>in</em> goes to 1, <em>out</em> does too, and because it’s connected to another input it stays that way, even if <em>in</em> goes back to 0. In other words, it remembers.</p>
<div class="image"><a id="ch03fig03"/><img src="../images/03fig03.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-3: An OR gate latch</em></p>
<p class="indent">Of course, this scheme needs some work because there’s no way to make <em>out</em> be 0 again. We need a way to reset it by disconnecting the feedback, as shown in <a href="ch03.xhtml#ch03fig04">Figure 3-4</a>.</p>
<div class="image"><a id="ch03fig04"/><img src="../images/03fig04.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-4: An AND-OR gate latch</em></p>
<p class="indent">Note that we’ve labeled the output of the inverter <span class="bar"><em>reset</em></span>. Putting a line over a symbol is hardware-speak meaning “the opposite.” It means that something is true when it’s a 0 and false when it’s a 1. Sometimes this is referred to as <em>active low</em> instead of <em>active high</em>, meaning that it does its thing when it’s 0 instead of 1. The line is pronounced “bar,” so in speech the signal would be referred to as “reset bar.”</p>
<p class="indent">When <em>reset</em> is low, <span class="bar"><em>reset</em></span> is high, so the output from the OR gate is fed back into the input. When <em>reset</em> goes high, <span class="bar"><em>reset</em></span> goes low, breaking that feedback so that <em>out</em> goes to 0.</p>
<p class="indent"><a href="ch03.xhtml#ch03fig05">Figure 3-5</a> shows an <em>S-R latch</em>, a slightly cleverer way of building a bit of memory. <em>S-R</em> stands for <em>set-reset</em>. It has active low inputs and <em>complementary</em> outputs, meaning one is active low and one is active high. You could build a version of this that has active high inputs by using NOR gates, but NOR gates are often more power-hungry than NAND gates, in addition to being more complicated and expensive to build.</p>
<div class="image"><a id="ch03fig05"/><img src="../images/03fig05.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-5: An S-R latch</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_73"/>The case where both <span class="bar"><em>set</em></span> and <span class="bar"><em>reset</em></span> are active is weird and not intended for use, because both outputs are true. Also, if both inputs become inactive (that is, transition from 0 to 1) at the same time, the state of the outputs is not predictable because it’s dependent on the propagation delays.</p>
<p class="indent">The circuit in <a href="ch03.xhtml#ch03fig05">Figure 3-5</a> has a nice property that the circuit in <a href="ch03.xhtml#ch03fig04">Figure 3-4</a> does not, which is that its design is symmetrical. That means the propagation delays are similar for both the <em>set</em> and <em>reset</em> signals.</p>
<h4 class="h4" id="ch03lev2sec4"><strong><em>Gated Latches</em></strong></h4>
<p class="noindent">Now that we have some way of remembering information, let’s look at what it takes to remember something at a point in time. The circuit in <a href="ch03.xhtml#ch03fig06">Figure 3-6</a> has an extra pair of gates added to the inputs.</p>
<div class="image"><a id="ch03fig06"/><img src="../images/03fig06.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-6: A gated S-R latch</em></p>
<p class="indent">As you can see, when the <span class="bar"><em>gate</em></span> input is inactive (high), it doesn’t matter what <span class="bar"><em>set</em></span> and <span class="bar"><em>reset</em></span> are doing; the outputs won’t change because the inputs to the S and R gates will both be 1.</p>
<p class="indent">Because we want to remember one bit of information, the next improvement we can make is to add an inverter between the <span class="bar"><em>set</em></span> and <span class="bar"><em>reset</em></span> inputs so that we need only a single data input, which we’ll abbreviate as <em>D</em>. This modification is shown in <a href="ch03.xhtml#ch03fig07">Figure 3-7</a>.</p>
<div class="image"><a id="ch03fig07"/><img src="../images/03fig07.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-7: A gated</em> D <em>latch</em></p>
<p class="indent">Now, if <em>D</em> is a 1 when the <span class="bar"><em>gate</em></span> is low, the <em>Q</em> output will be set to 1. Likewise, if <em>D</em> is a 0 when the <span class="bar"><em>gate</em></span> is low, the <em>Q</em> output will be set to 0. Changes on <em>D</em> <span epub:type="pagebreak" id="page_74"/>when <span class="bar"><em>gate</em></span> is high have no effect. That means we can remember the state of <em>D</em>. You can see this in the timing diagram shown in <a href="ch03.xhtml#ch03fig08">Figure 3-8</a>.</p>
<div class="image"><a id="ch03fig08"/><img src="../images/03fig08.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-8: A gated</em> D <em>latch timing diagram</em></p>
<p class="indent">The problem with this circuit is that changes in <em>D</em> pass through whenever the <span class="bar"><em>gate</em></span> is low, as you can see in the shaded section. This means we have to count on <em>D</em> being “well-behaved” and not changing when the “gate” is “open.” It would be better if we could make the opening instantaneous. We’ll see how to do that in the next section.</p>
<h4 class="h4" id="ch03lev2sec5"><strong><em>Flip-Flops</em></strong></h4>
<p class="noindent">As we discussed in the last section, we want to minimize the chances of getting incorrect results due to changing data. The way that’s commonly done is to use the transition between logic levels to grab the data instead of grabbing it when the logic level has a particular value. These transitions are called <em>edges</em>. You can think of an edge as a decision criterion for time. Back in <a href="ch03.xhtml#ch03fig08">Figure 3-8</a>, you can see the almost-instantaneous transition between logic levels. Edge-triggered latches are called <em>flip-flops</em>.</p>
<p class="indent">Latches are a building block used to make flip-flops. We can construct a positive edge-triggered flip-flop called a <em>D flip-flop</em> by cleverly combining three S-R latches, as shown in <a href="ch03.xhtml#ch03fig09">Figure 3-9</a>. <em>Positive edge-triggered</em> means that the flip-flop operates on the transition from a logic 0 to a logic 1; a <em>negative edge-triggered</em> flip-flop would operate on the transition from a logic 1 to a logic 0.</p>
<div class="image"><a id="ch03fig09"/><img src="../images/03fig09.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-9: A</em> D <em>flip-flop design</em></p>
<span epub:type="pagebreak" id="page_75"/>
<p class="noindent">This circuit can be somewhat mind-boggling. The two gates on the right form an S-R latch. We know from <a href="ch03.xhtml#ch03fig05">Figure 3-5</a> that those outputs won’t change unless either <span class="bar"><em>S</em></span> or <span class="bar"><em>R</em></span> goes low.</p>
<p class="indent"><a href="ch03.xhtml#ch03fig10">Figure 3-10</a> shows how the circuit behaves for various values of <em>D</em> and <em>clock</em>. The thin lines show logic 0s; the thick lines are logic 1s.</p>
<div class="image"><a id="ch03fig10"/><img src="../images/03fig10.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-10: A</em> D <em>flip-flop operation</em></p>
<p class="indent">Starting at the left, you can see that when the clock is 0, the value of <em>D</em> doesn’t matter because both <span class="bar"><em>S</em></span> and <span class="bar"><em>R</em></span> are high, so the state of the latch on the right-hand side of <a href="ch03.xhtml#ch03fig09">Figure 3-9</a> is unchanged. Moving toward the right, you can see in the next two diagrams that if <span class="bar"><em>R</em></span> is low, changing the value of <em>D</em> has no effect. Likewise, the two rightmost diagrams show that if <span class="bar"><em>S</em></span> is low, changing the value of <em>D</em> has no effect. The upshot is that changes to <em>D</em> have no effect when the clock is either high or low.</p>
<p class="indent">Now, let’s look at what happens when the clock changes from low to high, as shown in <a href="ch03.xhtml#ch03fig11">Figure 3-11</a>.</p>
<div class="image"><a id="ch03fig11"/><img src="../images/03fig11.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-11: A</em> D <em>flip-flop positive edge operation</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_76"/>You can see on the left that when the clock is low and <em>D</em> is high, <span class="bar"><em>S</em></span> and <span class="bar"><em>R</em></span> are high, so nothing changes. But when the clock changes to 1, <span class="bar"><em>S</em></span> goes low, which changes the state of the flip-flop. On the right, you can see similar behavior when <em>D</em> is low and the clock goes high, causing <span class="bar"><em>R</em></span> to go low and changing the flip-flop state. You saw in <a href="ch03.xhtml#ch03fig10">Figure 3-10</a> that no other changes matter.</p>
<p class="indent">In 1918 British physicists William Eccles and Frank Jordan invented the first electronic version of a flip-flop, which used vacuum tubes. <a href="ch03.xhtml#ch03fig12">Figure 3-12</a> shows the diagram for a slightly less antique <em>D flip-flop</em> called the 7474.</p>
<div class="image"><a id="ch03fig12"/><img src="../images/03fig12.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-12: A D flip-flop</em></p>
<p class="indent">The D flip-flop has complementary <em>Q</em> and <span class="bar"><em>Q</em></span> (outputs and <span class="bar"><em>S</em></span> (set) and <span class="bar"><em>R</em></span> (reset) inputs. It’s a little confusing, as the diagram shows <em>S</em> and <em>R</em>; it’s the combination of those with the <span class="ent">○</span> that make them <span class="bar"><em>S</em></span> and <span class="bar"><em>R</em></span>. So, except for the mysterious things on the left-hand side, it’s just like our S-R latch. The mysterious things are two extra inputs, <em>D</em> for data and <em>CK</em> for clock, which is represented by the triangle. It’s positive edge-triggered, so the value of the <em>D</em> input is stored whenever the signal on the <em>CK</em> goes from a 0 to a 1.</p>
<p class="indent">Edge-triggered devices have other timing considerations in addition to propagation delay. There is the <em>setup time</em>, which is the amount of time before the clock edge that the signal must be stable, and the <em>hold time</em>, which is the amount of time after the clock edge that the signal must be stable. These are shown in <a href="ch03.xhtml#ch03fig13">Figure 3-13</a>.</p>
<div class="image"><a id="ch03fig13"/><img src="../images/03fig13.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-13: Setup and hold times</em></p>
<p class="indent">As you can see, we don’t have to care what’s happening on the <em>D</em> input except during the setup and hold times surrounding the clock edge. And, as with all other logic, the output is stable after the propagation delay time and stays stable independent of the <em>D</em> input. Setup and hold times are typically denoted by t<sub>setup</sub> and t<sub>hold</sub>.</p>
<p class="indent">The edge behavior of flip-flops works well with clocks. We’ll see an example in the next section.</p>
<h4 class="h4" id="ch03lev2sec6"><span epub:type="pagebreak" id="page_77"/><strong><em>Counters</em></strong></h4>
<p class="noindent">Counting is a common application of flip-flops. For example, we could count time from an oscillator and drive a display with a decoder to make a digital clock. <a href="ch03.xhtml#ch03fig14">Figure 3-14</a> shows a circuit that produces a 3-bit number (C<sub>2</sub>, C<sub>1</sub>, C<sub>0</sub>) that is the count of the number of times the <em>signal</em> changes from 0 to 1. The <span class="bar"><em>reset</em></span> signal can be used to set the counter to 0.</p>
<div class="image"><a id="ch03fig14"/><img src="../images/03fig14.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-14: A 3-bit ripple counter</em></p>
<p class="indent">This counter is called a <em>ripple counter</em> because the result ripples from left to right, not because it’s useful for counting bottles of cheap wine. C<sub>0</sub> changes C<sub>1</sub>, C<sub>1</sub> changes C<sub>2</sub>, and so on if there are more bits. Since the <em>D</em> input of each flip-flop is connected to its <span class="bar"><em>Q</em></span> output, it will change state on every positive transition of the <em>CK</em> signal.</p>
<p class="indent">This is also called an <em>asynchronous</em> counter because everything just happens when it gets around to it. The problem with asynchronous systems is that it’s hard to know when to look at the result. The outputs (C<sub>2</sub>, C<sub>1</sub>, C<sub>0</sub>) are invalid during rippling. You can see how it takes longer to get a result for each successive bit in <a href="ch03.xhtml#ch03fig15">Figure 3-15</a>, where the gray areas represent undefined values due to propagation delay.</p>
<div class="image"><a id="ch03fig15"/><img src="../images/03fig15.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-15: Ripple counter timing</em></p>
<p class="indent">The <em>timing diagram</em> on the left shows that we get a valid 3-bit number after the propagation delays settle out. But on the right, you can see that we’re trying to count faster than the propagation delays permit, so there are times where no valid number is produced.</p>
<p class="indent">This is a variation of the problem we saw with the ripple-carry adder back in <a href="ch02.xhtml#ch02fig41">Figure 2-41</a>. Just as we were able to solve that problem with the carry look-ahead design, we can address the ripple problem with a <em>synchronous</em> counter design.</p>
<p class="indent"><span epub:type="pagebreak" id="page_78"/>Unlike the ripple counter, the synchronous counter outputs all change at the same time (in sync). This implies that all the flip-flops are clocked in parallel. A 3-bit synchronous counter is shown in <a href="ch03.xhtml#ch03fig16">Figure 3-16</a>.</p>
<div class="image"><a id="ch03fig16"/><img src="../images/03fig16.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-16: A 3-bit synchronous counter</em></p>
<p class="indent">You can see that all the flip-flops in the counter change state at the same time because they’re all clocked at the same time. Although propagation delay is still a factor in knowing when the outputs are valid, the cascade effect has been eliminated.</p>
<p class="indent">Counters are yet another functional building block, which means they have their own schematic symbol. In this case it’s yet another rectangular box, as you can see in <a href="ch03.xhtml#ch03fig17">Figure 3-17</a>.</p>
<div class="image"><a id="ch03fig17"/><img src="../images/03fig17.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-17: A counter schematic symbol</em></p>
<p class="indent">The figure includes a number of inputs we haven’t seen before. Counters are available that have some or all of these inputs. Most counters have a <em>CLR</em> input that clears the counter, setting it to 0. Also common is an <em>EN</em> input that enables the counter—the counter doesn’t count unless enabled. Some counters can count in either direction; the <em>U/</em><span class="bar"><em>D</em></span> input selects up or down. Finally, some counters have data inputs <em>D</em><sub>0–n</sub> and a load signal <em>LD</em> that allows the counter to be set to a specific value.</p>
<p class="indent">Now that we have counters, we can use them to keep track of time. But that’s not the only thing we can do with flip-flops. We’ll start learning how to remember large amounts of information in the next section.</p>
<h4 class="h4" id="ch03lev2sec7"><strong><em>Registers</em></strong></h4>
<p class="noindent">D flip-flops are good for remembering things. It’s a common enough application that you can get <em>registers</em>, which are a bunch of D flip-flops in a single package that share a common clock. <a href="ch03.xhtml#ch03fig18">Figure 3-18</a> shows an example of a register holding the result of addition using the adder circuit discussed earlier.</p>
<span epub:type="pagebreak" id="page_79"/>
<div class="image"><a id="ch03fig18"/><img src="../images/03fig18.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-18: A register holding an adder result</em></p>
<p class="indent">Once the output of the adder has been clocked into the register, the operands can change without changing the result. Note that registers often have <em>enable</em> inputs similar to those we saw for counters.</p>
<h3 class="h3" id="ch03lev1sec2"><strong>Memory Organization and Addressing</strong></h3>
<p class="noindent">We’ve seen that flip-flops are useful when we need to remember a bit and that registers are handy when we need to remember a collection of bits. What do we do when we need to remember a lot more information, though? For example, what if we want to be able to store several different addition results?</p>
<p class="indent">Well, we can start with a big pile of registers. But now we have a new problem: how to specify the register we want to use. This situation looks like <a href="ch03.xhtml#ch03fig19">Figure 3-19</a>.</p>
<div class="image"><a id="ch03fig19"/><img src="../images/03fig19.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-19: Multiple registers</em></p>
<p class="indent">One way to solve this problem is to assign each register a number, as in the figure. We can have this number or <em>address</em> specify the register using one of our standard building blocks, the decoder from “<a href="ch02.xhtml#ch02lev2sec17">Building Decoders</a>” on <a href="ch02.xhtml#page_63">page 63</a>. The decoder outputs are connected to the enable inputs on the registers.</p>
<p class="indent">Next we need to be able to select the output from the addressed register. Fortunately, we learned how to build selectors in “<a href="ch02.xhtml#ch02lev2sec19">Building Selectors</a>” on <a href="ch02.xhtml#page_65">page 65</a>, and they’re just what we need.</p>
<p class="indent"><span epub:type="pagebreak" id="page_80"/>Systems often have multiple memory components that need to be hooked together. Time for yet another of our standard building blocks: the <em>tri-state</em> output.</p>
<p class="indent">Putting it all together, a memory component looks like <a href="ch03.xhtml#ch03fig20">Figure 3-20</a>.</p>
<div class="image"><a id="ch03fig20"/><img src="../images/03fig20.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-20: A memory component</em></p>
<p class="indent">Memory components have a lot of electrical connections. If we want to do something with 32-bit numbers, we would need 32 connections each for the inputs and the outputs, plus connections for the address, control signals, and power. Programmers don’t have to worry about how to fit circuitry into packages or how to route wires, but hardware designers do. We can cut down on the number of connections by realizing that memory rarely needs to be read and written at the same time. We can get by with one set of data connections plus a <em>read/</em> <span class="bar"><em>write</em></span> control. <a href="ch03.xhtml#ch03fig21">Figure 3-21</a> shows a schematic of a simplified memory chip. The <em>enable</em> control turns the whole thing on and off so that multiple memory chips can be connected together.</p>
<div class="image"><a id="ch03fig21"/><img src="../images/03fig21.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-21: A simplified memory chip</em></p>
<p class="indent">You’ll notice that the figure uses big fat arrows for the address and data instead of showing the individual signals. We call groups of related signals <em>buses</em>, so the memory chip has an <em>address bus</em> and a <em>data bus</em>. Yup, it’s mass transit for bits.</p>
<p class="indent">The next challenge in memory chip packaging comes when the memory size increases and lots of address bits need connections. Referring back to <a href="ch01.xhtml#ch01tab02">Table 1-2</a> in <a href="ch01.xhtml#ch01">Chapter 1</a>, we’d need 32 address connections for a 4-GiB memory component.</p>
<p class="indent">Memory designers and road planners deal with similar traffic-management issues. Many cities are organized into grids, and that’s also how memory chips are laid out internally. You can see several rectangular <span epub:type="pagebreak" id="page_81"/>regions that are chunks of memory in the CPU photomicrograph shown back in <a href="ch02.xhtml#ch02fig03">Figure 2-3</a>. The address is partitioned into two chunks: a <em>row</em> address and a <em>column</em> address. A memory location is addressed internally using the intersection of the row and column, as shown in <a href="ch03.xhtml#ch03fig22">Figure 3-22</a>.</p>
<div class="image"><a id="ch03fig22"/><img src="../images/03fig22.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-22: Row and column addressing</em></p>
<p class="indent">Obviously we don’t need to worry about the number of address lines in the 16-location memory shown in this figure. But what if there were a lot more? We could halve the number of address lines by <em>multiplexing</em> the row and column addresses. All we would need is registers on the memory chip to save them, as shown in <a href="ch03.xhtml#ch03fig23">Figure 3-23</a>.</p>
<div class="image"><a id="ch03fig23"/><img src="../images/03fig23.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-23: Memory with address registers</em></p>
<p class="indent">Since the address comes in two parts, it follows that performance would be better if we only had to change one part, such as by setting the row address and then varying the column address. This is what we find in today’s large memory chips.</p>
<p class="indent">Memory chips are described by their size in depth × width format. For example, a 256 × 8 chip would have 256 8-bit wide memory locations; a 64 Mib × 1 chip would have 64 mebibits.</p>
<h4 class="h4" id="ch03lev2sec8"><span epub:type="pagebreak" id="page_82"/><strong><em>Random-Access Memory</em></strong></h4>
<p class="noindent">The memory we’ve talked about so far is called <em>random-access memory</em>, or <em>RAM.</em> With RAM, the entire width of any memory location can be read or written in any order.</p>
<p class="indent"><em>Static RAM</em>, or <em>SRAM</em>, is expensive but fast. It takes six transistors for each bit. Because transistors take up space, SRAM isn’t a great choice for storing billions or trillions of bits.</p>
<p class="indent"><em>Dynamic memory (DRAM)</em> is a clever hack. Electrons are stored in microscopic buckets called <em>capacitors</em>, using only one transistor for the lids. The problem is, these buckets leak, so it’s necessary to <em>refresh</em> the memory every once in a while, which means regularly topping off the buckets. You have to be careful that the topping off doesn’t occur at a critical time that would conflict with accessing the memory; this was a problem with one of the first DRAM-based computers, the DEC LSI-11. One of the interesting side effects of DRAM is that the buckets leak more when light shines on them. This enables them to be used as digital cameras.</p>
<p class="indent">DRAM is used for large memory chips because of its high density (number of bits per area). Large memory chips mean lots of addresses, which means that DRAM chips use the multiplexed addressing scheme discussed in the previous section. Because of other internal design considerations, it’s only faster to save the row address using the row address strobe and then to vary the column address via the column address strobe. It’s an overused term, but rows are sometimes called <em>pages</em>. It’s comparable to reading a book like this one; it’s much easier to scan a page than it is to flip pages. Or, as stated by the great performance pioneer Jimmy Durante, best performance is a-ras-a-ma-cas. This is a very important consideration in programming: keeping things that are used together in the same row greatly improves performance.</p>
<p class="indent">Both SRAM and DRAM are <em>volatile</em> memory, which means that data can be lost when the power is interrupted. <em>Core</em> memory is an antique <em>nonvolatile</em> type of RAM that stores bits in <em>toroidal</em> (doughnut-shaped) pieces of iron, which you can see in <a href="ch03.xhtml#ch03fig24">Figure 3-24</a>. Toroids were magnetized in one direction for a 0 and the other for a 1. The physics of toroids is cool because they’re very resistant to electromagnetic interference from outside the doughnut. In this type of memory, cores were arranged in a grid called a <em>plane</em> with row and column wires through them. There was also a third wire, called the <em>sense</em> wire, because the only way to read the state of a bit was to try to change it and then sense what happened. Of course, if you sensed that it changed, you had to change it back or the data would be lost, making the bit useless. That required a lot of circuitry in addition to all the stitching. Core was actually three-dimensional memory, as planes were assembled into bricks.</p>
<p class="indent">While core is antique technology, the nonvolatile characteristic is still prized, and research continues making commercially practical <em>magnetoresistive</em> memory that combines the best of core memory and RAM.</p>
<span epub:type="pagebreak" id="page_83"/>
<div class="image"><a id="ch03fig24"/><img src="../images/03fig24.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-24: Core memory</em></p>
<h4 class="h4" id="ch03lev2sec9"><strong><em>Read-Only Memory</em></strong></h4>
<p class="noindent"><em>Read-only memory</em>, or <em>ROM</em>, is not a very accurate name. Memory that could only be read but never written wouldn’t be useful. Even though the name has stuck, it’s more accurate to say that ROM is write-once memory. ROM can be written once and then read multiple times. ROM is important for devices that need to have a program built in, such as a microwave oven; you wouldn’t want to have to program your microwave every time you needed popcorn.</p>
<p class="indent">One of the early forms of ROM was the Hollerith card, which later became known as the <em>IBM card</em>, shown in <a href="ch03.xhtml#ch03fig25">Figure 3-25</a>. Bits were punched into pieces of paper. Really! They were pretty cheap because American inventor Herman Hollerith (1860–1929) was big into cutting corners. Hollerith invented the card in the late 1800s, although it might be more accurate to say that he appropriated the idea from the Jacquard loom, which was invented by Joseph Marie Jacquard in 1801. The Jacquard loom used punched cards to control the weaving pattern. Of course, Jacquard borrowed the idea from Basile Bouchon, who had invented a punched paper tape–controlled loom in 1725. Sometimes it’s hard to distinguish between invention and appropriation, because the future is built on the past. Keep this in mind when you hear people arguing for longer and more restrictive patent and copyright laws; progress slows if we can’t build on the past.</p>
<div class="image"><a id="ch03fig25"/><img src="../images/03fig25.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-25: An IBM card</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_84"/>Early IBM card readers used switches to read the bits. Cards would be slid under a row of springy wires that poked through the holes and made contact with a piece of metal on the other side. Later versions, which worked by shining light through the holes onto a row of <em>photodetectors</em> on the other side, were considerably faster.</p>
<p class="indent"><em>Punched paper tape</em> is a related ROM technology; rolls of paper tape with holes punched in it were used to represent bits (see <a href="ch03.xhtml#ch03fig26">Figure 3-26</a>). Tape had an advantage over cards in that dropping a deck of cards would scramble the data. Then again, tape could tear and was difficult to repair; many a masking tape repair job clogged up the works.</p>
<div class="image"><a id="ch03fig26"/><img src="../images/03fig26.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-26: Punched paper tape</em></p>
<p class="indent">Cards and tape were very slow because they had to be physically moved in order to be read.</p>
<p class="indent">A ROM variation called <em>core rope memory</em> was used in the Apollo flight computer (see <a href="ch03.xhtml#ch03fig27">Figure 3-27</a>). Because it could be written only by sewing, it was impervious to interference—which is important in the harsh environment of space.</p>
<div class="image"><a id="ch03fig27"/><img src="../images/03fig27.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-27: Core rope memory from the Apollo guidance computer</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_85"/>IBM cards and paper tape were <em>sequential</em> memory; that is, the data was read in order. Card readers couldn’t go backward, so they were really only good for long-term storage of data. The contents had to be read into some sort of RAM in order to be used. The first commercial availability of a single-chip microprocessor, the Intel 4004 in 1971, created demand for better program storage technology. These first microprocessors were used for devices like calculators that ran a fixed program. Along came <em>mask-programmable</em> ROM. A <em>mask</em> is a stencil used as part of the integrated circuit–manufacturing process. You’d write a program and send the bit pattern off to a semiconductor manufacturer along with a really big check. They’d turn it into a mask, and you’d get back a chip containing your program. It was read-only because there was no way to change it without writing another big check and having a different mask made. Mask-programmable ROM could be read in a random-access manner.</p>
<p class="indent">Masks were so expensive that they could be justified only for high-volume applications. Along came <em>programmable read-only memory (PROM)</em>, ROM chips that you could program yourself, but only once. The original mechanism for PROM involved melting nichrome (a nickel-chromium alloy) fuses on the chip. Nichrome is the same stuff that makes the glowing wires in your toaster.</p>
<p class="indent">People would go through a big pile of PROM chips quickly when developing a program. Engineers are pain-adverse, so next came <em>erasable programmable read-only memory (EPROM)</em>. These chips were like PROMs, except that they had a quartz window on top and you could erase them by putting them under a special ultraviolet light.</p>
<p class="indent">Life got better with the introduction of <em>electrically erasable programmable read-only memory</em> (what a mouthful!), or <em>EEPROM</em>. This is an EPROM chip that can be erased electrically—no light, no quartz window. Erasing EEPROM is comparatively very slow, though, so it’s not something you want to do a lot. EEPROMs are technically RAM, since it’s possible to read and write the contents in any order. But because they’re slow to write and more expensive than RAM, they’re used as a substitute for ROMs.</p>
<h3 class="h3" id="ch03lev1sec3"><strong>Block Devices</strong></h3>
<p class="noindent">It takes time to talk to memory. Imagine you had to go to the store every time you needed a cup of flour. It’s much more practical to go to the store once and bring home a whole sack of flour. Larger memory devices use this principle. Think warehouse shopping for bits.</p>
<p class="indent"><em>Disk drives</em>, also known as <em>mass storage</em>, are great for storing immense amounts of data. An 8-TB drive cost less than $200 when this book was written. They’re often referred to as <em>mass storage</em>. Some religious institutions use mass storage for their ceremonies in between use. Disk drives store bits on rotating magnetic platters, sort of like a lazy Susan. Bits periodically come around to where you’re sitting, and you use your hand to pluck them off or put them on. In a disk drive, your hand is replaced by the <em>disk head</em>.</p>
<p class="indent"><span epub:type="pagebreak" id="page_86"/>Disk drives are relatively slow compared to other types of memory. If you want something that just passed by the head, you have to wait almost an entire rotation for it to come around again. Modern disks spin at 7,200 rotations per minute (RPM), which means a rotation takes slightly longer than 8 milliseconds. The big problem with disk drives is that they’re mechanical and wear out. Bearing wear is one of the big causes of disk failure. The difference between commercial and consumer-grade devices is primarily the amount of grease in the bearing—manufacturers are able to charge hundreds of dollars for something that costs less than a penny. Disk drives store data by magnetizing areas on the disk, which makes them nonvolatile just like core memory.</p>
<p class="indent">Disk drives are a trade-off between speed and density. They’re slow because of the time it takes for the bits you want to show up under the head, but because the data is being brought to the head, no space is required for address and data connections, unlike, for example, in a DRAM. <a href="ch03.xhtml#ch03fig28">Figure 3-28</a> shows the insides of a disk drive. They’re built in sealed containers because dust and dirt would cause them to fail.</p>
<div class="image"><a id="ch03fig28"/><img src="../images/03fig28.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-28: A disk drive</em></p>
<p class="indent">Disks are block-addressable rather than byte-addressable. A <em>block</em> (historically called a <em>sector</em>) is the smallest unit that can be accessed. Disks have historically had 512-byte sectors, although newer devices have 4,096-byte sectors. That means in order to change a byte on a disk, you have to read an entire block, change the byte, and then write back the entire block. Disks contain one or more <em>platters</em> that are laid out as shown in <a href="ch03.xhtml#ch03fig29">Figure 3-29</a>.</p>
<span epub:type="pagebreak" id="page_87"/>
<div class="image"><a id="ch03fig29"/><img src="../images/03fig29.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-29: Disk layout</em></p>
<p class="indent">Since all of the sectors contain the same number of bits, the <em>bit density</em> (bits/mm<sup>2</sup>) is greater at the center of each platter than it is at the outer edge. This is wasteful because there’s clearly room to cram more bits onto the outer tracks. Newer disks address this problem by dividing the disk into a set of <em>radial zones</em>, effectively having more sectors in the outer zones than in the inner ones.</p>
<p class="indent">There are a couple of numbers that describe the performance of disk drives. Modern disks have a head on an actuator arm that moves radially across the disk; the position of the head divides the disks into tracks. The <em>seek time</em> is the amount of time that it takes to move the head from one track to another. It would, of course, be much faster to have one head per track so that seeking wasn’t necessary; you could get that on very old disk drives, but the tracks are too close together on modern disks to make that practical. In addition to the seek time, there’s the time it takes for the part of the disk you’re interested in to rotate so that it’s under the head, called <em>rotational latency</em>, which as we saw above is in the millisecond range.</p>
<p class="indent">Disk drives are often called <em>hard drives</em>. Originally, all disk drives were hard drives. The distinction arose when cheap removable storage devices called <em>floppy disks</em> appeared on the scene. Floppy disks were bendable, so calling the other type “hard” made them easy to differentiate.</p>
<p class="indent">An antiquated variation on disk drives is <em>magnetic drum</em> storage, which was just what it sounds like: a rotating magnetic drum with stripes of heads on it.</p>
<p class="indent"><em>Magnetic tape</em> is another nonvolatile storage technology that uses reels of magnetized tape. It is way slower than a disk drive, and it can take a long time to wind the tape to the requested position. Early Apple computers used consumer-grade audio cassettes for magnetic tape storage.</p>
<p class="indent"><em>Optical disks</em> are similar to magnetic disks except that they use light instead of magnetism. You know these as CDs and DVDs. A big advantage of optical disks is that they can be mass-produced via printing. Preprinted <span epub:type="pagebreak" id="page_88"/>disks are ROMs. PROM-equivalent versions that can be written once (CD-R, DVD-R) are also available, as are versions that can be erased and rewritten (CD-RW). <a href="ch03.xhtml#ch03fig30">Figure 3-30</a> shows a close-up of a portion of an optical disk.</p>
<div class="image"><a id="ch03fig30"/><img src="../images/03fig30.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-30: Optical disk data</em></p>
<h3 class="h3" id="ch03lev1sec4"><strong>Flash Memory and Solid State Disks</strong></h3>
<p class="noindent"><em>Flash memory</em> is the most recent incarnation of EEPROM. It’s good solution for some applications, like music players and digital cameras. It works by storing electrons in buckets just like DRAM. In this case, the buckets are bigger and better built so they don’t leak. But the lid hinges on the buckets eventually wear out if they’re opened and closed too many times. Flash memory can be erased more quickly than EEPROM and is cheaper to make. It works like RAM for reading and also for writing a blank device filled with 0s. But although 0s can be turned into 1s, they can’t be turned back without being erased first. Flash memory is internally divided into blocks, and only blocks can be erased, not individual locations. Flash memory devices are random-access for reads, and block-access for writes.</p>
<p class="indent">Disk drives are slowly being replaced by <em>solid-state disk drives</em>, which are pretty much just flash memory packaged up to look like a disk drive. Right now their price per bit is much higher than spinning disks, but that’s expected to change. Because flash memory wears out, solid-state drives include a processor that keeps track of the usages in different blocks and tries to even it out so that all blocks wear out at the same rate.</p>
<h3 class="h3" id="ch03lev1sec5"><strong>Error Detection and Correction</strong></h3>
<p class="noindent">You never know when a stray cosmic ray is going to hit a piece of memory and corrupt the data. It would be nice to know when this happens and even nicer to be able to repair the damage. Of course, such improvements cost money and are not typically found in consumer-grade devices.</p>
<p class="indent">We’d like to be able to detect errors without having to store a complete second copy of the data. And that wouldn’t work anyway, because we wouldn’t know which copy was correct. We could store two extra copies and assume that the matching pair (if any) is the right one. Computers designed for very harsh environments do this. They also use a more expensive circuit <span epub:type="pagebreak" id="page_89"/>design that doesn’t burn up when hit by a proton. For example, the space shuttle had redundant computers and a voting system in the event that an error was detected.</p>
<p class="indent">We can test for a 1-bit error using a method called <em>parity</em>. The idea is to add up the number of bits that are set to 1 and use an extra bit to store whether that sum is an odd or even number. We can do this by taking the XOR of the bits. There are two forms of this: in <em>even parity</em> the sum of the bits is used, and in <em>odd parity</em> the complement of the sum of the bits is used. This choice may seem, well, odd, but the nomenclature comes from the number of 1s or 0s including the parity bit.</p>
<p class="indent">The left half of <a href="ch03.xhtml#ch03fig31">Figure 3-31</a> shows the calculation of even parity; there are four 1s, so the parity is 0. The right half shows the checking of the parity; a 0 out means that the data is good, or at least as good as we can tell with parity. The big problem with parity is that it’s one place where two wrongs sure look like a right; it only catches odd numbers of errors.</p>
<div class="image"><a id="ch03fig31"/><img src="../images/03fig31.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 3-31: Even parity generation and checking</em></p>
<p class="indent">There are more complicated methods, such as Hamming codes, invented by American mathematician Richard Hamming (1915–1998), which take more bits and allow for more errors to be detected and for some to be corrected. <em>Error checking and correcting (ECC)</em> memory chips are available that include this circuitry. They’re typically used in big data centers, not in consumer devices.</p>
<p class="indent">Methods like parity are good for data that is constantly changing. There are less expensive methods that allow for verification of static block data, such as a computer program. The simplest of these is the <em>checksum</em>, where the contents of every data location are summed into some <em>n</em>-bit value and the overflow bits are thrown away. The checksum can be compared against the program, usually just before it is run. The larger the checksum value (that is, larger <em>n</em>), the lower the chance of getting a false positive.</p>
<p class="indent"><em>Cyclic redundancy checks</em>, or <em>CRCs</em>, are a mathematically better replacement for checksums. Hash codes are another. The goal is to calculate a verification number that is unique enough for the data so that for most changes, the check will no longer be correct.</p>
<h3 class="h3" id="ch03lev1sec6"><span epub:type="pagebreak" id="page_90"/><strong>Hardware vs. Software</strong></h3>
<p class="noindent">The techniques used to make PROMs, EEPROMs, and flash aren’t just limited to memory. We’ll soon see how computer hardware is constructed from logic circuits. And since you’re learning programming, you know that programs include logic in their code, and you may know that computers expose logic to programs via their instruction sets. What’s the difference between doing that in hardware versus software? It’s a blurry line. To a large degree, there is little distinction except that it’s much easier to build software since there are no additional costs other than design time.</p>
<p class="indent">You’ve probably heard the term <em>firmware</em>, which originally just referred to software in a ROM. But most firmware now lives in flash memory or even RAM, so the difference is minimal. And it’s even more complicated than that. It used to be that chips were designed by geeks who laid out circuits by sticking colored masking tape on big sheets of clear Mylar. In 1979 American scientists and engineers Carver Mead and Lynn Conway changed the world with their publication of <em>Introduction to VLSI Systems</em>, which helped kick-start the electronic design automation (EDA) industry. Chip design became software. Chips today are designed using specialized programming languages such as Verilog, VHDL, and SystemC.</p>
<p class="indent">Much of the time, a computer programmer is simply given a piece of hardware to use. But you might get the opportunity to participate in the design of a system that includes both hardware and software. The design of the interface between hardware and software is critical. There are countless examples of chips with unusable, unprogrammable, and unnecessary features.</p>
<p class="indent">Integrated circuits are expensive to make. In the early days, all chips were <em>full custom</em> designs. Chips are built up in layers, with the actual components on the bottom and metal layers on top to wire them together. <em>Gate arrays</em> were an attempt to lower the cost for some applications; a set of predesigned components was available, and only the metal layers were custom. Just like with memory, these were supplanted by PROM-equivalent versions that you could program yourself. And there was an EPROM equivalent that could be erased and reprogrammed.</p>
<p class="indent">Modern <em>field-programmable gate arrays (FPGAs)</em> are the flash memory equivalent; they can be reprogrammed in software. In many cases, using an FPGA is cheaper than using other components. FPGAs are very rich in features; for example, you can get a large FPGA that contains a couple of ARM processor cores. Intel recently purchased Altera and may include FPGAs on its processor chips. There’s a good chance you’ll work on a project containing one of these devices, so be prepared to turn your software into hardware.</p>
<h3 class="h3" id="ch03lev1sec7"><span epub:type="pagebreak" id="page_91"/><strong>Summary</strong></h3>
<p class="noindent">In this chapter, you’ve learned where computers get their sense of time. You were introduced to sequential logic, which, along with combinatorial logic from <a href="ch02.xhtml#ch02">Chapter 2</a>, provides us with all of the fundamental hardware building blocks. And you’ve learned something about how memory is built. We’ll put all of this knowledge together to make a computer in <a href="ch04.xhtml#ch04">Chapter 4</a>.<span epub:type="pagebreak" id="page_92"/></p>
</body></html>