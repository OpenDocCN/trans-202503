- en: '**1'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: THE NATURE OF RANDOMNESS**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Random processes power the systems we’ll develop later in this book. This chapter
    introduces specific random processes, from those that are truly random to those
    that are deterministic but still random enough to use—that is, pseudorandom and
    quasirandom processes.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll begin with a brief discussion of the relationship between probability
    and randomness. After learning how to determine whether a process is random, we’ll
    explore truly random processes, meaning processes strongly influenced by true
    randomness. We’ll also learn the difference between pseudorandom and quasirandom
    processes. Finally, we’ll use Python to create the `RE` class, the randomness
    engine we’ll use in all of our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: '**Probability and Randomness**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Probability distributions* represent the possible values a *random variable*
    can take and how likely each value is to occur. For us, a random variable is the
    output of a random process.'
  prefs: []
  type: TYPE_NORMAL
- en: Probability distributions come in two varieties. *Continuous* probability distributions
    return values from an infinite set, meaning any real number in the allowed range.
    Here, the word *real* means elements of the set of real numbers, denoted ℝ, which
    are all the numbers on the number line. *Discrete* probability distributions are
    restricted to returning values from a finite set of values, like the heads or
    tails of a coin or the numbers on a die.
  prefs: []
  type: TYPE_NORMAL
- en: Random processes generate values, known as *samples*, that come from some kind
    of probability distribution, be it continuous or discrete. For example, a coin
    flip delivers samples that are either heads or tails, while a die delivers samples
    from the set {1, 2, 3, 4, 5, 6} (assuming a standard six-sided die).
  prefs: []
  type: TYPE_NORMAL
- en: If a random process returns a single number, how do we know what distribution
    it’s sampling from? In some cases, we have theoretical knowledge, but in other
    cases, all we can do is generate many samples. Over time, the relative frequency
    with which each possible outcome appears will become evident and serve as a stand-in
    for the true probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '***Discrete Distributions***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As an example of a discrete probability distribution, suppose that, thanks to
    the generosity of a local wizard, I have in my possession a one-of-a-kind die
    with only three sides. The numbers 0, 1, or 2 appear on the faces of my three-sided
    die. Therefore, each roll of the die results in one of the three sides face up.
    When I roll my die 50,000 times, keeping a tally of the number of times each side
    appears face up, I get the results in [Table 1-1](ch01.xhtml#ch01tab01).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1-1:** Rolling a Three-Sided Die 50,000 Times'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Face** | **Count** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 33,492 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 8,242 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 8,266 |'
  prefs: []
  type: TYPE_TB
- en: 'The results indicate that the probability of getting a 0, 1, or 2 is not equal:
    0 appeared 33,492/50,000 = 66.984 percent of the time, 1 appeared 16.484 percent
    of the time, and 2 appeared 16.532 percent of the time. If I were to repeat the
    experiment, the exact count for each outcome would be slightly different, but
    it’s clear that many such experiments would lead to outcomes in which 0 appears
    approximately 67 percent of the time and 1 and 2 each appears 16.5 percent of
    the time. Notice that 67 percent + 16.5 percent + 16.5 percent = 100 percent,
    as it should if the only possible outcomes from rolling my magic die are 0, 1,
    and 2 in the proportion 67 : 16.5 : 16.5.'
  prefs: []
  type: TYPE_NORMAL
- en: Rolling the three-sided die is a random process that samples from a probability
    distribution, one where the likelihood of a 0 is 67 percent and the likelihood
    of a 1 or a 2 is 16.5 percent each. There are a finite number of possible outputs,
    so the probability distribution is discrete.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try two more experiments that sample from discrete probability distributions.
    The first flips a fair coin 50,000 times. The second rolls a standard six-sided
    die 50,000 times. As before, let’s tally the different outcomes, using 0 for tails
    and 1 for heads. [Table 1-2](ch01.xhtml#ch01tab02) shows the results for the coin
    flips.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1-2:** Flipping a Fair Coin 50,000 Times'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Side** | **Count** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 25,040 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 24,960 |'
  prefs: []
  type: TYPE_TB
- en: '[Table 1-3](ch01.xhtml#ch01tab03) shows the results for the die rolls.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1-3:** Rolling a Standard Die 50,000 Times'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Face** | **Count** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 8,438 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 8,252 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 8,292 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 8,367 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 8,336 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 8,315 |'
  prefs: []
  type: TYPE_TB
- en: Just as my magic three-sided die’s outcomes are not equally likely, the coin
    flips and standard die rolls deliver each outcome with equal probability. The
    counts are nearly uniform. Such *uniform distributions*, those that produce each
    possible outcome with equal probability, are far and away the most common type
    of distribution we’ll harness in this book.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*To run this experiment, I didn’t actually flip a coin and roll a die 50,000
    times. Instead, I used a pseudorandom generator, discussed later in this chapter.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Continuous Distributions***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Think of continuous probability distributions as the limit of discrete distributions.
    For example, a six-sided die selects from six possible outcomes. A 20-sided die,
    sometimes called a D20, selects from 20 possible outcomes. If we could somehow
    let the number of sides extend to infinity, then such a die would choose, on each
    roll, from an infinite number of outcomes. This is what a continuous probability
    distribution does.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose a random process generates real numbers in the range 0 to 1 such that
    all real numbers are equally likely. In that case, the random process is sampling
    from a continuous uniform distribution, just as a die samples from a discrete
    uniform distribution. We’ll make heavy use of the continuous uniform distribution
    as we proceed through the book. Likewise, we’ll occasionally use the *normal*
    (sometimes called *Gaussian*) distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing some math notation at this point will make it easier to understand
    what comes later. The uniform distribution we’ll use most often draws samples
    from [0, 1), meaning the sample is greater than or equal to 0 and strictly less
    than 1, that is, 0 ≤ *x* < 1\. When writing a range like [0, 1), note whether
    a square bracket or a parenthesis is used. The square bracket means the limit
    is included in the range, but a parenthesis excludes the limit. Therefore, (0,
    1) specifies a range where all possible real numbers except 0 and 1 are allowed.
    Similarly, [0, 1] includes both 0 and 1, while [0, 1) includes 0 but excludes
    1\. The pseudorandom and quasirandom processes described later in the chapter
    usually produce outputs in the range [0, 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'A uniform distribution is straightforward, whether continuous or discrete:
    each possible outcome is equally likely to appear. However, in a normal distribution,
    some values are more likely to be produced than others.'
  prefs: []
  type: TYPE_NORMAL
- en: The best way to appreciate a normal distribution is to examine its histogram.
    For example, [Figure 1-1](ch01.xhtml#ch01fig01) shows the distribution of 60 million
    samples from a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/01fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-1: A histogram showing normal (curve) and uniform (line) distributions*'
  prefs: []
  type: TYPE_NORMAL
- en: The values in [Figure 1-1](ch01.xhtml#ch01fig01) range from approximately −6
    to 6\. Values around 0 are most likely, with those at the extremes least likely.
    The normal distribution is widespread; many physical phenomena follow this distribution.
    Crucially, when we take a large set of samples from *any* distribution, the mean
    values will follow a normal distribution. This is the *central limit theorem*,
    and it’s fundamental to statistics.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, [Table 1-2](ch01.xhtml#ch01tab02) and [Table 1-3](ch01.xhtml#ch01tab03)
    tally the number of heads and tails and the number of times each side of a die
    appeared. A histogram is a graphical representation of such a table. The possible
    outputs are placed into bins of some specified width. For a die, the natural bin
    width is 1 so that each side falls into the bin with the same label.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a continuous distribution, the bins cover a range. For example, if we have
    a process that generates numbers in the range [0, 1), and we want 10 bins, then
    we might make each bin 0.1 wide. A sample of *x* = 0.3052 will then fall into
    bin 3, counting from 0, because:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0005-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Likewise, a sample of *x* = 0.0451 will fall into bin 0, and so on. When all
    samples have been placed in a bin, the histogram plots either the count in each
    bin or the fraction of samples that fell into that bin. The fraction is found
    by dividing each bin’s count by the sum of all the bins. A histogram using fractions
    per bin approximates the true probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s return to [Figure 1-1](ch01.xhtml#ch01fig01). The figure uses 1,000 bins,
    which explains why the curve looks more like a curve than a bar plot. The figure
    plots the fraction in each bin, not the count, which lets us compare different
    distributions without worrying about the number of samples used to generate the
    histogram. As the number of samples increases, such a histogram becomes a better
    approximation of the true probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal line in [Figure 1-1](ch01.xhtml#ch01fig01) represents a continuous
    uniform distribution selecting values in the range [–6, 6). If each value is equally
    likely, then on average, each bin will be equally populated and 1/1,000 = 0.001,
    thereby explaining the *y*-axis value for the uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: We need only remember that the random processes powering our experiments generate
    values according to some distribution, primarily the uniform or normal distributions.
    There are many other standard distributions, but we won’t explore them in this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*To learn more about probability and statistics, I recommend* Statistics Done
    Wrong *by Alex Reinhart (2015) or my book* Math for Deep Learning *(2021), both
    available from No Starch Press. You’ll find discussions of probability and statistics,
    along with differential calculus, all of which we’ll touch on at various points
    throughout this book.*'
  prefs: []
  type: TYPE_NORMAL
- en: We need to know how close our random processes come to being, well, random.
    Before we dive into randomness engines proper, let’s consider how to approach
    testing the output of a random process.
  prefs: []
  type: TYPE_NORMAL
- en: '**Testing for Randomness**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'How can we know that the output of a random process is truly random? The short
    answer is: we can’t. However, that shouldn’t deter us. We’re not attempting to
    solve deep philosophical issues, as interesting as they might be. Instead, we
    seek what’s good enough to accomplish our immediate goals and no more.'
  prefs: []
  type: TYPE_NORMAL
- en: Is this string of binary digits random?
  prefs: []
  type: TYPE_NORMAL
- en: '0101010011100000110000011101101111111011100000'
  prefs: []
  type: TYPE_NORMAL
- en: Well, it looks kind of random, but how can we tell? Earlier, we used the frequency
    of each possible output to tell us whether the sample matches expectations. A
    random process generating 0 or 1 with expected equal likelihood should match as
    well. In this case, there are 23 zeros and 23 ones. Does that indicate that the
    sequence is random?
  prefs: []
  type: TYPE_NORMAL
- en: 'You begin to see what I mean when I say we can’t tell whether a process is
    truly random. All we can do is apply tests to increase our confidence in our belief
    that the sequence israndom. We can check the expected frequency of the various
    outputs, but that’s not sufficient. For example, the following sequences also
    have equal numbers of zeros and ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0006-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Most of us wouldn’t consider either sequence to be particularly random.
  prefs: []
  type: TYPE_NORMAL
- en: In truth, none of the previous three sequences are the output of a random process.
    The first one is the binary representation of the operation codes for a 6502 microprocessor
    program to display the letter A on the screen of an old Apple II computer. The
    bit patterns for the opcodes are not random, but depend critically on the internal
    architecture of the microprocessor. I made the other two sequences by hand to
    have equal numbers of zeros and ones.
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, researchers have invented many statistical tests designed, collectively,
    to detect whether a sequence of values is worthy of being called random. We have
    no space here to dive into these tests, but they go far beyond frequencies and
    consider short- and long-term correlations of all kinds. A few such test suites
    are DieHarder, TestU01, and PractRand. These suites generally require vast collections
    of values, far more than we can work with here.
  prefs: []
  type: TYPE_NORMAL
- en: So what’s a person to do? We cannot prove that a randomness engine is generating
    random outputs, but we can gain enough confidence to believe more or less strongly.
    To accomplish this, we’ll use a command line program called `ent`, from the word
    *entropy*. It applies a small set of statistical tests that might influence our
    beliefs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many Linux distributions include `ent`, but if yours doesn’t, install it using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the following website for a compiled Windows version (along with a link
    to its GitHub repository, should you wish to examine `ent`’s source code): *[https://www.fourmilab.ch/random](https://www.fourmilab.ch/random)*.
    To install `ent` on macOS in the previous command, replace `sudo apt-get` with
    `brew`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `ent` program requires a file of bytes, which it assumes are uniformly distributed
    in the range [0, 255]. This means that the random process under test must have
    its output converted into a set of uniformly distributed bytes. We’ll learn how
    to do this later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, the book’s GitHub page includes a file we can use to understand `ent`’s
    output, *ent_test.bin*. Pass it to `ent` like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The file *ent_test.bin* contains bytes generated by a good, but rarely used,
    pseudorandom number generator called MWC (multiply-with-carry). We might expect
    `ent` to report that the file is random. However, `ent` doesn’t make that determination
    for us. Instead, `ent` runs a set of six statistical tests and reports the results,
    leaving it up to us to decide whether those results warrant a belief toward randomness.
  prefs: []
  type: TYPE_NORMAL
- en: The first test measures the entropy of the bytes. *Entropy* is a measure of
    disorder in a system. To a physicist, entropy is related to the number of microstates
    of a system—for example, the position and momentum of the molecules in a gas leading
    to the same macroscopic values of large-scale properties like temperature and
    pressure and the ways in which those molecules can be arranged. The entropy reported
    by `ent`, however, is deeper than that. It is the *Shannon entropy*, a measure
    of information content. In this case, it’s expressed in bits. There are 8 bits
    in a byte, so a maximally random sequence would have an entropy of 8.0, meaning
    information content is maximized. Our test file has an entropy of 7.999996 bits
    per byte, which is extremely close to 8, a good sign.
  prefs: []
  type: TYPE_NORMAL
- en: We use the entropy reported by `ent` to estimate how much it’s possible to compress
    the file. It’s an alternative presentation of the entropy. Compression algorithms
    work by taking advantage of the information contained in a file, as measured by
    its entropy. The lower the entropy, the more redundant the data, and the lower
    the information content. If the information content is low, there is another way
    to express the information that takes less space. However, if the file is random
    and entropy is maximized, there is no alternative way to express the file’s contents,
    so it cannot be compressed.
  prefs: []
  type: TYPE_NORMAL
- en: Next, `ent` applies a *χ*² test. The important part here is the percentage reported.
    If this percentage is below 5 percent or above 95 percent, then the expected frequencies—that
    is, the number of times each byte value appears—is suspect. Here, we have 72 percent,
    so we’re again on solid ground.
  prefs: []
  type: TYPE_NORMAL
- en: If the sequence of bytes is random, we might, correctly, expect the average
    value of the bytes to be 255/2 = 127.5\. Here, we have an average value of 127.5064,
    which is quite close.
  prefs: []
  type: TYPE_NORMAL
- en: It’s possible to estimate *π* with random numbers; `ent` uses this as another
    test of randomness. In this case, `ent` arrives at an estimate 0.01 percent off
    from the number of digits shown. If there is something in the sequence of bytes
    that biases the simulation, then it should manifest itself in the calculated *π*
    value. We’ll use random numbers to estimate *π* in [Chapter 3](ch03.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: The final output line applies a statistical test to measure how related byte
    *n* is to byte *n* + 1; that is, it pays attention to the order of the bytes.
    If the bytes are not serially correlated, at least to the level of one to the
    next, the resulting coefficient will be zero. Here, it’s ever so slightly negative
    but quite close to zero.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, then, `ent`’s report gives us strong confidence that the contents
    of the file *ent_test.bin* are worthy of being called random. Would you secure
    your bank account with this level of confidence? I sincerely hope not, but we’re
    not interested in cryptography; we’re interested in random processes, real or
    synthetic, that are sufficiently random to power our experiments. For that, `ent`
    is the only tool we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, `ent`’s output is needlessly verbose, especially since we’ll use it
    often in this chapter. Let’s define an abbreviated version. Instead of the output
    shown earlier, I’ll report `ent` results like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s put our nifty randomness detector to work. We’ll begin with truly random
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Truly Random Processes**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we’ll review several sources generally accepted as true random
    processes: experimenting with coins, dice, electrical noise in different forms,
    and the decay of radioactive elements. We’ll use the datasets we create here for
    experiments later in the book. The random processes covered in this section also
    provide a comparison against the next section’s pseudorandom processes, which
    merely give the appearance of randomness.'
  prefs: []
  type: TYPE_NORMAL
- en: Humanity has, over the centuries, developed multiple ways of generating randomness,
    including coin flips and dice rolls. Let’s consider these to see if we might trust
    them as randomness engines.
  prefs: []
  type: TYPE_NORMAL
- en: '***Flipping Coins***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most people consider a coin flip to be a reasonable source of randomness, but
    is that really the case? In 2009, two undergraduate students at the University
    of California, Berkeley, flipped a coin a total of 40,000 times while keeping
    track of the coin’s starting orientation, either heads up or tails up. [Table
    1-4](ch01.xhtml#ch01tab04) shows what they found (data used with permission).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1-4:** Flipping 40,000 Coins by Hand'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Orientation** | **Heads** | **Tails** | **p-value** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Heads up | 10,231 | 9,770 | 0.0011 |'
  prefs: []
  type: TYPE_TB
- en: '| Tails up | 9,985 | 10,016 | 0.8265 |'
  prefs: []
  type: TYPE_TB
- en: A glance at [Table 1-4](ch01.xhtml#ch01tab04) shows that when the coin was facing
    heads up, there were more heads at the end of the flip. The same is true for tails
    up; more tails were measured. We can use the *χ*² test to see if these proportions
    are in concert with the expected 50-50 split of a fair coin. The resulting p-values
    are in the rightmost column.
  prefs: []
  type: TYPE_NORMAL
- en: A p-value is the probability of measuring the observed number of heads and tails,
    given that the null hypothesis is true. In statistical testing, the *null hypothesis*
    is the hypothesis being tested. In this case, for the *χ*² test, the null hypothesis
    is that the observed number of heads and tails is consistent with equal likelihoods.
    The p-value provides evidence for or against this hypothesis. If the p-value is
    below the standard, somewhat arbitrary rule-of-thumb threshold of 0.05 (5 percent),
    then we say that the p-value is *statistically significant*, and we claim evidence
    against the null hypotheses.
  prefs: []
  type: TYPE_NORMAL
- en: The smaller the p-value, the stronger our evidence becomes. For a p-value threshold
    of 0.05, we might expect to falsely reject the null hypothesis about 1 time in
    20, while for a p-value of 0.01, the false rejection rate becomes 1 in 100, and
    so on as the p-values get smaller and smaller. However, only death and taxes are
    certain. A small p-value is not *proof* of anything; it’s only an indicator, a
    reason to believe or not to believe, though with perhaps strong evidence.
  prefs: []
  type: TYPE_NORMAL
- en: Look again at the Heads up row in [Table 1-4](ch01.xhtml#ch01tab04). The p-value
    is 0.0011, or 0.11 percent. According to the *χ*² test, there is a 0.11 percent
    probability of the observed counts (or a more extreme difference), given the null
    hypothesis is true. Therefore, we have evidence in favor of rejecting the null
    hypothesis. In other words, we have evidence for believing that Subject 1, who
    did the heads-up portion of the experiment, was not random, but was instead biased
    toward heads.
  prefs: []
  type: TYPE_NORMAL
- en: Subject 2, however, produced results consistent with the null hypothesis. For
    her, the p-value was 0.8265, or 83 percent. Again, the p-value, in this case,
    means the *χ*² test reports a probability of 83 percent for observing the counts,
    given the null hypothesis is true. This makes perfect sense, so we have evidence
    supporting the null hypothesis for the tails-up case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *χ*² test compared the tallies with the expected 50-50 split. We can do
    one more test: a *t-test*. The t-test compares two datasets and returns a p-value
    we interpret as the likelihood the datasets were generated by the same process.
    In this case, the t-test between the heads-up and tails-up data-sets returned
    a p-value of 0.0139, or 1.39 percent, again below the standard 0.05 threshold.
    This serves as evidence that the two datasets are likely drawn from different
    processes.'
  prefs: []
  type: TYPE_NORMAL
- en: What does that mean in this case? We have a single set of flips from two subjects,
    and each subject only flipped coins with the same side up each time. It’s conceivable,
    but not proven, that Subject 1 was highly consistent in her flips and, as such,
    biased the coin tosses so that heads were favored when heads were the starting
    condition. For us, this fun example serves as an indication that humans are not
    to be trusted to act randomly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have evidence that Subject 1 biased the coin flips. Are we stuck with the
    bias? Actually, no. American mathematician and computer scientist John von Neumann
    came up with a clever algorithm to make a biased coin fair. The algorithm is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: Flip the biased coin twice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If both flips came up the same—that is, both heads or both tails—start again
    from step 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Otherwise, keep the result of the first flip and disregard the second.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applying this algorithm to the sequence of heads and tails generated by Subject
    1 leaves us with 2,475 heads and 2,538 tails. The *χ*² test delivers a p-value
    of 0.37, which is well above 0.05 and strong evidence that the resulting dataset
    is now acting as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why does von Neumann’s algorithm work? Consider a biased coin where the probability
    of getting a heads is not 0.5 but 0.8, implying that the probability of a tails
    is 0.2, since probabilities add to 1\. In that case, flipping the coin twice will
    lead to the four possible combinations of heads and tails with the following probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0010-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Recall that if events are independent, as even the flips of a biased coin are,
    then the probabilities multiply. Also, the probability of getting heads followed
    by tails equals that of getting tails followed by heads. Therefore, consistently
    selecting the first (or the second) in either of these cases must lead to selecting
    heads or tails with equal likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: The file *40000cointosses.csv* contains the dataset used in this experiment
    with the associated code in *40000cointosses.py*.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Please review the original web page for other comments on how the experiment
    was conducted, including suitable warnings about taking the results as anything
    more than a hint that further experimentation might uncover something interesting:*
    [https://www.stat.berkeley.edu/∼ldous/Real-World/coin_tosses.html](https://www.stat.berkeley.edu/∼ldous/Real-World/coin_tosses.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '***Rolling Dice***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Flipping a fair coin is a random process, as is rolling a fair die. But is there
    really such a thing as a “fair” die? Imperfections in the manufacturing process,
    a slight deviation in shape, or unequal density throughout the die’s body might
    lead to a bias. Still, overall, and especially for our purposes, we might believe
    that dice rolls are random enough to be useful.
  prefs: []
  type: TYPE_NORMAL
- en: I gathered 14 six-sided dice and rolled them, en masse, using a dice cup from
    a game. I then took a picture of the results so I could count how many of each
    face appeared. I repeated this process 50 times for a total of 700 dice rolls.
    [Table 1-5](ch01.xhtml#ch01tab05) shows the results.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1-5:** Tallying Dice Rolls'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Outcome** | **Count** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 122 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 98 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 106 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 126 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 119 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 129 |'
  prefs: []
  type: TYPE_TB
- en: 'As before, if the dice are fair, we expect each count to be the same. For 700
    rolls, we expect each possible output to appear 700/6 ≈ 117 times—a spartan number
    because of the small scale of the experiment, but sufficient for us to acknowledge
    the chief concern of this section: to master our appreciation of truly random
    processes.'
  prefs: []
  type: TYPE_NORMAL
- en: The counts in [Table 1-5](ch01.xhtml#ch01tab05) are not 117 but deviate, often
    by quite a bit. Does this mean the dice are loaded? Perhaps, but we’ll never know
    for sure; we can only gain evidence favoring one answer over another. The *χ*²
    test is our tool of choice here, as it was for the preceding coin flips. Applying
    it returns a p-value of 0.28, well above the threshold of 0.05 that is generally
    accepted as statistically significant. Therefore, we do not reject the null hypothesis
    and believe that the dice are reasonably fair and, consequently, a potential source
    of true randomness.
  prefs: []
  type: TYPE_NORMAL
- en: Macroscopic physical systems are likely insufficient if the goal is to create
    truly random numbers; there are too many biases involved, though we can use an
    algorithm like von Neumann’s to improve the situation. Also, a physical randomness
    engine, perhaps based on automatic dice rolls, will degrade over time, further
    introducing bias. Therefore, we must search in a different direction. Let’s now
    consider processes more suitable for a randomness engine.
  prefs: []
  type: TYPE_NORMAL
- en: '**ROULETTE WHEELS**'
  prefs: []
  type: TYPE_NORMAL
- en: Roulette is another physical process that comes to mind when contemplating potential
    sources of randomness. In roulette, people bet on the position in which a marble
    will ultimately land when rolled against a spinning wheel. It is a favorite target
    for people trying to game the system because players can make bets while the wheel
    is spinning. On the face of it, a roulette wheel should be as random as rolling
    dice, but mechanical defects, especially if the wheel is tilted even the tiniest
    bit, bias the final ball position to the advantage of the clever.
  prefs: []
  type: TYPE_NORMAL
- en: The first occurrence of gaming a roulette wheel that I could find happened around
    1880 when Englishman Joseph Jagger, a textile worker, realized that imperfections
    in the construction of the roulette wheels in Monte Carlo enabled him to predict
    the final ball position reliably enough to win more often than he lost. His success
    forced improvements to the design of roulette wheels.
  prefs: []
  type: TYPE_NORMAL
- en: Around 1960, Edward Thorp, working with Claude Shannon, constructed what is
    likely the first wearable computer for the sole purpose of gaming roulette. The
    full account is in Thorp’s paper, titled “The Invention of the First Wearable
    Computer,” published in 1998\. The computer was small, with only 12 transistors,
    and was operated by a footswitch concealed in a shoe. As the roulette wheel turned,
    the footswitch started a timer that delivered one of eight tones to a tiny earpiece,
    each tone indicting a predicted octant in which the ball was more likely to fall.
    The system, though fragile, worked and was tried in Las Vegas in 1961 with some
    success.
  prefs: []
  type: TYPE_NORMAL
- en: In the 1970s, J. Doyne Farmer and Norman Packard essentially repeated that experiment
    using a microcomputer. Like Thorp and Shannon, they were similarly successful
    in the casinos; see Farmer’s brief page at *[http://www.doynefarmer.com/roulette](http://www.doynefarmer.com/roulette)*
    or the more detailed account in Thomas Bass’s book *The Eudaemonic Pie*, which
    is available online via the Internet Archive (*[https://archive.org](https://archive.org)*).
  prefs: []
  type: TYPE_NORMAL
- en: '***Using Voltage***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Most desktop computers have a microphone input jack. A program like Audacity
    can record samples from this input device. We might expect recording when no microphone
    is connected to give us an empty file, but it doesn’t. The microphone input is
    an analog input and is susceptible to electronic noise: minor, random variations
    in voltage due to the nature of the components involved and other environmental
    factors. We’ll use the variation of this voltage as a randomness engine.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This experiment requires you to record a WAV file. It doesn’t matter which
    tool you use to do so. I used Audacity, an open source sound editor available
    for most systems. Install it under Ubuntu with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Visit *[https://www.audacityteam.org](https://www.audacityteam.org)* to install
    Audacity for Windows and macOS.
  prefs: []
  type: TYPE_NORMAL
- en: We want to record from the microphone input using a single channel (mono) and
    at a high sampling rate, like 176,400 Hz (samples per second). For Audacity, this
    means changing the project rate to 176,400 (see the lower left of the screen)
    and changing the recording channel drop-down menu to **Mono**. We also need to
    select the microphone as the input source. The name of this device on your system
    is beyond my powers of clairvoyance, but experimentation with the drop-down menu
    next to the microphone icon will likely turn up the proper device. To test the
    input, select **Click to Start Monitoring**. You should see a slowly varying sound
    bar for only one channel (for example, the left channel). If not, play around
    a bit more until you do.
  prefs: []
  type: TYPE_NORMAL
- en: To record a sample, click the red record button. I suggest recording for several
    minutes. To stop, click the square stop button. Use the appropriate option on
    the File menu to export the recording as a WAV file, setting the output format
    in the File Save dialog to **32-bit float PCM**. If you can’t use this method
    to record a WAV file, there are several small ones available on the book’s website
    that you can use with the upcoming code.
  prefs: []
  type: TYPE_NORMAL
- en: Audio signals are represented as a continuous voltage that changes with time.
    Sampling an audio signal means measuring the instantaneous voltage at a set time
    interval, the sampling rate, and turning that voltage into a number over some
    range. For example, a compact disc uses 16-bit samples, so each voltage is assigned
    a number in the range −32,768 to 32,767\. The sampling rate decides how frequently
    the voltage is measured.
  prefs: []
  type: TYPE_NORMAL
- en: When a digitized signal is played back, meaning converted back into a voltage
    to drive a speaker, for instance, the quality of the sound depends on how many
    numbers were used to represent the signal and how often it was sampled. For our
    experiment, the samples are represented as 32-bit floating-point numbers, and
    the sampling rate is 176,400 Hz. By comparison, a compact disc samples at 44,100
    Hz.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-2](ch01.xhtml#ch01fig02) shows a subset of audio samples and their
    corresponding histogram.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/01fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-2: The audio samples (left) and corresponding histogram (right)*'
  prefs: []
  type: TYPE_NORMAL
- en: The *x*-axis in [Figure 1-2](ch01.xhtml#ch01fig02) is time, that is, the sample
    number, and the *y*-axis is the floating-point sample value, or the digitized
    value representing the voltage at that time. The horizontal line is the mean value
    of all the samples in the file; [Figure 1-2](ch01.xhtml#ch01fig02) shows only
    the first 200\. As we might expect, the mean value is virtually 0.
  prefs: []
  type: TYPE_NORMAL
- en: The right side of [Figure 1-2](ch01.xhtml#ch01fig02) shows us the histogram
    of all the samples in a two-second clip. We’ve seen this shape before; it’s nearly
    identical to [Figure 1-1](ch01.xhtml#ch01fig01), and it tells us that the noise
    samples are normally distributed with a mean value of 0\. We’ll use this observation
    to convert the audio stream into a file of random bytes.
  prefs: []
  type: TYPE_NORMAL
- en: We cannot use the recording as is; we must process the samples to make them
    more random, using the code in *silence.py*. Let’s walk through it part by part.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we `import` some library routines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `sys` module provides an interface to the command line; `numpy` we know.
    To read WAV files, we need the `read` function from SciPy. Here, I’m renaming
    it `wavread`.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the bottom of the script, we load the desired WAV file and process it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `wavread` function returns the sampling rate (`s`) and the samples themselves
    as a NumPy vector (`d`). We display the sampling rate, then split the samples
    into two halves and pass each half to `MakeBytes` before assigning the return
    values `a` and `b`, respectively. `MakeBytes` turns a vector of audio samples
    into a vector of bytes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final set of bytes is in `c`. This is the exclusive-OR (XOR) of the bytes
    in `a` and `b`. XOR is a logical operation on the bits of an integer. If one of
    the inputs to XOR is 1 and the other 0, then the output is 1\. If the inputs are
    the same, the output is 0\. I remember the phrase “one or the other, but not both.”
    XOR is different from the standard OR operation, which outputs 1 if any input
    is 1, including if both are 1, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 XOR 1 = 0, but 1 OR 1 = 1
  prefs: []
  type: TYPE_NORMAL
- en: Using one part of the generated stream of bytes to modify the other is a powerful
    way to alter the bit patterns, which adds to the random nature of the output.
    In *silence.py*, one of the byte streams is reversed (`[::-1]`) before XOR to
    add that much more randomness to the process.
  prefs: []
  type: TYPE_NORMAL
- en: The number of bytes returned by `MakeBytes` depends on the actual samples passed
    to it, not the number of samples. Therefore, it is likely that the vector `a`
    will be of different length than `b`, hence the `if` statement and indexing based
    on `len`. When `c` is ready, it’s written to disk in binary via `tofile`.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the action is in the `MakeBytes` function. Converting the stream of audio
    samples into bytes requires four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Subtract the overall mean from each sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Convert each sample to a bit based on its sign: 1 if positive, 0 if negative.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: De-bias the bits using the von Neumann algorithm from the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Group each set of 8 bits into an output byte.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The function `MakeBytes` performs each of these steps, using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The code passes in the samples as `A`, a NumPy vector. Now come the four steps.
    First, we subtract any mean value (`t`) ➊. Next, we define `thresh` to be 1 percent
    of the maximum range of the samples. I’ll explain why momentarily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2 says to use the sign of each sample as a bit ➋. We observed that the
    samples are normally distributed, and the mean is now zero because we subtracted
    it, so roughly half the samples will be negative and half positive. This sounds
    a lot like a coin flip: either one value or another. Therefore, we make the positive
    samples ones and the negative samples zeros. But what’s the point of `thresh`?'
  prefs: []
  type: TYPE_NORMAL
- en: The bits are collected in the list `w`, initially empty. The loop after initializing
    `w` examines each sample and asks if the sample’s absolute value is less than
    1 percent of the maximum range. If it is, we ignore that sample (`continue`);
    otherwise, we use the sample’s sign to add a new bit to `w`. There may be a tiny,
    nonrandom signal with a low amplitude in the samples. Ignoring samples close to
    zero helps remove any such signal. In effect, we are saying we’re interested in
    only “large” deviations from the mean of zero.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a long list of individual bits (`w`). These bits might be biased,
    representing a collection of unfair coin tosses. To handle this, we de-bias by
    using the von Neumann algorithm, which makes a new list of bits in `b` ➌.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we convert each set of eight bits in `b` into a new list of bytes in
    `z` ➍ by reshaping `b` into an *N*×8 array before multiplying each row of that
    array, column by column, by the value of each bit position in a byte. When all
    is said and done, the code converts the list of bytes into a NumPy array and returns
    it.
  prefs: []
  type: TYPE_NORMAL
- en: You may want to review the code again to ensure you follow each step. The central
    concept is that we took advantage of the fact that the samples are normally distributed
    to generate a stream of bits that, when de-biased, form the stream of bytes used
    to make the final output of the code.
  prefs: []
  type: TYPE_NORMAL
- en: To test out this code, I made a 30-minute recording using Audacity and saved
    the samples in the file *silence.wav*. I then used *silence.py* to convert this
    large WAV file into *silence.bin*.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The file* silence.wav *is too large to include in the book’s repository. However,
    if your heart is set on having it, contact me, and I’ll see what I can do.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the command line I used to convert the WAV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `wavread` function tends to complain about WAV file elements it doesn’t
    understand. Adding `-W ignore` to the command line suppresses these warnings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting output file, *silence.bin*, is not quite 6MB. It’s a collection
    of random bytes, so we can pass it to `ent` to get a report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: These are reasonably good values. The only value that is not where we might
    want it to be is *χ*², but we can live with that. Keep *silence.bin* on hand to
    use as a randomness engine later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: '**Random Physical Processes**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many physical processes are random, though possibly biased. In this section,
    we’ll explore three physical processes leading to randomness: atmospheric radio
    frequency noise, plasma and charged particle rates as detected by the *Voyager
    1* and *Voyager 2* spacecraft during their 40-plus-year mission, and the decay
    of radioactive isotopes.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Atmospheric Radio Frequency Noise***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 1997, Mads Haahr, a computer science professor at Trinity College, Dublin,
    started *[https://www.random.org](https://www.random.org)*, a website dedicated
    to generating truly random numbers from atmospheric radio frequency noise. The
    site has been running ever since and offers free random numbers to internet users,
    along with a host of paid services involving random numbers. You can take a look
    at the services offered at *[https://www.random.org](https://www.random.org)*,
    but for the purposes of this book, we’ll stick to the free collection of random
    bytes.
  prefs: []
  type: TYPE_NORMAL
- en: There are two primary ways to get random data from the site. First, random data
    in 16KB chunks are available at *[https://www.random.org/bytes](https://www.random.org/bytes)*.
    Simply select the desired format and download. However, since we need larger collections
    of bytes to use as a randomness engine for our experiments, we are better served
    by using the archive available at *[https://archive.random.org](https://archive.random.org)*.
    Downloading files directly from the site requires a small fee. However, if you
    are comfortable using torrents, the files can be legally acquired for free. Our
    reference Linux distribution includes Transmission, a client for accessing torrents.
    The application is also available for other operating systems; visit *[https://transmissionbt.com](https://transmissionbt.com)*
    for download instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a test, I used Transmission to download several months’ worth of random
    bytes (the Binary Files option). I then combined all the binary files into one
    using the `cat` command. For example, a command like this will merge all binary
    files ending in *.bin* into the file *random_bytes*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'I then passed *random_bytes* (about 126MB) to `ent` to get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that `ent` is quite happy with *random_bytes*. I have a much larger
    collection of bits from *[random.org](http://random.org)*, including bits from
    two previous years. It’s over 500MB in size, and `ent` likes this one as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The code shows that the entropy is maximized, with 8 bits per byte.
  prefs: []
  type: TYPE_NORMAL
- en: '#### ***Voyager Plasma and Charged Particle Data***'
  prefs: []
  type: TYPE_NORMAL
- en: In 1977, NASA launched the twin *Voyager* spacecraft to explore the outer solar
    system. *Voyager 1* encountered Jupiter and Saturn before heading out of the solar
    system. *Voyager 2* passed by Jupiter, Saturn, Uranus, and Neptune on its grand
    tour. To date, *Voyager 2* is the only spacecraft to explore Uranus and Neptune.
    In August 2012, *Voyager 1* became the first human-made object to leave the solar
    system and enter interstellar space. As of this writing, October 2023, both spacecraft
    are still performing well and have enough power for perhaps a decade.
  prefs: []
  type: TYPE_NORMAL
- en: The *Voyager*s are best known for the fantastic images they returned. However,
    both spacecraft carry multiple scientific instruments for measuring the environment
    through which they travel. This includes devices for measuring plasma protons
    and the flux of other charged particles and nuclei. These measurements are not
    entirely random but vary around certain values, much like the microphone input
    we used previously. Therefore, it’s possible to use the *Voyager* data to construct
    a binary file suitable for use as a source of randomness.
  prefs: []
  type: TYPE_NORMAL
- en: The file *voyager_plasma_lecp.bin* contains bytes formed from a set of *Voyager*
    datafiles, including plasma proton counts, density, and temperature for 1977 through
    1980, and low-energy charged particle fluxes (particles passing through an area
    over time) from 1977 through 2021\. I used the code in *process_vgr_data.py* to
    merge several smaller files to process the individual plasma and charged particle
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each type of data—plasma protons, low-energy protons, low-energy ions,
    and cosmic ray protons—the process was the same: find the median value over the
    time interval and mark each observation as a 1 bit if above the median and a 0
    bit if below. To expand the collection of bits, I repeated this process for the
    median along with 80 percent of the median and 120 percent of the median. I then
    de-biased the final collection of bits using the von Neumann algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting file contains 77,265 bytes, and `ent` reports the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: These are reasonable values.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-3](ch01.xhtml#ch01fig03) shows sample *Voyager* data from the plasma
    and low-energy charged particle experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/01fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-3: Sample* Voyager *data. Clockwise from top left: low-energy ions,
    cosmic ray protons, plasma proton temperature, and plasma proton speed.*'
  prefs: []
  type: TYPE_NORMAL
- en: The dashed line in each plot marks the median value over the dataset. Observations
    above the median became 1 bits, and those below the median 0 bits. The plot on
    the upper right shows the flux of cosmic ray protons over the entire mission.
    The vertical dashed line marks August 2012, the date when *Voyager 1* officially
    left the solar system. Notice the increase in cosmic ray protons after this time.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The* Voyager *datasets were gathered from multiple websites. They are presented,
    typically, in text format and required a fair bit of processing on my part to
    whip them into shape. The files used inconsistent formatting, had typos in places,
    and, on occasion, used data from the wrong year. If you want the files as I used
    them, please contact me directly.*'
  prefs: []
  type: TYPE_NORMAL
- en: '#### ***Radioactive Decay***'
  prefs: []
  type: TYPE_NORMAL
- en: 'From 1996 through December 2022, the HotBits website (*[https://www.fourmilab.ch/hotbits](https://www.fourmilab.ch/hotbits)*)
    delivered truly random data to the public using the most random of random processes:
    radioactive decay.'
  prefs: []
  type: TYPE_NORMAL
- en: Radioactive elements, like the cesium-137 used by HotBits, are unstable. Eventually,
    all such atoms decay by some process to another element, in this case, barium-137\.
    The number of protons in an atom determines which element it is. Cesium has 55
    protons in its nucleus; that’s its *atomic number*. *Isotopes* are versions of
    an element where the number of neutrons, also in the nucleus, varies. The sum
    of the two, ignoring the very light electrons, gives the *atomic mass*. If cesium-137
    has 55 protons, it must have 82 neutrons. When an atom of cesium-137 decays, one
    of the neutrons converts into a proton, changing the atomic number to 56 and thereby
    converting the atom to barium-137, an atom with 56 protons and 81 neutrons. Barium-137
    is stable, so the decay process stops. This process varies for different elements.
    For example, the decay of uranium passes through many stages to reach a stable
    isotope of lead.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a neutron becomes a proton, it releases a beta particle (an electron)
    and an antineutrino. Neutrinos have virtually no mass and are almost undetectable.
    However, a Geiger counter easily detects beta particles. This is how the HotBits
    site generates random bits. To be complete, the decay from cesium to barium passes
    through two stages: the barium nucleus begins in a metastable state, then, about
    two minutes later, returns to the ground state by emitting a gamma ray. A gamma
    ray is a high-energy photon, that is, light.'
  prefs: []
  type: TYPE_NORMAL
- en: The time when a particular decay will happen is governed by quantum physics
    and all the “weirdness” we associate with it. For radioactive decay, the weirdness
    at play is *quantum tunneling*, the fact that even though the cesium atom lacks
    the energy to, in effect, push itself up and out of the bowl it’s in, it nevertheless
    has a nonzero probability of doing so. There is no classical physics analog for
    quantum tunneling.
  prefs: []
  type: TYPE_NORMAL
- en: HotBits takes advantage of this unpredictability by using the timing between
    two pairs of detections to output a 1 or a 0\. First, a beta particle is detected,
    then another. The time interval between the two detections is denoted as *T*[1].
    Next, another pair is detected with that time interval labeled *T*[2]. If *T*[1]
    ≠ *T*[2], a bit is generated. If *T*[1] < *T*[2], the bit is a 0; otherwise, *T*[1]
    > *T*[2] and the output bit is a 1\. The sense of the comparison is reversed after
    each bit to frustrate any systematic bias introduced by the physical setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'HotBits required an API key to request up to 2,048 bytes at a time. There was
    a strict limit to the number of bytes granted in a 24-hour period. I downloaded
    bytes daily as I worked on this book and now have one 3,033,216-byte file. It’s
    pretty good data according to `ent`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we’ve discussed several different processes that generate random
    data. While all of them are, as we’ll see later, useful, none can generate massive
    quantities of random numbers—and we’ll sometimes need millions of random numbers
    later in the book. We have no choice but to turn to what we might call synthetic
    random processes and generate random numbers using deterministic means. The use
    of the word *deterministic* in the previous sentence should bother you, but I
    suspect you’ll be more comfortable with the idea of simulating a random process
    via a deterministic process by the end of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deterministic Processes**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As von Neumann once said, “Anyone who attempts to generate random numbers by
    deterministic means is, of course, living in a state of sin.” The very idea of
    a random process is its unpredictability, such that knowledge of what came before
    is of no utility in predicting what will come after. By definition, a deterministic
    process follows a predictable algorithm; therefore, it cannot possibly be a true
    random process.
  prefs: []
  type: TYPE_NORMAL
- en: Why use deterministic processes, then? Even if the process is deterministic,
    it can approximate a random process to the level where the outputs are helpful.
    A *pseudorandom process* approximates a random process. We’ll make heavy use of
    pseudorandom processes throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: This section also covers how pseudorandom processes are related to *quasirandom
    processes*. Finally, we’ll discuss two hybrid processes likely already available
    on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: '***Pseudorandom Numbers***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are many ways to approximate a random process that delivers pseudorandom
    numbers on demand. Here, I’ll introduce one such approach for use in our future
    experiments.
  prefs: []
  type: TYPE_NORMAL
- en: A *linear congruential generator (LCG)* is a simple approach to creating a sequence
    of pseudorandom numbers. The numbers generated by an LCG are good enough for a
    video game and for many of our experiments, but they are statistically weak and
    not recommended for serious use, like in cryptography.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to understand an LCG is to dive right in:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x*[*i* + 1] = (*ax[i]* + *c*) mod *m*'
  prefs: []
  type: TYPE_NORMAL
- en: The entire generator is that single equation, where *x* is the value produced
    by the generator. The subscript means that the next value in the sequence, *x*[*i*
    + 1], is derived from the previous, *x[i]*. The initial value, *x*[0], is the
    *seed*, the value that primes the generator. Virtually all pseudorandom generators
    use a seed of some kind.
  prefs: []
  type: TYPE_NORMAL
- en: The equation consists of two parts. The first is *ax[i]* + *c*, where *a* and
    *c* are carefully chosen positive integers. The second part takes the result of
    the first, *y*, and calculates *y* mod *m*, where *m* is another carefully chosen
    positive integer. The mod operator refers to the modulo, which is nothing more
    than the remainder. To calculate it, the generator first finds *y*/*m* using integer
    division, then the remainder, which must be a number in the range [0, *m*). The
    final result is then used as both the output of the generator and the value used
    to calculate the following output.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example of how this works, let me show you an LCG in action. I’ll use
    small numbers, which helps in understanding, but would be terrible choices in
    practice, as we’ll see:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x*[*i* + 1] = (3*x[i]* + 0) mod 7'
  prefs: []
  type: TYPE_NORMAL
- en: 'The seed must be less than 7, so let’s use *x*[0] = 4\. Here’s the sequence
    produced by this LCG:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0022-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The output might seem a bit random, but after *x*[5], the sequence begins to
    repeat. All pseudorandom generators repeat eventually. The number of outputs before
    repeating determines the generator’s *period*. Here the period is 6, with 4, 5,
    1, 3, 2, 6 repeating forever.
  prefs: []
  type: TYPE_NORMAL
- en: With properly chosen constants, the period can be much larger. For our experiments,
    we’ll use a set of constants popular in the 1980s, which came to be called the
    *MINSTD (minimum standard generator)*. It produces a sequence of unsigned integers
    with a period of 2^(31) ≈ 10⁹. That’s a reasonable period for many applications,
    but there’s more to a good generator than its period. The closer the pseudorandom
    generator is to a truly random sequence, the better. Statistical test suites like
    `ent`, or more professional ones, seek to uncover all manner of correlations in
    the sequence spit out by the generator. With those test suites, it quickly becomes
    evident that MINSTD is a poor generator for serious work, but sufficient for our
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'I created a file of 100 million bytes using MINSTD and handed it to `ent`.
    I got the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The output is very reasonable, so MINSTD will likely be of use to us.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator uses *a* = 48,271, *c* = 0, and *m* = 2,147,483,647 to make the
    generating equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x*[*i* + 1] = 48271*x[i]* mod 2147483647'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re familiar with how computers store numbers internally, you’ll notice
    that *m* is not using all 32 bits of a 32-bit integer. Instead, *m* = 2^(31) –
    1 is the largest positive integer that can be stored in a *signed* 32-bit value.
    Because of the modulo operation, the output of the generator must be in the range
    [0, 2^(31) – 1), implying the generator can create at most only some 2 billion
    unique values.
  prefs: []
  type: TYPE_NORMAL
- en: If we need to generate a sequence of bytes, [0, 255], then we’re good. But what
    if we want to create a sequence of 64-bit floating-point values, what the C language
    calls a `double`? In that case, we have to do more work. The simplest approach
    is to divide *x* by *m*, since that must produce a number in the range [0, 1).
    For many applications, that’s sufficient, but it still only delivers a value selected
    from a set of 2 billion or so numbers. You don’t need to know the details of this,
    but a 64-bit floating-point value can store more than that because it uses a 52-bit
    base-2 mantissa. If we want to make full use of what a 64-bit floating-point value
    can give us, we need to generate two outputs from MINSTD and assign 52 bits extracted
    from both of them to the mantissa of a floating-point value with an exponent of
    0\. Thankfully, we don’t need that much precision, but it’s worth noting that
    simply dividing by *m* isn’t giving you all that you might think it is.
  prefs: []
  type: TYPE_NORMAL
- en: I previously mentioned that pseudorandom generators use seed values, a starting
    value. This requirement is a double-edged sword. Setting the seed to a specific
    number causes the generator to repeatedly output the same sequence of values,
    but the downside is that sometimes we might need to jump through hoops to make
    sure we aren’t using the same seed repeatedly.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, MINSTD with a seed of *x* = 8,675,309 will produce this sequence
    each time:'
  prefs: []
  type: TYPE_NORMAL
- en: 0.00304057,  0.77134655,  0.66908364,  0.33651287,  0.8128977, . . .
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, using a seed of *x* = 1,234 will consistently produce:'
  prefs: []
  type: TYPE_NORMAL
- en: 0.02773777,  0.93004224,  0.06911496,  0.24831591,  0.45733623, . . .
  prefs: []
  type: TYPE_NORMAL
- en: The floating-point numbers are produced by dividing *x* by *m* = 2,147,483,647.
  prefs: []
  type: TYPE_NORMAL
- en: Often, we’ll use a pseudorandom generator, like those built into NumPy, without
    specifying a seed value. In that case, we want an unrepeatable sequence, at least
    unrepeatable for us, because we won’t know the seed selected. If no seed value
    is supplied, NumPy selects a value from `/dev/urandom`, a special system device
    discussed later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudorandom generator we’ll use most often is PCG64, the generator NumPy
    uses by default. It’s a good generator, with a period of 2^(128). NumPy’s old
    default generator was the Mersenne Twister. Statistically, it’s still quite good,
    much better than MINSTD. The Mersenne Twister’s period is 2^(19,937) – 1, which
    explains its more common name: MT19937\. To be precise, its period is a huge number,
    6,002 digits, and is utterly without meaning in human terms. PCG64’s period isn’t
    anything to scoff at either:'
  prefs: []
  type: TYPE_NORMAL
- en: 2^(128) = 340, 282, 366, 920, 938, 463, 463, 374, 607, 431, 768, 211, 456
  prefs: []
  type: TYPE_NORMAL
- en: That’s a number similarly without meaning in human terms. None of our experiments
    will come anywhere close to exhausting the sequence generated by either MT19937
    or PCG64, both of which we’ll use frequently throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of these generators follows a uniform distribution. Any other distribution,
    like a normal distribution, can be formed from a collection of uniformly distributed
    numbers. For example, it is straightforward to generate normally distributed numbers
    via the Box-Muller transformation, which maps two uniformly distributed numbers,
    *u*[1] and *u*[2], to two normally distributed numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0024-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There’s much more to say about pseudorandom generators; many computer scientists
    have spent their careers working with them. However, we now understand all we
    need for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '***Quasirandom Sequences***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For some experiments, we’ll want random data that is *space-filling*. By that
    I mean a generator producing a sequence that appears random but that will, over
    time, fill space more or less evenly. A purely random process offers us no such
    guarantee, but a quasirandom sequence does.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between a random or pseudorandom sequence and a quasirandom sequence
    is best understood via example. [Figure 1-4](ch01.xhtml#ch01fig04) compares two
    sequences of 40 points, one generated pseudorandomly and one generated quasirandomly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/01fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-4: Two sets of 40 points, random (at 0.1) and quasirandom (at 0.2)*'
  prefs: []
  type: TYPE_NORMAL
- en: The lower sequence, at *y* = 0.1, was generated pseudorandomly. It covers the
    range from 0 to 1 (the *x*-axis), but does so with gaps. The second sequence,
    at *y* = 0.2, is a quasirandom sequence. It covers the same range, but is more
    consistent, with no crowding or gaps. Given enough samples, both sequences will
    eventually fill in the entire range, but the quasirandom sequence does so by scattering
    values more or less evenly throughout.
  prefs: []
  type: TYPE_NORMAL
- en: Our quasirandom process uses what is known as a *Halton sequence*. A Halton
    sequence is based on a prime number that divides the interval [0, 1) first by
    the base (the prime), then the base squared, then cubed, and so on. For a base
    of 2, the interval is first divided in half, then quarters, then eighths, and
    so on. This process will eventually cover the interval in the infinite limit and
    fills the interval approximately evenly. For example, [Table 1-6](ch01.xhtml#ch01tab06)
    shows the first few values of the Halton sequence for the given base (rounded
    to three digits).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1-6:** Halton Sequences'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Base** | **Sequence** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `0, 0.5 ,  0.25 , 0.75 , 0.125, 0.625, 0.375, 0.875` |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `0, 0.333, 0.667, 0.111, 0.444, 0.778, 0.222, 0.556` |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | `0, 0.2 ,  0.4 ,  0.6 ,  0.8 ,  0.04 , 0.24 , 0.44` |'
  prefs: []
  type: TYPE_TB
- en: Each sequence begins with 0 and is entirely deterministic, like a pseudorandom
    generator with a fixed seed.
  prefs: []
  type: TYPE_NORMAL
- en: If the quasirandom sequence is so predictable, why is it useful? There are times
    when it is more important to fill space evenly than purely randomly. For example,
    some of our experiments will involve searching a multidimensional space to locate
    a point in that space that we consider best, for some definition of best. In that
    case, it often makes more sense to initialize the search so that the locations
    evaluated at first represent all of the space more or less equally.
  prefs: []
  type: TYPE_NORMAL
- en: Quasirandom sequences are most useful in combination. [Figure 1-4](ch01.xhtml#ch01fig04)
    used a single quasirandom sequence with a base of 2 to fill in a one-dimensional
    space, the interval from 0 to 1\. What if, instead, we wanted to fill in a two-dimensional
    space, like the *xy*-plane? For that, we need pairs of numbers. The seemingly
    obvious thing to do is sample twice to use the first number as the *x*-coordinate
    and the second as the *y*-coordinate. Let’s see what happens when we apply this
    approach to pseudorandom and quasi-random numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-5](ch01.xhtml#ch01fig05) attempts to fill 2D space with points.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/01fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1-5: From left to right: examples of bad quasirandom, pseudorandom,
    and good quasirandom sequences*'
  prefs: []
  type: TYPE_NORMAL
- en: In the middle of [Figure 1-5](ch01.xhtml#ch01fig05), I plotted 500 pairs sampled
    from a pseudorandom generator. The points are randomly distributed in space, but
    there are regions with fewer points and regions with more points, as expected.
    Next, I repeated this exercise using a quasirandom sequence with base 2\. That
    is, I asked the sequence for two samples, one after the other, and plotted them
    as a point. The result is on the left in [Figure 1-5](ch01.xhtml#ch01fig05). Clearly,
    something strange is happening.
  prefs: []
  type: TYPE_NORMAL
- en: The plot isn’t a mistake; the quasirandom sequence is doing precisely what it’s
    supposed to do. We’ve defined a random process as one where knowledge of previous
    values is of no utility in predicting subsequent values. That’s what the pseudorandom
    generator supplied; therefore, we can use sequentially generated values as coordinates
    for a point in 2D space. However, the quasirandom sequence offers no assurance
    that previous values do not indicate what comes next. Instead, the sequence is
    entirely predictable. Pairs of samples will be in a simple relationship to each
    other, which is what we see on the left in [Figure 1-5](ch01.xhtml#ch01fig05).
  prefs: []
  type: TYPE_NORMAL
- en: This does not mean we cannot use quasirandom sequences beyond one-dimensional
    cases. The trick is to use a different base for each dimension. Here, we have
    two dimensions, so we need *two* quasirandom sequences, each with a different
    base. The plot on the right in [Figure 1-5](ch01.xhtml#ch01fig05) was generated
    this way. The *x*-coordinates are the first 500 samples from a quasirandom sequence
    with base 2, and the *y*-coordinates are the first 500 from a quasi-random sequence
    using base 3\. Recall that the base must be a prime number. Since the two quasirandom
    sequences are not using the same base, the values are not (simply) correlated,
    so the points fill in the 2D space. Likewise, as each sequence is filling in the
    interval from [0, 1), and we are plotting them in pairs, it makes intuitive sense
    that they will fill in the 2D space as well, which is what the plot shows.
  prefs: []
  type: TYPE_NORMAL
- en: The moral of the story is that we can use quasirandom sequences, one base per
    dimension of the problem, to fill in some space in a way that seems random.
  prefs: []
  type: TYPE_NORMAL
- en: '***Combining Deterministic and Truly Random Processes***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Truly random processes are the gold standard, but they tend to be relatively
    slow, at least given how quickly a computer works. A pseudorandom generator is
    a good substitute, but it’s only a truly random process wannabe. Why not merge
    the two? This section will examine two approaches to combining truly random processes
    with pseudorandom generators. I call these *hybrid processes*. We’ll discuss `/dev/urandom`,
    the Linux operating system’s approach, and `RDRAND`, a CPU-based hybrid processor
    instruction supported by many newer Intel and AMD processors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both `/dev/urandom`, henceforth `urandom`, and `RDRAND` operate in the same
    way: they use a slow source of true randomness to reseed a cryptographically secure
    pseudorandom number generator. We learned earlier in the chapter that pseudorandom
    generators have seeds, something that sets the initial state of the generator.
    Set the seed the same way, and the sequence of values generated repeats. The hybrid
    approaches frequently reseed the generator with the intention of altering the
    sequence to the point where, even if an adversary were to figure out what the
    pseudorandom generator was doing, it wouldn’t matter in practice because the seed
    would be random, making any knowledge of the state of the generator useless after
    a short time interval.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I slipped a new phrase into the previous paragraph: *cryptographically secure*.
    Our experiments are not worried about adversaries and cryptography (well, mostly);
    we’re only concerned that the pseudorandom generator is “pretty good” in the sense
    that our experiments perform well using the generator.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A cryptographically secure pseudorandom generator is a high-quality pseudorandom
    generator with the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: An attacker’s knowledge of the generator’s state at time *t[c]* offers no ability
    for the attacker to know anything about the state of the generator at any previous
    time *t* < *t[c]*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For any output bit *i*, there is no polynomial-time algorithm operating on all
    previously generated bits, 0, . . . , *i* – 1, that can predict *i* with better
    than 50 percent accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second property is known as the *next-bit* test. It’s what marks the cryptographically
    secure generator as a high-quality generator.
  prefs: []
  type: TYPE_NORMAL
- en: The phrase *polynomial-time algorithm* comes from the study of algorithms and
    how they perform regarding time and space. Polynomial-time algorithms are the
    nice ones; they are the algorithms that might finish sometime before the heat
    death of the universe. The classic bubble sort algorithm is a polynomial-time
    algorithm because its runtime scales as the square of the number of elements to
    sort; that is, sorting *n* elements takes time on the order of *n*², a polynomial.
    Any algorithm whose time or space resources are bounded by a polynomial is also
    a polynomial-time algorithm. For example, the Quicksort algorithm scales as *n*
    log *n* for *n* items. That’s a function easily bounded by a polynomial, like
    *n*², so Quicksort is also a polynomial-time algorithm. An algorithm that scales
    as 2*^n* is not a polynomial-time algorithm because 2*^n* is an exponential, not
    a polynomial. Exponential algorithms are bad because they quickly become intractable,
    which is precisely what people concerned about attacks on their pseudorandom number
    generators want.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmically, a hybrid process uses a strong pseudorandom generator to deliver
    random values on demand while periodically reseeding said generator from a truly
    random source. Let’s explore the `urandom` and `RDRAND` approaches to this problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reading Random Bytes from urandom**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Unix systems treat many things that are not files as if they were files, and
    `urandom` is no exception. To acquire random bytes from `urandom`, we need to
    treat it like a file. For example, let’s run Python, open `urandom`, and dump
    60 million bytes from it to a disk file so `ent` can evaluate them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output file is *ttt.bin*, a throwaway name. When I ran the code, `ent`
    reported the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As expected, `urandom` performs well and will work for our purposes as a randomness
    engine.
  prefs: []
  type: TYPE_NORMAL
- en: The Linux kernel maintains an *entropy pool*, a collection of bytes derived
    from system operations that it uses to update the seed of the pseudorandom generator
    every 300 seconds, according to kernel file *random.c*, which is available at
    *[https://github.com/torvalds/linux/blob/master/drivers/char/random.c](https://github.com/torvalds/linux/blob/master/drivers/char/random.c)*.
    The same file also reveals the kernel’s entropy sources, if curiosity strikes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can monitor the size of the entropy pool by reading the contents of the
    file *entropy_avail*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we’re told that at the particular moment I executed the command,
    there were 3,693 bytes in the entropy pool, meaning that 3,693 bytes were available
    to reseed the generator when the reseed interval expired.
  prefs: []
  type: TYPE_NORMAL
- en: Linux uses the ChaCha20 pseudorandom number generator. It’s a reasonably new
    generator that performs exceptionally well overall, even when subjected to more
    intensive test suites. The details don’t concern us here, only that `urandom`
    delivers the goods.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you spend much time reviewing resources related to* /dev/urandom, *you’ll
    run across its cousin,* /dev/random*. The latter device was intended for small
    amounts of high-quality random data, and it will block until enough entropy is
    available. This is contrary to our requirements, so we’re ignoring* /dev/random
    *entirely. Indeed, it was announced in March 2022 that future versions of the
    Linux kernel will make* /dev/random *nothing more than another name for* /dev/urandom.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Using the RDRAND Instruction**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The RDRAND instruction, if available for your CPU, provides access to high-quality
    random numbers using much the same approach as the Linux kernel and `urandom`.
    The key differences are that the entropy source is part and parcel of the CPU
    itself and the pseudorandom generator is updated at an interval no longer than
    after returning 1,022 random values. That is, `RDRAND` is reseeded based on the
    number of samples returned, not on a fixed time interval like `urandom`.
  prefs: []
  type: TYPE_NORMAL
- en: The pseudorandom number generator used is not ChaCha20 but CTR_DRBG, a generator
    developed by the National Institute of Standards and Technology (NIST), part of
    the United States Department of Commerce. The close association between NIST and
    the US government has caused some to distrust `RDRAND`’s output, but as we are
    not concerned with cryptographic security, `RDRAND` is fair game.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike `urandom`, which is accessed as if it were a binary file and therefore
    available immediately to all programming languages, `RDRAND` is a CPU instruction,
    so we need to access it via lower-level code or by installing a Python library
    that uses the instruction for us. As it happens, the `rdrand` library will do
    nicely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The class `RdRandom` returns an interface to `RDRAND` and supports the `random`
    method to return a random float. Alternatively, the `rdrand_get_bytes` function
    returns the requested number of random bytes, here 60 million, so we can compare
    `ent`’s report with the output of `urandom`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Passing the bytes in *ttt.bin* to `ent` gives us the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: For our purposes, these results are indistinguishable from those of `urandom`;
    therefore, we’ll rely on `RDRAND` from time to time as well.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re looking to access `RDRAND` from C, the code in *drng.c* will serve
    as a guide. Note the `gcc` compile instructions at the top of the file.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, I’ll detail the `RE` class, a Python wrapper for different
    randomness sources, and the randomness engine we’ll use consistently for all the
    book’s experiments.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*For other ways to generate pseudorandom numbers, see my book* Random Numbers
    and Computers *(Springer, 2018). It includes a thorough treatment of all things
    related to pseudorandom number generation, including code for specific algorithms.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Book’s Randomness Engine**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we build the `RE` class, the randomness engine that will power
    most of our experiments throughout the remainder of the book. If you’d prefer
    to jump right in, read through *RE.py*, and then come back here to fill in any
    details you missed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to design a class that will accomplish the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide a vector of a specified number of uniform random samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate pseudorandom output using PCG64, MT19937, or MINSTD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate hybrid random output using `urandom` or `RDRAND`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate quasirandom output for any specified prime base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Produce floats or integers in any range, [*a*, *b*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Produce bytes ([0, 255]) or bits ([0, 1])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow a seed value to generate the same output repeatedly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Substitute a file of bytes as the randomness source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s review `RE`’s source code before taking it for a brief test drive.
  prefs: []
  type: TYPE_NORMAL
- en: '***The RE Class***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `RE` class is configured via its constructor and provides only one method
    meant for public use: `random`. This method takes a single argument, an integer
    specifying the number of samples to return as a NumPy vector.'
  prefs: []
  type: TYPE_NORMAL
- en: The `RE` class’s private methods implement the possible randomness sources,
    shown in [Table 1-7](ch01.xhtml#ch01tab07).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1-7:** `RE`’s Methods'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Fetch` | Sample from a disk file |'
  prefs: []
  type: TYPE_TB
- en: '| `MINSTD` | Sample from MINSTD |'
  prefs: []
  type: TYPE_TB
- en: '| `Urandom` | Read from `/dev/urandom` |'
  prefs: []
  type: TYPE_TB
- en: '| `RDRAND` | Read from `RDRAND` |'
  prefs: []
  type: TYPE_TB
- en: '| `Quasirandom` | Sample from the Halton sequence |'
  prefs: []
  type: TYPE_TB
- en: '| `NumPyGen` | Sample from PCG64 or MT19937 |'
  prefs: []
  type: TYPE_TB
- en: Each private method accepts a single argument, the number of samples to return.
    Also, aside from `Fetch`, each private method returns a floating-point vector
    of samples in the range [0, 1).
  prefs: []
  type: TYPE_NORMAL
- en: '**NumPyGen and MINSTD**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s look first at `NumPyGen` and `MINSTD`, as they are straightforward. To
    save space, I’ve removed comments and doc strings. They are present in *RE.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let’s begin with `NumPyGen`, as it’s about as simple as you can get. The `RE`
    class constructor, which we’ll discuss later, creates a NumPy generator, if that’s
    the source desired, and stores it in the `g` member variable. Don’t give me that
    look; we’re experimenting, and `g` is a perfectly good variable name.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy already knows how to return vectors of floating-point numbers, so all
    that remains is to tell NumPy we want `N` of them, which we immediately return
    to the caller, `random`, which we’ll also discuss later on.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, `MINSTD`’s job is also to return a vector of floating-point samples.
    We first create a vector to hold the samples and then loop, applying the LCG equation
  prefs: []
  type: TYPE_NORMAL
- en: '*x*[*i*+1] = (*ax[i]*) mod *m*'
  prefs: []
  type: TYPE_NORMAL
- en: with *a* = 48,271 and *m* = 2^(31) – 1 = 2,147,483,647\. To convert *x*[*i*+1]
    to a float in [0, 1), we divide by *m* or, as here, multiply by 1/*m*.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few things to notice. First, the `seed` member variable *is x* if
    the MINSTD generator is selected. That’s why it’s updated. Second, the MINSTD
    equation is iterative; to get *x*[*i*+1] we need *x[i]* as pseudorandom values
    must be generated sequentially (usually). In practice, this means that our implementation
    of MINSTD is a bit slow, but that’s fine for our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that MINSTD deals with 32-bit integers, meaning floating-point numbers
    returned by `MINSTD` are only as precise as 32-bit floats. If you know C, this
    means it returns `float`, not `double`. A Python `float` is a C `double`, and
    likewise uses 64 bits (52 for the mantissa or significand). For most of what we
    do in this book, the loss of precision isn’t important, but you should be aware
    of it if you wish to use `RE` in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: '**RDRAND**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `RDRAND` method needs to use the CPU’s RDRAND instruction, which it does
    via the `rdrand` module. When the `RE` class is imported, Python tries to load
    `rdrand` (see the top of *RE.py*). If present, the `RDRAND` method uses the library
    to access samples. If the library isn’t present, `RDRAND` still works but falls
    back to NumPy’s PCG64 generator and issues a suitable warning message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at `RDRAND`, ignoring the part where `rdrand` isn’t available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `rdrand` module supports a handful of methods, but we’re restricting ourselves
    to `random`, which returns a single float in [0, 1), using all 52 bits of the
    mantissa. However, as `random` is returning a single number, we need a loop, so
    `RDRAND` isn’t particularly fast. If you want bytes from `rdrand`, you’ll be better
    served by using the module’s `rdrand_get_bytes` function directly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Urandom**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The `RE` class uses `/dev/urandom` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The code first reads four times as many bytes as requested into a Python byte
    array. Then, it tells NumPy to treat the byte array as a buffer and read 32-bit
    unsigned integers from it. The resulting array is divided by 2^(32) to change
    it into a vector in [0, 1). Like `MINSTD`, we’re treating `urandom` as a source
    of 32-bit floats.
  prefs: []
  type: TYPE_NORMAL
- en: '**Quasirandom**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Quasirandom numbers are a bit different, as we saw earlier in the chapter,
    but the implementation is similar to the other sources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The output vector (`v`) is constructed sample by sample. The `while` loop calls
    `Halton`, a function contained within `Quasirandom`. This function returns a specific
    number in the Halton sequence for the given prime base. The next number to use
    is in member variable `qnum`. We’ll return to `qnum` later when discussing the
    `RE` constructor.
  prefs: []
  type: TYPE_NORMAL
- en: '**Random**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The `RE` class’s only public method is `random`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If we’re not using a disk file, then the process is the same, regardless of
    the randomness source: first, generate the requested number of floating-point
    samples in [0, 1), and then modify them to be in the desired range and of the
    desired type.'
  prefs: []
  type: TYPE_NORMAL
- en: As all randomness sources accept the same argument and return the same [0, 1)
    vector, we store references to the particular methods in the dictionary `generators`.
    Then, to get `v`, we need only call the appropriate method indexing by `kind`
    and passing the number of samples (`N`), which defaults to 1.
  prefs: []
  type: TYPE_NORMAL
- en: With `v` in hand, we modify it according to the selected configuration. If we
    want floating-point, we multiply by `high` and add `low`, which default to 1 and
    0, respectively. This returns a float in the range [low, high).
  prefs: []
  type: TYPE_NORMAL
- en: For integers, we first map `v` to [0, high) before adding `low` to give an integer
    in the range [low, high). The upper limit is not present in both cases, thereby
    following Python convention.
  prefs: []
  type: TYPE_NORMAL
- en: To get bytes, multiply by 256 and round. Finally, to get bits, round `v`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fetch**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'What about `Fetch`, you ask? It’s a mix of `Urandom` and `random`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The code is split into three paragraphs. The first calculates the number of
    bytes to read from the disk file (`nbytes`). As with `Urandom`, we’re restricting
    ourselves to 32-bit floats.
  prefs: []
  type: TYPE_NORMAL
- en: The second paragraph reads the bytes from disk and stores them in `b`. If we
    run out of file, we start again from the beginning. Remember this to ensure you
    don’t ask for far more samples than the file can supply. Think of the size of
    the file divided by four as the period of the generator.
  prefs: []
  type: TYPE_NORMAL
- en: The third paragraph massages the data accordingly. If we want bytes, we’re done;
    we simply convert the list `b` into a NumPy vector and return it. Otherwise, we
    first treat the bytes as a buffer and read them as unsigned 32-bit integers, which
    we divide by 2^(32) to make `v` in the range [0, 1). We then convert the data
    to the final output format as in the `random` method. There is some code duplicated
    between `random` and `Fetch`, but pedagogically, the clarity this provides is
    worth it.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Constructor**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The `RE` class constructor configures the randomness engine. The first part
    defines defaults and builds the dictionary of private methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `mode` defines what `RE` returns. It’s a string: `float`, `int`, `byte`,
    or `bit`. Use `kind` to define the source. Possible values are the keys of `generators`
    or a pathname to a disk file. Case matters here.'
  prefs: []
  type: TYPE_NORMAL
- en: Use `low` and `high` to set the output range, ignored for bytes and bits. Remember,
    the output does not include `high`. To get deterministic sequences, set the `seed`.
    Finally, use `base` if working with a quasirandom sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second part handles source-specific things:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: I leave parsing the second part to you. I’ll just point out that for quasi-random
    sequences, the seed value is used to set the initial Halton sequence value, which
    is random if a negative seed is given. Use a negative seed to spice up the deterministic
    Halton sequence with a bit of randomness.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’re finished with the implementation of `RE`. Now let’s learn how
    to use it.
  prefs: []
  type: TYPE_NORMAL
- en: '***RE Class Examples***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following examples illustrate how we’ll use the `RE` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first example imports `RE` and defines a generator using all the defaults—PCG64
    with floating-point output in [0, 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The second example creates an instance of the MT19937 generator and uses it
    to return five floating-point values in [–3, 5):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use `urandom` to return integers in [–3, 5) and use `RDRAND` to sample
    bytes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s an example that specifies a seed value to return the same sequence each
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'For a quasirandom sequence, if no seed is given, the sequence begins at zero
    each time. If the seed is less than zero, the starting position is set randomly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This example sets `kind` to a filename to sample from a disk file, here called
    *hotbits.bin*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know how to use `RE`, let’s start experimenting.
  prefs: []
  type: TYPE_NORMAL
- en: '### **Summary**'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focused on generating randomness, that is, on random processes.
    First, we explored the relationship between probability and randomness and learned
    that random processes sample from probability distributions, either continuous
    or discrete. We learned that there is generally no concrete answer to the question,
    How do we know if the output of a process is random? However, there are ways to
    test sequences to enhance our belief one way or the other. In practice, we declared
    the output of `ent` as our standard, since our experiments do not need state-of-the-art
    random processes.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we discussed truly random processes. We began with classical approaches
    like coin flips and dice rolls, and then shifted our attention to physical processes
    such as random fluctuations in an analog signal, radio frequency noise due to
    atmospheric effects, and the decay of radioactive elements.
  prefs: []
  type: TYPE_NORMAL
- en: Pseudorandom and quasirandom sequences are random process mimics. Though they
    pretend to be the output of truly random processes, they are not. However, they
    do shadow truly random processes with sufficient fidelity to make them the primary
    drivers for our experiments. After learning the basics of pseudorandom and quasirandom
    generators, we covered hybrid generators, the marriage of pseudorandomness and
    truly random processes. Hybrid generators provide cryptographically secure sequences.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter concluded with a walkthrough of the design and code for the `RE`
    class, the randomness engine that will power all of our experiments.
  prefs: []
  type: TYPE_NORMAL
