<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
	<head>
		<title>Chapter 9: Going On-Call</title>
		<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:7db14923-61d0-434f-baa0-3e20bf74259e" name="Adept.expected.resource"/>
	</head>
	<body epub:type="bodymatter chapter">
		<section>
			<header>
				<h1 class="chapter"><span class="ChapterNumber"><span epub:type="pagebreak" id="Page_149" title="149"/>9</span><br/><span class="ChapterTitle">Going On-Call</span></h1>
			</header>
			<p class="BodyFirst"><span class="DropCap">M</span>any companies ask engineers to go on-call. On-call engineers are the first line of defense for any unplanned work, be it production issues or ad hoc support requests. Separating deep work from operational work lets the majority of the team focus on development while on-call engineers focus only on unpredictable operational issues and support tasks. Effective on-call engineers are prized by their teammates and managers, and they grow quickly from the relationship-building and learning opportunities that on-call rotations provide.</p>
			<p>This chapter covers the basic knowledge and skills you’ll need to participate in on-call, incident handling, and support. We’ll explain how on-call rotations work and teach you important on-call skills. Then, we’ll go in-depth on a real-world incident to give you a practical example of how an incident is handled. After incidents, we’ll teach you support best practices. The on-call experience can cause burnout, so we end with a warning about the temptation to be a hero.</p>
			<p><span epub:type="pagebreak" id="Page_150" title="150"/>If you are in a role where an on-call rotation does not exist, read this chapter anyway. On-call skills apply in any urgent situation.</p>
			<h2 id="h1-501836c09-0001">How On-Call Works</h2>
			<p class="BodyFirst">On-call developers rotate based on a schedule. The length of a rotation can be as short as a day, though more often it’s a week or two. Every qualified developer takes part in the rotation. Developers who are new to the team or lack necessary skills are often asked to “shadow” a few primary on-call rotations to learn the ropes.</p>
			<p>Some schedules have a primary and a secondary on-call developer; the secondary acts as a backup when the primary is unavailable. (Needless to say, developers who routinely cause the secondary on-call to step in are not looked upon kindly.) Some organizations have a tiered response structure: the support team might get alerted first, and then the issue would get escalated to operations engineers, followed by the development team.</p>
			<p>Most of an on-call’s time is spent fielding ad hoc support requests such as bug reports, questions about how their team’s software behaves, and usage questions. On-calls triage these requests and respond to the most urgent.</p>
			<p>However, every on-call will eventually be hit with an operational incident (critical problem with production software). An incident is reported to on-call by an alert from an automated monitoring system or by a support engineer who manually observes a problem. On-call developers must triage, mitigate, and resolve incidents.</p>
			<p>
				On-call developers get paged when critical alerts fire. <em>Paging</em> is an anachronism from before cell phones—these days, an alert is routed through channels such as chat, email, phone calls, or text messages. Make sure you enter the alerting service’s phone number into your contacts if, like us, you don’t answer calls from unknown numbers!</p>
			<p>
				All on-call rotations should begin and end with a handoff. The previous on-call developer summarizes any current operational incidents <span epub:type="pagebreak" id="Page_151" title="151"/>and provides context for any open tasks to the next on-call developer. If you’ve tracked your work well, handoffs are a nonevent.</p>
			<h2 id="h1-501836c09-0002">Important On-Call Skills</h2>
			<p class="BodyFirst">On-call can be a rapid-fire, high-stress experience. Luckily, you can apply a common set of skills to handle both incidents and support requests. You’ll need to make yourself available and be on the lookout for incidents. You’ll also need to prioritize work so the most critical items get done first. Clear communication will be essential, and you’ll need to write down what you’re doing as you go. In this section, we’ll give you some tips to help you grow these skills.</p>
			<h3 id="h2-501836c09-0001">Make Yourself Available</h3>
			<p class="BodyFirst">“Your best ability is availability.” This old saying is key to a successful on-call. An on-call’s job is to respond to requests and alerts. Don’t ignore requests or try to hide. Expect to be interrupted, and accept that you can’t do as much deep work while on-call.</p>
			<p>Some on-call developers are expected to be near a computer 24/7 (though this doesn’t mean staying awake all night waiting for the alerts to fire. It means you’re reachable, able to react, and can adjust your nonwork plans accordingly). Larger companies have “follow the sun” on-call rotations that shift to developers in different time zones as the day goes on. Figure out what on-call expectations are, and don’t get caught in a situation where you can’t respond.</p>
			<p>Being available does not mean immediately dropping whatever you are doing to address the latest question or problem. For many requests, it’s perfectly acceptable to acknowledge that you’ve received the query and respond with an approximate time when you should be able to look at the problem: “I am currently assisting someone else; can I get back to you in 15 minutes?” A fast response is generally expected from the on-call engineer, but not necessarily a fast resolution.</p>
			<h3 id="h2-501836c09-0002"><span epub:type="pagebreak" id="Page_152" title="152"/>Pay Attention</h3>
			<p class="BodyFirst">Information relevant to on-call work comes in through many channels: chat, email, phone calls, text messages, tickets, logs, metrics, monitoring tools, and even meetings. Pay attention to these channels so you’ll have context when debugging and troubleshooting.</p>
			<p>Proactively read release notes and chat or email channels that list operational information like software deployments or configuration changes. Keep an eye on chat rooms in which operations teams discuss unusual observations and announce adjustments they are making. Read meeting notes, particularly operational scrum digests that track ongoing incidents and maintenance for the day. Keep operational dashboards up in the background or on a nearby TV so you can establish a baseline for normal behavior. When incidents do occur, you’ll be able to tell which graphs look odd.</p>
			<p>Create a list of resources that you can rely on in an emergency: direct links to critical dashboards and runbooks for your services, instructions for accessing logs, important chat rooms, and troubleshooting guides. A separate “on-call” bookmark folder that you keep up-to-date will come in handy. Share your list with the team so others can use and improve it.</p>
			<h3 id="h2-501836c09-0003">Prioritize Work</h3>
			<p class="BodyFirst">Work on the highest-priority tasks first. As tasks are finished or become blocked, work your way down the list from highest to lowest priority. As you work, alerts will fire, and new questions will come in. Quickly triage the interruption: either set it aside or begin working on it if it’s an emergency. If the new request is higher priority than your current task but isn’t critical, try to finish your current task, or at least get it to a good stopping point before context switching.</p>
			<p>Some support requests are extremely urgent, while others are fine getting done in a week. If you can’t tell how urgent a request is, ask what the impact of the request is. The impact will determine the priority. If you disagree with the requestor about an issue’s prioritization, discuss it with your manager.</p>
			<p><span epub:type="pagebreak" id="Page_153" title="153"/>On-call work is categorized by priority: P0, P1, P2, and so on. Prioritizing work into categories helps define a task’s urgency. Category names and meaning vary by company, but P0 tasks are the big ones. Google Cloud’s support priority ladder offers one example of how priority levels may be defined (<a class="LinkURL" href="https://cloud.google.com/support/docs/best-practice#setting_the_priority_and_escalating/">https://cloud.google.com/support/docs/best-practice#setting_the_priority_and_escalating/</a>):</p>
			<ul>
				<li>P1: Critical Impact—Service Unusable in Production</li>
				<li>P2: High Impact—Service Use Severely Impaired</li>
				<li>P3: Medium Impact—Service Use Partially Impaired</li>
				<li>P4: Low Impact—Service Fully Usable</li>
			</ul>
			<p>
				Service level indicators, objectives, and agreements also help prioritize operational work. <em>Service level indicators</em><em> (SLIs</em><em>)</em> such as error rate, request latency, and requests per second are the easiest way to see if an application is healthy. <em>Service level objectives</em><em> (SLOs</em><em>)</em> define SLI targets for healthy application behavior. If error rate is an SLI for an application, an SLO might be a request error rate less than 0.001 percent. <em>Service level agreements</em><em> (SLAs</em><em>)</em> are agreements about what happens when an SLO is missed. (Companies that violate SLAs with their customers usually need to return money and may even face contract termination.) Learn the SLIs, SLOs, and SLAs for your applications. SLIs will point you to the most important metrics. SLOs and SLAs will help you prioritize incidents.</p>
			<h3 id="h2-501836c09-0004">Communicate Clearly</h3>
			<p class="BodyFirst">Clear communication is critical when dealing with operational tasks. Things happen quickly, and miscommunication can cause major problems. To communicate clearly, be polite, direct, responsive, and thorough.</p>
			<p>Under a barrage of operational tasks and interruptions, developers get stressed and grumpy—it’s human nature. Be patient and polite when responding to support tasks. While it might be your 10th interruption of the day, it’s the requestor’s first interaction with you.</p>
			<p><span epub:type="pagebreak" id="Page_154" title="154"/>Communicate in concise sentences. It can feel uncomfortable to be direct, but being direct doesn’t mean being rude. Brevity ensures that your communication is read and understood. If you don’t know an answer, say so. If you do know the answer, speak up.</p>
			<p>Respond to requests quickly. Responses don’t have to be solutions. Tell the requestor that you’ve seen their request, and make sure you understand the problem:</p>
			<blockquote class="review trade">
				<p class="Blockquote">Thanks for reaching out. To clarify: the login service is getting 503 response codes from the profile service? You’re not talking about auth, right? They’re two separate services, but confusingly named.</p>
			</blockquote>
			<p>Post status updates periodically. Updates should include what you’ve found since your last update and what you’re planning on doing next. Every time you make an update, provide a new time estimate:</p>
			<blockquote class="review trade">
				<p class="Blockquote">I looked at the login service. I don’t see a spike in error rate, but I’ll take a look at the logs and get back to you. Expect an update in an hour.</p>
			</blockquote>
			<h3 id="h2-501836c09-0005">Track Your Work</h3>
			<p class="BodyFirst">Write down what you’re doing as you work. Each item that you work on while on-call should be in an issue tracker or the team’s on-call log. Track progress as you work by writing updates in each ticket. Include the final steps that mitigated or resolved the issue in the ticket so you’ll have the solution documented if the issue appears again. Tracking progress reminds you where you left off when you come back to a ticket after an interruption. The next on-call will be able to see the state of ongoing work by reading your issues, and anyone you ask for help can read the log to catch up. Logged questions and incidents also create a searchable knowledge base that future on-calls can refer to.</p>
			<p>
				Some companies use chat channels like Slack for operational incidents and support. Chat is a good way to communicate, but chat transcripts are hard to read later, so make sure to summarize everything in a <span epub:type="pagebreak" id="Page_155" title="155"/>ticket or document. Don’t be afraid to redirect support requests to appropriate channels. Be direct: “I’ll start looking into this right now. Could you open a ticket so this is counted when we evaluate our support workload?”</p>
			<p>Close finished issues so dangling tickets don’t clutter on-call boards and skew on-call support metrics. Ask the requestor to confirm that their issue has been addressed before closing their ticket. If a requestor isn’t responding, say that you’re going to close the ticket in 24 hours due to lack of response; then do so.</p>
			<p>Always include timestamps in your notes. Timestamps help operators correlate events across the system when debugging issues. Knowing that a service was restarted at 1 PM is useful when customers begin reporting latency at 1:05 PM.</p>
			<h2 id="h1-501836c09-0003">Handling Incidents</h2>
			<p class="BodyFirst">Incident handling is an on-call’s most important responsibility. Most developers think handling an incident is about fixing a production problem. Resolving the problem is important, but in a critical incident, the top objective is to mitigate the impact of the problem and restore service. The second objective is to capture information to later analyze how and why the problem happened. Determining the cause of the incident, proving it to be the culprit, and fixing the underlying problem is only your <em>third</em> priority.</p>
			<p>Incident response is broken into these five steps:</p>
			<ol class="decimal">
				<li value="1"><b>Triage</b>: Engineers must find the problem, decide its severity, and determine who can fix it.</li>
				<li value="2"><b>Coordination</b>: Teams (and potentially customers) must be notified of the issue. If the on-call can’t fix the problem themselves, they must alert those who can.</li>
				<li value="3"><b>Mitigation</b>: Engineers must get things stable as quickly as possible. Mitigation is not a long-term fix; you are just trying to <span epub:type="pagebreak" id="Page_156" title="156"/>“stop the bleeding.” Problems can be mitigated by rolling back a release, failing over to another environment, turning off misbehaving features, or adding hardware resources.</li>
				<li value="4"><b>Resolution</b>: After the problem is mitigated, engineers have some time to breathe, think, and work toward a resolution. Engineers continue to investigate the problem to determine and address underlying issues. The incident is resolved once the immediate problem has been fixed.</li>
				<li value="5"><b>Follow-up</b>: An investigation is conducted into the root cause—why it happened in the first place. If the incident was severe, a formal postmortem, or retrospective, is conducted. Follow-up tasks are created to prevent the root cause (or causes) from happening again. Teams look for any gaps in process, tooling, or documentation. The incident is not considered done until all follow-up tasks have been completed.</li>
			</ol>
			<p>
				The phases of an incident can sound abstract. To make things clear, we’ll walk you through a real incident and point out the different phases as we go. The incident occurs when data fails to load into a data warehouse. <em>Data warehouses</em> are databases meant to serve analytical queries for reports and machine learning. This particular data warehouse is kept up-to-date by a stream of updates in a real-time messaging system. Connectors read messages from the streaming system and write them into the warehouse. The data in the data warehouse is used by teams across the company for both internal and customer-facing reports, machine learning, application debugging, and more.</p>
			<h3 id="h2-501836c09-0006">Triage</h3>
			<p class="BodyFirst">Determine a problem’s priority by looking at its impact: How many people is it affecting, and how detrimental is it? Use your company’s prioritization categories and SLO/SLA definitions to prioritize the issue, with the help of SLIs and the metric that triggered the alert, if applicable.</p>
			<aside epub:type="sidebar">
				<div class="top hr">
					<hr/>
				</div>
				<section class="box trade">
					<h2><span epub:type="pagebreak" id="Page_157" title="157"/>Real-World Example</h2>
					<p class="BoxBodyFirst">The operations team gets paged when monitors detect data in the messaging system that isn’t in the data warehouse. The incident triggers the first on-call step: <em>triage</em> (acknowledging the issue and understanding its impact so it can be prioritized properly). The on-call engineer acknowledges the page and begins investigating the problem to determine the priority. Since the alert shows data is missing in tables used to generate customer reports, they deem the issue high priority.</p>
					<p>The triage phase has now ended. The engineer acknowledged the alert and determined the priority but did not try to solve the problem; they simply looked to see which tables were impacted.</p>
					<div class="bottom hr">
						<hr/>
					</div>
				</section>
			</aside>
			<p>If you’re having trouble determining issue severity, ask for help. Triage is not the time to prove you can figure things out on your own; time is of the essence.</p>
			<p>Likewise, triage is not the time to troubleshoot problems. Your users will continue to suffer while you troubleshoot. Save troubleshooting for the mitigation and resolution phases.</p>
			<h3 id="h2-501836c09-0007">Coordination</h3>
			<aside epub:type="sidebar">
				<div class="top hr">
					<hr/>
				</div>
				<section class="box trade">
					<h2>Real-World Example</h2>
					<p class="BoxBodyFirst">At this point, the on-call engineer switches into <em>coordination</em> mode. They post an announcement in the operations chat channel stating that they are seeing a gap in data for customer-facing tables. A cursory investigation shows that the connector that loads data into the data warehouse is running, and the logs don’t indicate any faults.</p>
					<p><span epub:type="pagebreak" id="Page_158" title="158"/>The on-call engineer asks for help from some of the connector developers and pulls in another engineer with connector experience. The engineering manager that the on-call engineer reports to steps in to act as the incident manager. The team sends out an email to the company notifying everyone that data is missing from the warehouse for several tables. The incident manager works with account management and operations to post a notice on the customer-facing status page.</p>
					<div class="bottom hr">
						<hr/>
					</div>
				</section>
			</aside>
			<p>
				Coordination starts by figuring out who’s in charge. For lower-priority incidents, the on-call is in charge and will coordinate. For larger incidents, an <em>incident commander</em> will take charge. Commanders keep track of who is doing what and what the current state of the investigation is.</p>
			<p>Once someone takes charge, all relevant parties must be notified of the incident. Contact everyone needed to mitigate or resolve the problem—other developers or SREs. Internal stakeholders such as technical account managers, product managers, and support specialists might need to be notified. Impacted users might need to be alerted through status pages, emails, Twitter alerts, and so on.</p>
			<p>
				Many different conversations will happen in parallel, which makes it difficult to follow what’s happening. Large incidents have war rooms to help with communication. <em>War rooms</em> are virtual or physical spaces used to coordinate incident response. All interested parties join the war room to coordinate response.</p>
			<p>
				Track communication in written form in a central location: a ticketing system or chat. Communicating helps everyone track progress, saves you from constantly answering status questions, prevents duplicate work, and enables others to provide helpful suggestions. Share both your observations and your actions, and state what you are about to do before you do it. Communicate your work even if you are working <span epub:type="pagebreak" id="Page_159" title="159"/>alone—someone might join later and find the log helpful, and a detailed record will help to reconstruct the timeline afterward.</p>
			<h3 id="h2-501836c09-0008">Mitigation</h3>
			<aside epub:type="sidebar">
				<div class="top hr">
					<hr/>
				</div>
				<section class="box trade">
					<h2>Real-World Example</h2>
					<p class="BoxBodyFirst">While notifications are sent, the engineers move on to <em>mitigation</em>. The decision is made to bounce (restart) the connector to see if it becomes unwedged (unstuck), but the issue remains. A stack dump shows the connector reading and deserializing (decoding) messages. The machine running the connector has a completely saturated CPU (100 percent usage), so the engineers guess that the connector is getting stuck on a large or corrupt message that is causing it to chew up all of the CPU during deserialization.</p>
					<p>The engineers decide to try to mitigate the problem by running a second connector with just the known good streams. There are 30 streams, and the engineers don’t know which streams have bad messages. They decide to binary search to find the corrupt streams: half are added, and then the set is adjusted based on the connector’s behavior. Eventually, the team finds the stream that is causing the problem. The connector is restarted with all the healthy streams, and their table data catches up. The impact of the problem is now limited to a single stream and table.</p>
					<div class="bottom hr">
						<hr/>
					</div>
				</section>
			</aside>
			<p>Your goal in the mitigation phase is to reduce the problem’s impact. Mitigation isn’t about fixing the problem; it’s about reducing its severity. Fixing a problem can take a lot of time, while mitigating it can usually be done quickly.</p>
			<p>
				Incidents are commonly mitigated by rolling back a software release to a “last known good” version or by shifting traffic away from the problem. Depending on the situation, mitigation might involve turning off <span epub:type="pagebreak" id="Page_160" title="160"/>a feature flag, removing a machine from a pool, or rolling back a just-deployed service.</p>
			<p>
				Ideally, the software you’re working with will have a runbook for the problem. <em>Runbooks</em> are predefined step-by-step instructions to mitigate common problems and perform actions such as restarts and rollbacks. Make sure you know where runbooks and troubleshooting guides can be found.</p>
			<p>Capture what data you can as you work to mitigate the problem. Once mitigated, the problem might be hard to reproduce. Quickly saving telemetry data, stack traces, heap dumps, logs, and screenshots of dashboards will help with debugging and root-cause analysis later.</p>
			<p>You’ll often find gaps in metrics, tooling, and configuration while trying to mitigate the problem. Important metrics might be missing, incorrect permissions might be granted, or systems might be misconfigured. Quickly write down any gaps that you find—anything that would have made your life better while troubleshooting. Open tickets during the follow-up phase to address these gaps.</p>
			<h3 id="h2-501836c09-0009">Resolution</h3>
			<aside epub:type="sidebar">
				<div class="top hr">
					<hr/>
				</div>
				<section class="box trade">
					<h2>Real-World Example</h2>
					<p class="BoxBodyFirst">The engineers now drive to a <em>resolution</em>. One stream appears to have a bad message, and the data warehouse table for this stream still isn’t getting data.</p>
					<p>Engineers remove all healthy streams from the original connector to try to reproduce the issue. The team can now see the message that the connector is stuck on, so they manually read the message using a command line tool. Everything looks fine.</p>
					<p>
						At this point, the team has an epiphany—how is it that the command line tool can read the message but the connector can’t? It appears that the connector includes some code that isn’t used in the command line tool—a fancy date deserializer. The date <span epub:type="pagebreak" id="Page_161" title="161"/>deserializer infers a message header’s data type (integer, string, date, and so on) using complicated logic. The command line tool doesn’t print message headers by default. Engineers rerun the tool with message headers enabled and discover that the bad message has a header with a single key but an empty value.</p>
					<p>
						The header’s key hints that the message header is injected by an <em>application performance management (APM)</em> tool. APMs sit inside applications and tell developers about the behavior of a running application: memory usage, CPU usage, and stack traces. Unbeknownst to the engineers, the APM daemon is injecting headers into all messages.</p>
					<p>The team contacts outside support. Support engineers for the streaming system tell the engineers that the command line tool has a bug: it won’t print message headers that contain a null-terminated byte string. The engineers believe there are bytes in headers that are causing the type inference to get stuck.</p>
					<p>The team disables header decoding in the connector to test the theory. Data for the last remaining table quickly loads into the data warehouse. All tables are now up-to-date, and the monitors begin to pass their data quality checks. The team notifies the support channel of the resolution and updates the customer-facing status page.</p>
					<div class="bottom hr">
						<hr/>
					</div>
				</section>
			</aside>
			<p>Once mitigation is complete, the incident is no longer an emergency. You can take time to troubleshoot and resolve the underlying issues. In our example incident, the priority was dropped once the customer-facing streams recovered. This gave breathing room to the engineers so they could investigate the problem.</p>
			<p>During the resolution phase, focus on the immediate technical problems. Focus on what is needed to recover without the temporary measures put in place during mitigation. Set aside larger technical and process problems for the follow-up phase.</p>
			<p><span epub:type="pagebreak" id="Page_162" title="162"/>Use the scientific method to troubleshoot technical problems. Chapter 12 of Google’s <em>Site Reliability Engineering</em> book offers a <em>hypothetico-deductive</em> model of the scientific method. Examine the problem, make a diagnosis, and then test and treat. If the treatment is successful, the problem is cured; if not, you reexamine and start again. The team in our example applied the scientific method when they formed a hypothesis that the connector was having deserialization issues and not dropping data. They looked at metric data and did their binary-search experiment to find the bad stream. If they came up empty-handed, the team would need to reformulate a new hypothesis.</p>
			<p>Ideally, you can quarantine a misbehaving program instance and examine its misbehavior. The engineers in our connector example did this when they isolated the bad stream to a separate connector. Your goal during resolution is to understand the symptoms of the problem and try to make it reproducible. Use all the operational data at your disposal: metrics, logs, stack traces, heap dumps, change notifications, issue tickets, and communications channels.</p>
			<p>
				Once you have a clear view of the symptoms, diagnose the problem by looking for the causes. Diagnosis is a search, and like any search, you can use search algorithms to troubleshoot. For small problems, a linear search—examining components front to back—is fine. Use divide and conquer or a binary search (also called <em>half-splitting</em>) on bigger systems. Find a point halfway through the call stack and see if the problem is upstream or downstream of the issue. If the problem is upstream, pick a new component halfway upstream; if it’s downstream, do the reverse. Keep iterating until you find the component where you believe the problem is occurring.</p>
			<p>
				Next, test your theory. Testing isn’t treatment—you’re not fixing the problem yet. Instead, see if you can control the bad behavior. Can you reproduce it? Can you change a configuration to make the problem go away? If so, you’ve located the cause. If not, you’ve eliminated one potential cause—go back, reexamine, and formulate a new diagnosis to test. <span epub:type="pagebreak" id="Page_163" title="163"/>Once the team in the connector example believed they had narrowed the problem to a header deserialization issue, they tested their theory by disabling header decoding in the connector configuration.</p>
			<p>After a successful test, you can decide on the best course of treatment. Perhaps a configuration change is all that’s needed. Often, a bug fix will need to be written, tested, and applied. Apply the treatment and verify that it’s working as expected. Keep an eye on metrics and logs until you’re convinced everything is stable.</p>
			<h3 id="h2-501836c09-0010">Follow-Up</h3>
			<aside epub:type="sidebar">
				<div class="top hr">
					<hr/>
				</div>
				<section class="box trade">
					<h2>Real-World Example</h2>
					<p class="BoxBodyFirst">The engineering manager responsible for the connector schedules <em>follow-up</em> work, and the on-call engineer writes a postmortem draft document. A postmortem meeting is scheduled. Eventually, through the postmortem process, tickets are opened to investigate why the APM was using message headers, why the connect consumer couldn’t deserialize them, and why the manual consumer couldn’t print headers with null strings.</p>
					<div class="bottom hr">
						<hr/>
					</div>
				</section>
			</aside>
			<p>
				Incidents are a big deal, so they need follow-up. The goal is to learn from the incident and to prevent it from happening again. A <em>postmortem</em> document is written and reviewed, and tasks are opened to prevent recurrence.</p>
			<aside epub:type="sidebar">
				<div class="top hr">
					<hr/>
				</div>
				<section class="note">
					<h2><span class="NoteHead">Note</span></h2>
					<p>
						The term <em>postmortem</em> is borrowed from the medical field, where an <em>after</em><em>-</em><em>death</em> examination is conducted and written up when a patient dies. Fortunately, in our case, stakes are less dire. A perfectly acceptable alternative term is <em>retrospective</em>, which has the added benefit of being the term we use for other after-the-fact discussions, like the sprint retrospectives we discuss in Chapter 12.</p>
					<div class="bottom hr">
						<hr/>
					</div>
				</section>
			</aside>
			<p><span epub:type="pagebreak" id="Page_164" title="164"/>The on-call engineer who dealt with the incident is responsible for drafting a postmortem document, which should capture what happened, what was learned, and what needs to be done to prevent the incident from happening again. There are many approaches and templates for writing a postmortem. One good example is Atlassian’s postmortem template (<a class="LinkURL" href="https://www.atlassian.com/incident-management/postmortem/templates/">https://www.atlassian.com/incident-management/postmortem/templates/</a>). The template has sections and examples describing the lead-up, fault, impact, detection, response, recovery, timeline, root cause, lessons learned, and corrective actions needed.</p>
			<p>
				A critical section of any postmortem document is the <em>root-cause analysis (RCA)</em> section. Root-cause analysis is performed using the five whys. This technique is pretty simple: keep asking why. Take a problem and ask why it happened. When you get an answer, ask why again. Keep asking why until you get to the root cause. The “five” is anecdotal—most problems take about five iterations to get to the root cause.</p>
			<p class="ListHead"><b>Problem: Data Missing from Data Warehouse</b></p>
			<ol class="decimal">
				<li value="1"><b>Why?</b> The connector wasn’t loading data into the data warehouse.</li>
				<li value="2"><b>Why?</b> The connector couldn’t deserialize incoming messages.</li>
				<li value="3"><b>Why?</b> The incoming messages had bad headers.</li>
				<li value="4"><b>Why?</b> The APM was inserting headers into the messages.</li>
				<li value="5"><b>Why?</b> The APM defaulted to this behavior without developer knowledge.</li>
			</ol>
			<p>In this example, the root cause was the APM’s accidental message header configuration.</p>
			<aside epub:type="sidebar">
				<div class="top hr">
					<hr/>
				</div>
				<section class="note">
					<h2><span class="NoteHead">Note</span></h2>
					<p>Root-cause analysis is a popular but misleading term. Incidents are rarely caused by a single issue. In practice, the five whys might lead to many different causes. This is fine; just document everything.</p>
					<div class="bottom hr">
						<hr/>
					</div>
				</section>
			</aside>
			<p><span epub:type="pagebreak" id="Page_165" title="165"/>After a postmortem document is written, one of the managers or tech leads schedules a review meeting with all interested parties. The postmortem document author leads the review, and participants discuss each section in detail. The author adds missing information and new tasks as they’re discovered during discussion.</p>
			<p>It’s easy to get upset and cast blame in high-stress situations. Do your best to provide constructive feedback. Point out areas for improvement, but avoid blaming individuals or teams for problems. “Peter didn’t disable message headers” assigns blame, while “Message header config changes aren’t going through code review” is an area for improvement. Don’t let postmortems turn into unhealthy vent fests.</p>
			<p>
				Good postmortem meetings also keep “solutioning” separate from the review meeting. <em>Solutioning</em>—figuring out how to solve a problem—takes a long time and distracts from the purpose of the meeting: to discuss problems and assign tasks. “The message had a bad header” is a problem, while “We should put bad messages in a dead letter queue” is a solution. Any solutioning should happen in follow-up tasks.</p>
			<p>After a postmortem meeting, follow-up tasks must be completed. If tasks are assigned to you, work with your manager and the postmortem team to prioritize them properly. An incident can’t be closed until all remaining follow-up tasks have been finished.</p>
			<p>
				Old postmortem documents are a great way to learn. Some companies even share their postmortem documents publicly as valuable resources for the whole community to learn from. Look at Dan Luu’s collection for inspiration (<a class="LinkURL" href="https://github.com/danluu/post-mortems">https://github.com/danluu/post-mortems</a>). You might find postmortem reading groups at your company, too. Teams get together to review postmortem documents with a wider audience. Some teams even use old postmortem documents to simulate production issues to train new engineers.</p>
			<h2 id="h1-501836c09-0004"><span epub:type="pagebreak" id="Page_166" title="166"/>Providing Support</h2>
			<p class="BodyFirst">When on-call engineers aren’t dealing with incidents, they spend time handling support requests. These requests come both from within the organization and from external customers, and they run the gamut from simple “Hey, how does this work?” questions to difficult troubleshooting problems. Most requests are bug reports, questions about business logic, or technical questions about how to use your software.</p>
			<p>Support requests follow a pretty standard flow. When a request comes in, you should acknowledge that you’ve seen it and ask questions to make sure you understand the problem. Once you’ve got a grasp on the problem, give a time estimate on the next update: “I’ll get back to you by 5 PM with an update.” Next, start investigating, and update the requestor as you go. Follow the same mitigation and resolution strategies that we outlined earlier. When you think the issue is resolved, ask the requestor to confirm. Finally, close out the request. Here’s an example:</p>
			<blockquote class="blockquote trade">
				<p class="Blockquote">[3:48 PM] Sumeet: I’m getting reports from customers saying page loads are slow.</p>
				<p class="Blockquote">[4:12 PM] Janet: Hi, Sumeet. Thanks for reporting this. Can I get a few more pieces of information? Can you give me one or two customer IDs that reported slowness and any particular pages they are seeing the issue on? Our dashboards aren’t showing any widespread latency.</p>
				<p class="Blockquote">[5:15 PM] Sumeet: Customer IDs 1934 and 12305. Pages: the ops main page (/ops) and the APM chart (/ops/apm/dashboard). They’re saying loads are taking &gt; 5 seconds.</p>
				<p class="Blockquote">[5:32 PM] Janet: Great, thanks. I’ll have an update for you by 10 tomorrow morning.</p>
				<p class="Blockquote">[8:15 AM] Janet: Okay, I think I know what’s going on. We had maintenance on the database that powers the APM dashboard yesterday afternoon. It impacted the ops main page since we show a roll-up there, <span epub:type="pagebreak" id="Page_167" title="167"/>too. The maintenance finished around 8 PM. Can you confirm that the customer is no longer seeing the issue?</p>
				<p class="Blockquote">[9:34 AM] Sumeet: Awesome! Just confirmed with a couple of the customers who reported the issue. Page loads are now much better.</p>
			</blockquote>
			<p>
				This example illustrates many practices covered in “Important On-Call Skills.” Janet, the on-call, is <em>paying attention</em> and <em>makes herself available</em>; she responds within a half hour of the first request. Janet <em>communicates clearly</em> by asking clarifying questions to understand the problem and its impact so she can properly <em>prioritize the issue</em>. She posts an ETA for the next update when she has enough information to investigate. Once Janet believes the problem is solved, she <em>tracks her work </em>by describing what happened and asks the requestor to confirm it’s not an issue anymore.</p>
			<p>Support can feel like a distraction, since your “real” job is programming. Think of support as an opportunity to learn. You’ll get to see how your team’s software is used in the real world and the ways in which it fails or confuses users. Answering support requests will take you to parts of the code you were not familiar with; you’ll have to think hard and experiment. You will notice patterns that cause problems, which will help you create better software in the future. Support rotations will make you a better engineer. Plus, you get to help others and build relationships and your reputation. Fast, high-quality support responses do not go unnoticed.</p>
			<h2 id="h1-501836c09-0005">Don’t Be a Hero</h2>
			<p class="BodyFirst">We’ve spent quite a few words in this chapter encouraging you not to shy away from on-call responsibilities and ad hoc support requests. There is another extreme we want to warn you about: doing too much. On-call activities can feel gratifying. Colleagues routinely thank you for helping <span epub:type="pagebreak" id="Page_168" title="168"/>them with issues, and managers praise efficient incident resolution. However, doing too much can lead to burnout.</p>
			<p>For some engineers, jumping into “firefighting” mode becomes a reflex as they become more experienced. Talented firefighting engineers can be a godsend to a team: everyone knows that when things get tough, all they need to do is ask the firefighter, and they’ll fix it. Depending on a firefighter is not healthy. Firefighters who are pulled into every issue effectively become permanently on-call. The long hours and high stakes will cause burnout. Firefighter engineers also struggle with their programming or design work because they are constantly being interrupted. And teams that rely on a firefighter won’t develop their own expertise and troubleshooting abilities. Firefighter heroics can also cause fixes for serious underlying problems to be deprioritized because the firefighter is always around to patch things up.</p>
			<p>If you feel that you are the only one who can fix a problem or that you are routinely involved in firefighting when not on-call, you might be becoming a “hero.” Talk to your manager or tech lead about ways to find better balance and get more people trained and available to step in. If there’s a hero on your team, see if you can learn from them and pick up some of the burden; let them know when you are okay struggling a bit: “Thanks, Jen. I actually want to try to figure this out on my own for a bit so I can skill up . . . Can I ask for your help in 30 minutes if this is still a mystery?”</p>
			<h2 id="h1-501836c09-0006">Do’s and Don’ts</h2>
			<table border="1" class="trade" id="tabular-501836c09-0001">
				<thead>
					<tr>
						<td><b>Do’s</b></td>
						<td><b>Don’ts</b></td>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td><b>DO</b> add known “pager” numbers to your phone’s contacts.
						</td>
						<td><b>DON’T</b> ignore alerts.
						</td>
					</tr>
					<tr>
						<td><b>DO</b> use priority categories, SLIs, SLOs, and SLAs to prioritize incident response.
						</td>
						<td><b>DON’T</b> try to troubleshoot during triage.
						</td>
					</tr>
					<tr>
						<td><span epub:type="pagebreak" id="Page_169" title="169"/><b>DO</b> triage, coordinate, mitigate, resolve, and follow up on critical incidents.
						</td>
						<td><b>DON’T </b>leave a problem unmitigated while you search for the root cause.
						</td>
					</tr>
					<tr>
						<td><b>DO</b> use the scientific method to troubleshoot.
						</td>
						<td><b>DON’T</b> cast blame during postmortems.
						</td>
					</tr>
					<tr>
						<td><b>DO</b> ask “the five whys” when following up on an incident.
						</td>
						<td><b>DON’T</b> hesitate to close nonresponsive support requests.
						</td>
					</tr>
					<tr>
						<td><b>DO</b> acknowledge support requests.
						</td>
						<td><b>DON’T</b> ask support requestors what their priority is; ask about the impact of the problem.
						</td>
					</tr>
					<tr>
						<td><b>DO </b>give time estimates and periodic updates.
						</td>
						<td><b>DON’T</b> be a hero who has to fix all the things.
						</td>
					</tr>
					<tr>
						<td><b>DO</b> confirm a problem is fixed before closing a support request ticket.
						</td>
						<td/>
					</tr>
					<tr>
						<td><b>DO</b> redirect support requests to the appropriate communication channels.
						</td>
						<td/>
					</tr>
				</tbody>
			</table>
			<h2 id="h1-501836c09-0007">Level Up</h2>
			<p class="BodyFirst">The five phases of incident response in our “Handling Incidents” section come from an <em>Increment</em> article, “What Happens When the Pager Goes Off?” (<a class="LinkURL" href="https://increment.com/on-call/when-the-pager-goes-off/">https://increment.com/on-call/when-the-pager-goes-off/</a>). The article has more quotes and detail around how different companies handle incidents.</p>
			<p>
				In more nascent operations settings, developers might need to define SLIs and SLOs themselves. If you find yourself responsible for SLIs and SLOs, we highly recommend Chapter 4 of Google’s <em>Site Reliability Engineering</em> book.</p>
			<p>
				Chapters 11, 13, 14, and 15 of <em>Site Reliability Engineering</em> cover on-call, emergency response, incident handling, and postmortems. We’ve included the most important information for new engineers in our chapter, but Google’s book provides more detail if you want to go deeper.</p>
		</section>
	</body>
</html>