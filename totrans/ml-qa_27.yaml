- en: '**23'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DATA DISTRIBUTION SHIFTS**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What are the main types of data distribution shifts we may encounter after model
    deployment?
  prefs: []
  type: TYPE_NORMAL
- en: '*Data distribution shifts* are one of the most common problems when putting
    machine learning and AI models into production. In short, they refer to the differences
    between the distribution of data on which a model was trained and the distribution
    of data it encounters in the real world. Often, these changes can lead to significant
    drops in model performance because the model’s predictions are no longer accurate.'
  prefs: []
  type: TYPE_NORMAL
- en: There are several types of distribution shifts, some of which are more problematic
    than others. The most common are covariate shift, concept drift, label shift,
    and domain shift; all discussed in more detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: '**Covariate Shift**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose *p*(*x*) describes the distribution of the input data (for instance,
    the features), *p*(*y*) refers to the distribution of the target variable (or
    class label distribution), and *p*(*y*|*x*) is the distribution of the targets
    *y* given the inputs *x*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Covariate shift* happens when the distribution of the input data, *p*(*x*),
    changes, but the conditional distribution of the output given the input, *p*(*y*|*x*),
    remains the same.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 23-1](ch23.xhtml#ch23fig1) illustrates covariate shift where both the
    feature values of the training data and the new data encountered during production
    follow a normal distribution. However, the mean of the new data has changed from
    the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/23fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 23-1: Training data and new data distributions differ under covariate
    shift.*'
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we trained a model to predict whether an email is spam
    based on specific features. Now, after we embed the email spam filter in an email
    client, the email messages that customers receive have drastically different features.
    For example, the email messages are much longer and are sent from someone in a
    different time zone. However, if the way those features relate to an email being
    spam or not doesn’t change, then we have a covariate shift.
  prefs: []
  type: TYPE_NORMAL
- en: Covariate shift is a very common challenge when deploying machine learning models.
    It means that the data the model receives in a live or production environment
    is different from the data on which it was trained. However, because the relationship
    between inputs and outputs, *p*(*y*|*x*), remains the same under covariate shift,
    techniques are available to adjust for it.
  prefs: []
  type: TYPE_NORMAL
- en: A common technique to detect covariate shift is *adversarial validation*, which
    is covered in more detail in [Chapter 29](ch29.xhtml). Once covariate shift is
    detected, a common method to deal with it is *importance weighting*, which assigns
    different weights to the training example to emphasize or de-emphasize certain
    instances during training. Essentially, instances that are more likely to appear
    in the test distribution are given more weight, while instances that are less
    likely to occur are given less weight. This approach allows the model to focus
    more on the instances representative of the test data during training, making
    it more robust to covariate shift.
  prefs: []
  type: TYPE_NORMAL
- en: '**Label Shift**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Label shift*, sometimes referred to as *prior probability shift*, occurs when
    the class label distribution *p*(*y*) changes, but the class-conditional distribution
    *p*(*y*|*x*) remains unchanged. In other words, there is a significant change
    in the label distribution or target variable.'
  prefs: []
  type: TYPE_NORMAL
- en: As an example of such a scenario, suppose we trained an email spam classifier
    on a balanced training dataset with 50 percent spam and 50 percent non-spam email.
    In contrast, in the real world, only 10 percent of email messages are spam.
  prefs: []
  type: TYPE_NORMAL
- en: A common way to address label shifts is to update the model using the weighted
    loss function, especially when we have an idea of the new distribution of the
    labels. This is essentially a form of importance weighting. By adjusting the weights
    in the loss function according to the new label distribution, we are incentivizing
    the model to pay more attention to certain classes that have become more common
    (or less common) in the new data. This helps align the model’s predictions more
    closely with the current reality, improving its performance on the new data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Concept Drift**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Concept drift* refers to the change in the mapping between the input features
    and the target variable. In other words, concept drift is typically associated
    with changes in the conditional distribution *p*(*y*|*x*), such as the relationship
    between the inputs *x* and the output *y*.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the example of the spam email classifier from the previous section, the
    features of the email messages might remain the same, but *how* those features
    relate to whether an email is spam might change. This could be due to a new spamming
    strategy that wasn’t present in the training data. Concept drift can be much harder
    to deal with than the other distribution shifts discussed so far since it requires
    continuous monitoring and potential model retraining.
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain Shift**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The terms *domain shift* and *concept drift* are used somewhat inconsistently
    across the literature and are sometimes taken to be interchangeable. In reality,
    the two are related but slightly different phenomena. *Concept drift* refers to
    a change in the function that maps from the inputs to the outputs, specifically
    to situations where the relationship between features and target variables changes
    as more data is collected over time.
  prefs: []
  type: TYPE_NORMAL
- en: In *domain shift*, the distribution of inputs, *p*(*x*), and the conditional
    distribution of outputs given inputs, *p*(*y*|*x*), both change. This is sometimes
    also called *joint distribution shift* due to the joint distribution *p*(*x* and
    *y*) = *p*(*y*|*x*) *· p*(*x*). We can thus think of domain shift as a combination
    of both covariate shift and concept drift.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, since we can obtain the marginal distribution *p*(*y*) by integrating
    over the joint distribution *p*(*x*, *y*) over the variable *x* (mathematically
    expressed as *p*(*y*) = ∫ *p*(*x*, *y*) *dx*), covariate drift and concept shift
    also imply label shift. (However, exceptions may exist where the change in *p*(*x*)
    compensates for the change in *p*(*y*|*x*) such that *p*(*y*) may not change.)
    Conversely, label shift and concept drift usually also imply covariate shift.
  prefs: []
  type: TYPE_NORMAL
- en: To return once more to the example of email spam classification, domain shift
    would mean that the features (content and structure of email) *and* the relationship
    between the features and target both change over time. For instance, spam email
    in 2023 might have different features (new types of phishing schemes, new language,
    and so forth), and the definition of what constitutes spam might have changed
    as well. This type of shift would be the most challenging scenario for a spam
    filter trained on 2020 data, as it would have to adjust to changes in both the
    input data and the target concept.
  prefs: []
  type: TYPE_NORMAL
- en: Domain shift is perhaps the most difficult type of shift to handle, but monitoring
    model performance and data statistics over time can help detect domain shifts
    early. Once they are detected, mitigation strategies include collecting more labeled
    data from the target domain and retraining or adapting the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Types of Data Distribution Shifts**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 23-2](ch23.xhtml#ch23fig2) provides a visual summary of different types
    of data shifts in the context of a binary (2-class) classification problem, where
    the black circles refer to examples from one class and the diamonds refer to examples
    from another class.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/23fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 23-2: Different types of data shifts in a binary classification context*'
  prefs: []
  type: TYPE_NORMAL
- en: As noted in the previous sections, some types of distribution shift are more
    problematic than others. The least problematic among them is typically covariate
    shift. Here, the distribution of the input features, *p*(*x*), changes between
    the training and testing data, but the conditional distribution of the output
    given the inputs, *p*(*y*|*x*), remains constant. Since the underlying relationship
    between the inputs and outputs remains the same, the model trained on the training
    data can still apply, in principle, to the testing data and new data.
  prefs: []
  type: TYPE_NORMAL
- en: The most problematic type of distribution shift is typically joint distribution
    shift, where both the input distribution *p*(*x*) and the conditional output distribution
    *p*(*y*|*x*) change. This makes it particularly difficult for a model to adjust,
    as the learned relationship from the training data may no longer hold. The model
    has to cope with both new input patterns and new rules for making predictions
    based on those patterns.
  prefs: []
  type: TYPE_NORMAL
- en: However, the “severity” of a shift can vary widely depending on the real-world
    context. For example, even a covariate shift can be extremely problematic if the
    shift is severe or if the model cannot adapt to the new input distribution. On
    the other hand, a joint distribution shift might be manageable if the shift is
    relatively minor or if we have access to a sufficient amount of labeled data from
    the new distribution to retrain our model.
  prefs: []
  type: TYPE_NORMAL
- en: In general, it’s crucial to monitor our models’ performance and be aware of
    potential shifts in the data distribution so that we can take appropriate action
    if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**23-1.** What is the big issue with importance weighting as a technique to
    mitigate covariate shift?'
  prefs: []
  type: TYPE_NORMAL
- en: '**23-2.** How can we detect these types of shifts in real-world scenarios,
    especially when we do not have access to labels for the new data?'
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recommendations and pointers to advanced mitigation techniques for avoiding
    domain shift: Abolfazl Farahani et al., “A Brief Review of Domain Adaptation”
    (2020), *[https://arxiv.org/abs/2010.03978](https://arxiv.org/abs/2010.03978)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
