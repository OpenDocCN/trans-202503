- en: '**5'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CHARACTER REPRESENTATION**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/comm1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although computers are famous for their “number-crunching” capabilities, the
    truth is that most computer systems process character data far more often than
    numbers. The term *character* refers to a human- or machine-readable symbol that
    is typically a non-numeric entity. In general, a character is any symbol that
    you can type on a keyboard or show on a video display. In addition to alphabetic
    characters, character data includes punctuation marks, numeric digits, spaces,
    tabs, carriage returns (the ENTER key), other control characters, and other special
    symbols.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter looks at how to represent characters, strings, and character sets
    within a computer system. It also discusses various operations on these data types.
  prefs: []
  type: TYPE_NORMAL
- en: '**5.1 Character Data**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most computer systems use a 1-byte or multibyte binary sequence to encode the
    various characters. Windows, macOS, and Linux fall into this category, using the
    ASCII or Unicode character sets, whose members can all be represented with 1-
    or multibyte binary sequences. The EBCDIC character set, in use on IBM mainframes
    and minicomputers, is another example of a single-byte character code.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will discuss all three of these character sets and their internal
    representations, as well as how to create your own character sets.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.1.1 The ASCII Character Set***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ASCII (American Standard Code for Information Interchange) character set
    maps 128 characters to the unsigned integer values 0 through 127 (`$0` through
    `$7F`). Although the exact mapping of characters to numeric values is arbitrary
    and unimportant, a standardized mapping allows you to communicate between programs
    and peripheral devices. The standard ASCII codes are useful because nearly everyone
    uses them. If you use the ASCII code `65` to represent the character *A*, for
    example, you can be confident that some peripheral device (such as a printer)
    will correctly interpret this value as an *A*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the ASCII character set provides only 128 different characters, you
    might be wondering: “What do we do with the additional 128 values (`$80..$FF`)
    that we can represent with a byte?” One option is to ignore those extra values,
    and that’s the primary approach of this book. Another possibility is to extend
    the ASCII character set by an additional 128 characters. Of course, unless you
    can get everyone to agree upon a particular extension of the character set^([1](footnotes.xhtml#fn5_1a))
    (a difficult task indeed), the whole purpose of having a standardized character
    set will be defeated.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite some major shortcomings, such as the inability to represent all characters
    and alphabets in use today, ASCII data is *the* standard for data interchange
    across computer systems and programs. Most programs can accept ASCII data, and
    most programs can produce it. Because you’ll probably be dealing with ASCII characters
    in your programs, it would be wise to study the layout of the character set and
    memorize a few key ASCII codes (such as those for *0*, *A*, and *a*).
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Table A-1 in [Appendix A](app01.xhtml#app01) lists all the characters in the
    standard ASCII character set.*'
  prefs: []
  type: TYPE_NORMAL
- en: The ASCII character set is divided into four groups of 32 characters. The first
    32 characters, ASCII codes `$0` through `$1F` (0 through 31), form a special set
    of nonprinting characters called the *[control characters](gloss01.xhtml#gloss01_59)*.
    As their name implies, these characters perform various printer and display control
    operations rather than displaying symbols. Examples of control characters include
    the carriage return, which positions the cursor at the beginning of the current
    line of characters;^([2](footnotes.xhtml#fn5_2a)) line feed, which moves the cursor
    down one line on the output device; and backspace, which moves the cursor back
    one position to the left. Unfortunately, because there’s very little standardization
    among output devices, different control characters perform different operations
    on different output devices. To find out exactly how a particular control character
    affects a certain device, consult the device’s manual.
  prefs: []
  type: TYPE_NORMAL
- en: The second group of 32 ASCII character codes comprises various punctuation symbols,
    special characters, and the numeric digits. The most notable characters in this
    group include the space character (ASCII code `$20`) and the numeric digits (ASCII
    codes `$30..$39`).
  prefs: []
  type: TYPE_NORMAL
- en: The third group of 32 ASCII characters contains the uppercase alphabetic characters.
    The ASCII codes for the characters *A* through *Z* lie in the range `$41` through
    `$5A`. Because there are only 26 different alphabetic characters, the remaining
    six codes hold various special symbols.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth and final group of 32 ASCII character codes represents the lowercase
    alphabetic symbols, five additional special symbols, and another control character
    (delete). The lowercase character symbols use the ASCII codes `$61` through `$7A`.
    If you convert the codes for the upper- and lowercase characters to binary, you’ll
    notice that the uppercase symbols differ from their lowercase equivalents in exactly
    one bit position. For example, consider the character codes for *E* and *e* in
    [Figure 5-1](ch05.xhtml#ch05fig01).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-1: ASCII codes for E and e*'
  prefs: []
  type: TYPE_NORMAL
- en: These two codes differ only in bit 5\. Uppercase alphabetic characters always
    contain a `0` in bit 5; lowercase alphabetic characters always contain a `1` in
    bit 5\. To quickly convert an alphabetic character between upper- and lowercase,
    simply invert bit 5\. To force an uppercase character to lowercase, set bit 5
    to `1`. Likewise, you can force a lowercase character to uppercase by setting
    bit 5 to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Bits 5 and 6 determine the character’s group (see [Table 5-1](ch05.xhtml#ch05tab01)).
    Therefore, you can convert any upper- or lowercase (or special) character to its
    corresponding control character by setting bits 5 and 6 to `0` (for example, *A*
    becomes CTRL-A when you set bits 5 and 6 to `0`; that is, `0x41` becomes `0x01`).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-1:** ASCII Character Groups Determined by Bits 5 and 6'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Bit 6** | **Bit 5** | **Group** |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `0` | Control characters |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `1` | Digits and punctuation |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `0` | Uppercase and special |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `1` | Lowercase and special |'
  prefs: []
  type: TYPE_TB
- en: Bits 5 and 6 aren’t the only bits that encode useful information. Consider,
    for a moment, the ASCII codes of the numeric digit characters in [Table 5-2](ch05.xhtml#ch05tab02).
    The decimal representations of these ASCII codes are not very enlightening. However,
    the hexadecimal representation reveals something very important—the LO nibble
    is the binary equivalent of the represented number. By stripping away (setting
    to `0`) the HO nibble of the ASCII code, you obtain the binary representation
    of that digit. Conversely, you can convert a binary value in the range `0` through
    `9` to its ASCII character representation by simply setting the HO nibble to `%0011`,
    or the decimal value `3`. You can use the logical AND operation to force the HO
    bits to `0`; likewise, you can use the logical OR operation to force the HO bits
    to `%0011`. For more information on string-to-numeric conversions, see [Chapter
    2](ch02.xhtml#ch02).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-2:** ASCII Codes for the Numeric Digits'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Character** | **Decimal** | **Hexadecimal** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | `48` | `$30` |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `49` | `$31` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `50` | `$32` |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `51` | `$33` |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | `52` | `$34` |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | `53` | `$35` |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | `54` | `$36` |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | `55` | `$37` |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | `56` | `$38` |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | `57` | `$39` |'
  prefs: []
  type: TYPE_TB
- en: Despite the fact that it is a “standard,” simply encoding your data using ASCII
    characters does not guarantee compatibility across systems. An *A* on one machine
    is most likely an *A* on another system; but, of the 32 control codes in the first
    group of ASCII codes, plus the delete code in the last group, only 4 control codes
    are commonly supported by most devices and applications—backspace (BS), tab, carriage
    return (CR), and line feed (LF). Worse still, different machines often use these
    “supported” control codes in different ways. End-of-line is a particularly troublesome
    example. Windows, MS-DOS, CP/M, and other systems mark end-of-line by the two-character
    sequence CR/LF. The original Apple Macintosh OS and many other systems mark end-of-line
    by a single CR character. Linux, BeOS, macOS, and other Unix systems mark end-of-line
    with a single LF character.
  prefs: []
  type: TYPE_NORMAL
- en: Exchanging simple text files between different systems can be an exercise in
    frustration. Even if you use standard ASCII characters in all your files, you
    still need to convert the data when exchanging files between systems. Fortunately,
    many text editors automatically handle files with different line endings (many
    available freeware utilities will also do this conversion for you). If you have
    to do this in your own software, simply copy all characters except the end-of-line
    sequence from one file to another, and then emit the new end-of-line sequence
    whenever you encounter an old end-of-line sequence in the input file.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.1.2 The EBCDIC Character Set***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although the ASCII character set is, unquestionably, the most popular character
    representation, it’s certainly not the only one available. For example, IBM uses
    the *[EBCDIC](gloss01.xhtml#gloss01_86)* code on many of its mainframe and mini-computer
    lines. However, you’ll rarely encounter it on personal computer systems, so we’ll
    consider it only briefly in this book.
  prefs: []
  type: TYPE_NORMAL
- en: EBCDIC (pronounced “Eb-suh-dic”) stands for *Extended Binary Coded Decimal Interchange
    Code*. If you’re wondering whether there was an unextended version of this character
    code, the answer is yes. Earlier IBM systems and keypunch machines used *BCDIC
    (Binary Coded Decimal Interchange Code)*, a character set based on punched cards
    and decimal representation (for IBM’s older decimal machines).
  prefs: []
  type: TYPE_NORMAL
- en: BCDIC existed long before modern digital computers; it was born on old-fashioned
    IBM keypunches and tabulator machines. EBCDIC extended that encoding to provide
    a character set for IBM’s computers. However, EBCDIC inherited several traits
    from BCDIC that seem strange in the context of modern computers. For example,
    the encodings of the alphabetic characters are not contiguous. Originally, the
    alphabetic characters probably did have a sequential encoding; however, when IBM
    expanded the character set, it used some binary combinations that aren’t present
    in the BCD format (like `%1010..%1111`). These binary values appear between two
    otherwise sequential BCD values, which explains why certain character sequences
    (such as the alphabetic characters) aren’t contiguous in the EBCDIC encoding.
  prefs: []
  type: TYPE_NORMAL
- en: EBCDIC is not a single character set; rather, it is a family of character sets.
    While the EBCDIC character sets have a common core (for example, the encodings
    for the alphabetic characters are usually the same), different versions, known
    as *[code pages](gloss01.xhtml#gloss01_51)*, have different encodings for punctuation
    and special characters. Because of the limited number of encodings available in
    a single byte, different code pages reuse some of the character encodings for
    their own special set of characters. So, if you’re given a file that contains
    EBCDIC characters and someone asks you to translate it to ASCII, you’ll quickly
    discover that it’s not a trivial task.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the weirdness of the EBCDIC character set, many common algorithms
    that work well on ASCII characters simply don’t work with EBCDIC. However, keep
    in mind that EBCDIC functional equivalents exist for most ASCII characters. Check
    out the IBM literature for more details.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.1.3 Double-Byte Character Sets***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because a byte can represent a maximum of 256 characters, some computer systems
    use *double-byte character sets (DBCSs)* to represent more than 256 characters.
    DBCSs do not encode every character using 16 bits; instead, they use a single
    byte for most character encodings and use double-byte codes only for certain characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical double-byte character set uses the standard ASCII character set along
    with several additional characters in the range `$80` through `$FF`. Certain values
    in this range are used as extension codes that tell the software that a second
    byte immediately follows. Each extension byte allows the DBCS to support another
    256 different character codes. With three extension values, for example, the DBCS
    can support up to 1,021 different characters: 256 characters for each of the extension
    bytes, and 253 (256 – 3) characters for the standard single-byte set (we subtract
    3 because the three extension byte values each consume one of the 256 combinations,
    and they don’t count as characters).'
  prefs: []
  type: TYPE_NORMAL
- en: Back in the days when terminals and computers used memory-mapped character displays,
    double-byte character sets weren’t very practical. Hardware character generators
    really want each character to be the same size, and they want to process a limited
    number of characters. However, as bitmapped displays with software character generators
    became prevalent (such as Windows, Macintosh, Unix/XWindows machines, tablets,
    and smartphones), it became possible to process DBCSs.
  prefs: []
  type: TYPE_NORMAL
- en: Although DBCSs can compactly represent a large number of characters, more computing
    resources are required to process text in a DBCS format. For example, determining
    the length of a zero-terminated string containing DBCS characters (typical in
    the C/C++ languages) can be considerable work. Some characters in the string consume
    2 bytes, while most others consume only 1 byte, so a string length function has
    to scan the string byte-by-byte to locate any extension values indicating that
    a single character consumes 2 bytes. This process more than doubles the time a
    high-performance string length function takes to execute.
  prefs: []
  type: TYPE_NORMAL
- en: Worse still, many common algorithms used to manipulate string data fail when
    applied to DBCSs. For example, a common C/C++ trick to step through characters
    in a string is to either increment or decrement a pointer to the string using
    expressions like `++ptrChar` or `--ptrChar`. This won’t work with DBCSs. While
    someone using a DBCS probably has a set of standard C library routines that work
    on DBCSs, it’s also quite likely that other character functions they or others
    have written don’t work properly with the extended characters.
  prefs: []
  type: TYPE_NORMAL
- en: The other big problem with DBCSs is the lack of consistent standard. Different
    DBCSs use the same exact encoding for different characters. For these reasons,
    if you need a standardized character set that supports more than 256 characters,
    you’re far better off using the Unicode character set.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.1.4 The Unicode Character Set***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A few decades back, engineers at Aldus, NeXT, Sun, Apple Computer, IBM, Microsoft,
    the Research Library Group, and Xerox realized that their new computer systems
    with bitmaps and user-selectable fonts could display far more than 256 different
    characters at one time. At the time, DBCSs were the most common solution, but—as
    just noted—they had a couple of compatibility problems. So, the engineers sought
    a different route.
  prefs: []
  type: TYPE_NORMAL
- en: The solution they came up with was the Unicode character set. The engineers
    who originally developed Unicode chose a 2-byte character size. Like DBCSs, this
    approach still required special library code (existing single-byte string functions
    would not always work with double-byte characters), but other than changing the
    size of a character, most existing string algorithms would still work with 2-byte
    characters. The Unicode definition included all of the (known/living) character
    sets at the time, giving each character a unique encoding, to avoid the consistency
    problems that plagued differing DBCSs.
  prefs: []
  type: TYPE_NORMAL
- en: The original Unicode standard used a 16-bit word to represent each character.
    Therefore, Unicode supported up to 65,536 different character codes—a huge advance
    over the 256 possible codes that are representable with an 8-bit byte. Furthermore,
    Unicode is upward compatible from ASCII. If the HO 9 bits^([3](footnotes.xhtml#fn5_3a))
    of a Unicode character’s binary representation contain `0`, then the LO 7 bits
    use the standard ASCII code. If the HO 9 bits contain some nonzero value, then
    the 16 bits form an extended character code (extended from ASCII, that is). If
    you’re wondering why so many different character codes are necessary, note that,
    at the time, certain Asian character sets contained 4,096 characters. The Unicode
    character set even provided a set of codes you could use to create an application-defined
    character set. Approximately half of the 65,536 possible character codes have
    been defined, and the remaining character encodings are reserved for future expansion.
  prefs: []
  type: TYPE_NORMAL
- en: Today, Unicode is a universal character set, long replacing ASCII and older
    DBCSs. All modern operating systems (including macOS, Windows, Linux, iOS, Android,
    and Unix), web browsers, and most modern applications provide Unicode support.
    Unicode Consortium, a nonprofit corporation, maintains the Unicode standard. By
    maintaining the standard, Unicode, Inc. (*[https://home.unicode.org/](https://home.unicode.org/)*),
    helps guarantee that a character you write on one system will display as you expect
    on a different system or application.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.1.5 Unicode Code Points***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Alas, as well thought-out as the original Unicode standard was, it couldn’t
    have anticipated the explosion in characters that would occur. Emojis, astrological
    symbols, arrows, pointers, and a wide variety of symbols introduced for the internet,
    mobile devices, and web browsers have greatly expanded the Unicode symbol repertoire
    (along with a desire to support historic, obsolete, and rare scripts). In 1996,
    systems engineers discovered that 65,536 symbols were insufficient. Rather than
    require 3 or 4 bytes for each Unicode character, those in charge of the Unicode
    definition gave up on trying to create a fixed-size representation of characters
    and allowed for opaque (and multiple) encodings of Unicode characters. Today,
    Unicode defines 1,112,064 code points, far exceeding the 2-byte capacity originally
    set aside for Unicode characters.
  prefs: []
  type: TYPE_NORMAL
- en: A Unicode *[code point](gloss01.xhtml#gloss01_53)* is simply an integer value
    that Unicode associates with a particular character symbol; you can think of it
    as the Unicode equivalent of the ASCII code for a character. The convention for
    Unicode code points is to specify the value in hexadecimal with a `U+` prefix;
    for example, `U+0041` is the Unicode code point for the letter *A*.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*See* [https://en.wikipedia.org/wiki/Unicode#General_Category_property](https://en.wikipedia.org/wiki/Unicode#General_Category_property)
    *for more details on code points.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***5.1.6 Unicode Code Planes***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because of its history, blocks of 65,536 characters are special in Unicode—they
    are known as a *multilingual plane*. The first multilingual plane, `U+000000`
    to `U+00FFFF`, roughly corresponds to the original 16-bit Unicode definition;
    the Unicode standard calls this the *[Basic Multilingual Plane (BMP)](gloss01.xhtml#gloss01_24)*.
    Planes 1 (`U+010000` to `U+01FFFF`), 2 (`U+020000` to `U+02FFFF`), and 14 (`U+0E0000`
    to `U+0EFFFF`) are supplementary planes. Unicode reserves planes 3 through 13
    for future expansion and planes 15 and 16 for user-defined character sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Unicode standard defines code points in the range `U+000000` to `U+10FFFF`.
    Note that `0x10ffff` is 1,114,111, which is where most of the 1,112,064 characters
    in the Unicode character set come from; the remaining 2,048 code points are reserved
    for use as *surrogates*, which are Unicode extensions. *Unicode scalar*, another
    term you might hear, is a value from the set of all Unicode code points *except*
    the 2,048 surrogate code points. The HO two hexadecimal digits of the six-digit
    code point value specify the multilingual plane. Why 17 planes? The reason, as
    you’ll see in a moment, is that Unicode uses special multiword entries to encode
    code points beyond `U+FFFF`. Each of the two possible extensions encodes 10 bits,
    for a total of 20 bits; 20 bits gives you 16 multilingual planes, which, plus
    the original BMP, produces 17 multilingual planes. This is also why code points
    fall in the range `U+000000` to `U+10FFFF`: it takes 21 bits to encode the 16
    multilingual planes plus the BMP.'
  prefs: []
  type: TYPE_NORMAL
- en: '***5.1.7 Surrogate Code Points***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As noted earlier, Unicode began life as a 16-bit (2-byte) character set encoding.
    When it became apparent that 16 bits were insufficient to handle all the possible
    characters that existed at the time, an expansion was necessary. As of Unicode
    v2.0, the Unicode, Inc., organization extended the definition of Unicode to include
    multiword characters. Now Unicode uses surrogate code points (`U+D800` through
    `U+DFFF`) to encode values larger than `U+FFFF`. [Figure 5-2](ch05.xhtml#ch05fig02)
    shows the encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-2: Surrogate code point encoding for Unicode planes 1–16*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the two words (unit 1/high surrogate and unit 2/low surrogate) always
    appear together. The unit 1 value (with HO bits `%110110`) specifies the upper
    10 bits (`b`[`10`]..`b`[`19`]) of the Unicode scalar, and the unit 2 value (with
    HO bits `%110111`) specifies the lower 10 bits (`b`[`0`]..`b`[`9`]) of the Unicode
    scalar. Therefore, the value of bits `b`[`16`] through `b`[`19`] plus 1 specifies
    Unicode plane 1 through 16\. Bits `b`[`0`] through `b`[`15`] specify the Unicode
    scalar value within the plane.
  prefs: []
  type: TYPE_NORMAL
- en: Note that surrogate codes appear only in the BMP. None of the other multilingual
    planes contain surrogate codes. Bits `b`[`0`] through `b`[`19`], extracted from
    the unit 1 and 2 values, always specify a Unicode scalar value (even if the values
    fall in the range `U+D800` through `U+DFFF`).
  prefs: []
  type: TYPE_NORMAL
- en: '***5.1.8 Glyphs, Characters, and Grapheme Clusters***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each Unicode code point has a unique name. For example, `U+0045` has the name
    “LATIN CAPITAL LETTER A.” Note that the symbol *A* is *not* the name of the character.
    *A* is a *glyph*—a series of strokes (one horizontal and two slanted strokes)
    that a device draws in order to represent the character.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different glyphs for the single Unicode character “LATIN CAPITAL
    LETTER A.” For example, a Times Roman letter A and a Times Roman Italic letter
    *A* have different glyphs, but Unicode doesn’t differentiate between them (or
    between *A* characters in any two different fonts). The character “LATIN CAPITAL
    LETTER A” remains `U+0045` regardless of the font or style you use to draw it.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an interesting side note, if you have access to the Swift programming language,
    you can print the name of any Unicode character using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'So, what exactly is a character in Unicode? Unicode scalars are Unicode characters,
    but there’s a difference between what you’d normally call a character and the
    definition of a scalar. For example, is *©* one character or two? Consider the
    following Swift code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`"\u{301}"` is the Swift syntax for specifying a Unicode scalar value within
    a string; in this particular case `301` is the hexadecimal code for the *combining
    acute accent* character.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first `print` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: prints the character (producing `©` on the output, as we expect).
  prefs: []
  type: TYPE_NORMAL
- en: 'The second `print` statement prints the number of characters Swift determines
    are present in the string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This prints `1` to the standard output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third `print` statement prints the number of elements (UTF-16 elements^([4](footnotes.xhtml#fn5_4a)))
    in the string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This prints `2` on the standard output, because the string holds two words of
    UTF-16 data.
  prefs: []
  type: TYPE_NORMAL
- en: So, again, is this one character or two? Internally (assuming UTF-16 encoding),
    the computer sets aside 4 bytes of memory for this single character (two 16-bit
    Unicode scalar values).^([5](footnotes.xhtml#fn5_5a)) On the screen, however,
    the output takes only one character position and looks like a single character
    to the user. When this character appears within a text editor and the cursor is
    immediately to the right of the character, the user expects that pressing the
    backspace key will delete it. From the user’s perspective, then, this is a single
    character (as Swift reports when you print the `count` attribute of the string).
  prefs: []
  type: TYPE_NORMAL
- en: In Unicode, however, a character is largely equivalent to a code point. This
    is not what people normally think of as a character. In Unicode terminology, a
    *grapheme cluster* is what people commonly call a character—it’s a sequence of
    one or more Unicode code points that combine to form a single language element
    (that is, a single character). So, when we talk about characters with respect
    to symbols that an application displays to an end user, we’re really talking about
    grapheme clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Grapheme clusters can make life miserable for software developers. Consider
    the following Swift code (a modification of the earlier example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This code produces the same `©` and `1` outputs from the first two `print`
    statements. The following produces `©`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: and this `print` statement produces `1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the third `print` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: displays `3` rather than `2` (as in the original example).
  prefs: []
  type: TYPE_NORMAL
- en: There are definitely three Unicode scalar values in this string (`U+0065`, `U+0301`,
    and `U+0301`). When printing, the operating system combines the `e` and the two
    acute accent combining characters to form the single character `©` and then outputs
    the character to the standard output device. Swift is smart enough to know that
    this combination creates a single output symbol on the display, so printing the
    result of the `count` attribute continues to output `1`. However, there are (undeniably)
    three Unicode code points in this string, so printing `utf16.count` produces `3`
    on output.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.1.9 Unicode Normals and Canonical Equivalence***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Unicode character *©* actually existed on personal computers long before
    Unicode came along. It’s part of the original IBM PC character set and also part
    of the Latin-1 character set (used, for example, on old DEC terminals). As it
    turns out, Unicode uses the Latin-1 character set for the code points in the range
    `U+00A0` to `U+00FF`, and `U+00E9` just happens to correspond to the *©* character.
    Therefore, we can modify the earlier program as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The outputs from this program are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Ouch! Three different strings all producing `©` but containing a different number
    of code points. Imagine how this complicates programming strings containing Unicode
    characters. For example, if you have the following three strings (Swift syntax)
    and you try to compare them, what will the result be?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To the user, all three strings look the same on the screen. However, they clearly
    contain different values. If you compare them to see if they are equal, will the
    result be `true` or `false`?
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, that depends upon whose string libraries you’re using. Most current
    string libraries would return `false` if you compared these strings for equality.
    Interestingly enough, Swift will claim that `eAccent1` is equal to `eAccent2`,
    but it isn’t smart enough to report that `eAccent1` is equal to `eAccent3` or
    that `eAccent2` is equal to `eAccent3`—despite the fact that it displays the same
    symbol for all three strings. Many languages’ string libraries simply report that
    all three strings are unequal.
  prefs: []
  type: TYPE_NORMAL
- en: The three Unicode/Swift strings `"\{E9}"`, `"e\{301}"`, and `"e\{301}\{301}"`
    all produce the same output on the display; therefore, they are canonically equivalent
    according to the Unicode standard. Some string libraries won’t report any of these
    strings as being equivalent, however. Others, like the one for Swift, will handle
    small canonical equivalences (such as `"\{E9}" == "e\{301}"`) but not arbitrary
    sequences that should be equivalent.^([6](footnotes.xhtml#fn5_6a))
  prefs: []
  type: TYPE_NORMAL
- en: Unicode defines *normal forms* for Unicode strings. One aspect of normal form
    is to replace canonically equivalent sequences with an equivalent sequence—for
    example, replace `"e\u{309}"` by `"\u{E9}"` or replace `"\u{E9}"` by `"e\u{309}"`
    (usually, the shorter form is preferable). Some Unicode sequences allow multiple
    combining characters. Often, the order of the combining characters is irrelevant
    to producing the desired grapheme cluster. However, it’s easier to compare two
    such strings if the combining characters are in a specified order. Normalizing
    Unicode strings may also produce results whose combining characters always appear
    in a fixed order (thereby improving efficiency of string comparisons).
  prefs: []
  type: TYPE_NORMAL
- en: '***5.1.10 Unicode Encodings***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As of Unicode v2.0, the standard supports a 21-bit character space capable of
    handling over a million characters (though most of the code points remain reserved
    for future use). Rather than use a fixed-size 3-byte (or worse, 4-byte) encoding
    to allow the larger character set, Unicode, Inc., allows different encodings—UTF-32,
    UTF-16, and UTF-8—each with its own advantages and disadvantages.^([7](footnotes.xhtml#fn5_7a))
  prefs: []
  type: TYPE_NORMAL
- en: UTF-32 uses 32-bit integers to hold Unicode scalars. The advantage to this scheme
    is that a 32-bit integer can represent every Unicode scalar value (which requires
    only 21 bits). Programs that require random access to characters in strings—without
    having to search for surrogate pairs—and other constant-time operations are (mostly)
    possible with UTF-32\. The obvious drawback to UTF-32 is that each Unicode scalar
    value requires 4 bytes of storage—twice that of the original Unicode definition
    and four times that of ASCII characters. It may seem that using two or four times
    as much storage (over ASCII and the original Unicode) is a small price to pay.
    After all, modern machines have several orders of magnitude more storage than
    they did when Unicode first appeared. However, that extra storage has a huge impact
    on performance, because those additional bytes quickly consume cache storage.
    Furthermore, modern string processing libraries often operate on character strings
    8 bytes at a time (on 64-bit machines). With ASCII characters, that means a given
    string function can process up to eight characters concurrently; with UTF-32,
    that same string function can operate on only two characters concurrently. As
    a result, the UTF-32 version will run four times slower than the ASCII version.
    Ultimately, even Unicode scalar values are insufficient to represent all Unicode
    characters (that is, many Unicode characters require a sequence of Unicode scalars),
    so using UTF-32 doesn’t solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: The second encoding format the Unicode supports is UTF-16\. As the name suggests,
    UTF-16 uses 16-bit (unsigned) integers to represent Unicode values. To handle
    scalar values greater than `0xFFFF`, UTF-16 uses the surrogate pair scheme to
    represent values in the range `0x010000` to `0x10FFFF` (see “[Surrogate Code Points](#sec5_1_7)”
    on page [102](#sec5_1_7)). Because the vast majority of useful characters fit
    into 16 bits, most UTF-16 characters require only 2 bytes. For those rare cases
    where surrogates are necessary, UTF-16 requires 2 words (32 bits) to represent
    the character.
  prefs: []
  type: TYPE_NORMAL
- en: The last encoding, and unquestionably the most popular, is UTF-8\. The UTF-8
    encoding is forward compatible from the ASCII character set. In particular, all
    ASCII characters have a single-byte representation (their original ASCII code,
    where the HO bit of the byte containing the character contains a `0` bit). If
    the UTF-8 HO bit is `1`, then UTF-8 requires between 1 and 3 additional bytes
    to represent the Unicode code point. [Table 5-3](ch05.xhtml#ch05tab03) provides
    the UTF-8 encoding schema.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-3:** UTF Encoding'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Bytes** | **Bits for code point** | **First code point** | **Last code
    point** | **Byte 1** | **Byte 2** | **Byte 3** | **Byte 4** |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `7` | `U+00` | `U+7F` | `0`xxxxxxx |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `11` | `U+80` | `U+7FF` | `110`xxxxx | `10`xxxxxx |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| `3` | `16` | `U+800` | `U+FFFF` | `1110`xxxx | `10`xxxxxx | `10`xxxxxx |  |'
  prefs: []
  type: TYPE_TB
- en: '| `4` | `21` | `U+10000` | `U+10FFFF` | `11110`xxx | `10`xxxxxx | `10`xxxxxx
    | `10`xxxxxx |'
  prefs: []
  type: TYPE_TB
- en: The “xxx . . .” bits are the Unicode code point bits. For multibyte sequences,
    byte 1 contains the HO bits, byte 2 contains the next HO bits (LO bits compared
    to byte 1), and so on. For example, the 2-byte sequence (`%11011111`, `%10000001`)
    corresponds to the Unicode scalar `%0000_0111_1100_0001` (`U+07C1`).
  prefs: []
  type: TYPE_NORMAL
- en: UTF-8 encoding is probably the most common encoding in use. Most web pages use
    it. Most C standard library string functions will operate on UTF-8 text without
    modification (although some C standard library functions can produce malformed
    UTF-8 strings if the programmer isn’t careful with them).
  prefs: []
  type: TYPE_NORMAL
- en: Different languages and operating systems use different encodings as their default.
    For example, macOS and Windows tend to use UTF-16 encoding, whereas most Unix
    systems use UTF-8\. Some variants of Python use UTF-32 as their native character
    format. By and large, though, most programming languages use UTF-8 because they
    can continue to use older ASCII-based character processing libraries to process
    UTF-8 characters. Apple’s Swift is one of the first programming languages that
    attempts to do Unicode right (though there is a huge performance hit for doing
    so).
  prefs: []
  type: TYPE_NORMAL
- en: '***5.1.11 Unicode Combining Characters***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although UTF-8 and UTF-16 encodings are much more compact than UTF-32, the CPU
    overhead and algorithmic complexities of dealing with multibyte (or multiword)
    characters sets complicates their use, introducing bugs and performance issues.
    Despite the issues of wasting memory (especially in the cache), why not simply
    define characters as 32-bit entities and be done with it? This seems like it would
    simplify string processing algorithms, improving performance and reducing the
    likelihood of defects in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with this theory is that you cannot represent all possible grapheme
    clusters with only 21 bits (or even 32 bits) of storage. Many grapheme clusters
    consist of several concatenated Unicode code points. Here’s an example from Chris
    Eidhof and Ole Begemann’s *Advanced Swift* (CreateSpace, 2017):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of these Unicode grapheme clusters produces an identical character: an
    `ó` with a dot underneath the character (this is a character from the Yoruba character
    set). The character sequence (`U+1ECD`, `U+300`) is an `o` with a dot under it
    followed by a combining acute. The character sequence (`U+F2`, `U+323`) is an
    `ó` followed by a combining dot. The character sequence (`U+6F`, `U+323`, `U+300`)
    is an `o` followed by a combining dot, followed by a combining acute. Finally,
    the character sequence (`U+6F`, `U+300`, `U+323`) is an `o` followed by a combining
    acute, followed by a combining dot. All four strings produce the same output.
    Indeed, the Swift string comparisons treat all four strings as equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note that there is not a single Unicode scalar value that will produce this
    character. You must combine at least two Unicode scalars (or as many as three)
    to produce this grapheme cluster on the output device. Even if you used UTF-32
    encoding, it would still require two (32-bit) scalars to produce this particular
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Emojis present another challenge that can’t be solved using UTF-32\. Consider
    the Unicode scalar `U+1F471`. This prints an emoji of a person with blond hair.
    If we add a skin color modifier to this, we obtain (`U+1F471`, `U+1F3FF`), which
    produces a person with a dark skin tone (and blond hair). In both cases we have
    a single character displaying on the screen. The first example uses a single Unicode
    scalar value, but the second example requires two. There is no way to encode this
    with a single UTF-32 value.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom line is that certain Unicode grapheme clusters will require multiple
    scalars, no matter how many bits we assign to the scalar (it’s possible to combine
    30 or 40 scalars into a single grapheme cluster, for example). That means we’re
    stuck dealing with multiword sequences to represent a single “character” regardless
    of how hard we try to avoid it. This is why UTF-32 has never really taken off.
    It doesn’t solve the problem of random access into a string of Unicode characters.
    If you’ve got to deal with normalizing and combining Unicode scalars, it’s more
    efficient to use UTF-8 or UTF-16 encodings.
  prefs: []
  type: TYPE_NORMAL
- en: Again, most languages and operating systems today support Unicode in one form
    or another (typically using UTF-8 or UTF-16 encoding). Despite the obvious problems
    with dealing with multibyte character sets, modern programs need to deal with
    Unicode strings rather than simple ASCII strings. Swift, which is almost “pure
    Unicode,” doesn’t even offer much in the way of standard ASCII character support.
  prefs: []
  type: TYPE_NORMAL
- en: '**5.2 Character Strings**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After integers, character strings are probably the most common type in use
    in modern programs. In general, a *[character string](gloss01.xhtml#gloss01_47)*
    is a sequence of characters with two main attributes: a *length* and the *character
    data*.'
  prefs: []
  type: TYPE_NORMAL
- en: Character strings may also possess other attributes, such as the *maximum length*
    allowable for that particular variable or a *reference count* specifying how many
    different string variables refer to the same character string. We’ll look at these
    attributes and how programs can use them in this section, which describes various
    string formats and some of the possible string operations.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.2.1 Character String Formats***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different languages use different data structures to represent strings. Some
    string formats use less memory, others allow faster processing, some are more
    convenient to use, and still others provide additional functionality for the programmer
    and operating system. To help you better understand the reasoning behind the design
    of character strings, let’s look at some common string representations popularized
    by various high-level languages.
  prefs: []
  type: TYPE_NORMAL
- en: '**5.2.1.1 Zero-Terminated Strings**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Without question, *[zero-terminated strings](gloss01.xhtml#gloss01_271)* are
    the most common string representation in use today, because this is the native
    string format for C, C++, and several other languages. In addition, you’ll find
    zero-terminated strings in programs written in languages that don’t have a specific
    native string format, such as assembly language.
  prefs: []
  type: TYPE_NORMAL
- en: 'A zero-terminated ASCII string is a sequence containing zero or more 8-bit
    character codes ending with a byte containing `0` (or, in the case of UTF-16,
    a sequence containing zero or more 16-bit character codes and ending with a 16-bit
    word containing `0`). For example, in C/C++, the ASCII string `"abc"` requires
    4 bytes: 1 byte for each of the three characters `a`, `b`, and `c`, and a `0`
    byte.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zero-terminated strings have a few advantages over other string formats:'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-terminated strings can represent strings of any practical length with only
    one byte of overhead (2 bytes in UTF-16, 4 in UTF-32).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the popularity of the C/C++ programming languages, high-performance string
    processing libraries are available that work well with zero-terminated strings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero-terminated strings are easy to implement. As far as the C and C++ languages
    are concerned, strings are just arrays of characters. That’s probably why C’s
    designers chose this format in the first place—so they wouldn’t have to clutter
    up the language with string operators.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can easily represent zero-terminated strings in any language able to create
    an array of characters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, zero-terminated strings also have disadvantages that mean they are
    not always the best choice for representing character string data:'
  prefs: []
  type: TYPE_NORMAL
- en: String functions that need to know the length of a string before working on
    the string data often aren’t very efficient when operating on zero-terminated
    strings. The only reasonable way to compute the length of a zero-terminated string
    is to scan the string from the beginning to the end. The longer your strings are,
    the slower this function runs, so the zero-terminated string format isn’t the
    best choice if you need to process long strings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although it’s a minor problem, you cannot easily represent the character code
    `0` (such as the NUL character in ASCII and Unicode) with the zero-terminated
    string format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero-terminated strings don’t contain any information that tells you how long
    the string can grow beyond the terminating `0` byte. Therefore, some string functions,
    like concatenation, can only extend the length of an existing string variable
    and check for overflow if the caller explicitly passes the maximum length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**5.2.1.2 Length-Prefixed Strings**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A second string format, *[length-prefixed strings](gloss01.xhtml#gloss01_135)*,
    overcomes some of the problems with zero-terminated strings. Length-prefixed strings
    are common in languages like Pascal; they generally consist of a single byte that
    specifies the length of the string, followed by zero or more 8-bit character codes.
    In a length-prefixed scheme, the string `"abc"` consists of 4 bytes: the length
    byte (`$03`), followed by `a`, `b`, and `c`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Length-prefixed strings solve two of the problems associated with zero-terminated
    strings: they allow you to represent the NUL character, and string operations
    are more efficient. Another advantage to length-prefixed strings is that the length
    is usually located at position `0` in the string (if we view the string as an
    array of characters), so the first character of the string begins at index `1`
    in the array representation of the string. For many string functions, having a
    `1`-based index into the character data is much more convenient than a `0`-based
    index (which zero-terminated strings use).'
  prefs: []
  type: TYPE_NORMAL
- en: The principal drawback of length-prefixed strings that they are limited to a
    maximum of 255 characters in length (assuming a 1-byte length prefix). You can
    remove this limitation by using a 2- or 4-byte length value, but doing so increases
    the amount of overhead data from 1 to 2 or 4 bytes.
  prefs: []
  type: TYPE_NORMAL
- en: '**5.2.1.3 Seven-Bit Strings**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The 7-bit string format is an interesting option that works for 7-bit encodings
    like ASCII. It uses the (normally unused) higher-order bit of the characters in
    the string to indicate the end of the string. All but the last character code
    in the string has its HO bit clear, and the last character in the string has its
    HO bit set.
  prefs: []
  type: TYPE_NORMAL
- en: 'This 7-bit string format has several disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: You have to scan the entire string in order to determine the length of the string.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You cannot have zero-length strings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few languages provide literal string constants for 7-bit strings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’re limited to a maximum of 128 character codes, though this is fine when
    you’re using plain ASCII.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, a big advantage of 7-bit strings is that they don’t require any overhead
    bytes to encode the length. Assembly language (using a macro to create literal
    string constants) is probably the best language to use when dealing with 7-bit
    strings. Because the benefit of 7-bit strings is that they’re compact and assembly
    language programmers tend to worry most about compactness, this is a good match.
    Here’s an HLA macro that converts a literal string constant to a 7-bit string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**5.2.1.4 HLA Strings**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As long as you’re not too concerned about a few extra bytes of overhead per
    string, you can create a string format that combines the advantages of both length-prefixed
    and zero-terminated strings without their respective disadvantages. The High-Level
    Assembly language has done this with its native string format.^([8](footnotes.xhtml#fn5_8a))
  prefs: []
  type: TYPE_NORMAL
- en: 'The biggest drawback to the HLA character string format is the amount of overhead
    required for each string: 9 bytes per string,^([9](footnotes.xhtml#fn5_9a)) which
    can be significant, percentage-wise, if you’re in a memory-constrained environment
    and you process many small strings.'
  prefs: []
  type: TYPE_NORMAL
- en: The HLA string format uses a 4-byte length prefix, allowing character strings
    to be just over four billion characters long (obviously, this is far more than
    any practical HLA application will use). HLA also appends a `0` byte to the character
    string data. The additional 4 bytes of overhead contain the maximum legal length
    for that string. Having this extra field allows HLA string functions to check
    for string overflow, if necessary. In memory, HLA strings take the form shown
    in [Figure 5-3](ch05.xhtml#ch05fig03).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-3: HLA string format*'
  prefs: []
  type: TYPE_NORMAL
- en: The 4 bytes immediately before the first character of the string contain the
    current string length. The 4 bytes preceding the current string length contain
    the maximum string length. Immediately following the character data is a `0` byte.
    Finally, HLA always ensures that the string data structure’s length is a multiple
    of 4 bytes long (for performance reasons), so there may be up to 3 additional
    bytes of padding at the end of the object in memory. (Note that the string in
    [Figure 5-3](ch05.xhtml#ch05fig03) requires only 1 byte of padding to ensure that
    the data structure is a multiple of 4 bytes in length.)
  prefs: []
  type: TYPE_NORMAL
- en: 'HLA string variables are pointers that contain the byte address of the first
    character in the string. To access the length fields, you load the value of the
    string pointer into a 32-bit register, then access the `Length` field at offset
    –4 from the base register and the `MaxLength` field at offset –8 from the base
    register. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As read-only objects, HLA strings are compatible with zero-terminated strings.
    For example, if you have a function written in C that’s expecting you to pass
    it a zero-terminated string, you can call that function and pass it an HLA string
    variable, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The only catch is that the C function must not make any changes to the string
    that would affect its length (because the C code won’t update the `Length` field
    of the HLA string). Of course, you can always call a C `strlen()` function upon
    returning to update the length field yourself, but generally, it’s best not to
    pass HLA strings to a function that modifies zero-terminated strings.
  prefs: []
  type: TYPE_NORMAL
- en: '**5.2.1.5 Descriptor-Based Strings**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The string formats we’ve considered up to this point have kept the attribute
    information (that is, the lengths and terminating bytes) for a string in memory
    along with the character data. A slightly more flexible scheme is to maintain
    such information in a record structure, known as a *[descriptor](gloss01.xhtml#gloss01_73)*,
    that also contains a pointer to the character data. Consider the following Pascal/Delphi
    data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note that this data structure does not hold the actual character data. Instead,
    the `strData` pointer contains the address of the first character of the string.
    The `curLength` field specifies the current length of the string. You could add
    any other fields you like to this record, like a maximum length field, though
    a maximum length isn’t usually necessary because most string formats employing
    a descriptor are *dynamic* (as the next section will discuss). Most string formats
    employing a descriptor just maintain the `Length` field.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting attribute of a descriptor-based string system is that the actual
    character data associated with a string could be part of a larger string. Because
    there are no length or terminating bytes within the actual character data, it’s
    possible to have the character data for two strings overlap (see [Figure 5-4](ch05.xhtml#ch05fig04)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-4: Overlapping strings using descriptors*'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, there are two strings—`"Hello World"` and `"World"`—that overlap.
    This can save memory and make certain functions, like `substring()`, very efficient.
    Of course, when strings overlap like this, you can’t modify the string data because
    that could wipe out part of some other string.
  prefs: []
  type: TYPE_NORMAL
- en: '**5.2.1.6 Java Strings**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Java uses a descriptor-based string form. The actual `String` data type (that
    is, the structure/class that defines the internal representation of a Java string)
    is *opaque*, which means you really aren’t supposed to know about or mess with
    it. It’s a very bad idea to attempt to manipulate Java strings other than via
    the Java String API, because the Java standard has changed their internal representation
    on a couple of occasions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, Java originally defined the `String` type as a descriptor with
    four items: a pointer to an array of 16-bit (original) Unicode characters (no
    extension beyond 16 bits), a count field, an offset field, and a hash code field.
    The offset and count fields allowed efficient substring operations, since all
    substrings into a larger string would share the same array of characters. Unfortunately,
    this format produced memory leaks in some degenerate cases, so Java’s designers
    changed the format and eliminated these fields. If you had code that used the
    offset and count fields (again, a bad idea), your code was broken by this change.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Java also switched from the original Unicode 2-byte definition to UTF-16 encoding
    once it became apparent that 16-bit characters were insufficient. However, after
    a bit of research into a wide variety of Java programs on the internet, Oracle
    (Java’s owner) discovered that most programs use only the Latin-1 character set
    (basically, ASCII). In Oracle’s own words:'
  prefs: []
  type: TYPE_NORMAL
- en: Data from different applications suggests that strings are a major component
    of Java heap usage and that most `java.lang.String` objects contain only Latin-1
    characters. Such characters require only one byte of storage. As a result, half
    of the space in the internal character arrays of `java.lang.String` objects are
    not used. The compact strings feature, introduced in Java SE 9, reduces the memory
    footprint, and reduces garbage collection activity.
  prefs: []
  type: TYPE_NORMAL
- en: This change was largely transparent to Java users and their programs. Oracle
    added a new field to the `String` descriptor to specify whether the encoding was
    UTF-16 or Latin-1\. Once again, if your programs depended on the internal representation,
    they broke.
  prefs: []
  type: TYPE_NORMAL
- en: Always assume that Java `String`s are proper Unicode strings (typically using
    UTF-16 encoding). Java does not try to hide the ugliness of multiword characters.
    As a Java programmer, you must be aware of the difference between the number of
    characters, code points, and grapheme clusters in a string. Java provides functions—for
    example, `String.length()`, `String.codePointCount()`, and `BreakIterator.getCharacterInstance()`—to
    compute all these values for you, but your code must explicitly call them.
  prefs: []
  type: TYPE_NORMAL
- en: '**5.2.1.7 Swift Strings**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Like Java, the Swift programming language uses Unicode characters in its strings.
    Swift 4.x and earlier used a UTF-16 encoding, which is native to macOS (on which
    Apple developed Swift); with Swift v5.0, Apple switched to UTF-8 as the native
    encoding for Swift strings. As with Java, Swift’s `String` type is opaque, so
    you shouldn’t attempt to mess with (or otherwise use) its internal representation.
  prefs: []
  type: TYPE_NORMAL
- en: '**5.2.1.8 C# Strings**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The C# programming language uses UTF-16 encoding for characters in its strings.
    As with Java and Swift, C#’s `string` type is opaque and you shouldn’t attempt
    to mess with (or otherwise use) its internal representation. That being said,
    the Microsoft documentation does claim that C# strings are an array of (Unicode)
    characters.
  prefs: []
  type: TYPE_NORMAL
- en: '**5.2.1.9 Python Strings**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Python programming language originally used UCS-2 (original 16-bit Unicode,
    BMP-only) encoding for strings. Then Python was modified to support UTF-16 or
    UTF-32 encodings (the language was compiled in “narrow” or “wide” versions for
    16- or 32-bit characters). Today, modern versions of Python use a special string
    format that tracks the characters in strings and stores them as ASCII, UTF-8,
    UTF-16, or UTF-32, based on the most compact representation. You can’t really
    access the internal string representation directly within Python, so the caveats
    of opaque types aren’t relevant.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.2.2 Types of Strings: Static, Pseudo-Dynamic, and Dynamic***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Based on the various string formats covered thus far, we can now define three
    string types according to when the system allocates storage for the string. There
    are static, pseudo-dynamic, and dynamic strings.
  prefs: []
  type: TYPE_NORMAL
- en: '**5.2.2.1 Static Strings**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Pure *static strings* are those whose maximum size a programmer chooses when
    writing the program. Pascal strings and Delphi “short” strings fall into this
    category. Arrays of characters that you use to hold zero-terminated strings in
    C/C++ also fall into this category. Consider the following declaration in Pascal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And here’s an example in C/C++:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: While the program is running, there’s no way to increase the maximum sizes of
    these static strings. Nor is there any way to reduce the storage they will use;
    these string objects will consume 256 bytes at runtime, period. One advantage
    to pure static strings is that the compiler can determine their maximum length
    at compile time and implicitly pass this information to a string function so it
    can test for bounds violations at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: '**5.2.2.2 Pseudo-Dynamic Strings**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A pseudo-dynamic string is one whose length the system sets at runtime by calling
    a memory management function like `malloc()` to allocate storage for it. However,
    once the system allocates storage for the string, the maximum length of the string
    is fixed. HLA strings generally fall into this category.^([10](footnotes.xhtml#fn5_10a))
    An HLA programmer typically calls the `stralloc()` function to allocate storage
    for a string variable, after which that particular string object has a fixed length
    that cannot change.^([11](footnotes.xhtml#fn5_11a))
  prefs: []
  type: TYPE_NORMAL
- en: '**5.2.2.3 Dynamic Strings**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Dynamic string systems, which typically use a descriptor-based format, automatically
    allocate sufficient storage for a string object whenever you create a new string
    or otherwise do something that affects an existing string. Operations like string
    assignment and substring are relatively trivial in dynamic string systems—generally
    they copy only the string descriptor data, so these operations are fast. However,
    as noted in the section “[Descriptor-Based Strings](#sec5_2_1_5)” on page [114](#sec5_2_1_5),
    when using strings this way, you cannot store data back into a string object,
    because it could modify data that is part of other string objects in the system.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this problem is to use the copy-on-write technique. Whenever
    a string function needs to change characters in a dynamic string, the function
    first makes a copy of the string and then makes the necessary modifications to
    that copy. Research suggests that copy-on-write semantics can improve the performance
    of many typical applications, because operations like string assignment and substring
    extraction (which is just a partial string assignment) are far more common than
    the modification of character data within strings. The only drawback to this approach
    is that after several modifications to string data in memory, there may be sections
    of the string heap area that contain character data that’s no longer in use. To
    avoid a *[memory leak](gloss01.xhtml#gloss01_152)*, dynamic string systems employing
    copy on write usually provide *[garbage collection](gloss01.xhtml#gloss01_103)*
    code, which scans the string heap area looking for *stale* character data in order
    to recover that memory for other purposes. Unfortunately, depending on the algorithms
    in use, garbage collection can be quite slow.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.2.3 Reference Counting for Strings***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider the case where you have two string descriptors (or pointers) pointing
    at the same string data in memory. Clearly, you can’t *deallocate* (that is, reuse
    for a different purpose) the storage associated with one pointer while the program
    is still using the other pointer to access the same data. One common solution
    is to make the programmer responsible for keeping track of such details. Unfortunately,
    as applications become more complex, this approach often leads to dangling pointers,
    memory leaks, and other pointer-related problems in the software. A better solution
    is to allow the programmer to deallocate the storage for the character data in
    the string and to have the actual deallocation process hold off until the programmer
    releases the last pointer referencing that data. To accomplish this, a string
    system can use reference counters, which track the pointers and their associated
    data.
  prefs: []
  type: TYPE_NORMAL
- en: A *[reference counter](gloss01.xhtml#gloss01_214)* is an integer that counts
    the number of pointers that reference a string’s character data in memory. Every
    time you assign the address of the string to some pointer, you increment the reference
    counter by 1\. Likewise, whenever you wish to deallocate the storage associated
    with the character data for the string, you decrement the reference counter. Deallocation
    of the storage for the character data doesn’t happen until the reference counter
    decrements to 0.
  prefs: []
  type: TYPE_NORMAL
- en: Reference counting works great when the language handles the details of string
    assignment automatically for you. If you try to implement reference counting manually,
    you must be sure to always increment the reference counter when you assign a string
    pointer to some other pointer variable. The best way to do this is to never assign
    pointers directly, but rather to handle all string assignments via some function
    (or macro) call that updates the reference counters in addition to copying the
    pointer data. If your code fails to update the reference counter properly, you’ll
    wind up with dangling pointers or memory leaks.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.2.4 Delphi Strings***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although Delphi provides a “short string” format that is compatible with the
    length-prefixed strings in earlier versions of Delphi, later versions of Delphi
    (4.0 and later) use dynamic strings. While this string format is unpublished (and,
    therefore, subject to change), indications are that Delphi’s string format is
    very similar to HLA’s. Delphi uses a zero-terminated sequence of characters with
    a leading string length and a reference counter (rather than a maximum length
    as HLA uses). [Figure 5-5](ch05.xhtml#ch05fig05) shows the layout of a Delphi
    string in memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-5: Delphi string data format*'
  prefs: []
  type: TYPE_NORMAL
- en: As with HLA, Delphi string variables are pointers that point to the first character
    of the actual string data. To access the length and reference counter fields,
    the Delphi string routines use a negative offset of – 4 and –8 from the character
    data’s base address. However, because this string format is not published, applications
    should never access the length or reference counter fields directly. Delphi provides
    a length function that extracts the string length for you, and there’s really
    no need for your applications to access the reference counter field because the
    Delphi string functions maintain it automatically.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.2.5 Custom String Formats***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Typically, you’ll use the string format your language provides, unless you have
    special requirements. If that’s the case, you’ll find that most languages provide
    user-defined data-structuring capabilities that enable you to create your own
    custom string formats.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the language will probably insist on a single string format for literal
    string constants. However, you can usually write a short conversion function that
    will translate the literal strings in your language to whatever format you choose.
  prefs: []
  type: TYPE_NORMAL
- en: '**5.3 Character Set Data Types**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Like strings, character set data types (or just *character sets*) are a composite
    data type built upon the character data type. A *character set* is a mathematical
    set of characters. Membership in a set is a binary relation: a character is either
    in the set or not, and you can’t have multiple copies of the same character in
    a character set. Furthermore, the concept of sequence (whether one character comes
    before another, as in a string) is foreign to a character set. If two characters
    are members of a set, their order in the set is irrelevant.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 5-4](ch05.xhtml#ch05tab04) lists some common operations that applications
    perform on character sets.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-4:** Common Character Set Functions'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Function/operator** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Membership (in) | Checks to see if a character is a member of a character
    set (returns `true`/`false`). |'
  prefs: []
  type: TYPE_TB
- en: '| Intersection | Returns the intersection of two character sets (that is, the
    set of characters that are members of both sets). |'
  prefs: []
  type: TYPE_TB
- en: '| Union | Returns the union of two character sets (that is, all the characters
    that are members of either set or both sets). |'
  prefs: []
  type: TYPE_TB
- en: '| Difference | Returns the difference of two sets (that is, those characters
    in one set that are not in the other). |'
  prefs: []
  type: TYPE_TB
- en: '| Extraction | Extracts a single character from a set. |'
  prefs: []
  type: TYPE_TB
- en: '| Subset | Returns `true` if one character set is a subset of another. |'
  prefs: []
  type: TYPE_TB
- en: '| Proper subset | Returns `true` if one character set is a proper subset of
    another. |'
  prefs: []
  type: TYPE_TB
- en: '| Superset | Returns `true` if one character set is a superset of another.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Proper superset | Returns `true` if one character set is a proper superset
    of another. |'
  prefs: []
  type: TYPE_TB
- en: '| Equality | Returns `true` if one character set is equal to another. |'
  prefs: []
  type: TYPE_TB
- en: '| Inequality | Returns `true` if one character set is not equal to another.
    |'
  prefs: []
  type: TYPE_TB
- en: '***5.3.1 Powerset Representation of Character Sets***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are many different ways to represent character sets. Several languages
    implement them using an array of Boolean values (one Boolean value for each possible
    character code). Each Boolean value determines whether its corresponding character
    is (`true`) or is not (`false`) a member of the character set. To conserve memory,
    most character set implementations allocate only a single bit for each character
    in the set; therefore, they consume 16 bytes (128 bits) of memory when supporting
    128 characters, or 32 bytes (256 bits) when supporting up to 256 possible characters.
    This representation of a character set is known as a *[powerset](gloss01.xhtml#gloss01_199)*.
  prefs: []
  type: TYPE_NORMAL
- en: The HLA language uses an array of 16 bytes to represent the 128 possible ASCII
    characters, which is organized in memory as shown in [Figure 5-6](ch05.xhtml#ch05fig06).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-6: HLA character set representation*'
  prefs: []
  type: TYPE_NORMAL
- en: Bit 0 of byte 0 corresponds to ASCII code `0` (the NUL character). If this bit
    is `1`, then the character set contains the NUL character; if this bit is `0`,
    then the character set does not contain the NUL character. Likewise, bit 1 of
    byte 8 corresponds to ASCII code `65`, an uppercase *A*. Bit 65 will contain a
    `1` if *A* is a current member of the character set, and `0` if it is not.
  prefs: []
  type: TYPE_NORMAL
- en: Pascal (for example, Delphi) uses a similar scheme to represent character sets.
    Delphi allows up to 256 characters in a character set, so Delphi character sets
    consume 256 bits (or 32 bytes) of memory.
  prefs: []
  type: TYPE_NORMAL
- en: While there are other ways to implement character sets, this bit vector (array)
    implementation makes it very easy to perform set operations like union, intersection,
    difference comparison, and membership tests.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.3.2 List Representation of Character Sets***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes a powerset bitmap just isn’t the right representation for a character
    set. For example, if your sets are always very small (no more than three or four
    members), using 16 or 32 bytes to represent each of them can be overkill. In this
    case, you’d be better off using a character string to represent a list of characters.^([12](footnotes.xhtml#fn5_12a))
    If you rarely have more than a few characters in a set, scanning through a string
    to locate a particular character is probably efficient enough for most applications.
    Likewise, if your character set has a large number of possible characters, then
    the powerset representation could become huge (for example, implementing the original
    Unicode UCS-2 character set as a powerset would require 8,192 bytes of memory,
    even if there was only a single character in the set). In this situation, a list
    or character string representation could be more appropriate than a powerset,
    as you don’t need to reserve memory for all possible members of the set (only
    those that are actually present).
  prefs: []
  type: TYPE_NORMAL
- en: '**5.4 Designing Your Own Character Set**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very little is sacred about the ASCII, EBCDIC, and Unicode character sets. Their
    primary advantage is that they are international standards to which many systems
    adhere. If you stick with one of these standards, chances are good you’ll be able
    to exchange information with other people, which is what these codes were designed
    for.
  prefs: []
  type: TYPE_NORMAL
- en: However, they were not designed to make various character computations easy.
    ASCII and EBCDIC were developed with now-antiquated hardware in mind—mechanical
    teletypewriters’ keyboards and punched-card systems, respectively. Given that
    such equipment is found mainly in museums today, the layout of the codes in these
    character sets has almost no benefit in modern computer systems. If we could design
    our own character sets today, they’d be considerably different from ASCII or EBCDIC.
    They’d probably be based on modern keyboards (so they’d include codes for common
    keys, like LEFT ARROW, RIGHT ARROW, page up, and page down). They’d also be laid
    out to make various common computations a whole lot easier.
  prefs: []
  type: TYPE_NORMAL
- en: Although the ASCII and EBCDIC character sets are not going away any time soon,
    there’s nothing stopping you from defining your own application-specific character
    set. Of course, such a set is, well, application-specific, and you won’t be able
    to share text files containing characters encoded in your custom character set
    with applications that are ignorant of your private encoding. But it’s fairly
    easy to translate between different character sets using a lookup table, so you
    can convert between your application’s internal character set and an external
    character set (like ASCII) when performing I/O operations. Assuming you pick a
    reasonable encoding that makes your programs more efficient overall, the loss
    of efficiency during I/O can be worthwhile. But how do you choose an encoding?
  prefs: []
  type: TYPE_NORMAL
- en: The first question you have to ask yourself is, “How many characters do I want
    to support in my character set?” Obviously, the number of characters you choose
    will directly affect the size of your character data. An easy choice is 256 possible
    characters, because bytes are the most common primitive data type that software
    uses to represent character data. Keep in mind, however, that if you don’t really
    need 256 characters, you probably shouldn’t try to define that many in your character
    set. For example, if you can get by with 128, or even 64, characters in your custom
    character set, then “text files” you create with it will compress better. Likewise,
    data transmissions using it will be faster if you only have to transmit 6 or 7
    bits for each character instead of 8\. If you need more than 256 characters, you’ll
    have to weigh the advantages and disadvantages of using multiple code pages, double-byte
    character sets, or 16-bit characters. And keep in mind that Unicode provides support
    for user-defined characters. So, if you need more than 256 characters in your
    character set, you might consider inserting it into Unicode to remain “somewhat
    standard” with the rest of the world.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll define a character set containing 128 characters using
    an 8-bit byte. For the most part, we’ll simply rearrange the codes in the ASCII
    character set to make them more convenient for several calculations, and we’ll
    rename a few of the control codes so they make sense on modern systems instead
    of the old mainframes and teletypes for which they were created. We’ll also add
    a few new characters beyond those defined by the ASCII standard. Again, the main
    purpose of this exercise is to make various computations more efficient, not create
    new characters. We’ll call this the *HyCode* character set.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*This point bears repeating: the use of HyCode in this chapter is not an attempt
    to create some new character set standard. It’s simply a demonstration of how
    you can create a custom, application-specific character set to improve your programs.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***5.4.1 Designing an Efficient Character Set***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We should think about several things when designing a new character set. For
    example, do we need to be able to represent strings of characters using an existing
    string format? This can influence the encoding of our strings—if you want to use
    function libraries that operate on zero-terminated strings, then you need to reserve
    encoding `0` in your custom character set for use as an end-of-string marker.
    Keep in mind, however, that a fair number of string functions won’t work with
    your new character set, no matter what you do. Functions like `stricmp()` work
    only if you use the same representation for alphabetic characters as ASCII (or
    some other common character set). Therefore, you shouldn’t feel hampered by the
    requirements of some particular string representation, because you’re going to
    have to write many of your own string functions to process your custom characters
    anyway. The HyCode character set doesn’t reserve code `0` for an end-of-string
    marker, and that’s okay because zero-terminated strings are not very efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at programs that use character functions, you’ll see that certain
    functions occur frequently, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Check a character to see if it is a digit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert a digit character to its numeric equivalent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert a numeric digit to its character equivalent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check a character to see if it is alphabetic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check a character to see if it is a lowercase character.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check a character to see if it is an uppercase character.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare two characters (or strings) using a *case-insensitive* comparison.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sort a set of alphabetic strings (case-sensitive and case-insensitive sorting).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check a character to see if it is alphanumeric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check a character to see if it is legal in an identifier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check a character to see if it is a common arithmetic or logical operator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check a character to see if it is a bracketing character (that is, one of *(*,
    *)*, *[*, *]*, *{*, *}*, *<*, or *>*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check a character to see if it is a punctuation character.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check a character to see if it is a *whitespace* character (such as a space,
    tab, or newline).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check a character to see if it is a cursor control character.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check a character to see if it is a scroll control key (such as PGUP, PGDN,
    HOME, and END).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check a character to see if it is a function key.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll design the HyCode character set to make these types of operations as efficient
    and easy as possible. One huge improvement we can make over the ASCII character
    set is to assign contiguous character codes to characters belonging to the same
    type, such as alphabetic characters and control characters, so we can do any of
    the preceding tests by using a pair of comparisons. For example, it would be nice
    if we could determine that a particular character is some sort of punctuation
    mark by comparing against two values that represent upper and lower bounds of
    the entire range of such characters, which we can’t do in ASCII because the punctuation
    marks are spread throughout the character set. While it’s not possible to satisfy
    every conceivable range comparison this way, we can design our character set to
    accommodate the most common tests with as few comparisons as possible.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.4.2 Grouping the Character Codes for Numeric Digits***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can achieve the first three functions in the previous list by reserving the
    character codes `0` through `9` for the characters 0 through 9\. First, by using
    a single unsigned comparison to check if a character code is less than or equal
    to `9`, we can see if a character is a digit. Next, converting between characters
    and their numeric representations is trivial, because the character code and the
    numeric representation are one and the same.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.4.3 Grouping Alphabetic Characters***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The ASCII character set, though nowhere near as bad as EBCDIC, just isn’t well
    designed for dealing with alphabetic character tests and operations. Here are
    some problems with ASCII that we’ll solve with HyCode:'
  prefs: []
  type: TYPE_NORMAL
- en: The alphabetic characters lie in two disjoint ranges. Tests for an alphabetic
    character require four comparisons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lowercase characters have ASCII codes that are greater than the uppercase
    characters. If we’re going to do a case-sensitive comparison, it’s more intuitive
    to treat lowercase characters as being less than uppercase characters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All lowercase characters have a greater value than any individual uppercase
    character. This leads to counterintuitive results, such as *a* being greater than
    *B*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HyCode solves these problems in a couple of interesting ways. First, HyCode
    uses encodings `$4C` through `$7F` to represent the 52 alphabetic characters.
    Because HyCode uses only 128 character codes (`$00..$7F`), the alphabetic codes
    consume the last 52 character codes. This means that we can test a character to
    see if it is alphabetic by comparing whether the code is greater than or equal
    to `$4C`. In a high-level language, you’d write the comparison like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, if your compiler supports the HyCode character set, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In assembly language, you could use a pair of instructions like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'HyCode interleaves the lowercase and uppercase characters (that is, the sequential
    encodings are for the characters *a*, *A*, *b*, *B*, *c*, *C*, and so on). This
    makes sorting and comparing strings very easy, regardless of whether you’re doing
    a case-sensitive or case-insensitive search. The interleaving uses the LO bit
    of the character code to determine whether the character code is lowercase (LO
    bit is `0`) or uppercase (LO bit is `1`). HyCode uses the following encodings
    for alphabetic characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Checking for an uppercase or lowercase alphabetic using HyCode is more work
    than checking whether a character is alphabetic, but in assembly it’s still less
    work than the equivalent ASCII comparison. To test a character to see if it’s
    a member of a single case, you need two comparisons—first to see if it’s alphabetic,
    then to determine its case. In C/C++ you can use statements like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The subexpression `(c & 1)` evaluates `true` (`1`) if the LO bit of `c` is
    `1`, meaning we have an uppercase character if `c` is alphabetic. Likewise, `!(c
    & 1)` evaluates `true` if the LO bit of `c` is `0`, meaning we have a lowercase
    character. If you’re working in 80x86 assembly language, you can test a character
    to see if it’s uppercase or lowercase by using three machine instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Very few languages provide the equivalent of an `ror()` operation, and only
    a few allow you to (easily) treat character values as signed and unsigned within
    the same code sequence. Therefore, this sequence is probably limited to assembly
    language programs.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.4.4 Comparing Alphabetic Characters***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The HyCode grouping of alphabetic characters means that lexicographical ordering
    (“dictionary ordering”) is almost free. Sorting your strings by comparing the
    HyCode character values gives you lexicographical order, because HyCode defines
    the following relations on the alphabetic characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This is exactly the relationship you want for lexicographical ordering, and
    it’s also the one most people would intuitively expect. To do a case-insensitive
    comparison, you simply mask out the LO bits (or force them both to `1`) of the
    alphabetic characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the benefit of the HyCode character set when doing case-insensitive
    comparisons, let’s first take a look at what the standard case-insensitive character
    comparison would look like in C/C++ for two ASCII characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This code doesn’t look too bad, but consider what the `toupper()` function (or,
    usually, macro) expands to:^([13](footnotes.xhtml#fn5_13a))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'With this macro, you wind up with the following once the C preprocessor expands
    the previous `if` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This expands to 80x86 code similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In HyCode, case-insensitive comparisons are much simpler. Here’s what the HLA
    assembly code would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the HyCode sequence uses half the instructions for a case-insensitive
    comparison of two characters.
  prefs: []
  type: TYPE_NORMAL
- en: '***5.4.5 Grouping Other Characters***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Because alphabetic characters are at one end of the character code range and
    numeric characters are at the other, it takes two comparisons to check a character
    to see if it’s alphanumeric (which is still better than the four comparisons necessary
    in ASCII). Here’s the Pascal/Delphi code you’d use to see if a character is alphanumeric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Several programs (beyond compilers) need to efficiently process strings of characters
    that represent program identifiers. Most languages allow alphanumeric characters
    in identifiers, and, as you just saw, we can check a character to see if it’s
    alphanumeric using only two comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: Many languages also allow underscores within identifiers, and some languages,
    such as MASM, allow other characters like the “at” character (`@`) and dollar
    sign (`$`) to appear within identifiers. Therefore, by assigning the underscore
    character the value `75`, and by assigning the `$` and `@` characters the respective
    codes `73` and `74`, we can still test for an identifier character using only
    two comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: For similar reasons, HyCode groups together the cursor control keys, the whitespace
    characters, the bracketing characters (parentheses, brackets, braces, and angle
    brackets), the arithmetic operators, the punctuation characters, and so on. [Table
    5-5](ch05.xhtml#ch05tab05) lists the complete HyCode character set. If you study
    the numeric codes assigned to each character, you’ll see that they allow for efficient
    computation of most of the character operations described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-5:** The HyCode Character Set'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Binary** | **Hex** | **Decimal** | **Character** | **Binary** | **Hex**
    | **Decimal** | **Character** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_0000` | `00` | `0` | `0` | `0001_1110` | `1E` | `30` | End |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_0001` | `01` | `1` | `1` | `0001_1111` | `1F` | `31` | Home |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_0010` | `02` | `2` | `2` | `0010_0000` | `20` | `32` | PgDn |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_0011` | `03` | `3` | `3` | `0010_0001` | `21` | `33` | PgUp |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_0100` | `04` | `4` | `4` | `0010_0010` | `22` | `34` | Left |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_0101` | `05` | `5` | `5` | `0010_0011` | `23` | `35` | Right |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_0110` | `06` | `6` | `6` | `0010_0100` | `24` | `36` | Up |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_0111` | `07` | `7` | `7` | `0010_0101` | `25` | `37` | Down/linefeed
    |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_1000` | `08` | `8` | `8` | `0010_0110` | `26` | `38` | Nonbreaking
    space |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_1001` | `09` | `9` | `9` | `0010_0111` | `27` | `39` | Paragraph |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_1010` | `0A` | `10` | Keypad | `0010_1000` | `28` | `40` | Carriage
    return |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_1011` | `0B` | `11` | Cursor | `0010_1001` | `29` | `41` | Newline/enter
    |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_1100` | `0C` | `12` | Function | `0010_1010` | `2A` | `42` | Tab |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_1101` | `0D` | `13` | Alt | `0010_1011` | `2B` | `43` | Space |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_1110` | `0E` | `14` | Control | `0010_1100` | `2C` | `44` | `(` |'
  prefs: []
  type: TYPE_TB
- en: '| `0000_1111` | `0F` | `15` | Command | `0010_1101` | `2D` | `45` | `)` |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_0000` | `10` | `16` | Len | `0010_1110` | `2E` | `46` | `[` |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_0001` | `11` | `17` | Len128 | `0010_1111` | `2F` | `47` | `]` |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_0010` | `12` | `18` | Bin128 | `0011_0000` | `30` | `48` | `{` |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_0011` | `13` | `19` | Eos | `0011_0001` | `31` | `49` | `}` |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_0100` | `14` | `20` | Eof | `0011_0010` | `32` | `50` | `<` |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_0101` | `15` | `21` | Sentinel | `0011_0011` | `33` | `51` | `>` |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_0110` | `16` | `22` | Break/interrupt | `0011_0100` | `34` | `52` |
    `=` |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_0111` | `17` | `23` | Escape/cancel | `0011_0101` | `35` | `53` | `^`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_1000` | `18` | `24` | Pause | `0011_0110` | `36` | `54` | `&#124;`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_1001` | `19` | `25` | Bell | `0011_0111` | `37` | `55` | `&` |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_1010` | `1A` | `26` | Back tab | `0011_1000` | `38` | `56` | `-` |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_1011` | `1B` | `27` | Backspace | `0011_1001` | `39` | `57` | `+` |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_1100` | `1C` | `28` | Delete |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| `0001_1101` | `1D` | `29` | Insert |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| `0011_1010` | `3A` | `58` | `*` | `0101_1101` | `5D` | `93` | `I` |'
  prefs: []
  type: TYPE_TB
- en: '| `0011_1011` | `3B` | `59` | `/` | `0101_1110` | `5E` | `94` | `j` |'
  prefs: []
  type: TYPE_TB
- en: '| `0011_1100` | `3C` | `60` | `%` | `0101_1111` | `5F` | `95` | `J` |'
  prefs: []
  type: TYPE_TB
- en: '| `0011_1101` | `3D` | `61` | `~` | `0110_0000` | `60` | `96` | `k` |'
  prefs: []
  type: TYPE_TB
- en: '| `0011_1110` | `3E` | `62` | `!` | `0110_0001` | `61` | `97` | `K` |'
  prefs: []
  type: TYPE_TB
- en: '| `0011_1111` | `3F` | `63` | `?` | `0110_0010` | `62` | `98` | `l` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_0000` | `40` | `64` | `,` | `0110_0011` | `63` | `99` | `L` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_0001` | `41` | `65` | `.` | `0110_0100` | `64` | `100` | `m` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_0010` | `42` | `66` | `:` | `0110_0101` | `65` | `101` | `M` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_0011` | `43` | `67` | `;` | `0110_0110` | `66` | `102` | `n` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_0100` | `44` | `68` | `"` | `0110_0111` | `67` | `103` | `N` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_0101` | `45` | `69` | `''` | `0110_1000` | `68` | `104` | `o` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_0110` | `46` | `70` | `` ` `` | `0110_1001` | `69` | `105` | `O` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_0111` | `47` | `71` | `\` | `0110_1010` | `6A` | `106` | `p` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_1000` | `48` | `72` | `#` | `0110_1011` | `6B` | `107` | `P` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_1001` | `49` | `73` | `$` | `0110_1100` | `6C` | `108` | `q` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_1010` | `4A` | `74` | `@` | `0110_1101` | `6D` | `109` | `Q` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_1011` | `4B` | `75` | `_` | `0110_1110` | `6E` | `110` | `r` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_1100` | `4C` | `76` | `a` | `0110_1111` | `6F` | `111` | `R` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_1101` | `4D` | `77` | `A` | `0111_0000` | `70` | `112` | `s` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_1110` | `4E` | `78` | `b` | `0111_0001` | `71` | `113` | `S` |'
  prefs: []
  type: TYPE_TB
- en: '| `0100_1111` | `4F` | `79` | `B` | `0111_0010` | `72` | `114` | `t` |'
  prefs: []
  type: TYPE_TB
- en: '| `0101_0000` | `50` | `80` | `c` | `0111_0011` | `73` | `115` | `T` |'
  prefs: []
  type: TYPE_TB
- en: '| `0101_0001` | `51` | `81` | `C` | `0111_0100` | `74` | `116` | `u` |'
  prefs: []
  type: TYPE_TB
- en: '| `0101_0010` | `52` | `82` | `d` | `0111_0101` | `75` | `117` | `U` |'
  prefs: []
  type: TYPE_TB
- en: '| `0101_0011` | `53` | `83` | `D` | `0111_0110` | `76` | `118` | `v` |'
  prefs: []
  type: TYPE_TB
- en: '| `0101_0100` | `54` | `84` | `e` | `0111_0111` | `77` | `119` | `V` |'
  prefs: []
  type: TYPE_TB
- en: '| `0101_0101` | `55` | `85` | `E` | `0111_1000` | `78` | `120` | `w` |'
  prefs: []
  type: TYPE_TB
- en: '| `0101_0110` | `56` | `86` | `f` | `0111_1001` | `79` | `121` | `W` |'
  prefs: []
  type: TYPE_TB
- en: '| `0101_0111` | `57` | `87` | `F` | `0111_1010` | `7A` | `122` | `x` |'
  prefs: []
  type: TYPE_TB
- en: '| `0101_1000` | `58` | `88` | `g` | `0111_1011` | `7B` | `123` | `X` |'
  prefs: []
  type: TYPE_TB
- en: '| `0101_1001` | `59` | `89` | `G` | `0111_1100` | `7C` | `124` | `y` |'
  prefs: []
  type: TYPE_TB
- en: '| `0101_1010` | `5A` | `90` | `h` | `0111_1101` | `7D` | `125` | `Y` |'
  prefs: []
  type: TYPE_TB
- en: '| `0101_1011` | `5B` | `91` | `H` | `0111_1110` | `7E` | `126` | `z` |'
  prefs: []
  type: TYPE_TB
- en: '| `0101_1100` | `5C` | `92` | `i` | `0111_1111` | `7F` | `127` | `Z` |'
  prefs: []
  type: TYPE_TB
- en: '**5.5 For More Information**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hyde, Randall. “HLA Standard Library Reference Manual.” n.d. *[http://www.plantation-productions.com/Webster/HighLevelAsm/HLADoc/](http://www.plantation-productions.com/Webster/HighLevelAsm/HLADoc/)*
    or *[https://bit.ly/2W5G1or](https://bit.ly/2W5G1or).*
  prefs: []
  type: TYPE_NORMAL
- en: IBM. “ASCII and EBCDIC Character Sets.” n.d. *[https://ibm.co/33aPn3t](https://ibm.co/33aPn3t)*.
  prefs: []
  type: TYPE_NORMAL
- en: Unicode, Inc. “Unicode Technical Site.” Last updated March 4, 2020\. *[https://www.unicode.org/](https://www.unicode.org/)*.
  prefs: []
  type: TYPE_NORMAL
