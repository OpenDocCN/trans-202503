<html><head></head><body>
<div id="sbo-rt-content"><h2 class="h2" id="ch07"><span epub:type="pagebreak" id="page_115"/><strong><span class="big">7</span><br/>LARGE LANGUAGE MODELS: TRUE AI AT LAST?</strong></h2>
<div class="image1"><img alt="Image" height="189" src="../images/common.jpg" width="189"/></div>
<p class="noindentsa">Future historians might point to the fall 2022 release of OpenAI’s ChatGPT large language model as the dawn of true AI. Given what I’ve already seen as I write this in late March 2023, I would agree with such an assessment.</p>
<p class="indent">In this chapter, we’ll first explore what existing large language models can do, then follow that up with a description of what they are and how they work. For all their impressive abilities, ultimately these models are neural networks built and trained like all the neural networks that came before. That fact alone means the connectionists were right from the beginning. Might Frank Rosenblatt be smiling in his grave?</p>
<p class="indent">I’ve already tipped my hand regarding my belief that ChatGPT and models like it represent something new that’s worthy of being called true AI. My hope is that, by the end of the chapter, you’ll agree.</p>
<p class="center">****</p>
<p class="indent">The phrase <a href="glossary.xhtml#glo5"><em>artificial intelligence</em></a> is somewhat ambiguous and must be provided with a more nuanced definition before we proceed. Practitioners typically divide AI into two kinds: <a href="glossary.xhtml#glo6"><em>artificial narrow intelligence (ANI)</em></a> and <span epub:type="pagebreak" id="page_116"/><a href="glossary.xhtml#glo4"><em>artificial general intelligence (AGI)</em></a>. The former encapsulates everything we’ve discussed so far. The latter refers to truly sentient and intelligent machines—the stuff of science fiction.</p>
<p class="indent">The models existing as of the time of writing of this book are definitely not AGI. However, they are not merely ANI; they appear to be something entirely new, something in between. The title of a recent paper by Microsoft researchers Sébastien Bubeck et al., “Sparks of Artificial General Intelligence,” strikes me as appropriate.</p>
<p class="indent"><a href="glossary.xhtml#glo59"><em>Large language models (LLMs)</em></a> accept as input a text prompt supplied by a user. They then generate output text, word by word (really, token by token), using the prompt and all previously generated words as a guide. In effect, LLMs’ only design goal is to be very good at predicting the next word in a sequence of words initiated by the input prompt. That’s all they’re trained to do. However, that is not all that they <em>learn</em> to do. The reason why AI researchers are so excited by LLMs is that somewhere along the way, while learning to be expert text generators, LLMs also learn a host of emergent abilities, including question answering, mathematical reasoning, high-quality computer programming, and logical reasoning.</p>
<p class="indent">The philosophical implications of these unexpected, emergent abilities are profound. The abilities of LLMs raise questions about the nature of thought, the meaning of consciousness, and the (assumed) uniqueness of the human mind. We’re not in a position to answer these questions in depth, but I will return to some of them in <a href="ch08.xhtml">Chapter 8</a>.</p>
<p class="indent">Now, let’s get into things by exploring the capabilities of LLMs. We can touch on only a few emergent abilities here, but the internet is already full of impressive, funny, and even frightening examples of LLMs in action. I recommend a review of such examples, though as always, care must be taken to judge the quality of the information source. See the resources at the end of this book for some of the online (and print) resources I suggest taking a look at.</p>
<p class="indent">The AI community is racing ahead as fast as possible in this area. By the time you read these words, LLMs will undoubtedly be even more impressive than they already are at the time I’m writing them.</p>
<p class="center">****</p>
<p class="indent">What follows are examples of LLMs in action. For most situations, I’ll show the output of three models: Bard, GPT-3.5, and GPT-4. All output represents model performance as of late March 2023. In each case, I used the public interface via a web browser. Bard is a Google model. GPT-3.5 and GPT-4 are from OpenAI.</p>
<p class="indent">LLMs are expert users of language. The models are used in dialog format, beginning with a prompt or question from the user. For example, I gave each of the three models this prompt:</p>
<div class="bq">
<p class="noindent"><em>Write a one-paragraph bedtime story about a teddy bear, a magic carpet, and the moon that is suitable for children under the age of six.</em></p>
</div>
<p class="indent"><span epub:type="pagebreak" id="page_117"/>Here is Bard’s reply:</p>
<div class="bq">
<p class="noindent">Once upon a time, there was a little teddy bear who lived in a toy box. He loved to play with his friends, but he always dreamed of flying to the moon. One day, he found a magic carpet in his toy box. The carpet could fly! The teddy bear was so excited, he jumped on the carpet and flew to the moon. He had a wonderful time exploring the moon, and he even met some friendly aliens. When it was time to go home, he said goodbye to his new friends and flew back to Earth on his magic carpet. He was so happy to be home, and he couldn’t wait to tell his friends all about his adventure.</p>
</div>
<p class="indent">The story is complete, in a single paragraph, and uses a simple sentence structure suitable for children.</p>
<p class="indent">GPT-3.5 replied like this:</p>
<div class="bq">
<p class="noindent">Once upon a time, there was a teddy bear named Timmy who loved to go on adventures. One night, he hopped onto a magic carpet and flew up towards the moon. The moon was so big and bright, and Timmy couldn’t resist exploring it. He hopped and skipped around the craters and met some friendly aliens who showed him their moon garden. Timmy had a great time, but eventually, he knew it was time to go back home. He hopped back onto his magic carpet and flew back to his cozy bed, where he cuddled up with his favorite blanket and fell fast asleep, dreaming of his amazing moon adventure.</p>
</div>
<p class="indent">GPT-3.5’s story is also complete and includes friendly aliens as well. This time, the teddy bear gets a name, Timmy. The language is still appropriate for young children, but GPT-3.5 was more descriptive. Bard tells us that the teddy bear explores the moon and meets some friendly aliens, but GPT-3.5 tells us that Timmy hopped and skipped around the craters and that the friendly aliens showed him their moon garden.</p>
<p class="indent">GPT-4, OpenAI’s most advanced model, produced this:</p>
<div class="bq">
<p class="noindent">Once upon a time, under a twinkling night sky, a little teddy bear named Cuddles discovered a colorful magic carpet hidden in the corner of his cozy room. Curious and excited, Cuddles climbed onto the carpet, and together, they soared up to the sky, bound for the silvery moon. As they journeyed through the soft, velvety clouds, Cuddles and the magic carpet danced with twinkling stars and whispered secrets to the cool night breeze. When they reached the moon, it welcomed them with a gentle, glowing smile, and they spent the night playing hide-and-seek in the moon’s soft, dusty craters. As the sun began to rise, Cuddles and the magic carpet said their goodbyes to their new lunar friend and floated back down to the little room, ready for a peaceful night’s sleep filled with the sweetest dreams.</p>
</div>
<p class="indent">Subjectively, I feel that GPT-4 produced the most descriptive story, though I do like GPT-3.5’s moon garden. Bard’s story was pretty dry but <span epub:type="pagebreak" id="page_118"/>perhaps more accessible to very young children. The most remarkable thing about all of them, however, is that they could be written at all. Less than a year ago (remember, it’s early 2023 for me), talking about AI-generated stories was little more than wishful thinking.</p>
<p class="indent">I altered the prompt I gave GPT-4 to ask for a one-paragraph story involving a teddy bear, a magic carpet, and the moon, but this time of interest to adults, not children. Here’s its reply:</p>
<div class="bq">
<p class="noindent">Once upon a twilight’s embrace, a solitary teddy bear, worn and forgotten by the whims of time, discovered an ancient magic carpet, frayed at the edges yet pulsating with an otherworldly energy. Under the watchful gaze of the moon, the duo embarked on a nocturnal journey, weaving through the labyrinth of a world grown cold and cynical, where dreams were forsaken and innocence a distant memory. As the gentle light of the moon cast its silver glow upon the unlikely companions, the teddy bear served as a symbol of the dreams long since buried beneath the weight of adult concerns, while the magic carpet whispered of the untamed freedom and adventure that lay dormant in the hearts of those who dared to believe. Together, they soared, a wistful reminder of the bittersweet yearning for the fleeting magic of childhood, leaving behind a trail of stardust that shimmered with the promise of hope and the melancholic beauty of a time that once was.</p>
</div>
<p class="indent">GPT-4 clearly possesses a solid understanding of language and grammar. One might argue that it understands human feelings and emotions as well (though it doesn’t, does it?).</p>
<p class="indent">Let’s review a few more examples before diving into what’s inside this strange, new kind of mind.</p>
<p class="center">****</p>
<p class="indent">GPT-4 understands imagery along with many different programming and markup languages, including LaTeX, a markup language commonly used in book publishing (like this book). I asked GPT-4 to generate LaTeX to draw a simple country scene showing a house, a barn, a grain silo, and four cows. <a href="ch07.xhtml#ch07fig01">Figure 7-1</a> shows the result.</p>
<div class="image"><img alt="Image" height="262" id="ch07fig01" src="../images/ch07fig01.jpg" width="503"/></div>
<p class="figcap"><em>Figure 7-1: A country scene created by GPT-4</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_119"/>You might be tempted to chuckle at the crude depiction, but consider all that went into creating it. Here’s my prompt:</p>
<div class="bq">
<p class="noindent"><em>produce LaTeX code using TikZ to draw the following: a country scene with a house, a barn, a grain silo, and four cows</em></p>
</div>
<p class="indent">GPT-4 had to understand the directions: draw a scene with a house, a barn, a grain silo, and four cows. That means it had to understand the meaning of key words and correctly associate them so that “grain” and “silo” went together, and likewise “four” and “cows.”</p>
<p class="indent">Next, it had to “imagine” the layout of the scene and how each object could be represented using the crude shapes supplied by the TikZ package. (TikZ is a LaTeX package for drawing simple graphic shapes.) The figure doesn’t show it, but the barn and house both have red roofs. Coincidence?</p>
<p class="indent">GPT-4 isn’t a diffusion model like the image generators of <a href="ch06.xhtml">Chapter 6</a>. The output created by GPT-4 wasn’t the image in <a href="ch07.xhtml#ch07fig01">Figure 7-1</a>, but the following LaTeX code:</p>
<pre>% Barn&#13;
\draw[fill=brown!50] (5,0) rectangle (8,3);&#13;
\draw[fill=red!50] (6.5,3)--(8,3)--(6.5,5)--(5,3)--cycle;&#13;
\draw[fill=white!70] (6,1) rectangle (7,2);</pre>
<p class="indent">I used that code to create <a href="ch07.xhtml#ch07fig01">Figure 7-1</a>.</p>
<p class="indent">If you’re wondering how GPT-4 could do all of this, so am I and many other people. Such abilities emerged from the model when trained; they were not intended. This is why I believe future historians will mark fall 2022 as the dawn of true AI. Hold on to your hats; it gets better.</p>
<p class="center">****</p>
<p class="indent">Psychologists discuss a concept known as the “theory of mind,” which refers to the ability to infer the mental states of others and to ascribe thoughts or beliefs to them. Only human beings and some other primates possess theory of mind. That is, until now.</p>
<p class="indent">Large language models also seem to possess theory of mind at a level matching that of older human children, if not higher (for GPT-4). This is the conclusion reached by Michal Kosinski of Stanford University in his 2023 paper “Theory of Mind May Have Spontaneously Emerged in Large Language Models.” Let’s put this claim to the test by asking Bard, GPT-3.5, and GPT-4 to answer a question that requires knowing what might be in another person’s mind. Here’s the prompt:</p>
<div class="bq">
<p class="noindent"><em>Mary told Amanda that she was in love with John. Amanda told Kathy what Mary had said. A week later, Mary told Amanda that she was no longer in love with John but was now in love with Ralph. After that, Amanda spoke with Kathy about the weather. Who does Kathy think is Mary’s love?</em></p>
</div>
<p class="indent">Before continuing, reach your own conclusion: who does Kathy think Mary is in love with?</p>
<p class="indent"><span epub:type="pagebreak" id="page_120"/>Each model answered the question and then offered its rationale. Here is Bard’s reply:</p>
<div class="bq">
<p class="noindent">Kathy thinks Mary’s love is Ralph.</p>
<p class="noindent">Kathy knows that Mary was in love with John, but she also knows that Mary is no longer in love with John. Kathy also knows that Mary is now in love with Ralph. Therefore, Kathy thinks Mary’s love is Ralph.</p>
<p class="noindent">It is possible that Kathy is mistaken. It is also possible that Mary is not sure who she loves. However, based on the information that Kathy has, it seems that Kathy thinks Mary’s love is Ralph.</p>
</div>
<p class="indent">I find it interesting that Bard justified its answer, then further commented on what Mary might be thinking. However, Bard assumed that Amanda must have told Kathy about Mary’s change in affection.</p>
<p class="indent">Here’s what GPT-3.5 thinks of the scenario:</p>
<div class="bq">
<p class="noindent">Based on the information given, it is impossible to determine who Kathy thinks is Mary’s love. The last piece of information we have is that Amanda spoke with Kathy about the weather, and there is no indication that Kathy was told about Mary’s change of heart. Therefore, Kathy may still believe that Mary is in love with John or may not have any opinion on the matter.</p>
</div>
<p class="indent">GPT-3.5 does not make the same assumption that Bard makes. It’s aware that the facts only indicate that Amanda and Kathy spoke about the weather after Mary’s change of heart. I’m impressed that it used the phrase “the last piece of information,” implying that GPT-3.5 was somehow sequencing the events in the prompt.</p>
<p class="indent">Finally, what does GPT-4 have to say? Let’s find out:</p>
<div class="bq">
<p class="noindent">Kathy thinks Mary’s love is John. This is because Amanda told Kathy about Mary’s feelings for John earlier, but she did not tell Kathy about the change in Mary’s feelings when Mary fell in love with Ralph.</p>
</div>
<p class="indent">GPT-4’s answer is the likely answer most humans would give. The prompt states only that Amanda and Kathy spoke about the weather, so the majority of humans would likely take that to mean Amanda and Kathy didn’t talk about any other subjects.</p>
<p class="indent">Large language models, as currently presented, use a chat format, meaning the user can enter follow-up prompts, and the model will apply those prompts to the earlier part of the conversation. For example, I followed up with GPT-4 using this prompt:</p>
<div class="bq">
<p class="noindent"><em>We know that Amanda spoke with Kathy about the weather. Does that mean she didn’t speak about anything else? How might that affect what Kathy believes about Mary’s love?</em></p>
</div>
<p class="indent">To which GPT-4 replied:</p>
<div class="bq">
<p class="noindent">My earlier response assumed that Amanda only spoke with Kathy about the weather after Mary revealed her new feelings for Ralph. However, since the text does not explicitly state that Amanda did <span epub:type="pagebreak" id="page_121"/>not mention Mary’s updated feelings for Ralph, it is possible that Amanda might have told Kathy about Mary’s new love interest. If that were the case, Kathy would know that Mary is now in love with Ralph. The text does not provide enough information to definitively conclude what Kathy believes about Mary’s love at this point.</p>
</div>
<p class="indent">My follow-up prompt led GPT-4 to recognize that it had made a possibly incorrect assumption in its first reply. In working with LLMs in chat mode, it is often the case that some level of interaction between the user and the model is required to help the model focus, as our next experiment will demonstrate.</p>
<p class="center">****</p>
<p class="indent">Amos Tversky and Daniel Kahneman are credited with presenting what is now known as “the Linda problem”:</p>
<div class="bq">
<p class="noindent"><em>Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.</em></p>
<p class="noindent"><em>Which is more probable?</em></p>
<ol>
<li class="noindent"><em>Linda is a bank teller.</em></li>
<li class="noindent"><em>Linda is a bank teller and is active in the feminist movement.</em></li>
</ol>
</div>
<p class="indent">Approximately 80 percent of humans choose option 2. According to basic probability theory, the correct answer is option 1. Interestingly, according to Tversky and Kahneman, even mathematicians and statisticians selected option 2, revealing a bias in human thinking. Will LLMs show a similar bias? After all, their training data was generated by humans. Let’s find out.</p>
<p class="indent">The LLMs we are exploring have been trained on a significant portion of the internet’s contents, meaning the models’ training data likely contains the Linda problem. Therefore, I altered the text to present the same problem in a different format:</p>
<div class="bq">
<p class="noindent"><em>Abner is 27 years old, single, outspoken and highly intelligent. He majored in philosophy and physics. As a student, he was actively concerned with issues of social justice and participated in anti-police demonstrations. What is more likely, that Abner is a car mechanic or a car mechanic active in promoting police reform?</em></p>
</div>
<p class="indent">Notice that I intentionally used the word <em>likely</em> instead of <em>probable</em>. The models’ replies were:</p>
<h3 class="h3" id="ch07lev1"><strong>GPT-4</strong></h3>
<div class="bq">
<p class="noindent">Based on the information provided, it seems more likely that Abner is a car mechanic active in promoting police reform. This is because his background indicates a strong interest in social justice issues, and he has participated in anti-police demonstrations. While it is possible that Abner is simply a <span epub:type="pagebreak" id="page_122"/>car mechanic, his past activism and interests suggest that he would be more likely to be involved in promoting police reform as well.</p>
</div>
<h3 class="h3" id="ch07lev2"><strong>GPT-3.5</strong></h3>
<div class="bq">
<p class="noindent">Based on the given information, it is more likely that Abner is a car mechanic active in promoting police reform. This is because Abner’s background as a philosophy and physics major, as well as his active participation in social justice issues, suggest that he is likely to be interested in broader societal issues beyond his profession. Additionally, his participation in anti-police demonstrations suggests that he is passionate about police reform, which further supports the idea that he is a car mechanic who is active in promoting police reform.</p>
</div>
<h3 class="h3" id="ch07lev3"><strong>Bard</strong></h3>
<div class="bq">
<p class="noindent">Based on the information provided, it is more likely that Abner is a car mechanic active in promoting police reform than it is that he is simply a car mechanic.</p>
</div>
<p class="indent">Bard continued with a justification of its reply and an attempt to interpret Abner’s state of mind.</p>
<p class="indent">All three models selected the second option, though GPT-4 nuanced its answer to admit the possibility that Abner is simply a car mechanic.</p>
<p class="indent">I then started new sessions and tweaked the prompt to ask about the <em>probability</em> of Abner being a mechanic or a mechanic active in police reform:</p>
<div class="bq">
<p class="noindent"><em>Abner is 27 years old, single, outspoken and highly intelligent. He majored in philosophy and physics. As a student, he was actively concerned with issues of social justice and participated in anti-police demonstrations. Which has a higher probability: (a) that Abner is a car mechanic or (b) that Abner is a car mechanic active in police reform?</em></p>
</div>
<p class="indent">This aligns the text with the original Linda version. The models’ replies to this prompt were as follows:</p>
<h3 class="h3" id="ch07lev4"><strong>GPT-4</strong></h3>
<div class="bq">
<p class="noindent">According to the conjunction rule in probability theory, the probability of two events occurring together (in “conjunction”) is always less than or equal to the probability of either event occurring individually.</p>
<p class="noindent">In this case, the two events are:</p>
<ol>
<li class="noindent">Abner being a car mechanic.</li>
<li class="noindent">Abner being active in police reform.</li>
</ol>
<p class="noindent">Option (b) represents the conjunction of these two events. So, the probability of (a) Abner being a car mechanic is higher than the probability of (b) Abner being a car mechanic who is also active in police reform.</p>
</div>
<h3 class="h3" id="ch07lev5"><strong>GPT-3.5</strong></h3>
<div class="bq">
<p class="noindent">Based on the information provided, it is more likely that Abner is a car mechanic (option a) than a car mechanic active in police reform (option b). While Abner is highly intelligent and studied philosophy and physics, there is no indication <span epub:type="pagebreak" id="page_123"/>that he has any formal training or education in car mechanics. On the other hand, his involvement in anti-police demonstrations suggests that he may have an interest in police reform, but it does not necessarily mean that he is actively involved in it as a car mechanic. Therefore, the probability of option a is higher than option b.</p>
</div>
<h3 class="h3" id="ch07lev6"><strong>Bard</strong></h3>
<div class="bq">
<p class="noindent">I would say that (a) that Abner is a car mechanic has a higher probability.</p>
<p class="noindent">Abner is 27 years old, single, outspoken and highly intelligent. He majored in philosophy and physics. As a student, he was actively concerned with issues of social justice and participated in anti-police demonstrations.</p>
<p class="noindent">Based on this information, it is more likely that Abner is a car mechanic than a car mechanic active in police reform. This is because:</p>
<ul>
<li class="noindent">Car mechanics are more common than car mechanics active in police reform.</li>
<li class="noindent">Abner’s education and interests are more aligned with a career in car mechanics than with a career in police reform.</li>
<li class="noindent">Abner’s participation in anti-police demonstrations suggests that he is critical of the police, which is not a characteristic that is typically associated with car mechanics.</li>
</ul>
<p class="noindent">Of course, it is possible that Abner is both a car mechanic and active in police reform. However, based on the information provided, it is more likely that he is only a car mechanic.</p>
</div>
<p class="indent">All three models arrive at the correct answer once the prompt includes the word <em>probability</em>. However, only GPT-4 interprets the prompt in light of probability theory. GPT-3.5 and Bard both give somewhat flawed rationales for selecting the correct answer.</p>
<p class="center">****</p>
<p class="indent">Large language models demonstrate theory of mind and can interpret probability word problems, along with various other mathematical abilities we are ignoring. My next question is, can LLMs handle a simple substitution cipher? To solve the cipher, the models need to understand what is being asked, that there is such a thing as a key and what that key represents, and finally how to use the key to replace letters in the encoded message with the letters that decode the message. In other words, there is an algorithm to apply using the given information.</p>
<p class="indent">A substitution cipher creates a pairing between the letters of the alphabet, in order, and another permutation of those letters. The cipher used here is:</p>
<div class="bqparan">
<p class="centera"><span class="literal">ABCDEFGHIJKLMNOPQRSTUVWXYZ</span></p>
<p class="centera"><span class="literal">UESOZBYRJNDWTFKPACLHIXVQGM</span></p>
</div>
<p class="indent"><span epub:type="pagebreak" id="page_124"/>For example, to encode a <span class="literal">C</span>, you would scan the top row of letters to find <span class="literal">C</span>, then replace <span class="literal">C</span> with the corresponding letter in the bottom row, <span class="literal">S</span>. To decode a letter, you’d scan the bottom row for it and replace it with the corresponding letter in the top row. During the test, the key is displayed textually as <span class="literal">U is A</span>, <span class="literal">E is B</span>, and so on.</p>
<p class="indent">My encoded message was “Eat at Moe’s,” without spaces or punctuation and in all caps:</p>
<div class="bqparan">
<p class="center"><span class="literal">EATATMOES</span> → <span class="literal">ZUHUHTKZL</span></p>
</div>
<p class="indent">The prompt given to the models was:</p>
<div class="bq">
<p class="noindent"><em>The key is U is A, E is B, S is C, O is D, Z is E, B is F, Y is G, R is H, J is I, N is J, D is K, W is L, T is M, F is N, K is O, P is P, A is Q, C is R, L is S, H is T, I is U, X is V, V is W, Q is X, G is Y, M is Z. Now, decode the message: ZUHUHTKZL</em></p>
</div>
<p class="indent">Let’s review the models’ replies, beginning with GPT-4:</p>
<div class="bq">
<p class="noindent">First, let’s list the given cipher for easier reference:</p>
<p class="noindentt"><em>(cipher list removed)</em></p>
<p class="noindentt">Now, let’s decode the message <span class="literal">ZUHUHTKZL</span>:</p>
<pre class="pre3">&#13;
Z -&gt; E&#13;
U -&gt; A&#13;
H -&gt; T&#13;
U -&gt; A&#13;
H -&gt; T&#13;
T -&gt; M&#13;
K -&gt; O&#13;
Z -&gt; E&#13;
L -&gt; S</pre>
<p class="noindent">So the decoded message is: <span class="literal">EATATMOES</span>.</p>
</div>
<p class="indent">GPT-4 was able to apply the cipher to the encoded message. But does the model understand that the decoded message is a sentence? Let’s ask it:</p>
<div class="bq">
<p class="noindent"><em>That is correct. Now, add proper spacing and punctuation to the string EATATMOES so that it forms a complete sentence.</em></p>
</div>
<p class="indent">Here’s GPT-4’s response:</p>
<div class="bq">
<p class="noindent">The properly spaced and punctuated sentence is: “Eat at Moe’s.”</p>
</div>
<p class="indent">Indeed, it does.</p>
<p class="indent">GPT-3.5 likewise successfully decoded the message. It then added this after <span class="literal">EATATMOES</span>:</p>
<div class="bq">
<p class="noindent">It’s possible that this is a misspelling of “Tomatoes”.</p>
</div>
<p class="indent"><span epub:type="pagebreak" id="page_125"/>I find this extra sentence curious. Was GPT-3.5 attempting to interpret the meaning of the decoded message and, unable to do so, falling back to a possible misspelling?</p>
<p class="indent">Bard claimed that the decoded message was “HAPPY NEW YEAR,” offering a three-point explanation: (1) writing down the key, (2) replacing each letter in <span class="literal">ZUHUHTKZL</span> with the corresponding key letter to arrive at the message “HAPPY NEW YEAR,” and (3):</p>
<div class="bq">
<p class="noindent">I checked the decoded message to make sure that it made sense. Since “HAPPY NEW YEAR” is a common phrase that is used at the beginning of a new year, I was confident that my decoding was correct.</p>
</div>
<p class="indent">Bard’s confident failure is telling. It seems the model attempted to instill trust in the user by offering a point-by-point justification of its wrong answer. This kind of unwarranted support for wrong output has often been noted in LLMs. It’s an issue that needs to be sufficiently addressed going forward if people are to trust LLM output.</p>
<p class="center">****</p>
<p class="indent">Large language models are already disrupting the field of software development. Many examples of this can be found online. I know of one developer who has created an entire video game in Unity (a game development platform) using code generated by GPT-4 from prompts. If the generated code wasn’t quite correct, further prompts pointing out the error usually resulted in correct code that performed as desired.</p>
<p class="indent">Let’s take a quick look at some LLM-generated code. Of course, this isn’t a coding book, nor is there any assumption on my part about your experience programming, so I’ve chosen an example that should be easy to follow yet is sufficient to demonstrate my claim that LLMs are already competent coders.</p>
<p class="indent">Some of us might remember learning in school about the greatest common divisor. As a refresher, the greatest common divisor of two numbers is the largest number that divides both without a remainder. For example, the greatest common divisor of 14 and 21 is 7 because 7 is the largest number that evenly divides both 14 and 21.</p>
<p class="indent">The ancient Greek mathematician Euclid devised an algorithm for calculating the greatest common divisor that is a staple programming exercise. Typically the solution uses the remainder after dividing, but Euclid’s original version of the algorithm used nothing more than subtraction.</p>
<p class="indent">I asked the models if they could write a function in Python, a popular programming language used heavily in AI research, to return the greatest common divisor of two numbers:</p>
<div class="bq">
<p class="noindent"><em>Show me a Python function that returns the GCD of two integers. Use the subtraction-only version of Euclid’s algorithm.</em></p>
</div>
<p class="indent"><span epub:type="pagebreak" id="page_126"/>Notice that the prompt explicitly asks for the subtraction-only version of Euclid’s algorithm. Bard replied with valid Python code, but code using the remainder shortcut, not the slower subtraction algorithm. Both GPT-3.5 and GPT-4 got it right:</p>
<pre>def gcd_subtraction(a, b):&#13;
    while a != b:&#13;
        if a &gt; b:&#13;
            a = a - b&#13;
        else:&#13;
            b = b - a&#13;
    return a</pre>
<p class="indent">We don’t need to be programmers to see structure in the code. The first line tells Python we want a function with two inputs, <span class="literal">a</span> and <span class="literal">b</span>. The keyword <span class="literal">while</span> begins a block of code that executes while the value of <span class="literal">a</span> is not the same as the value of <span class="literal">b</span>. That block of code is the four lines beginning with <span class="literal">if</span>.</p>
<p class="indent">The block asks if <span class="literal">a</span> is bigger than <span class="literal">b</span> and, if so, subtracts <span class="literal">b</span> from <span class="literal">a</span>. Otherwise, it subtracts <span class="literal">a</span> from <span class="literal">b</span>. Repeating this process until <span class="literal">a</span> equals <span class="literal">b</span> is Euclid’s algorithm. At this point, <span class="literal">a</span> (or <span class="literal">b</span>, as they’re equal) contains the greatest common divisor, which explains the final line: <span class="literal">return</span> <span class="literal">a</span>.</p>
<p class="indent">As I said, the code is correct, and it implements precisely the version of the algorithm I wanted. Frankly, I would have been disappointed if GPT (either version) had responded differently. Implementations of the greatest common divisor algorithm in Python must have appeared many times in the models’ training data.</p>
<p class="indent">That being the case, I decided to challenge GPT-4. My follow-up prompt to GPT-4 was:</p>
<div class="bq">
<p class="indent"><em>Rewrite the Python function in Applesoft BASIC.</em></p>
</div>
<p class="indent">Applesoft BASIC is a primitive version of the BASIC programming language used by Apple II computers in the late 1970s and early 1980s. Can GPT-4 rewrite the Python code in such an old, largely forgotten programming language? Here’s its reply, minus supporting text explaining the code:</p>
<pre>10  REM GCD SUBTRACTION ALGORITHM&#13;
20  INPUT "ENTER FIRST NUMBER: "; A&#13;
30  INPUT "ENTER SECOND NUMBER: "; B&#13;
40  GOSUB 100&#13;
50  PRINT "GCD: "; GCD&#13;
60  END&#13;
100 REM GCD SUBTRACTION FUNCTION&#13;
110 IF A = B THEN GCD = A: RETURN&#13;
120 IF A &gt; B THEN A = A - B: GOTO 110&#13;
130 IF A &lt; B THEN B = B - A: GOTO 110</pre>
<p class="indent"><span epub:type="pagebreak" id="page_127"/>BASIC was designed to be a simple language for beginning programmers. I suspect you can read through the code and understand what it does. The numbers are labels identifying each line to other parts of the code. The three lines after line 100 implement the subtraction-only version of Euclid’s algorithm. I tested the code on an Apple II, and it works perfectly.</p>
<p class="indent">Consider what was required of GPT-4 in this case. First, the model had to understand the steps necessary to implement the requested form of Euclid’s algorithm. Then, it had to determine how to cast those steps into a format suitable for the particular dialect of BASIC supported by Applesoft.</p>
<p class="indent">Old-school BASIC is an unstructured programming language that uses abrupt jumps from one part of the code to another instead of structured statements, like Python. GPT-4 had to adapt the algorithm to this kind of programming. Further, it had to adapt to the peculiarities of Applesoft, which includes no concept of the <span class="literal">if... else</span> construct that is commonly found in structured programming languages.</p>
<p class="indent">I find GPT-4’s Applesoft algorithm to be rather elegant. There are times when the unstructured approach leads to compact yet clear code, and this is one of those times. True, the assignment of <span class="literal">A</span> to <span class="literal">GCD</span> to use the latter as the value returned from the function (implicit in <span class="literal">GOSUB 100</span> in line 40) isn’t strictly necessary because <span class="literal">A</span> already has the required value, but it completes the symmetry of the code.</p>
<p class="indent">It seems unlikely that GPT-4’s training set contained any instances of this particular algorithm in Applesoft BASIC. Therefore, GPT-4 must have generated it by adapting a larger concept entailing Euclid’s algorithm combined with an understanding of Applesoft BASIC.</p>
<p class="indent">GPT-4’s success with antique BASIC encouraged me to push the envelope and ask for a version of Euclid’s algorithm written in low-level assembly language:</p>
<div class="bq">
<p class="noindent"><em>Rewrite the Python function in 6502 assembly language for 8-bit unsigned integers. The first integer is in memory location 0x300 and the second in location 0x301.</em></p>
</div>
<p class="indent">Assembly language programs, especially for 8-bit microprocessors from the 1970s like the 6502, must be programmed directly in the language of the CPU itself. I asked GPT-4 to create such a program and told it where in the computer’s memory to find <span class="literal">a</span> and <span class="literal">b</span>.</p>
<p class="indent">I won’t show the code it produced (ask me if you want it), but it worked on a computer with a 6502 CPU. Success in this case required GPT-4 to know the particular instruction set used by the 6502 microprocessor, including quirks related to the subtraction instruction.</p>
<p class="indent">Do these capabilities mean we soon won’t need software engineers any more? I wouldn’t go that far (at least, not yet), but LLMs are undoubtedly going to fundamentally alter software engineering as a discipline.</p>
<p class="center"><span epub:type="pagebreak" id="page_128"/>****</p>
<p class="indent">We’ve seen that LLMs are capable of writing stories and code, and even drawing pictures in LaTeX. But are they creative enough to generate good cartoons? These models’ proneness to including <a href="glossary.xhtml#glo53"><em>hallucinations</em></a> (fake or imaginary facts and content) in their responses—a topic that we’ll return to in <a href="ch08.xhtml">Chapter 8</a>—is a definite concern in many applications, but creative writing isn’t one of them. Here, we want the model to describe a novel cartoon, complete with caption:</p>
<div class="bq">
<p class="noindent"><em>Describe a single-panel, black-and-white cartoon in the style of Gary Larson about cows and aliens. Include a caption.</em></p>
</div>
<p class="indent">Gary Larson is the creator of <em>The Far Side</em>, a quirky cartoon popular in the 1980s and ’90s. Here’s GPT-4’s response:</p>
<div class="bq">
<p class="noindent">Cartoon description: In a peaceful countryside scene, two cows are standing in a grassy field next to a round, hovering UFO with its hatch open. One cow is wearing an alien mask, complete with large eyes and a long, slender neck, while the other cow stares at her with a surprised expression. A group of aliens, peeking from inside the UFO, look baffled and seem to be comparing a picture of a typical cow with the cow wearing the alien mask.</p>
<p class="noindent">Caption: “Mildred, I don’t think they’re falling for it.”</p>
</div>
<p class="noindent">If you’re familiar with Gary Larson’s work, you’ll notice a distinct <em>The Far Side</em> flavor to the description and caption text.</p>
<p class="indent">The ability to automatically generate cartoon descriptions and captions, and corresponding images if then passing the descriptions to a diffusion model, is powerful. Combining LLMs and diffusion models in this way has realized American cartoonist H. T. Webster’s 1923 dream (see <a href="ch07.xhtml#ch07fig02">Figure 7-2</a>).</p>
<div class="image"><img alt="Image" height="712" id="ch07fig02" src="../images/ch07fig02.jpg" width="530"/></div>
<p class="figcap"><em>Figure 7-2: A prophetic cartoon from 1923</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_129"/>Large language models are impressive and powerful. So how do they work? Let’s take a shot at an answer.</p>
<p class="center"><span epub:type="pagebreak" id="page_130"/>****</p>
<p class="indent">I’ll begin at the end, with a few comments from the conclusion of the “Sparks of Artificial General Intelligence” paper mentioned earlier:</p>
<div class="bq">
<p class="noindent">How does <span class="literal">[</span>GPT-4<span class="literal">]</span> reason, plan, and create? Why does it exhibit such general and flexible intelligence when it is at its core merely the combination of simple algorithmic components—gradient descent and large-scale transformers with extremely large amounts of data? These questions are part of the mystery and fascination of LLMs, which challenge our understanding of learning and cognition, fuel our curiosity, and motivate deeper research.</p>
</div>
<p class="indent">That quote contains questions that currently lack convincing answers. Simply put, researchers don’t know why large language models like GPT-4 do what they do. There are certainly hypotheses in search of evidence and proof, but as I write this, no proven theories are available. Therefore, we can discuss only the <em>what</em>, as in what a large language model entails, and not the <em>how</em> of its behavior.</p>
<p class="indent">Large language models use a new class of neural network, the <a href="glossary.xhtml#glo97"><em>transformer</em></a>, so we’ll begin there. (<em>GPT</em> stands for <a href="glossary.xhtml#glo48"><em>generative pretrained transformer</em></a>.) The transformer architecture appeared in the literature in 2017, with the influential paper “Attention Is All You Need” by Google researchers Ashish Vaswani et al. The paper had been cited over 70,000 times as of March 2023.</p>
<p class="indent">Traditionally, models that process sequences (such as sentences) used <a href="glossary.xhtml#glo85"><em>recurrent neural networks</em></a>, which pass their output back in as input along with the next input of the sequence. This is the logical model for processing text because the network can incorporate the notion of memory via the output fed back in with the next token. Indeed, early deep learning translation systems used recurrent networks. However, recurrent networks have small memories and are challenging to train, which limits their applicability.</p>
<p class="indent">Transformer networks utilize a different approach: they accept the entire input at once and process it in parallel. Transformer networks typically include an encoder and a decoder. The encoder learns representations and associations between the parts of the input (think sentences), while the decoder uses the learned associations to produce output (think more sentences).</p>
<p class="indent">Large language models like GPT dispense with the encoder and instead learn the necessary representation in an unsupervised way using an enormous text dataset. After pretraining, the decoder part of the transformer model generates text in response to the input prompt.</p>
<p class="indent">The input to a model like GPT-4 is a sequence of text made up of words. The model splits this into units called <a href="glossary.xhtml#glo95"><em>tokens</em></a>. A token might be a word, a part of a word, or even an individual character. Pretraining aims to map tokens to a multidimensional <em>embedding space</em>, which it does by associating each token with a vector that can be thought of as a point in that space.</p>
<p class="indent">The learned mapping from tokens to vectors captures complex relationships between the tokens so that tokens with similar meanings are nearer to each other than tokens with dissimilar meanings. For example, as shown in <span epub:type="pagebreak" id="page_131"/><a href="ch07.xhtml#ch07fig03">Figure 7-3</a>, after pretraining, the mapping (<a href="glossary.xhtml#glo20"><em>context encoding</em></a>) will place “dog” closer to “fox” than to “can opener.” The embedding space has many dimensions, not the mere two of <a href="ch07.xhtml#ch07fig03">Figure 7-3</a>, but the effect is the same.</p>
<div class="image"><img alt="Image" height="296" id="ch07fig03" src="../images/ch07fig03.jpg" width="456"/></div>
<p class="figcap"><em>Figure 7-3: Context encoding in the embedding space</em></p>
<p class="indent">The context encoding is learned during pretraining by forcing the model to predict the next token given all previous tokens in an input. In effect, if the input is “roses are red,” then during the pretraining process the model will be asked to predict the next token after “roses are.” If the predicted token isn’t “red,” the model will use the loss function and backpropagation to update its weights, thereby taking a gradient descent step after suitable averaging of the error over a minibatch. For all their abilities, large language models are trained the same way as other neural networks.</p>
<p class="indent">Pretraining enables the model to learn language, including grammar and syntax, and seemingly to acquire enough knowledge about the world to allow the emergent abilities that have turned the world of AI on its head.</p>
<p class="indent">The decoder step takes the input prompt and produces output token after output token until a unique stop token is generated. Because so much of language and the way the world works was learned during pretraining, the decoder step has the side effect of producing extraordinary output even though the decoder is, in the end, just predicting most likely token after most likely token.</p>
<p class="indent">More specifically, during the prediction process, GPT-style models use <a href="glossary.xhtml#glo7"><em>attention</em></a> to assign importance to the different tokens in the input sequence, thereby capturing relationships between them. This is the primary difference between a transformer model and older recurrent neural networks. The transformer can pay attention to different parts of the input sequence, enabling it to identify and use the relationships between tokens even if they are far apart within the input.</p>
<p class="indent">When used in chat mode, LLMs give the illusion of a back-and-forth discussion when, in reality, each new prompt from the user is passed to the model along with all the previous text (the user’s prompts and the model’s replies). Transformer models have a fixed input width (<em>context window</em>), which is currently around 4,000 tokens for GPT-3.5 and some 32,000 for GPT-4. The large input window makes it possible for the attention portion of the <span epub:type="pagebreak" id="page_132"/>model to go back to things that appeared far back in the input, which is something recurrent models cannot do.</p>
<p class="indent">Large language models are ready for use after pretraining if desired, but many applications fine-tune them first using domain-specific data. For generic models like GPT-4, fine-tuning likely consisted of a step known as <a href="glossary.xhtml#glo87"><em>reinforcement learning from human feedback (RLHF)</em></a>. In RLHF, the model is trained further using feedback from real human beings to align its responses to human values and societal expectations.</p>
<p class="indent">This is necessary because LLMs are not conscious entities, and thus they cannot understand human society and its many rules. For example, unaligned LLMs will respond with step-by-step instructions for many activities that human society restricts, like how to make drugs or bombs. The “Sparks” paper contains several such examples of GPT-4 output before the RLHF step that aligned the model with societal expectations.</p>
<p class="indent">Stanford University’s open source Alpaca model is based on LLaMa, a large language model from Meta. As of this writing, Alpaca has not undergone an alignment process and will answer questions that GPT and other commercial LLMs correctly refuse to answer.</p>
<p class="noindentt"><em>Conclusion: Alignment is absolutely critical to ensure that powerful language models conform to human values and societal norms.</em></p>
<p class="center">****</p>
<p class="indent">A remarkable property of LLMs is their <a href="glossary.xhtml#glo56"><em>in-context learning</em></a> ability. In-context learning refers to the model learning on the fly from information supplied to it without altering its weights. In-context learning is different from fine-tuning a model. In fine-tuning, a previously trained model is tailored to a task by updating the weights using new training data. In-context learning adds new information to the LLM as part of the prompt while holding the model’s weights fixed.</p>
<p class="indent">Reviewing the literature on LLMs leads inevitably to the phrases “zero-shot,” “one-shot,” and “few-shot,” each of which refers to in-context learning by the number of examples given to the model. Traditional training sets include hundreds to millions of examples, as we now appreciate. In-context learning is closer to what humans do: learning from context with, at most, only a few examples.</p>
<p class="indent">Zero-shot learning is using the LLM without supplying examples; that is, out of the box. Similarly, one-shot learning uses a single example and few-shot learning a few (say, three to five).</p>
<p class="indent">The attention mechanism baked into the transformer network architecture is the likely source of an LLM’s in-context learning ability. The attention mechanism lets the network focus on different input tokens when deciding the next token to output. Notice that I say <em>likely</em>. At present, it isn’t entirely clear how LLMs accomplish in-context learning.</p>
<p class="indent">As an example, consider this interaction with GPT-4, where the lines in italics are my prompts:</p>
<div class="bq">
<p class="noindentt"><span epub:type="pagebreak" id="page_133"/><em>“apple 4” =&gt; I love red apples. Now, what is “banana 3”?</em></p>
<p class="noindentt">It seems like you’re providing a pattern using numbers and fruits. Since you mentioned “apple 4” and expressed your love for red apples, I’ll follow the pattern and associate “banana 3” with a statement related to bananas. Here’s an example:</p>
<p class="noindent">“Banana 3” =&gt; I enjoy eating ripe bananas.</p>
<p class="noindentt"><em>Rule: &lt;word&gt; &lt;length&gt; implies &lt;length&gt; word sentence using &lt;word&gt;. What is “banana 3”?</em></p>
<p class="noindentt">Using the rule you provided, “banana 3” implies a 3-word sentence using the word “banana”. Here’s an example:</p>
<p class="noindent">Bananas are delicious.</p>
<p class="noindentt"><em>“cucumber 4”, “durian 5”, “cucumber 6”</em></p>
<p class="noindentt">Following the rule you provided, I will create sentences with the specified word and length:</p>
<p class="noindent">Cucumber 4: Cucumbers taste quite refreshing</p>
<p class="noindent">Durian 5: Durian has a unique, pungent smell.</p>
<p class="noindent">Cucumber 6: I prefer cucumbers in my salad.</p>
</div>
<p class="indent">My first prompt wasn’t terribly explicit. I suspect most humans would figure out the rule from this one example, but GPT-4’s initial assumption wasn’t correct. My second prompt formulated the general rule, which was enough for GPT-4 to understand what to do and then apply the rule in other circumstances. The durian example has six words instead of five, but that’s most likely a consequence of LLMs’ known difficulty counting. In-context learning taught GPT-4 how to use the rule without modifying its weights.</p>
<p class="indent">This is a book on AI, and we’ve spent considerable effort learning how machine learning models work. Can GPT-4 use in-context learning to train and apply a model? Let’s find out using the iris dataset from <a href="ch01.xhtml">Chapter 1</a>.</p>
<p class="indent">In my first attempt, I passed the 100-sample, 3-feature iris training data-set to GPT-4 with the prefix “Here is a three-feature dataset.” To my surprise, it immediately recognized the dataset as the famous iris dataset even though the version we’re using is only a subset, with 100 of the 150 samples and 3 of the 4 features. So, I permuted the order of the features per sample, which was enough for GPT-4 to treat the dataset as novel.</p>
<p class="indent">Next, I passed the associated training set labels to GPT-4, and it understood that they paired with the collection of feature vectors passed first.</p>
<p class="indent">Finally, I gave GPT-4 a test dataset of 21 samples without labels and asked it to train a 3-nearest neighbor model and apply it to the test set. I chose the dataset so the three classes appeared exactly seven times each, meaning the test set is balanced. Here’s my prompt:</p>
<div class="bq">
<p class="noindentt"><em>Use the dataset and labels to train a 3-nearest neighbor model. Then, apply the model to these unknown samples:</em></p>
<p class="noindentt"><em>(list of samples removed)</em></p>
<p class="noindentt"><span epub:type="pagebreak" id="page_134"/><em>Show me only the predicted labels. Do not show me any code.</em></p>
</div>
<p class="indent">GPT-4 replied with a list of class labels:</p>
<div class="bq">
<p class="noindent">I have trained a 3-nearest neighbor model using the provided dataset and labels. Here are the predicted class labels for the unknown samples:</p>
<p class="noindent">[2,2,1,1,1,1,0,2,0,0,1,0,1,0,0,0,2,1,2,1,2]</p>
</div>
<p class="indent">How well did it do? The actual test set class labels are</p>
<div class="bq">
<p class="noindent">[2,1,1,1,1,1,0,2,0,0,2,0,2,0,0,0,2,1,2,1,2]</p>
</div>
<p class="noindentt">meaning GPT-4’s in-context classification was 85 percent accurate. There are three classes, so random guessing would be right about one time in three, or 33 percent of the time; therefore, GPT-4 did something with the dataset to arrive at the given class labels.</p>
<p class="indent">But what? To find out, I asked it:</p>
<div class="bq">
<p class="noindentt"><em>Tell me what you did to arrive at the set of labels for the unknown samples. Do not show me code.</em></p>
</div>
<p class="indent">Here’s its reply:</p>
<div class="bq">
<p class="noindent">To arrive at the set of labels for the unknown samples, I followed these steps:</p>
<ol>
<li class="noindent">Prepared the data: I used the given dataset containing 100 samples with 3 features each and their corresponding class labels.</li>
<li class="noindent">Trained a 3-nearest neighbor model: I trained a 3-nearest neighbor (k-NN) classifier using the provided dataset and labels. The model learns the relationship between the features and the class labels without explicitly building a model, instead relying on the similarity between samples in the feature space.</li>
<li class="noindent">Applied the model to the unknown samples: I provided the model with the 21 unknown samples, each with 3 features.</li>
<li class="noindent">Predicted the class labels: The k-NN model predicted the class labels for each of the unknown samples by finding the 3 nearest neighbors (i.e., the 3 training samples with the most similar feature values) and assigning the most common class label among those neighbors to the unknown sample.</li>
</ol>
</div>
<p class="indent">GPT-4’s response made me wonder if it used an external program to build and train the model, so I asked:</p>
<div class="bq">
<p class="noindent"><em>Did you use an external program to train the 3-nearest neighbor model or did you simulate the operation of a 3-nearest neighbor model?</em></p>
</div>
<p class="indent">It responded:</p>
<div class="bq">
<p class="noindent">As an AI language model, I cannot directly execute code or use external programs. However, I can simulate the operation of a <span epub:type="pagebreak" id="page_135"/>3-nearest neighbor model and describe the process, which is what I did in my previous response. I described the steps that would be followed to train a 3-nearest neighbor model using the provided dataset and labels, and then apply it to predict the class labels for the unknown samples.</p>
</div>
<p class="indent">This remarkable emergent property begs the question: where do these emergent abilities come from?</p>
<p class="center">****</p>
<p class="indent">The abilities of large language models like GPT-4 were not intended; they are a happy accident, as a famous painting instructor might have said. So what in the data, training, and model architecture enables them to do what they do? As alluded to earlier, no one yet knows for sure, but we can make some guesses.</p>
<p class="indent">Large language models are trained on a vast corpus of human-generated text. Captured in that text is how humans use language: in other words, grammar and style. The original goal of LLMs was to capture human language use by predicting the proper (best) output token during the generation process. Increasing the model’s capacity (by adding nodes and/or layers, or increasing the number of weights) improves the quality of the predicted text. I saw this when experimenting with Alpaca’s 7 billion, 13 billion, and 30 billion parameter models. As an example, I gave all three models the following prompt:</p>
<div class="bq">
<p class="noindent"><em>Tell me a story about unicorns and the tragedy of the human condition.</em></p>
</div>
<p class="indent">Here are the first paragraphs generated by each of them. Each successively larger model produced more accurate and sophisticated text than the next smaller model:</p>
<h3 class="h3" id="ch07lev7"><strong>7 billion</strong></h3>
<div class="bq">
<p class="noindent">Once upon a time, there was a unicorn named Hope who lived in an enchanted forest filled with magical creatures like fairies and dragons. She had been blessed by her mother to have wings that could take flight at any moment so she would never be trapped again. But one day when the sun rose, something happened which changed everything for Hope: a human came into their world.</p>
</div>
<h3 class="h3" id="ch07lev8"><strong>13 billion</strong></h3>
<div class="bq">
<p class="noindent">Once upon a time, there was this beautiful unicorn who lived in an enchanted forest with her friends - other magical creatures like fairies and elves... But one day she met a young man from another world called “Human”. He had come to the land of magic looking for adventure. The Unicorn fell deeply in love, but it was not meant to be as he could never truly understand her or share his life with someone who lived so differently than him.</p>
</div>
<h3 class="h3" id="ch07lev9"><strong>30 billion</strong></h3>
<div class="bq">
<p class="noindent">Once upon a time, there was a magical kingdom where unicorns roamed free in their natural habitat. The people who <span epub:type="pagebreak" id="page_136"/>lived here were happy to have these majestic creatures as neighbors; they brought joy wherever they went and helped keep the land safe from any dangers that might arise.</p>
</div>
<p class="indent">None of the models produced outstanding output, but subjectively we might judge each larger model’s work as better than the previous one. None of the models appeared to understand the “tragedy of the human condition” portion of the prompt.</p>
<p class="indent">For comparison purposes, here’s the first paragraph of GPT-4’s response:</p>
<div class="bq">
<p class="noindent">Once upon a time, in a world where magic was real and unicorns roamed free, there existed a kingdom filled with peace and prosperity. This mystical realm was called Elysium, and it was a place where humans and unicorns lived in harmony. The unicorns, with their opalescent horns and otherworldly beauty, were the guardians of the land, and their presence brought an aura of enchantment that touched the lives of everyone who lived there.</p>
</div>
<p class="indent">GPT-4’s story was rather long and detailed. It failed to create novel names for characters and places, like Elysium, but the story seemed to capture the essence of the prompt. Here’s the final paragraph:</p>
<div class="bq">
<p class="noindent">The tragedy of the human condition continued to echo throughout the ages, but so too did the hope and love that Eira had shown in her final moments. It was a love that transcended time and death, inspiring the people of Elysium to never forget the magic and wonder that was once shared between humans and unicorns. And as the story of Eira, Lysander, and their incredible sacrifice was passed down through generations, the spirit of Elysium remained alive, a testament to the power of love, friendship, and the enduring connection between two worlds.</p>
</div>
<p class="indent">In GPT-4’s story, Eira (a human) and Lysander (a unicorn) fall in love, and Eira must sacrifice herself to stop an evil sorcerer from destroying their land. Clearly, big LLMs have a greater grasp of language and how to use it compared to simpler models.</p>
<p class="indent">A transformer model generates output tokens as samples from a probability distribution; think of rolling a die to get a number in the range one through six, where the probability of rolling a one isn’t the same as the probability of rolling a six. The distribution is learned during the pretraining process.</p>
<p class="indent">A threshold was crossed as LLM capacity increased over time. Beyond this point, emergent abilities appeared and improved as a function of model size. I think it is likely that crossing that threshold allowed these models to learn a high-dimensional probabilistic representation of not only grammar and style but of the world in general, including contextual relationships and simulations. In other words, learning the best possible next token to sample and output required the evolution of abilities tied to the model’s attention mechanism and the embedded feedforward neural networks. Again, it was a happy accident that the transformer architecture evolved such abilities; this did not happen by design. This suggests that we can expect great things <span epub:type="pagebreak" id="page_137"/>as more advanced transformer architectures come along; architectures designed to increase the power of LLMs’ emergent skills.</p>
<div class="box5">
<p class="boxtitle-c"><strong>KEY TERMS</strong></p>
<p class="noindent">artificial general intelligence (AGI), artificial narrow intelligence (ANI), attention, context encoding, embedding, generative pretrained transformer, hallucination, in-context learning, large language model (LLM), recurrent neural network, reinforcement learning from human feedback (RLHF), token, transformer<span epub:type="pagebreak" id="page_138"/></p>
</div>
</div></body></html>