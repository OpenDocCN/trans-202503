["```\n\n/* \"returns\" through pass-by-pointer param dev_ptr GPU memory of size bytes\n\n * returns cudaSuccess or a cudaError value on error\n\n */\n\ncudaMalloc(void **dev_ptr, size_t size);\n\n/* free GPU memory\n\n * returns cudaSuccess or cudaErrorInvalidValue on error\n\n */\n\ncudaFree(void *data);\n\n/* copies data from src to dst, direction is based on value of kind\n\n *   kind: cudaMemcpyHosttoDevice is copy from cpu to gpu memory\n\n *   kind: cudaMemcpyDevicetoHost is copy from gpu to cpu memory\n\n * returns cudaSuccess or a cudaError value on error\n\n */\n\ncudaMemcpy(void *dst, const void *src, size_t count, cudaMemcpyKind kind);\n```", "```\n\ndim3 blockDim(16,16);  // 256 threads per block, in a 16x16 2D arrangement\n\ndim3 gridDim(20,20);   // 400 blocks per grid, in a 20x20 2D arrangement\n```", "```\nret = do_something<<<gridDim,blockDim>>>(dev_array, 100);\n```", "```\nint row = blockIdx.y * blockDim.y + threadIdx.y;\n\nint col = blockIdx.x * blockDim.x + threadIdx.x;\n```", "```\nx = a * x    // where x is a vector and a is a scalar value\n```", "```\n\n#include <cuda.h>\n\n#define BLOCK_SIZE       64     /* threads per block */\n\n#define N              10240    /* vector size */\n\n// some host-side init function\n\nvoid init_array(int *vector, int size, int step);\n\n// host-side function: main\n\nint main(int argc, char **argv) {\n\n  int *vector, *dev_vector, scalar;\n\n  scalar = 3;     // init scalar to some default value\n\n  if(argc == 2) { // get scalar's value from a command line argument\n\n    scalar = atoi(argv[1]);\n\n  }\n\n  // 1\\. allocate host memory space for the vector (missing error handling)\n\n  vector = (int *)malloc(sizeof(int)*N);\n\n  // initialize vector in host memory\n\n  // (a user-defined initialization function not listed here)\n\n  init_array(vector, N, 7);\n\n  // 2\\. allocate GPU device memory for vector (missing error handling)\n\n  cudaMalloc(&dev_vector, sizeof(int)*N);\n\n  // 2\\. copy host vector to device memory (missing error handling)\n\n  cudaMemcpy(dev_vector, vector, sizeof(int)*N, cudaMemcpyHostToDevice;)\n\n  // 3\\. call the CUDA scalar_multiply kernel\n\n  // specify the 1D layout for blocks/grid (N/BLOCK_SIZE)\n\n  //    and the 1D layout for threads/block (BLOCK_SIZE)\n\n  scalar_multiply<<<(N/BLOCK_SIZE), BLOCK_SIZE===(dev_vector, scalar);\n\n  // 4\\. copy device vector to host memory (missing error handling)\n\n  cudaMemcpy(vector, dev_vector, sizeof(int)*N, cudaMemcpyDeviceToHost);\n\n  // ...(do something on the host with the result copied into vector)\n\n  // free allocated memory space on host and GPU\n\n  cudaFree(dev_vector);\n\n  free(vector);\n\n  return 0;\n\n}\n```", "```\n\n/*\n\n * CUDA kernel function that performs scalar multiply\n\n * of a vector on the GPU device\n\n *\n\n * This assumes that there are enough threads to associate\n\n * each array[i] element with a signal thread\n\n * (in general, each thread would be responsible for a set of data elements)\n\n */\n\n__global__ void scalar_multiply(int *array, int scalar) {\n\n  int index;\n\n  // compute the calling thread's index value based on\n\n  // its position in the enclosing block and grid\n\n  index = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // the thread's uses its index value is to\n\n  // perform scalar multiply on its array element\n\n  array[index] = array[index] * scalar;\n\n}\n```", "```\n\n#include <stdio.h>\n\n#include <unistd.h>\n\n#include \"mpi.h\"\n\nint main(int argc, char **argv) {\n\n    int rank, process_count;\n\n    char hostname[1024];\n\n    /* Initialize MPI. */\n\n    MPI_Init(&argc, &argv);\n\n    /* Determine how many processes there are and which one this is. */\n\n    MPI_Comm_size(MPI_COMM_WORLD, &process_count);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    /* Determine the name of the machine this process is running on. */\n\n    gethostname(hostname, 1024);\n\n    /* Print a message, identifying the process and machine it comes from. */\n\n    printf(\"Hello from %s process %d of %d\\n\", hostname, rank, process_count);\n\n    /* Clean up. */\n\n    MPI_Finalize();\n\n    return 0;\n\n}\n```", "```\n$ mpicc -o hello_world_mpi hello_world_mpi.c\n```", "```\n\n$ mpirun -np 8 --hostfile hosts.txt ./hello_world_mpi\n\nHello from lemon process 4 of 8\n\nHello from lemon process 5 of 8\n\nHello from orange process 2 of 8\n\nHello from lemon process 6 of 8\n\nHello from orange process 0 of 8\n\nHello from lemon process 7 of 8\n\nHello from orange process 3 of 8\n\nHello from orange process 1 of 8\n```", "```\n\nif (rank == 0) {\n\n    /* This code only executes at the boss. */\n\n}\n```", "```\n\n/* Boss sends the scalar value to every process with a broadcast. */\n\nMPI_Bcast(&scalar, 1, MPI_INT, 0, MPI_COMM_WORLD);\n```", "```\n\n/* Each process determines how many processes there are. */\n\nMPI_Comm_size(MPI_COMM_WORLD, &process_count);\n\n/* Boss sends the total array size to every process with a broadcast. */\n\nMPI_Bcast(&array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n/* Determine how many array elements each process will get.\n\n * Assumes the array is evenly divisible by the number of processes. */\n\nlocal_size = array_size / process_count;\n```", "```\n\nif (rank == 0) {\n\n    int i;\n\n    /* For each worker process, send a unique chunk of the array. */\n\n    for (i = 1; i < process_count; i++) {\n\n        /* Send local_size ints starting at array index (i * local_size) */\n\n        MPI_Send(array + (i * local_size), local_size, MPI_INT, i, 0,\n\n                 MPI_COMM_WORLD);\n\n    }\n\n} else {\n\n    MPI_Recv(local_array, local_size, MPI_INT, 0, 0, MPI_COMM_WORLD,\n\n             MPI_STATUS_IGNORE);\n\n}\n```", "```\n\nif (rank == 0) {\n\n    int i;\n\n    for (i = 1; i < process_count; i++) {\n\n        MPI_Recv(array + (i * local_size), local_size, MPI_INT, i, 0,\n\n                 MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n    }\n\n} else {\n\n    MPI_Send(local_array, local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\n}\n```", "```\n\n/* Boss scatters chunks of the array evenly among all the processes. */\n\nMPI_Scatter(array, local_size, MPI_INT, local_array, local_size, MPI_INT,\n\n            0, MPI_COMM_WORLD);\n```", "```\n\n/* Boss gathers the chunks from all the processes and coalesces the\n\n * results into a final array. */\n\nMPI_Gather(local_array, local_size, MPI_INT, array, local_size, MPI_INT,\n\n           0, MPI_COMM_WORLD);\n```", "```\n\n#include <stdio.h>\n\n#include <stdlib.h>\n\n#include \"mpi.h\"\n\n#define ARRAY_SIZE (40)\n\n#define SCALAR (10)\n\n/* In a real application, the boss process would likely read its input from a\n\n * data file.  This example program produces a simple array and informs the\n\n * caller of the size of the array through the array_size pointer parameter.*/\n\nint *build_array(int *array_size) {\n\n    int i;\n\n    int *result = malloc(ARRAY_SIZE * sizeof(int));\n\n    if (result == NULL) {\n\n        exit(1);\n\n    }\n\n    for (i = 0; i < ARRAY_SIZE; i++) {\n\n        result[i] = i;\n\n    }\n\n    *array_size = ARRAY_SIZE;\n\n    return result;\n\n}\n\n/* Print the elements of an array, given the array and its size. */\n\nvoid print_array(int *array, int array_size) {\n\n    int i;\n\n    for (i = 0; i < array_size; i++) {\n\n        printf(\"%3d \", array[i]);\n\n    }\n\n    printf(\"\\n\\n\");\n\n}\n\n/* Multiply each element of an array by a scalar value. */\n\nvoid scalar_multiply(int *array, int array_size, int scalar) {\n\n    int i;\n\n    for (i = 0; i < array_size; i++) {\n\n        array[i] = array[i] * scalar;\n\n    }\n\n}\n\nint main(int argc, char **argv) {\n\n    int rank, process_count;\n\n    int array_size, local_size;\n\n    int scalar;\n\n    int *array, *local_array;\n\n    /* Initialize MPI */\n\n    MPI_Init(&argc, &argv);\n\n    /* Determine how many processes there are and which one this is. */\n\n    MPI_Comm_size(MPI_COMM_WORLD, &process_count);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    /* Designate rank 0 to be the boss.  It sets up the problem by generating\n\n     * the initial input array and choosing the scalar to multiply it by. */\n\n    if (rank == 0) {\n\n        array = build_array(&array_size);\n\n        scalar = SCALAR;\n\n        printf(\"Initial array:\\n\");\n\n        print_array(array, array_size);\n\n    }\n\n    /* Boss sends the scalar value to every process with a broadcast.\n\n     * Worker processes receive the scalar value by making this MPI_Bcast\n\n     * call. */\n\n    MPI_Bcast(&scalar, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    /* Boss sends the total array size to every process with a broadcast.\n\n     * Worker processes receive the size value by making this MPI_Bcast\n\n     * call. */\n\n    MPI_Bcast(&array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    /* Determine how many array elements each process will get.\n\n     * Assumes the array is evenly divisible by the number of processes. */\n\n    local_size = array_size / process_count;\n\n    /* Each process allocates space to store its portion of the array. */\n\n    local_array = malloc(local_size * sizeof(int));\n\n    if (local_array == NULL) {\n\n        exit(1);\n\n    }\n\n    /* Boss scatters chunks of the array evenly among all the processes. */\n\n    MPI_Scatter(array, local_size, MPI_INT, local_array, local_size, MPI_INT,\n\n                0, MPI_COMM_WORLD);\n\n    /* Every process (including boss) performs scalar multiplication over its\n\n     * chunk of the array in parallel. */\n\n    scalar_multiply(local_array, local_size, scalar);\n\n    /* Boss gathers the chunks from all the processes and coalesces the\n\n     * results into a final array. */\n\n    MPI_Gather(local_array, local_size, MPI_INT, array, local_size, MPI_INT,\n\n               0, MPI_COMM_WORLD);\n\n    /* Boss prints the final answer. */\n\n    if (rank == 0) {\n\n        printf(\"Final array:\\n\");\n\n        print_array(array, array_size);\n\n    }\n\n    /* Clean up. */\n\n    if (rank == 0) {\n\n        free(array);\n\n    }\n\n    free(local_array);\n\n    MPI_Finalize();\n\n    return 0;\n\n}\n```", "```\n\n$ mpicc -o scalar_multiply_mpi scalar_multiply_mpi.c\n\n$ mpirun -np 8 --hostfile hosts.txt ./scalar_multiply_mpi\n\nInitial array:\n\n  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 19\n\n 20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38 39\n\nFinal array:\n\n  0  10  20  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170 180 190\n\n200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390\n```", "```\n'''\n\n    The typical way to perform\n\n    scalar multiplication\n\n'''\n\n# array is an array of numbers\n\n# s is an integer\n\ndef scalarMultiply(array, s):\n\n    for i in range(len(array)):\n\n        array[i] = array[i] * s\n\n    return array\n\n# call the scalarMultiply function:\n\nmyArray = [1, 3, 5, 7, 9]\n\nresult = scalarMultiply(myArray, 2)\n\n# prints [2, 6, 10, 14, 18]\n\nprint(result)\n```", "```\n'''\n\n    Equivalent program that\n\n    performs scalar multiplication\n\n    with list comprehension\n\n'''\n\n# multiplies two numbers together\n\ndef multiply(num1, num2):\n\n    return num1 * num2\n\n# array is an array of numbers\n\n# s is an integer\n\ndef scalarMultiply(array, s):\n\n    # using list comprehension\n\n    return [multiply(x, s) for x in array]\n\n# call the scalarMultiply function:\n\nmyArray = [1, 3, 5, 7, 9]\n\nresult = scalarMultiply(myArray, 2)\n\n# prints [2, 6, 10, 14, 18]\n\nprint(result)\n```", "```\n\nvoid map(char *key, char *value){\n\n    // key is document name\n\n    // value is string containing some words (separated by spaces)\n\n    int i;\n\n    int numWords = 0; // number of words found: populated by parseWords()\n\n    // returns an array of numWords words\n\n    char *words[] = parseWords(value, &numWords);\n\n    for (i = 0; i < numWords; i++) {\n\n        // output (word, 1) key-value intermediate to file system\n\n        emit(words[i], \"1\");\n\n    }\n\n}\n```", "```\n\nvoid reduce(char *key, struct Iterator values) {\n\n    // key is individual word\n\n    // value is of type Iterator (a struct that consists of\n\n    // a items array (type char **), and its associated length (type int))\n\n    int numWords = values.length();  // get length\n\n    char *counts[] = values.items(); // get counts\n\n    int i, total = 0;\n\n    for (i = 0; i < numWords; i++) {\n\n        total += atoi(counts[i]); // sum up all counts\n\n    }\n\n    char *stringTotal = itoa(total); // convert total to a string\n\n    emit(key, stringTotal); // output (word, total) pair to file system\n\n}\n```"]