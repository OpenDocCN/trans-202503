<html><head></head><body>
<h2 class="h2" id="ch09"><span epub:type="pagebreak" id="page_251"/><strong><span class="big">9</span><br/>CPU ARCHITECTURE</strong></h2>&#13;
<div class="image1"><img alt="Image" src="../images/comm1.jpg"/></div>&#13;
<p class="noindents">Without question, the design of the central processing unit (CPU) has the greatest impact on the performance of your software. To execute a particular instruction (or command), a CPU requires a certain amount of electronic circuitry specific to that instruction. As you increase the number of instructions the CPU can support, you also increase the CPU’s complexity and the amount of circuitry, or <em>logic gates</em>, needed to execute them. Therefore, to keep the number of logic gates and the associated costs reasonably small, CPU designers must restrict the number and complexity of the instructions the CPU can execute. This is known as the CPU’s <em>instruction set</em>.</p>&#13;
<p class="indent">This chapter, and the next, discusses the design of CPUs and their instruction sets—information that is absolutely crucial for writing high-performance software.</p>&#13;
<h3 class="h3" id="sec9_1"><span epub:type="pagebreak" id="page_252"/><strong>9.1 Basic CPU Design</strong></h3>&#13;
<p class="noindent">Programs in early computer systems were often hardwired into the circuitry. That is, the computer’s wiring determined exactly what algorithm the computer would execute. The computer had to be rewired in order to solve a different problem. This was a difficult task, something that only electrical engineers were able to do.</p>&#13;
<p class="indent">Thus, the next advance in computer design was the programmable computer system, in which a computer operator could easily “rewire” the computer using a panel of sockets and plug wires known as a <em>patch board</em>. A computer program consisted of rows of sockets, with each row representing one operation (instruction) during the program’s execution. To execute an instruction, the programmer inserted a wire into its corresponding socket (see <a href="ch09.xhtml#ch09fig01">Figure 9-1</a>).</p>&#13;
<div class="image"><img alt="image" src="../images/09fig01.jpg"/></div>&#13;
<p class="figcap"><a id="ch09fig01"/><em>Figure 9-1: Patch board programming</em></p>&#13;
<p class="indent">The number of possible instructions was limited by how many sockets could fit on each row. CPU designers quickly realized that with a small amount of additional logic circuitry, they could reduce the number of sockets required for specifying <em>n</em> different instructions from <em>n</em> sockets to log<sub>2</sub>(<em>n</em>) sockets. They did this by assigning a unique binary number to each instruction (for example, <a href="ch09.xhtml#ch09fig02">Figure 9-2</a> shows how to represent eight instructions using only 3 bits).</p>&#13;
<div class="image"><img alt="image" src="../images/09fig02.jpg"/></div>&#13;
<p class="figcap"><a id="ch09fig02"/><em>Figure 9-2: Encoding instructions</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_253"/>The example in <a href="ch09.xhtml#ch09fig02">Figure 9-2</a> requires eight logic functions to decode the <em>A</em>, <em>B</em>, and <em>C</em> bits on the patch board, but the extra circuitry (a single three- to eight-line decoder) is worth the cost, because it reduces the total number of sockets from eight to three for each instruction.</p>&#13;
<p class="indent">Many CPU instructions require operands. For example, the <span class="literal">mov</span> instruction moves data from one location in the computer to another, such as from one register to another, and therefore requires a source operand and a destination operand. The operands were encoded as part of the machine instruction, with sockets corresponding to the source and destination. <a href="ch09.xhtml#ch09fig03">Figure 9-3</a> shows one possible combination of sockets to handle a <span class="literal">mov</span> instruction.</p>&#13;
<div class="image"><img alt="image" src="../images/09fig03.jpg"/></div>&#13;
<p class="figcap"><a id="ch09fig03"/><em>Figure 9-3: Encoding instructions with source and destination fields</em></p>&#13;
<p class="indent">The <span class="literal">mov</span> instruction would move data from the source register to the destination register, the <span class="literal">add</span> instruction would add the value of the source register to the destination register, and so on. This scheme allowed the encoding of 128 different instructions with just seven sockets per instruction.</p>&#13;
<p class="indent">As noted earlier, a big problem with patch-board programming was that a program’s functionality was limited by the number of sockets available on the machine. Early computer designers recognized a relationship between the sockets on the patch board and bits in memory. They realized they could store the binary equivalent of a machine instruction in main memory, fetch that binary number when the CPU wanted to execute the instruction, and then load it into a special register to decode the instruction. Known as the <em>stored program computer</em>, this invention was another major advance in computer design.</p>&#13;
<p class="indent">The trick was to add more circuitry, called the <em>control unit (CU)</em>, to the CPU. The control unit uses a special register, the <em>instruction pointer</em>, to hold the address of an instruction’s binary numeric code (also known as an <span epub:type="pagebreak" id="page_254"/><em>operation code</em> or <em><a href="gloss01.xhtml#gloss01_182">opcode</a></em>). The control unit fetches the instruction’s opcode from memory and places it in the instruction decoding register for execution. After executing the instruction, the control unit increments the instruction pointer and fetches the next instruction from memory for execution.</p>&#13;
<h3 class="h3" id="sec9_2"><strong>9.2 Decoding and Executing Instructions: Random Logic vs. Microcode</strong></h3>&#13;
<p class="noindent">Once the control unit fetches an instruction from memory, traditional CPUs use two common approaches to execute the instruction: random logic (hardwired) and microcode (emulation). The 80x86 family, for example, uses both of these techniques.</p>&#13;
<p class="indent">The <em>random logic</em><sup><a href="footnotes.xhtml#fn9_1a" id="fn9_1">1</a></sup> or hardwired approach uses decoders, latches, counters, and other hardware logic devices to operate on the opcode data. Random logic is fast but poses a circuitry design challenge; for CPUs with large and complex instruction sets, it’s difficult to properly lay out the logic so that related circuits are close to one another in the two-dimensional space of the chip.</p>&#13;
<p class="indent">CPUs based on microcode contain a small, very fast <em>execution unit</em> (circuitry responsible for executing a particular function), known as a <em><a href="gloss01.xhtml#gloss01_156">microengine</a></em>, that uses the binary opcode to select a set of instructions from the microcode bank. This microcode executes one microinstruction per clock cycle, and the sequence of microinstructions executes all the steps to perform whatever calculations are necessary for that instruction.</p>&#13;
<p class="indent">Although this <em>microengine</em> itself is fast, it must fetch its instructions from the microcode ROM (read-only memory). Therefore, if memory technology is slower than the execution logic, the micro-engine must run at the same speed as the microcode ROM, which in turn limits the speed at which the CPU can run.</p>&#13;
<p class="indent">The random logic approach decreases the time to execute an opcode’s instruction, provided that typical CPU speeds are faster than memory speeds, but that doesn’t mean it’s necessarily faster than the microcode approach. Random logic often includes a sequencer that steps through several states (one state per clock cycle). Whether you use up clock cycles executing microinstructions or stepping through a random logic state machine, you’re still burning up time.</p>&#13;
<p class="indent">Which approach is better for CPU design depends entirely on the current state of memory technology. If memory technology is faster than CPU technology, the microcode approach probably makes more sense. If memory technology is slower than CPU technology, random logic tends to execute machine instructions more quickly.</p>&#13;
<h3 class="h3" id="sec9_3"><span epub:type="pagebreak" id="page_255"/><strong>9.3 Executing Instructions, Step by Step</strong></h3>&#13;
<p class="noindent">Regardless of which approach the CPU uses, you need to understand how a CPU executes individual machine instructions. To that end, we’ll consider four representative 80x86 instructions—<span class="literal">mov</span>, <span class="literal">add</span>, <span class="literal">loop</span>, and <span class="literal">jnz</span> (<em>jump if not zero</em>)—to give you a sense of how a CPU executes all the instructions in its instruction set.</p>&#13;
<p class="indent">As you saw earlier, the <span class="literal">mov</span> instruction copies data from a source operand to a destination operand. The <span class="literal">add</span> instruction adds the value of its source operand to its destination operand. <span class="literal">loop</span> and <span class="literal">jnz</span> are <em>conditional jump</em> instructions—they test some condition and, if it’s <span class="literal">true</span>, they jump to some other instruction in memory; if it’s <span class="literal">false</span>, they continue with the next instruction. The <span class="literal">jnz</span> instruction tests a Boolean variable within the CPU known as the <em>zero flag</em> and either transfers control to the target instruction (the instruction to jump to) if the zero flag contains <span class="literal">0</span>, or continues with the next instruction if the zero flag contains <span class="literal">1</span>. The program indicates the address of the target instruction by specifying the distance, in bytes, between it and the <span class="literal">jnz</span> instruction in memory.</p>&#13;
<p class="indent">The <span class="literal">loop</span> instruction decrements the value of the ECX register and, if the resulting value does not contain <span class="literal">0</span>, transfers control to a target instruction. This is a good example of a <em>complex instruction set computer (CISC)</em> instruction because it does more than one operation:</p>&#13;
<ol>&#13;
<li class="noindent">It subtracts 1 from ECX.</li>&#13;
<li class="noindent">It does a conditional jump if ECX does not contain <span class="literal">0</span>.</li></ol>&#13;
<p class="indent">That is, <span class="literal">loop</span> is roughly equivalent to the following instruction sequence:</p>&#13;
<p class="programs">sub( 1, ecx ); // On the 80x86, the sub instruction sets the zero flag<br/>&#13;
jnz SomeLabel; // the result of the subtraction is 0.</p>&#13;
<p class="indent">To execute the <span class="literal">mov</span>, <span class="literal">add</span>, <span class="literal">jnz</span>, and <span class="literal">loop</span> instructions, the CPU has to execute a number of different operations. Each operation requires a finite amount of time to execute, and the time required to execute the entire instruction generally amounts to one clock cycle per operation or <em>stage</em> (step) that the CPU executes. Obviously, the more stages needed for an instruction, the slower it will run. Because they have many execution stages, complex instructions generally run slower than simple instructions.</p>&#13;
<p class="indent">Although 80x86 CPUs differ and don’t necessarily execute the exact same steps, their sequence of operations is similar. This section presents some possible sequences, all starting with the same three execution stages:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the instruction’s opcode from memory.</li>&#13;
<li class="noindent">Update the EIP (extended instruction pointer) register with the address of the byte following the opcode.</li>&#13;
<li class="noindent">Decode the instruction’s opcode to see what instruction it specifies.</li></ol>&#13;
<h4 class="h4" id="sec9_3_1"><span epub:type="pagebreak" id="page_256"/><strong><em>9.3.1 The mov Instruction</em></strong></h4>&#13;
<p class="noindent">A decoded 32-bit 80x86 <span class="literal">mov(</span><span class="EmpItalic">srcReg</span><span class="literal">,</span> <span class="EmpItalic">destReg</span><span class="literal">);</span> instruction might use the following (additional) execution stages:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the data from the source register (<span class="EmpItalic">srcReg</span>).</li>&#13;
<li class="noindent">Store the fetched value into the destination register (<span class="EmpItalic">destReg</span>).</li></ol>&#13;
<p class="indent">The <span class="literal">mov(</span><span class="EmpItalic">srcReg</span><span class="literal">,</span> <span class="EmpItalic">destMem</span><span class="literal">);</span> instruction could use the following execution stages:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the displacement associated with the memory operand from the memory location immediately following the opcode.</li>&#13;
<li class="noindent">Update EIP to point at the first byte beyond the operand that follows the opcode.</li>&#13;
<li class="noindent">Compute the effective address of the destination memory location, if the <span class="literal">mov</span> instruction uses a complex addressing mode (for example, the indexed addressing mode).</li>&#13;
<li class="noindent">Fetch the data from <span class="EmpItalic">srcReg</span>.</li>&#13;
<li class="noindent">Store the fetched value into the destination memory location.</li></ol>&#13;
<p class="indent">A <span class="literal">mov(</span><span class="EmpItalic">srcMem</span><span class="literal">,</span> <span class="EmpItalic">destReg</span><span class="literal">);</span> instruction is very similar, simply swapping the register access for the memory access in these steps.</p>&#13;
<p class="indent">The <span class="literal">mov(</span><span class="EmpItalic">constant</span><span class="literal">,</span> <span class="EmpItalic">destReg</span><span class="literal">);</span> instruction could use the following execution stages:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the constant associated with the source operand from the memory location immediately following the opcode.</li>&#13;
<li class="noindent">Update EIP to point at the first byte beyond the constant that follows the opcode.</li>&#13;
<li class="noindent">Store the constant value into the destination register.</li></ol>&#13;
<p class="indent">Assuming each stage requires one clock cycle for execution, this sequence (including the three common stages) will require six clock cycles to execute.</p>&#13;
<p class="indent">The <span class="literal">mov(</span><span class="EmpItalic">constant</span><span class="literal">,</span> <span class="EmpItalic">destMem</span><span class="literal">);</span> instruction could use the following execution stages:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the displacement associated with the memory operand from the memory location immediately following the opcode.</li>&#13;
<li class="noindent">Update EIP to point at the first byte beyond the operand that follows the opcode.</li>&#13;
<li class="noindent">Fetch the constant operand’s value from the memory location immediately following the displacement associated with the memory operand.</li>&#13;
<li class="noindent">Update EIP to point at the first byte beyond the constant.</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_257"/>Compute the effective address of the destination memory location, if the <span class="literal">mov</span> instruction uses a complex addressing mode (for example, the indexed addressing mode).</li>&#13;
<li class="noindent">Store the constant value into the destination memory location.</li></ol>&#13;
<h4 class="h4" id="sec9_3_2"><strong><em>9.3.2 The add Instruction</em></strong></h4>&#13;
<p class="noindent">The <span class="literal">add</span> instruction is a little more complex. Here’s a typical set of operations (beyond the common set) that the decoded <span class="literal">add(</span><span class="EmpItalic">srcReg</span><span class="literal">,</span> <span class="EmpItalic">destReg</span><span class="literal">);</span> instruction must complete:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the value of the source register and send it to the <em>arithmetic logical unit (ALU)</em>, which handles arithmetic on the CPU.</li>&#13;
<li class="noindent">Fetch the value of the destination register operand and send it to the ALU.</li>&#13;
<li class="noindent">Instruct the ALU to add the values.</li>&#13;
<li class="noindent">Store the result back into the destination register operand.</li>&#13;
<li class="noindent">Update the flags register with the result of the addition operation.</li></ol>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The</em> flags register, <em>also known as the</em> condition-codes register <em>or</em> program-status word<em>, is an array of Boolean variables in the CPU that tracks whether the previous instruction produced an overflow, a zero result, a negative result, or other such condition.</em></p>&#13;
</div>&#13;
<p class="indent">If the source operand is a memory location instead of a register, and the <span class="literal">add</span> instruction takes the form <span class="literal">add(</span><span class="EmpItalic">srcMem</span><span class="literal">,</span> <span class="EmpItalic">destReg</span><span class="literal">);</span>, then the instruction sequence is slightly more complicated:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the displacement associated with the memory operand from the memory location immediately following the opcode.</li>&#13;
<li class="noindent">Update EIP to point at the first byte beyond the operand that follows the opcode.</li>&#13;
<li class="noindent">Compute the effective address of the source memory location, if the <span class="literal">add</span> instruction uses a complex addressing mode (for example, the indexed addressing mode).</li>&#13;
<li class="noindent">Fetch the source operand’s data from memory and send it to the ALU.</li>&#13;
<li class="noindent">Fetch the value of the destination register operand and send it to the ALU.</li>&#13;
<li class="noindent">Instruct the ALU to add the values.</li>&#13;
<li class="noindent">Store the result back into the destination register operand.</li>&#13;
<li class="noindent">Update the flags register with the result of the addition operation.</li></ol>&#13;
<p class="indent">If the source operand is a constant and the destination operand is a register, the <span class="literal">add</span> instruction takes the form <span class="literal">add(</span><span class="EmpItalic">constant</span><span class="literal">,</span> <span class="EmpItalic">destReg</span><span class="literal">);</span> and the CPU might deal with it as follows:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the constant operand that immediately follows the opcode in memory and send it to the ALU.</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_258"/>Update EIP to point at the first byte beyond the constant that follows the opcode.</li>&#13;
<li class="noindent">Fetch the value of the destination register operand and send it to the ALU.</li>&#13;
<li class="noindent">Instruct the ALU to add the values.</li>&#13;
<li class="noindent">Store the result back into the destination register operand.</li>&#13;
<li class="noindent">Update the flags register with the result of the addition operation.</li></ol>&#13;
<p class="indent">This instruction sequence requires nine cycles to complete.</p>&#13;
<p class="indent">If the source operand is a constant, and the destination operand is a memory location, then the <span class="literal">add</span> instruction takes the form <span class="literal">add(</span><span class="EmpItalic">constant</span><span class="literal">,</span> <span class="EmpItalic">destMem</span><span class="literal">);</span> and the sequence is slightly more complicated:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the displacement associated with the memory operand from memory immediately following the opcode.</li>&#13;
<li class="noindent">Update EIP to point at the first byte beyond the operand that follows the opcode.</li>&#13;
<li class="noindent">Compute the effective address of the destination memory location, if the <span class="literal">add</span> instruction uses a complex addressing mode (for example, the indexed addressing mode).</li>&#13;
<li class="noindent">Fetch the constant operand that immediately follows the memory operand’s displacement value and send it to the ALU.</li>&#13;
<li class="noindent">Fetch the destination operand’s data from memory and send it to the ALU.</li>&#13;
<li class="noindent">Update EIP to point at the first byte beyond the constant that follows the memory operand.</li>&#13;
<li class="noindent">Instruct the ALU to add the values.</li>&#13;
<li class="noindent">Store the result back into the destination memory operand.</li>&#13;
<li class="noindent">Update the flags register with the result of the addition operation.</li></ol>&#13;
<p class="indent">This instruction sequence requires 11 or 12 cycles to complete, depending on whether the effective address computation is necessary.</p>&#13;
<h4 class="h4" id="sec9_3_3"><strong><em>9.3.3 The jnz Instruction</em></strong></h4>&#13;
<p class="noindent">Because the 80x86 <span class="literal">jnz</span> instruction does not allow different types of operands, it needs only one sequence of steps. The <span class="literal">jnz</span> <span class="literal">label;</span> instruction might use the following additional execution stages once decoded:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the displacement value (the jump distance) and send it to the ALU.</li>&#13;
<li class="noindent">Update the EIP register to hold the address of the instruction following the displacement operand.</li>&#13;
<li class="noindent">Test the zero flag to see if it is clear (that is, if it contains <span class="literal">0</span>).</li>&#13;
<li class="noindent">If the zero flag was clear, copy the value in EIP to the ALU.</li>&#13;
<li class="noindent">If the zero flag was clear, instruct the ALU to add the displacement and EIP values.</li>&#13;
<li class="noindent">If the zero flag was clear, copy the result of the addition back to the EIP.</li></ol>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_259"/>Notice how the <span class="literal">jnz</span> instruction requires fewer steps, and thus runs in fewer clock cycles, if the jump is not taken. This is very typical for conditional jump instructions.</p>&#13;
<h4 class="h4" id="sec9_3_4"><strong><em>9.3.4 The loop Instruction</em></strong></h4>&#13;
<p class="noindent">Because the 80x86 <span class="literal">loop</span> instruction does not allow different types of operands, it needs only one sequence of steps. The decoded 80x86 <span class="literal">loop</span> instruction might use an execution sequence like the following:<sup><a href="footnotes.xhtml#fn9_2a" id="fn9_2">2</a></sup></p>&#13;
<ol>&#13;
<li class="noindent">Fetch the value of the ECX register and send it to the ALU.</li>&#13;
<li class="noindent">Instruct the ALU to decrement this value.</li>&#13;
<li class="noindent">Send the result back to the ECX register. Set a special internal flag if this result is nonzero.</li>&#13;
<li class="noindent">Fetch the displacement value (the jump distance) following the opcode in memory and send it to the ALU.</li>&#13;
<li class="noindent">Update the EIP register with the address of the instruction following the displacement operand.</li>&#13;
<li class="noindent">Test the special internal flag to see if ECX was nonzero.</li>&#13;
<li class="noindent">If the flag was set (that is, it contains <span class="literal">1</span>), copy the value in EIP to the ALU.</li>&#13;
<li class="noindent">If the flag was set, instruct the ALU to add the displacement and EIP values.</li>&#13;
<li class="noindent">If the flag was set, copy the result of the addition back to the EIP register.</li></ol>&#13;
<p class="indent">As with the <span class="literal">jnz</span> instruction, note that the <span class="literal">loop</span> instruction executes more rapidly if the branch is not taken, and the CPU continues execution with the instruction that immediately follows the <span class="literal">loop</span> instruction.</p>&#13;
<h3 class="h3" id="sec9_4"><strong>9.4 RISC vs. CISC: Improving Performance by Executing More, Faster, Instructions</strong></h3>&#13;
<p class="noindent">Early microprocessors (including the 80x86 and its predecessors) are examples of <em>complex instruction set computers (CISCs)</em>. At the time these CPUs were created, the thinking was that having each instruction do more work made programs run faster because they executed fewer instructions (as CPUs with less complex instructions had to execute more instructions to do the same amount of work). The Digital Equipment Corporation (DEC) PDP-11 and its successor, the VAX, epitomized this design philosophy.</p>&#13;
<p class="indent">In the early 1980s, computer architecture researchers discovered that this complexity came at a huge cost. All the hardware necessary to support these complex instructions wound up constraining the overall clock speed of the CPU. Experiments with the VAX 11-780 minicomputer demonstrated that programs executing multiple, simple, instructions were faster than those executing fewer, more complex, instructions. Those researchers <span epub:type="pagebreak" id="page_260"/>hypothesized that if they stripped the instruction set down to the bare essentials, using only simple instructions, they could boost the hardware’s performance (by increasing the clock speed). They called this new architecture <em>reduced instruction set computer (RISC)</em>.<sup><a href="footnotes.xhtml#fn9_3a" id="fn9_3">3</a></sup> So began the great “RISC versus CISC” debate: which architecture was really better?</p>&#13;
<p class="indent">On paper, at least, RISC CPUs looked better. In practice, they ran at slower clock speeds, because existing CISC designs had a huge head start (as their designers had had many more years to optimize them). By the time RISC CPU designs had matured enough to run at higher clock speeds, the CISC designs had evolved, taking advantage of the RISC research. Today, the 80x86 CISC CPU is still the high-performance king. RISC CPUs found a different niche: they tend to be more power efficient than CISC processors, so they typically wind up in portable and low-power designs (such as cell phones and tablets).</p>&#13;
<p class="indent">Though the 80x86 (a CISC CPU) remains the performance leader, it’s still possible to write programs with a larger number of simple 80x86 instructions that run faster than those with fewer, more complex 80x86 instructions. 80x86 designers have kept these legacy instructions around to allow you to execute older software that still contains them. Newer compilers, however, avoid these legacy instructions to produce faster-running code.</p>&#13;
<p class="indent">Nevertheless, one important takeaway from RISC research is that the execution time of each instruction is largely dependent upon the amount of work it does. The more internal operations an instruction requires, the longer it will take to execute. In addition to improving execution time by reducing the number of internal operations, RISC also prioritized internal operations that could execute concurrently—that is, <em>in parallel</em>.</p>&#13;
<h3 class="h3" id="sec9_5"><strong>9.5 Parallelism: The Key to Faster Processing</strong></h3>&#13;
<p class="noindent">If we can reduce the amount of time it takes for a CPU to execute the individual instructions in its instruction set, an application containing a sequence of those instructions will also run faster than it otherwise would.</p>&#13;
<p class="indent">An early goal of the RISC processors was to execute one instruction per clock cycle, on average. However, even if a RISC instruction is simplified, its actual execution still requires multiple steps. So how could the processors achieve this goal? The answer is parallelism.</p>&#13;
<p class="indent">Consider the following steps for a <span class="literal">mov(</span><span class="EmpItalic">srcReg</span><span class="literal">,</span> <span class="EmpItalic">destReg</span><span class="literal">);</span> instruction:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the instruction’s opcode from memory.</li>&#13;
<li class="noindent">Update the EIP register with the address of the byte following the opcode.</li>&#13;
<li class="noindent">Decode the instruction’s opcode to see what instruction it specifies.</li>&#13;
<li class="noindent">Fetch the data from <span class="EmpItalic">srcReg</span>.</li>&#13;
<li class="noindent">Store the fetched value into the destination register (<span class="EmpItalic">destReg</span>).</li></ol>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_261"/>The CPU must fetch the instruction’s opcode from memory before it updates the EIP register instruction with the address of the byte beyond the opcode, decode the opcode before it knows to fetch the value of the source register, and fetch the value of the source register before it can store the fetched value in the destination register.</p>&#13;
<p class="indent">All but one of the stages in the execution of this <span class="literal">mov</span> instruction are <em>serial</em>. That is, the CPU must execute one stage before proceeding to the next. The exception is step 2, updating the EIP register. Although this stage must follow the first stage, none of the following stages depend upon it. We could execute this step concurrently with any of the others, and it wouldn’t affect the operation of the <span class="literal">mov</span> instruction. By doing two of the stages in parallel, then, we can reduce this instruction’s execution time by one clock cycle. The following sequence illustrates one possible concurrent execution:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the instruction’s opcode from memory.</li>&#13;
<li class="noindent">Decode the instruction’s opcode to see what instruction it specifies.</li>&#13;
<li class="noindent">Fetch the data from <span class="EmpItalic">srcReg</span> <em>and</em> update the EIP register with the address of the byte following the opcode.</li>&#13;
<li class="noindent">Store the fetched value into the destination register (<span class="EmpItalic">destReg</span>).</li></ol>&#13;
<p class="indent">Although the remaining stages in the <span class="literal">mov(</span><span class="EmpItalic">srcReg</span><span class="literal">,</span> <span class="EmpItalic">destReg</span><span class="literal">);</span> instruction must be serialized, other forms of the <span class="literal">mov</span> instruction offer similar opportunities to save cycles by executing stages concurrently. For example, consider the 80x86 <span class="literal">mov([ebx+disp],</span> <span class="literal">eax);</span> instruction:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the instruction’s opcode from memory.</li>&#13;
<li class="noindent">Update the EIP register with the address of the byte following the opcode.</li>&#13;
<li class="noindent">Decode the instruction’s opcode to see what instruction it specifies.</li>&#13;
<li class="noindent">Fetch the displacement value for use in calculating the effective address of the source operand.</li>&#13;
<li class="noindent">Update EIP to point at the first byte after the displacement value in memory.</li>&#13;
<li class="noindent">Compute the effective address of the source operand.</li>&#13;
<li class="noindent">Fetch the value of the source operand’s data from memory.</li>&#13;
<li class="noindent">Store the result into the destination register operand.</li></ol>&#13;
<p class="indent">Once again, we can overlap the execution of several stages in this instruction. In the following example, we reduce the number of steps from eight to six by overlapping both updates of EIP with two other operations:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the instruction’s opcode from memory.</li>&#13;
<li class="noindent">Decode the instruction’s opcode to see what instruction it specifies, <em>and</em> update the EIP register with the address of the byte following the opcode.</li>&#13;
<li class="noindent">Fetch the displacement value for use in calculating the effective address of the source operand.</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_262"/>Compute the effective address of the source operand, <em>and</em> update EIP to point at the first byte after the displacement value in memory.</li>&#13;
<li class="noindent">Fetch the value of the source operand’s data from memory.</li>&#13;
<li class="noindent">Store the result into the destination register operand.</li></ol>&#13;
<p class="indent">As a last example, consider the <span class="literal">add(</span><span class="EmpItalic">constant</span><span class="literal">,</span> <span class="literal">[ebx+</span><span class="EmpItalic">disp</span><span class="literal">]);</span> instruction. Its serial execution looks like this:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the instruction’s opcode from memory.</li>&#13;
<li class="noindent">Update the EIP register with the address of the byte following the opcode.</li>&#13;
<li class="noindent">Decode the instruction’s opcode to see what instruction it specifies.</li>&#13;
<li class="noindent">Fetch the displacement value from the memory location immediately following the opcode.</li>&#13;
<li class="noindent">Update EIP to point at the first byte beyond the displacement operand that follows the opcode.</li>&#13;
<li class="noindent">Compute the effective address of the second operand.</li>&#13;
<li class="noindent">Fetch the constant operand that immediately follows the displacement value in memory and send it to the ALU.</li>&#13;
<li class="noindent">Fetch the destination operand’s data from memory and send it to the ALU.</li>&#13;
<li class="noindent">Update EIP to point at the first byte beyond the constant that follows the displacement operand.</li>&#13;
<li class="noindent">Instruct the ALU to add the values.</li>&#13;
<li class="noindent">Store the result back into the destination (second) operand.</li>&#13;
<li class="noindent">Update the flags register with the result of the addition operation.</li></ol>&#13;
<p class="indent">We can overlap several stages in this instruction because they don’t depend on the result of their immediate predecessor:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the instruction’s opcode from memory.</li>&#13;
<li class="noindent">Decode the instruction’s opcode to see what instruction it specifies <em>and</em> update the EIP register with the address of the byte following the opcode.</li>&#13;
<li class="noindent">Fetch the displacement value from the memory location immediately following the opcode.</li>&#13;
<li class="noindent">Update EIP to point at the first byte beyond the displacement operand that follows the opcode <em>and</em> compute the effective address of the memory operand (<span class="literal">ebx+</span><span class="EmpItalic">disp</span>).</li>&#13;
<li class="noindent">Fetch the constant operand that immediately follows the displacement value and send it to the ALU.</li>&#13;
<li class="noindent">Fetch the destination operand’s data from memory and send it to the ALU.</li>&#13;
<li class="noindent">Instruct the ALU to add the values <em>and</em> update EIP to point at the first byte beyond the constant value.</li>&#13;
<li class="noindent">Store the result back into the second operand <em>and</em> update the flags register with the result of the addition operation.</li></ol>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_263"/>Although it might seem like the CPU could fetch the constant and the memory operand in the same stage because their values do not depend upon each other, it can’t do this (yet!) because it has only a single data bus, and both values are coming from memory. In the next section you’ll see how we can overcome this problem.</p>&#13;
<p class="indent">By overlapping various execution stages, we’ve substantially reduced the number of steps, and consequently the number of clock cycles, that these instructions need to complete execution. This is a major key to improving CPU performance without cranking up the chip’s clock speed. However, there’s only so much to be gained from this approach alone, because instruction execution is still serialized. Starting with the next section, we’ll see how to overlap the execution of adjacent instructions in order to save additional cycles.</p>&#13;
<h4 class="h4" id="sec9_5_1"><strong><em>9.5.1 Functional Units</em></strong></h4>&#13;
<p class="noindent">As you’ve seen in the <span class="literal">add</span> instruction, the steps for adding two values and then storing their sum can’t be done concurrently, because you can’t store the sum until after you’ve computed it. Furthermore, there are some resources that the CPU can’t share between steps in an instruction. There is only one data bus, and the CPU can’t fetch an instruction’s opcode while it is trying to store data to memory. In addition, many of the steps that make up the execution of an instruction share functional units in the CPU.</p>&#13;
<p class="indent"><em>Functional units</em> are groups of logic that perform a common operation, such as the arithmetic logical unit and the control unit. A functional unit can do only one operation at a time; you can’t do two operations concurrently that use the same functional unit. To design a CPU that executes several stages in parallel, we must arrange those stages to reduce potential conflicts, or add extra logic so that two (or more) operations can occur simultaneously by executing in different functional units.</p>&#13;
<p class="indent">Consider again the steps that a <span class="literal">mov(</span><span class="EmpItalic">srcMem</span><span class="literal">,</span> <span class="EmpItalic">destReg</span><span class="literal">);</span> instruction might require:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the instruction’s opcode from memory.</li>&#13;
<li class="noindent">Update the EIP register to hold the address of the displacement value following the opcode.</li>&#13;
<li class="noindent">Decode the instruction’s opcode to see what instruction it specifies.</li>&#13;
<li class="noindent">Fetch the displacement value from memory to compute the source operand’s effective address.</li>&#13;
<li class="noindent">Update the EIP register to hold the address of the byte beyond the displacement value.</li>&#13;
<li class="noindent">Compute the effective address of the source operand.</li>&#13;
<li class="noindent">Fetch the value of the source operand.</li>&#13;
<li class="noindent">Store the fetched value into the destination register.</li></ol>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_264"/>The first operation uses the value of the EIP register, so we can’t overlap it with the subsequent step, which adjusts the value in EIP. In addition, the first operation uses the bus to fetch the instruction opcode from memory, and because every step that follows this one depends upon this opcode, it’s unlikely that we’ll be able to overlap it with any other.</p>&#13;
<p class="indent">The second and third operations don’t share any functional units, and the third operation doesn’t depend upon the value of the EIP register, which is modified in the second step. Therefore, we can modify the control unit so that it combines these steps, adjusting the EIP register at the same time that it decodes the instruction. This will shave one cycle off the execution of the <span class="literal">mov</span> instruction.</p>&#13;
<p class="indent">The third and fourth steps, which decode the instruction’s opcode and fetch the displacement value, don’t look like they can be done in parallel, because you must decode the instruction’s opcode to determine whether the CPU needs to fetch a displacement operand from memory. However, we can design the CPU to fetch the displacement anyway so that it’s available if we need it.</p>&#13;
<p class="indent">Of course, there’s no way to overlap the execution of steps 7 and 8 because the CPU must fetch the value before storing it away.</p>&#13;
<p class="indent">By combining all the steps that are possible, we might obtain the following sequence for a <span class="literal">mov</span> instruction:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the instruction’s opcode from memory.</li>&#13;
<li class="noindent">Decode the instruction’s opcode to see what instruction it specifies, <em>and</em> update the EIP register to hold the address of the displacement value following the opcode.</li>&#13;
<li class="noindent">Fetch the displacement value from memory to compute the source operand’s effective address, <em>and</em> update the EIP register to hold the address of the byte beyond the displacement value.</li>&#13;
<li class="noindent">Compute the effective address of the source operand.</li>&#13;
<li class="noindent">Fetch the value of the source operand from memory.</li>&#13;
<li class="noindent">Store the fetched value into the destination register.</li></ol>&#13;
<p class="indent">By adding a small amount of logic to the CPU, we’ve shaved one or two cycles off the execution of the <span class="literal">mov</span> instruction. This simple optimization works with most of the other instructions as well.</p>&#13;
<p class="indent">Now consider the <span class="literal">loop</span> instruction, which has several steps that use the ALU. If the CPU has only a single ALU, it must execute these steps sequentially. However, if the CPU has multiple ALUs (that is, multiple functional units), it can execute some of these steps in parallel. For example, the CPU could decrement the value in the ECX register (using the ALU) at the same time it updates the EIP value. Note that the <span class="literal">loop</span> instruction also uses the ALU to compare the decremented ECX value against <span class="literal">0</span> (to determine if it should branch). However, there’s a data dependency between incrementing ECX and comparing it with <span class="literal">0</span>; therefore, the CPU can’t perform both of these operations at the same time.</p>&#13;
<h4 class="h4" id="sec9_5_2"><span epub:type="pagebreak" id="page_265"/><strong><em>9.5.2 The Prefetch Queue</em></strong></h4>&#13;
<p class="noindent">Now that we’ve looked at some simple optimization techniques, consider what happens when the <span class="literal">mov</span> instruction executes on a CPU with a 32-bit data bus. If the <span class="literal">mov</span> instruction fetches an 8-bit displacement value from memory, the CPU may wind up fetching an additional 3 bytes along with the displacement value (the 32-bit data bus lets us fetch 4 bytes in a single bus cycle). The second byte on the data bus is actually the opcode of the next instruction. If we could save this opcode until the execution of the next instruction, we could shave a cycle off its execution time because it wouldn’t have to fetch the same opcode byte again.</p>&#13;
<h5 class="h5" id="sec9_5_2_1"><strong>9.5.2.1 Using Unused Bus Cycles</strong></h5>&#13;
<p class="noindent">There are still more improvements we can make. While the <span class="literal">mov</span> instruction is executing, the CPU isn’t accessing memory on every clock cycle. For example, while data is being stored into the destination register, the bus is idle. When the bus is idle, we can prefetch and save the instruction opcode and operands of the next instruction.</p>&#13;
<p class="indent">The hardware that does this is the <em><a href="gloss01.xhtml#gloss01_203">prefetch queue</a></em>. <a href="ch09.xhtml#ch09fig04">Figure 9-4</a> shows the internal organization of a CPU with a prefetch queue.</p>&#13;
<div class="image"><img alt="image" src="../images/09fig04.jpg"/></div>&#13;
<p class="figcap"><a id="ch09fig04"/><em>Figure 9-4: CPU design with a prefetch queue</em></p>&#13;
<p class="indent">The <em>bus interface unit (BIU)</em>, as its name implies, controls access to the address and data buses. The BIU acts as a “traffic cop” and handles simultaneous requests for bus access by different modules, such as the execution unit and the prefetch queue. Whenever some component inside the CPU wishes to access main memory, it sends this request to the BIU.</p>&#13;
<p class="indent">Whenever the execution unit is not using the BIU, the BIU can fetch additional bytes from the memory that holds the machine instructions and store them in the prefetch queue. Then, whenever the CPU needs an instruction opcode or operand value, it grabs <em>the next available byte</em> from <span epub:type="pagebreak" id="page_266"/>the prefetch queue. Because the BIU grabs multiple bytes at a time from memory, and because, per clock cycle, the CPU generally consumes fewer bytes from the prefetch queue than are available, instructions will normally be sitting in the prefetch queue for the CPU’s use.</p>&#13;
<p class="indent">However, there’s no guarantee that all instructions and operands will be sitting in the prefetch queue when we need them. For example, consider the 80x86 <span class="literal">jnz Label;</span> instruction. If the 2-byte form of the instruction appears at locations 400 and 401 in memory, the prefetch queue may contain the bytes at addresses 402, 403, 404, 405, 406, 407, and so on. If <span class="literal">jnz</span> transfers control to <span class="literal">Label</span> at target address 480, the bytes at addresses 402, 403, 404, and so on, won’t be of any use to the CPU. The system will have to pause for a moment to fetch the data at address 480 before it can go on. Most of the time, the CPU fetches sequential values from memory, though, so having the data in the prefetch queue saves time.</p>&#13;
<h5 class="h5" id="sec9_5_2_2"><strong>9.5.2.2 Overlapping Instructions</strong></h5>&#13;
<p class="noindent">Another improvement we can make is to overlap the processes of decoding the next instruction’s opcode and executing the last step of the previous instruction. After the CPU processes the operand, the next available byte in the prefetch queue is an opcode, which the CPU can decode because the instruction decoder is idle while the CPU executes the steps of the current instruction. Of course, if the current instruction modifies the EIP register, the time the CPU spends on the decoding operation goes to waste; however, because it occurs in parallel with other operations of the current instruction, this decoding doesn’t slow down the system (though it does require extra circuitry).</p>&#13;
<h5 class="h5" id="sec9_5_2_3"><strong>9.5.2.3 Summarizing Background Prefetch Events</strong></h5>&#13;
<p class="noindent">Our instruction execution sequence now assumes that the following CPU prefetch events are occurring (concurrently) in the background:</p>&#13;
<ol>&#13;
<li class="noindent">If the prefetch queue is not full (generally it can hold between 8 and 32 bytes, depending on the processor) and the BIU is idle on the current clock cycle, fetch the next double word located at the address found in the EIP register at the beginning of the clock cycle.</li>&#13;
<li class="noindent">If the instruction decoder is idle and the current instruction does not require an instruction operand, the CPU should begin decoding the opcode at the front of the prefetch queue. If the current instruction requires an instruction operand, then the CPU begins decoding the byte just beyond that operand in the prefetch queue.</li></ol>&#13;
<p class="indent">Now let’s reconsider our <span class="literal">mov(</span><span class="EmpItalic">srcreg</span><span class="literal">,</span> <span class="EmpItalic">destreg</span><span class="literal">);</span> instruction. Because we’ve added the prefetch queue and the BIU, we can overlap the fetch and decode stages of this instruction with specific stages of the previous instruction to get the following steps:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch and decode the instruction; this is overlapped with the previous instruction.</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_267"/>Fetch the source register and update the EIP register with the address of the next instruction.</li>&#13;
<li class="noindent">Store the fetched value into the destination register.</li></ol>&#13;
<p class="indent">The instruction execution timings in this example assume that the opcode is present in the prefetch queue and that the CPU has already decoded it. If either is not true, additional cycles will be necessary to fetch the opcode from memory and decode the instruction.</p>&#13;
<h4 class="h4" id="sec9_5_3"><strong><em>9.5.3 Conditions That Hinder the Performance of the Prefetch Queue</em></strong></h4>&#13;
<p class="noindent">When they transfer control to the target location, jump and conditional jump instructions are slower than other instructions, because the CPU can’t overlap the processes of fetching and decoding the opcode for the next instruction with the process of executing a jump instruction that transfers control. It may take several cycles after the execution of a jump instruction for the prefetch queue to reload.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>If you want to write fast code, avoid jumping around in your program as much as possible.</em></p>&#13;
</div>&#13;
<p class="indent">Conditional jump instructions invalidate the prefetch queue only if they actually transfer control to the target location. If the jump condition is <span class="literal">false</span>, execution continues with the next instruction and the values in the prefetch queue remain valid. Therefore, while writing the program, if you can determine which jump condition occurs most frequently, you should arrange your program so that the most common condition causes the program to continue with the next instruction rather than jump to a separate location.</p>&#13;
<p class="indent">In addition, instruction size (in bytes) can affect the performance of the prefetch queue. The larger the instruction, the faster the CPU will empty the prefetch queue. Instructions involving constants and memory operands tend to be the largest. If you execute a sequence of these instructions in a row, the CPU may end up having to wait because it is removing instructions from the prefetch queue faster than the BIU is copying data to the prefetch queue. So, whenever possible, try to use shorter instructions.</p>&#13;
<p class="indent">Finally, prefetch queues work best when you have a wide data bus. The 16-bit 8086 processor runs much faster than the 8-bit 8088 because it can keep the prefetch queue full with fewer bus accesses. Don’t forget, the CPU needs to use the bus for other purposes. Instructions that access memory compete with the prefetch queue for access to the bus. If you have a sequence of instructions that all access memory, the prefetch queue may quickly empty, and once that happens, the CPU must wait for the BIU to fetch new opcodes from memory before it can continue executing instructions.</p>&#13;
<h4 class="h4" id="sec9_5_4"><strong><em>9.5.4 Pipelining: Overlapping the Execution of Multiple Instructions</em></strong></h4>&#13;
<p class="noindent">Executing instructions in parallel using a BIU and an execution unit is a special case of pipelining. Most modern processors incorporate pipelining <span epub:type="pagebreak" id="page_268"/>to improve performance. With just a few exceptions, pipelining allows us to execute one instruction per clock cycle.</p>&#13;
<p class="indent">The advantage of the prefetch queue is that it lets the CPU overlap the processes of fetching and decoding the instruction opcode with the execution of other instructions. Assuming you’re willing to add hardware, you can execute almost all operations in parallel. That is the idea behind pipelining.</p>&#13;
<p class="indent">Pipelined operation improves an application’s average performance by executing several instructions concurrently. However, as you saw with the prefetch queue, certain instructions (and combinations thereof) fare better than others in a pipelined system. By understanding how pipelined operation works, you can organize your applications to run faster.</p>&#13;
<h5 class="h5" id="sec9_5_4_1"><strong>9.5.4.1 A Typical Pipeline</strong></h5>&#13;
<p class="noindent">Consider the steps necessary to do a generic operation, with each step taking one clock cycle:</p>&#13;
<ol>&#13;
<li class="noindent">Fetch the instruction’s opcode from memory.</li>&#13;
<li class="noindent">Decode the opcode <em>and</em>, if required, prefetch a displacement operand, a constant operand, or both.</li>&#13;
<li class="noindent">If required, compute the effective address for a memory operand (for example, <span class="literal">[ebx+</span><span class="EmpItalic">disp</span><span class="literal">]</span>).</li>&#13;
<li class="noindent">If required, fetch the value of any memory operand and/or register.</li>&#13;
<li class="noindent">Compute the result.</li>&#13;
<li class="noindent">Store the result into the destination register.</li></ol>&#13;
<p class="indent">Assuming you’re willing to pay for some extra silicon, you can build a little <em>miniprocessor</em> to handle each step. The organization would look something like <a href="ch09.xhtml#ch09fig05">Figure 9-5</a>.</p>&#13;
<div class="image"><img alt="image" src="../images/09fig05.jpg"/></div>&#13;
<p class="figcap"><a id="ch09fig05"/><em>Figure 9-5: A pipelined implementation of instruction execution</em></p>&#13;
<p class="indent">In stage 4, the CPU fetches both the source and destination operands. You can set this up by putting multiple data paths inside the CPU (such as from the registers to the ALU) and ensuring that no two operands ever compete for simultaneous use of the data bus (that is, there are no memory-to-memory operations).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_269"/>If you design a separate piece of hardware for each stage in the pipeline in <a href="ch09.xhtml#ch09fig05">Figure 9-5</a>, almost all of them can take place in parallel. Of course, you can’t fetch and decode the opcode for more than one instruction at the same time, but you can fetch the opcode of the next instruction while decoding the current instruction’s opcode. If you have an <em>n</em>-stage pipeline, you will usually have <em>n</em> instructions executing concurrently. <a href="ch09.xhtml#ch09fig06">Figure 9-6</a> shows pipelining in operation. T1, T2, T3, and so on, represent consecutive “ticks” (time = 1, time = 2, and so on) of the system clock.</p>&#13;
<div class="image"><img alt="image" src="../images/09fig06.jpg"/></div>&#13;
<p class="figcap"><a id="ch09fig06"/><em>Figure 9-6: Instruction execution in a pipeline</em></p>&#13;
<p class="indent">At time T = T1, the CPU fetches the opcode byte for the first instruction. At T = T2, the CPU begins decoding the opcode for the first instruction, and, in parallel, it fetches a block of bytes from the prefetch queue in the event that the first instruction has an operand. Also in parallel with the decoding of the first instruction, the CPU instructs the BIU to fetch the opcode of the second instruction because the first instruction no longer needs that circuitry.</p>&#13;
<p class="indent">Note that there is a minor conflict here. The CPU is attempting to fetch the next byte from the prefetch queue for use as an operand; at the same time, it is fetching operand data from the prefetch queue for use as an opcode. How can it do both at once? You’ll see the solution shortly.</p>&#13;
<p class="indent">At time T = T3, the CPU computes the address of any memory operand if the first instruction accesses memory. If the first instruction doesn’t access memory, the CPU does nothing. During T3, the CPU also decodes the opcode of the second instruction and fetches any operands in the second instruction. Finally, the CPU also fetches the opcode for the third instruction. With each advancing tick of the clock, another execution stage of each instruction in the pipeline completes, and the CPU fetches the opcode of yet another instruction from memory.</p>&#13;
<p class="indent">This process continues until, at T = T6, the CPU completes the execution of the first instruction, computes the result for the second, and fetches the opcode for the sixth instruction in the pipeline. The important thing to note is that after T = T5, the CPU completes an instruction on every clock cycle. Once the CPU fills the pipeline, it completes one instruction on each cycle. This is true even if there are complex addressing modes to be computed, memory operands to fetch, or other operations that consume cycles on a nonpipelined processor. All you need to do is add more stages to the pipeline, and you can still effectively process each instruction in one clock cycle.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_270"/>Now back to the small conflict in the pipeline organization I mentioned earlier. At T = T2, for example, the CPU attempts to prefetch a block of bytes containing any operands of the first instruction, and at the same time it fetches the opcode of the second instruction. Until the CPU decodes the first instruction, it doesn’t know how many operands the instruction requires or their length. Moreover, until it determines that information, the CPU doesn’t know what byte to fetch as the opcode of the second instruction. So how can the pipeline fetch the opcode of the next instruction in parallel with any address operands of the current instruction?</p>&#13;
<p class="indent">One solution is to disallow this simultaneous operation in order to avoid the potential data hazard. If an instruction has an address or constant operand, we can simply delay the start of the next instruction. Unfortunately, many instructions have these additional operands, so this approach will substantially hinder the CPU’s execution speed.</p>&#13;
<p class="indent">The second solution is to throw a lot more hardware at the problem. Operand and constant sizes usually come in 1-, 2-, and 4-byte lengths. Therefore, if we actually fetch the bytes in memory that are located at offsets 1, 3, and 5 bytes beyond the current opcode we are decoding, one of them will probably contain the opcode of the next instruction. Once we are through decoding the current instruction, we know how many bytes it consumes, and, therefore, we know the offset of the next opcode. We can use a simple data selector circuit to choose which of the three candidate opcode bytes we want to use.</p>&#13;
<p class="indent">In practice, we actually have to select the next opcode byte from more than three candidates because 80x86 instructions come in many different lengths. For example, a <span class="literal">mov</span> instruction that copies a 32-bit constant to a memory location can be 10 or more bytes long. Moreover, instructions vary in length from 1 to 15 bytes. And some opcodes on the 80x86 are longer than 1 byte, so the CPU may have to fetch multiple bytes in order to properly decode the current instruction. However, by throwing more hardware at the problem, we can decode the current opcode at the same time we’re fetching the next.</p>&#13;
<h5 class="h5" id="sec9_5_4_2"><strong>9.5.4.2 Stalls in a Pipeline</strong></h5>&#13;
<p class="noindent">Unfortunately, the scenario presented in the previous section is a little too simplistic. There are two problems that our simple pipeline ignores: competition between instructions for access to the bus (known as <em>bus contention</em>), and nonsequential instruction execution. Both problems may increase the average execution time of the instructions in the pipeline. By understanding how the pipeline works, you can write your software to avoid these pitfalls and improve the performance of your applications.</p>&#13;
<p class="indent">Bus contention can occur whenever an instruction needs to access an item in memory. For example, if a <span class="literal">mov(</span><span class="EmpItalic">reg</span><span class="literal">,</span> <span class="EmpItalic">mem</span><span class="literal">);</span> instruction needs to store data in memory and a <span class="literal">mov(</span><span class="EmpItalic">mem</span><span class="literal">,</span> <span class="EmpItalic">reg</span><span class="literal">);</span> instruction needs to fetch data from memory, contention for the address and data bus may develop because the CPU will be trying to do both operations simultaneously.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_271"/>One simplistic way to handle bus contention is through a <em><a href="gloss01.xhtml#gloss01_193">pipeline stall</a></em>. The CPU, when faced with contention for the bus, gives priority to the instruction farthest along in the pipeline. This stalls the later instruction in the pipeline, and it takes two cycles to execute that instruction (see <a href="ch09.xhtml#ch09fig07">Figure 9-7</a>).</p>&#13;
<div class="image"><img alt="image" src="../images/09fig07.jpg"/></div>&#13;
<p class="figcap"><a id="ch09fig07"/><em>Figure 9-7: A pipeline stall</em></p>&#13;
<p class="indent">There are many other cases of bus contention. For example, fetching operands for an instruction requires access to the prefetch queue at the same time that the CPU needs to access it to fetch the opcode of the next instruction. Given the simple pipelining scheme that we’ve outlined so far, it’s unlikely that most instructions would execute at one clock (cycle) per instruction (CPI).</p>&#13;
<p class="indent">As another example of a pipeline stall, consider what happens when an instruction <em>modifies</em> the value in the EIP register. For example, the <span class="literal">jnz</span> instruction might change the value in the EIP register if it transfers control to its target label, which implies that the next set of instructions to be executed does not immediately follow the <span class="literal">jnz</span> instruction. By the time the instruction <span class="literal">jnz label;</span> completes execution (assuming the zero flag is clear so that the branch is taken), we’ve already started five other instructions and we’re only one clock cycle away from completing the first of these. The CPU must not execute those instructions, or it will compute improper results.</p>&#13;
<p class="indent">The only reasonable solution is to <em>flush</em> the entire pipeline and begin fetching opcodes anew. However, doing so causes a severe execution time penalty. It will take the length of the pipeline (six cycles in our example) before the next instruction completes execution. The longer the pipeline is, the more you can accomplish per cycle in the system, but the slower a program will run if it jumps around quite a bit. Unfortunately, you can’t control the number of stages in the pipeline,<sup><a href="footnotes.xhtml#fn9_4a" id="fn9_4">4</a></sup> but you <em>can</em> control the number of transfer instructions in your programs, so it’s best to keep these to a minimum in a pipelined system.</p>&#13;
<h4 class="h4" id="sec9_5_5"><span epub:type="pagebreak" id="page_272"/><strong><em>9.5.5 Instruction Caches: Providing Multiple Paths to Memory</em></strong></h4>&#13;
<p class="noindent">System designers can resolve many problems with bus contention through the intelligent use of the prefetch queue and the cache memory subsystem. As you’ve seen, they can design the prefetch queue to buffer data from the instruction stream. However, they can also use a separate <em><a href="gloss01.xhtml#gloss01_124">instruction cache</a></em> (apart from the data cache) to hold machine instructions. As a programmer, you have no control over how your CPU’s instruction cache is organized, but knowing how it operates might prompt you to use certain instruction sequences that would otherwise create stalls.</p>&#13;
<p class="indent">Suppose the CPU has two separate memory spaces, one for instructions and one for data, each with its own bus. This is called the <em><a href="gloss01.xhtml#gloss01_107">Harvard architecture</a></em> because the first such machine was built at Harvard University. On a Harvard machine, there’s no contention for the bus; the BIU can continue to fetch opcodes on the instruction bus while accessing memory on the data/memory bus (see <a href="ch09.xhtml#ch09fig08">Figure 9-8</a>).</p>&#13;
<div class="image"><img alt="image" src="../images/09fig08.jpg"/></div>&#13;
<p class="figcap"><a id="ch09fig08"/><em>Figure 9-8: A typical Harvard machine</em></p>&#13;
<p class="indent">In the real world, there are very few true Harvard machines. The extra pins needed on the processor to support two physically separate buses increase the cost of the processor and introduce many other engineering problems. However, microprocessor designers have discovered that they can obtain many of the benefits of the Harvard architecture with few of its disadvantages by using separate on-chip caches for data and instructions. Advanced CPUs use an internal Harvard architecture and an external von Neumann architecture. <a href="ch09.xhtml#ch09fig09">Figure 9-9</a> shows the structure of the 80x86 with separate data and instruction caches.</p>&#13;
<div class="image"><img alt="image" src="../images/09fig09.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_273"/><a id="ch09fig09"/><em>Figure 9-9: Using separate code and data caches</em></p>&#13;
<p class="indent">Each path between the sections inside the CPU represents an independent bus, and data can flow on all paths concurrently. This means that the prefetch queue can pull instruction opcodes from the instruction cache while the execution unit is writing data to the data cache. However, it’s not always possible, even with a cache, to avoid bus contention. In the arrangement with two separate caches, the BIU still has to use the data/address bus to fetch opcodes from memory whenever they are not located in the instruction cache. Likewise, the data cache still has to buffer data from memory on occasion.</p>&#13;
<p class="indent">Although you can’t control the presence, size, or type of cache on a CPU, you must be aware of how the cache operates in order to write the best programs. On-chip, level-one (L1) instruction caches are generally quite small (between 4KB and 64KB on typical CPUs) compared to the size of main memory. Therefore, the shorter your instructions, the more of them will fit in the cache (tired of “shorter instructions” yet?). The more instructions you have in the cache, the less often bus contention will occur. Likewise, using registers to hold temporary results places less strain on the data cache, so it doesn’t need to flush data to memory or retrieve data from memory quite so often.</p>&#13;
<h4 class="h4" id="sec9_5_6"><strong><em>9.5.6 Pipeline Hazards</em></strong></h4>&#13;
<p class="noindent">There is another problem with using a pipeline: hazards. There are two types of hazards: control hazards and data hazards. We’ve actually discussed control hazards already, although we didn’t refer to them by name. <span epub:type="pagebreak" id="page_274"/>A control hazard occurs whenever the CPU branches to some new location in memory and consequently has to flush from the pipeline the instructions that are in various stages of execution. A data hazard occurs when two instructions attempt to access the same memory location out of sequence.</p>&#13;
<p class="indent">Let’s take a look at data hazards using the execution profile for the following instruction sequence:</p>&#13;
<p class="programs">mov( <span class="EmpItalic1">SomeVar</span>, ebx );<br/>&#13;
mov( [ebx], eax );</p>&#13;
<p class="indent">When these two instructions execute, the pipeline will look something like <a href="ch09.xhtml#ch09fig10">Figure 9-10</a>.</p>&#13;
<div class="image"><img alt="image" src="../images/09fig10.jpg"/></div>&#13;
<p class="figcap"><a id="ch09fig10"/><em>Figure 9-10: A data hazard</em></p>&#13;
<p class="indent">These two instructions attempt to fetch the 32-bit value whose address is held in the <span class="EmpItalic">SomeVar</span> pointer variable. <em>However, this sequence of instructions won’t work properly!</em> The second instruction accesses the value in EBX before the first instruction copies the address of memory location <span class="EmpItalic">SomeVar</span> into EBX (T5 and T6 in <a href="ch09.xhtml#ch09fig10">Figure 9-10</a>).</p>&#13;
<p class="indent">CISC processors, like the 80x86, handle hazards automatically. (Some RISC chips do not, and if you tried this sequence on certain RISC chips, you would store an incorrect value in EAX.) In order to handle the data hazard in this example, CISC processors stall the pipeline to synchronize the two instructions. The actual execution would look something like <a href="ch09.xhtml#ch09fig11">Figure 9-11</a>.</p>&#13;
<div class="image"><img alt="image" src="../images/09fig11.jpg"/></div>&#13;
<p class="figcap"><a id="ch09fig11"/><em>Figure 9-11: How a CISC CPU handles a data hazard</em></p>&#13;
<p class="indent">By delaying the second instruction by two clock cycles, the CPU guarantees that the load instruction will load EAX with the value at the proper address. Unfortunately, the <span class="literal">mov([ebx], eax);</span> instruction now executes in three clock cycles rather than one. However, requiring two extra clock cycles is better than producing incorrect results.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_275"/>Fortunately, you (or your compiler) can reduce the impact that hazards have on program execution speed within your software. A data hazard occurs when the source operand of one instruction was a destination operand of a previous instruction. There’s nothing wrong with loading EBX from <span class="EmpItalic">SomeVar</span> and then loading EAX from [EBX] (that is, the double-word memory location pointed at by EBX), <em>as long as they don’t occur one right after the other</em>. Suppose the code sequence had been:</p>&#13;
<p class="programs">mov( 2000, ecx );<br/>&#13;
mov( <span class="EmpItalic1">SomeVar</span>, ebx );<br/>&#13;
mov( [ebx], eax );</p>&#13;
<p class="indent">We could reduce the effect of the hazard in this code sequence by simply rearranging the instructions, as follows:</p>&#13;
<p class="programs">mov( <span class="EmpItalic1">SomeVar</span>, ebx );<br/>&#13;
mov( 2000, ecx );<br/>&#13;
mov( [ebx], eax );</p>&#13;
<p class="indent">Now the <span class="literal">mov([ebx], eax);</span> instruction requires only one additional clock cycle. By inserting yet another instruction between the <span class="literal">mov(</span><span class="EmpItalic">SomeVar</span><span class="literal">, ebx);</span> and the <span class="literal">mov([ebx], eax);</span> instructions, you can eliminate the effects of the hazard altogether (of course, the inserted instruction must not modify the values in the EAX and EBX registers).</p>&#13;
<p class="indent">On a pipelined processor, the order of instructions in a program may dramatically affect the program’s performance. If you’re writing assembly code, always look for possible hazards and eliminate them wherever possible by rearranging your instruction sequences. If you’re using a compiler, choose one that properly handles instruction ordering.</p>&#13;
<h4 class="h4" id="sec9_5_7"><strong><em>9.5.7 Superscalar Operation: Executing Instructions in Parallel</em></strong></h4>&#13;
<p class="noindent">With the pipelined architecture shown so far, we could achieve, at best, execution times of one CPI. Is it possible to execute instructions faster than this? At first you might think, “Of course not—we can do at most one operation per clock cycle, so there’s no way we can execute more than one instruction per clock cycle.” Keep in mind, however, that a single instruction is <em>not</em> a single operation. In the examples presented earlier, each instruction took between six and eight operations to complete. By adding seven or eight separate units to the CPU, we could effectively execute these eight operations in one clock cycle, yielding one CPI. If we add more hardware and execute, say, 16 operations at once, can we achieve 0.5 CPI? The answer is a qualified yes. A CPU that includes this additional hardware is a <em>superscalar</em> CPU, and it can execute more than one instruction during a single clock cycle. The 80x86 family began supporting superscalar execution with the introduction of the Pentium processor.</p>&#13;
<p class="indent">A superscalar CPU has several execution units (see <a href="ch09.xhtml#ch09fig12">Figure 9-12</a>). If it encounters in the prefetch queue two or more instructions that it can execute independently, it will do so.</p>&#13;
<div class="image"><img alt="image" src="../images/09fig12.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_276"/><a id="ch09fig12"/><em>Figure 9-12: A CPU that supports superscalar operation</em></p>&#13;
<p class="indent">There are a couple of advantages to going superscalar. Suppose you have the following instructions in the instruction stream:</p>&#13;
<p class="programs">mov( 1000, eax );<br/>&#13;
mov( 2000, ebx );</p>&#13;
<p class="indent">If there are no other problems or hazards in the surrounding code, and all 6 bytes for these two instructions are currently in the prefetch queue, there’s no reason why the CPU can’t fetch and execute both instructions in parallel. All it takes is extra silicon on the CPU chip to implement two execution units.</p>&#13;
<p class="indent">Besides speeding up independent instructions, a superscalar CPU can also speed up program sequences that have hazards. One limitation of normal CPUs is that once a hazard occurs, the offending instruction will completely stall the pipeline. Every instruction that follows the stalled instruction will also have to wait for the CPU to synchronize the execution of the offending instructions. With a superscalar CPU, however, instructions following the hazard may continue execution through the pipeline as long as they don’t have hazards of their own. This alleviates (though it does not eliminate) some of the need for careful instruction scheduling.</p>&#13;
<p class="indent">The way you write software for a superscalar CPU can dramatically affect its performance. First and foremost is that rule you’re probably sick of by now: <em>use short instructions</em>. The shorter your instructions, the more instructions the CPU can fetch in a single operation and, therefore, the more likely the CPU will execute faster than one CPI. Most superscalar CPUs do not completely duplicate the execution unit. There might be multiple ALUs, floating-point units, and so on, which means that certain <span epub:type="pagebreak" id="page_277"/>instruction sequences can execute very quickly, while others won’t. You have to study the exact composition of your CPU to decide which instruction sequences produce the best performance.</p>&#13;
<h4 class="h4" id="sec9_5_8"><strong><em>9.5.8 Out-of-Order Execution</em></strong></h4>&#13;
<p class="noindent">In a standard superscalar CPU, it is the programmer’s (or compiler’s) responsibility to arrange the instructions to avoid hazards and pipeline stalls. Fancier CPUs can actually remove some of this burden and improve performance by automatically rescheduling instructions while the program executes. To understand how this is possible, consider the following instruction sequence:</p>&#13;
<p class="programs">mov( <span class="EmpItalic1">SomeVar</span>, ebx );<br/>&#13;
mov( [ebx], eax );<br/>&#13;
mov( 2000, ecx );</p>&#13;
<p class="indent">There’s a data hazard between the first and second instructions. The second instruction must delay until the first instruction completes execution. This introduces a pipeline stall and increases the running time of the program. Typically, the stall affects every instruction that follows. However, the third instruction’s execution does not depend on the result from either of the first two instructions. Therefore, there’s no reason to stall the execution of the <span class="literal">mov(2000, ecx);</span> instruction. It can continue executing while the second instruction waits for the first to complete. This technique is called <em><a href="gloss01.xhtml#gloss01_187">out-of-order execution</a></em> because the CPU can execute instructions prior to the completion of instructions appearing previously in the code stream.</p>&#13;
<p class="indent">Keep in mind that the CPU can execute instructions out of sequence only if doing so produces exactly the same results as sequential execution. While there are many little technical issues that make this feature more difficult than it seems, with enough engineering effort you can implement it.</p>&#13;
<h4 class="h4" id="sec9_5_9"><strong><em>9.5.9 Register Renaming</em></strong></h4>&#13;
<p class="noindent">One problem that hampers the effectiveness of superscalar operation on the 80x86 CPU is its limited number of general-purpose registers. Suppose, for example, that the CPU had four different pipelines and, therefore, was capable of executing four instructions simultaneously. Presuming no conflicts existed among these instructions and they could all execute simultaneously, it would still be very difficult to actually achieve four instructions per clock cycle because most instructions operate on two register operands. For four instructions to execute concurrently, you’d need eight different registers: four destination registers and four source registers (none of the destination registers could double as source registers of other instructions). CPUs that have lots of registers can handle this task quite easily, but the limited register set of the 80x86 makes this difficult. Fortunately, there’s a trick to alleviate part of the problem: <em><a href="gloss01.xhtml#gloss01_215">register renaming</a></em>.</p>&#13;
<p class="indent">Register renaming is a sneaky way to give a CPU more registers than it actually has. Programmers won’t have direct access to these extra registers, <span epub:type="pagebreak" id="page_278"/>but the CPU can use them to prevent hazards in certain cases. For example, consider the following short instruction sequence:</p>&#13;
<p class="programs">mov( 0, eax );<br/>&#13;
mov( eax, i );<br/>&#13;
mov( 50, eax );<br/>&#13;
mov( eax, j );</p>&#13;
<p class="indent">There’s a data hazard between the first and second instructions as well as between the third and fourth instructions. Out-of-order execution in a superscalar CPU would normally allow the first and third instructions to execute concurrently, and then the second and fourth instructions could execute concurrently. However, there’s also a data hazard between the first and third instructions because they use the same register. The programmer could have easily solved this problem by using a different register (say, EBX) for the third and fourth instructions. However, let’s assume that the programmer was unable to do this because all the other registers were holding important values. Is this sequence doomed to executing in four cycles on a superscalar CPU that should require only two?</p>&#13;
<p class="indent">One advanced trick a CPU can employ is to create a bank of registers for each of the general-purpose registers on the CPU. That is, rather than having a single EAX register, the CPU could support an array of EAX registers; let’s call these registers EAX[0], EAX[1], EAX[2], and so on. Similarly, you could have an array of each of the other registers: EBX[0] through EBX[<em>n</em>], ECX[0] through ECX[<em>n</em>], and so on. The instruction set doesn’t permit the programmer to select one of these specific register array elements for a given instruction, but the CPU can automatically choose among them if doing so wouldn’t change the overall computation and could speed up program execution. This is known as <em>register renaming</em>. For example, consider the following sequence (with register array elements automatically chosen by the CPU):</p>&#13;
<p class="programs">mov( 0, eax[0] );<br/>&#13;
mov( eax[0], i );<br/>&#13;
mov( 50, eax[1] );<br/>&#13;
mov( eax[1], j );</p>&#13;
<p class="indent">Because EAX[0] and EAX[1] are different registers, the CPU can execute the first and third instructions concurrently. Likewise, the CPU can execute the second and fourth instructions concurrently.</p>&#13;
<p class="indent">Although this is a simple example, and different CPUs implement register renaming in many different ways, you can see how the CPU can use this technique to improve performance.</p>&#13;
<h4 class="h4" id="sec9_5_10"><strong><em>9.5.10 VLIW Architecture</em></strong></h4>&#13;
<p class="noindent">Superscalar operation attempts to schedule, in hardware, the execution of multiple instructions simultaneously. Another technique, which Intel is using in its IA-64 architecture, involves <em>very long instruction words (VLIW)</em>. <span epub:type="pagebreak" id="page_279"/>In a VLIW computer system, the CPU fetches a large block of bytes (41 bits in the case of the IA-64 Itanium CPU) and decodes and executes it all at once. This block of bytes usually contains two or more instructions (three in the case of the IA-64). VLIW computing requires the programmer or compiler to properly schedule the instructions in each block so that there are no hazards or other conflicts, but if all goes well, the CPU can execute three or more instructions per clock cycle.</p>&#13;
<h4 class="h4" id="sec9_5_11"><strong><em>9.5.11 Parallel Processing</em></strong></h4>&#13;
<p class="noindent">Most techniques for improving CPU performance via architectural advances involve the parallel execution of instructions. If programmers are aware of the underlying architecture, they can write code that runs faster, but these architectural advances often improve performance significantly even if programmers do not write special code to take advantage of them.</p>&#13;
<p class="indent">The only problem with ignoring the underlying architecture is that there’s only so much the hardware can do to parallelize a program that requires sequential execution for proper operation. To truly produce a parallel program, the programmer must specifically write parallel code, though, of course, this requires architectural support from the CPU. This section and the next touch on the types of support a CPU can provide.</p>&#13;
<p class="indent">Common CPUs use what’s known as the <em>single instruction, single data (SISD)</em> model. This means that the CPU executes one instruction at a time, and that instruction operates on a single piece of data.<sup><a href="footnotes.xhtml#fn9_5a" id="fn9_5">5</a></sup> Two common parallel models are the <em>single instruction, multiple data (SIMD)</em> and <em>multiple instruction, multiple data (MIMD)</em> models. Many modern CPUs, including the 80x86, include limited support for these parallel-execution models, providing a hybrid SISD/SIMD/MIMD architecture.</p>&#13;
<p class="indent">In the SIMD model, the CPU executes a single instruction stream, just like the pure SISD model, but operates on multiple pieces of data concurrently. For example, consider the 80x86 <span class="literal">add</span> instruction. This is a SISD instruction that operates on (that is, produces) a single piece of data. True, the instruction fetches values from two source operands, but the end result is that the <span class="literal">add</span> instruction stores a sum into only a single destination operand. An SIMD version of <span class="literal">add</span>, on the other hand, would compute several sums simultaneously. The 80x86 MMX and SIMD instruction extensions, the ARM’s Neon instructions, and the PowerPC’s AltiVec instructions, operate in exactly this fashion. With the <span class="literal">paddb</span> MMX instruction, for example, you can add up to eight separate pairs of values with the execution of a single instruction. Here’s an example of this instruction:</p>&#13;
<p class="programs">paddb( mm0, mm1 );</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_280"/>Although this instruction appears to have only two operands (like a typical SISD <span class="literal">add</span> instruction on the 80x86), the MMX registers (MM0 and MM1) actually hold eight independent byte values (the MMX registers are 64 bits wide but are treated as eight 8-bit values).</p>&#13;
<p class="indent">Unless you have an algorithm that can take advantage of SIMD instructions, they’re not that useful. Fortunately, high-speed 3D graphics and multimedia applications benefit greatly from these SIMD (and MMX) instructions, so their inclusion in the 80x86 CPU offers a huge performance boost for these important applications.</p>&#13;
<p class="indent">The MIMD model uses multiple instructions, operating on multiple pieces of data (usually with one instruction per data object, though one of these instructions could also operate on multiple data items). These multiple instructions execute independently of one another, so it’s very rare that a single program (or, more specifically, a single thread of execution) would use the MIMD model. However, if you have a multiprogramming environment with multiple programs attempting to execute concurrently, the MIMD model does allow each of those programs to execute its own code stream simultaneously. This type of parallel system is called a <em>multiprocessor system</em>.</p>&#13;
<h4 class="h4" id="sec9_5_12"><strong><em>9.5.12 Multiprocessing</em></strong></h4>&#13;
<p class="noindent">Pipelining, superscalar operation, out-of-order execution, and VLIW designs are all techniques that CPU designers use in order to execute several operations in parallel. These techniques support <em>fine-grained parallelism</em> and are useful for speeding up adjacent instructions in a computer system. If adding more functional units increases parallelism, what would happen if you added another CPU to the system? This approach, known as <em><a href="gloss01.xhtml#gloss01_169">multiprocessing</a></em>, can improve system performance, though not as uniformly as other techniques.</p>&#13;
<p class="indent">Multiprocessing doesn’t help a program’s performance unless that program is specifically written for use on a multiprocessor system. If you build a system with two CPUs, those CPUs cannot trade off executing alternate instructions within a single program. It is very expensive, time-wise, to switch the execution of a program’s instructions from one processor to another. Therefore, multiprocessor systems are effective only with an operating system that executes multiple processes or threads concurrently. To differentiate this type of parallelism from that afforded by pipelining and superscalar operation, we’ll call this <em>coarse-grained parallelism</em>.</p>&#13;
<p class="indent">Adding multiple processors to a system is not as simple as wiring two or more processors to the motherboard. To understand why this is so, consider two separate programs running on separate processors in a multiprocessor system. These two processors communicate with each other by writing to a block of shared physical memory. When CPU 1 writes to this block of memory it caches the data locally and might not actually write the data to physical memory for some time. If CPU 2 attempts to simultaneously read this block of shared memory, it winds up reading the old data out of main memory (or its local cache) rather than reading the updated data that CPU 1 wrote to its local cache. This is known as the <em>cache-coherency</em> problem. <span epub:type="pagebreak" id="page_281"/>In order for these two functions to operate properly, the two CPUs must notify each other whenever they make changes to shared objects, so the other CPU can update its own locally cached copy.</p>&#13;
<p class="indent">Multiprocessing is an area where the RISC CPUs have a big advantage over Intel’s CPUs. While Intel 80x86 systems reach a point of diminishing returns at around 32 processors, Sun SPARC and other RISC processors easily support 64-CPU systems (with more arriving, it seems, every day). This is why large databases and large web server systems tend to use expensive Unix-based RISC systems rather than 80x86 systems.</p>&#13;
<p class="indent">Newer versions of the Intel i-series and Xeon processors support a hybrid form of multiprocessing known as <em><a href="gloss01.xhtml#gloss01_113">hyperthreading</a></em>. The idea behind hyperthreading is deceptively simple—in a typical superscalar processor it’s rare for an instruction sequence to utilize all the CPU’s functional units on each clock cycle. Rather than allow those functional units to go unused, the CPU can run two separate threads of execution concurrently and keep all the functional units occupied. This allows a single CPU to effectively do the work of 1.5 CPUs in a typical multiprocessor system.</p>&#13;
<h3 class="h3" id="sec9_6"><strong>9.6 For More Information</strong></h3>&#13;
<p class="ref">Hennessy, John L., and David A. Patterson. <em>Computer Architecture: A Quantitative Approach</em>. 5th ed. Waltham, MA: Elsevier, 2012.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>One subject missing from this chapter is the design of the CPU’s actual instruction set. That is the subject of the next chapter.</em><span epub:type="pagebreak" id="page_282"/></p>&#13;
</div>&#13;
</body></html>