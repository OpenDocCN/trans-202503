<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="601" id="Page_601"/>21</span><br/>
<span class="ChapterTitle">Reinforcement Learning</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">There are many ways to train a machine learning system. When we have a set of labeled samples, we can use supervised learning to teach the computer to predict the right label for each sample. When we can’t offer any feedback, we can use unsupervised learning and let the computer do its best. But sometimes we’re somewhere in between these two extremes. Perhaps we know <em>something </em>about what we want the system to learn, but it’s not as clear-cut as having labels for samples. Perhaps all we know is how to tell a better solution from a worse one.</p>
<p>For example, we might be trying to teach a new kind of humanoid robot how to walk on two legs. We don’t know exactly how it ought to balance and how it should move, but we know we want it to be upright and not falling over. If the robot tries to slither on its belly, or hop on one leg, we <span epub:type="pagebreak" title="602" id="Page_602"/>can tell it that’s not the right way to proceed. If it starts with both legs on the ground and then uses them to make some forward progress, we can tell it that it’s on the right track and keep exploring those kinds of behaviors. This strategy of rewarding what we recognize as progress is called <em>reinforcement learning</em> <em>(RL)</em> (Sutton and Baro 2018). The term describes a general approach to learning, rather than a specific algorithm.</p>
<p>In this chapter we cover some of the basic ideas of this vast field. The key idea is that RL breaks up the simulated world into one entity that takes action and the rest of the world that responds to that action. To make this concrete, we will use RL to learn how to play a simple one-player game, and then dig into the details of the technique. We’ll begin with a simple algorithm that has some flaws and upgrade it into something that learns efficiently and well. </p>
<h2 id="h1-500723c21-0001">Basic Ideas</h2>
<p class="BodyFirst">Suppose that you’re playing a game of checkers with a friend, and it’s your turn. At this moment, you can move one of your pieces, and your friend has to wait. In reinforcement learning, we say that you’re the <em>actor </em>or <em>agent</em> since you have the choice of action. Everything else in the universe—the board, the pieces, the rules, and even your friend—are lumped together as the <em>environment</em>. These roles aren’t fixed. When it’s your friend’s turn to move, then they’re the agent, and everything else—the board, the pieces, the rules, and even you—are now part of the environment.</p>
<p>When the actor or agent chooses an action, they change the environment. In our checkers game, you’re the agent, so you move one of your pieces, and maybe you remove some of your opponent’s pieces. The result is that the world has changed. In reinforcement learning, after an agent’s action, they’re given a piece of <em>feedback,</em> also called a<em> reward,</em> that tells them how “good” their action was, using whatever criteria we like. The feedback or reward is usually just a single number. Since we’re creating this world, the feedback can mean anything we want. In a game of checkers, for instance, a move that wins the game would be assigned a huge positive reward, whereas a move that loses the game would be assigned a huge negative reward. In between, the more a move seems to lead to victory, the bigger the reward.</p>
<p>Through trial and error, an agent can discover which actions are better than others in different situations, and can thus gradually make better and better choices as it gains experience. This approach works particularly well for situations in which we don’t already know the best thing to do at all times. For example, consider the problem of scheduling the elevators in a tall and busy office building. Even just figuring out where elevator cars ought to go when they’re empty is hard. Should the cars always return to the ground floor? Should some wait at the top? Should they wait at floors evenly distributed between the top and bottom? Maybe these policies should change over time so in the early morning and just after lunch, the cars should be on the ground floor, waiting for people arriving off the <span epub:type="pagebreak" title="603" id="Page_603"/>street, but in the late afternoon, they should be higher up, ready to help people descend and head home. There’s no obvious answer to how we should schedule a particular building. It all depends on the average traffic pattern for that building (and that pattern itself might depend on the time, season, or weather).</p>
<p>This is an ideal problem for reinforcement learning. The elevator’s control system can try out a policy for directing the empty cars, and then use feedback from the environment (such as the number of people waiting for elevators, their average waiting time, the density of the elevator cars, etc.) to help it adjust that policy to perform as well as it can on the metrics we’re measuring.</p>
<p>Reinforcement learning can help us with problems for which we don’t know the best result. We may not have a measurement as clear as the winning conditions of a game, but only better and worse outcomes. This is a key point: we may not be able to find any objective, consistent “right” or “best” answer. Instead, we’re trying to find the best answer we can with the information we have according to whatever metrics we’re measuring by. In some situations, we may not even have any idea of how well we’re doing along the way. For example, in a complex game, we might not be able to tell if we’re ahead or behind until the surprising moment when we win or lose. In those cases, we can only evaluate our actions in light of how things finally work out when the task is done.</p>
<p>Reinforcement learning offers a nice way to model uncertainty. In simple rule-based games, we can, in principle, evaluate any board and select the best move, assuming that the other player always does the same. But in the real world, other players make moves that surprise us. And when we deal with the real world, where on some days more people need an elevator than on other days, we need to have strategies that can continue to perform well in the face of surprises. Reinforcement learning can be a good choice for these kinds of situations.</p>
<p>Let’s look at reinforcement learning in more detail with a specific example.</p>
<h2 id="h1-500723c21-0002">Learning a New Game</h2>
<p class="BodyFirst">Let’s see the steps involved in using reinforcement learning to teach a program how to play <em>tic-tac-toe</em><b> </b>(also called <em>naughts and crosses</em>, or <em>Xs and Os</em>). To play, the players alternate placing an X or O in the cells of a three by three grid, and the first to get three of their symbols in a row (in any direction) is the winner. In the examples of <a href="#figure21-1" id="figureanchor21-1">Figure 21-1</a>, we play O and our computer learner plays X.</p>
<figure>
<img src="Images/F21001.png" alt="F21001" width="694" height="89"/>
<figcaption><p><a id="figure21-1">Figure 21-1</a>: A game of tic-tac-toe, reading left to right. X moved first.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="604" id="Page_604"/>In this scenario, the program we’re training is the agent. It’s playing against the environment, which would probably be simulated by another program that knows all about the game and how to play. The agent doesn’t know the rules of the game, how to win or lose, or even how to make moves. But our agent won’t be completely in the dark. At the start of each of the agent’s turns, the environment gives it two important pieces of information: the current board, and the list of available moves. This is shown in steps 1 and 2 of <a href="#figure21-2" id="figureanchor21-2">Figure 21-2</a>.</p>
<figure>
<img src="Images/F21002.png" alt="F21002" width="689" height="433"/>
<figcaption><p><a id="figure21-2">Figure 21-2</a>: The basic information exchange loop between a player and the environment in a game of tic-tac-toe </p></figcaption>
</figure>
<p>In step 3, the agent picks a move, based on any methodology it likes. For example, it can pick at random, or consult an online resource, or use its own memory of previous games. Part of the challenge in reinforcement learning is designing an agent that can do a good job with the resources we have available for it.</p>
<p>Once the agent picks a move, it communicates that to the environment in step 4. The environment then follows step 5, starting with actually making the move by placing an X in the chosen cell. The environment then checks to see if the agent has won. If so, it sets the reward to something big. Otherwise, it computes a reward based on how good the move seems to be for the agent. Now the environment, simulating the other player, makes its own move. If it’s won, then it changes the reward to something very low. If the game ended as a result of the environment’s or agent’s move, we call the reward an <em>ultimate reward </em>or <em>final reward.</em> In step 6, the environment sends the reward (sometimes this is called the <em>reward signal)</em> to the agent, so the agent can learn how good its selected move was. If nobody’s won, we return to the start of the loop and the agent gets to take another turn.</p>
<p><span epub:type="pagebreak" title="605" id="Page_605"/>In some cases, we don’t give the agent the list of available moves. This might be because there are too many to list, or they have too many variations. Then we might give the agent some guidelines, or even no guidance at all. </p>
<p>Following this procedure, the agent will probably make useless or terrible actions when it starts learning, but using the techniques below we’d hope that the agent will gradually learn to find good actions. For our discussions, we’ll keep things simple and assume that the agent is given a list of possible actions to choose from.</p>
<h2 id="h1-500723c21-0003">The Structure of Reinforcement Learning</h2>
<p class="BodyFirst">Let’s reorganize and generalize our tic-tac-toe example into a more abstract description. This will let us embrace situations that go beyond turn-taking games. We’ll organize things into three steps, which we discuss in turn.</p>
<p>Before we begin, a bit of terminology. At the start of training, we place the environment into an <em>initial state</em>. In a board game, this is the setup for the start of a new game. In our elevator example, this might be placing all elevator cars on the ground floor. A full training cycle (such as a game from start to finish) is called an <em>episode</em>. We generally expect to teach the agent over a great many episodes.</p>
<h3 id="h2-500723c21-0001">Step 1: The Agent Selects an Action</h3>
<p class="BodyFirst">We begin with <a href="#figure21-3" id="figureanchor21-3">Figure 21-3</a>.</p>
<figure>
<img src="Images/F21003.png" alt="F21003" width="682" height="351"/>
<figcaption><p><a id="figure21-3">Figure 21-3</a>: The environment provides the agent with the current world state and a choice of actions. The agent chooses an action and communicates that to the environment.</p></figcaption>
</figure>
<p>Recall that the environment is the world in which all of our agent’s actions take place. The environment is completely described by a set of numbers that are collectively called the <em>environmental state</em>, the <em>state variables</em>, or simply the <em>state</em>. This might be a short list, or a very long one, depending on the complexity of the environment. In the case of a board game, the <span epub:type="pagebreak" title="606" id="Page_606"/>state is commonly made up of the position of all the markers on the board, plus any game assets (such as game money, power-ups, hidden cards, etc.) held by each player.</p>
<p>The agent then chooses one of the available actions. We often anthropomorphize the agent and talk about how the agent “wants” to achieve some result, such as winning a game or scheduling the elevators so nobody has to wait too long. In basic reinforcement learning, the agent is idle until the environment tells it that it’s time to take an action. The agent then chooses an action from the list of actions by using an algorithm called its <em>policy</em>, along with whatever <em>private information</em> the agent may have access to (including what it’s learned from previous episodes).</p>
<p>We usually think of the agent’s private information as a database. It might contain descriptions of possible strategies or some kind of history of the actions that were taken in previous states and the rewards that were returned. The policy, by contrast, is an algorithm that is usually controlled by a set of parameters. The parameters usually change over time as the agent plays and searches for improved action-choosing policies. </p>
<p>We usually don’t think of the agent implementing its action. Instead, the chosen action is reported to the environment, and the environment takes care of performing the action. This is because the environment is in charge of the state. Returning to our elevator example, if the agent directs a car to move from the 13th floor to the 8th floor, the agent doesn’t update the state to place the car at the 8th floor. Something might go wrong along the way, such as a mechanical failure causing the car to get stuck. The agent simply tells the environment what it wants to do, and the environment tries to make that happen, maintaining the state so it’s always a correct picture of the current situation. In our tic-tac-toe game, the state contains the current distribution of X and O markers on the board.</p>
<h3 id="h2-500723c21-0002">Step 2: The Environment Responds</h3>
<p class="BodyFirst"><a href="#figure21-4" id="figureanchor21-4">Figure 21-4</a> shows step 2 of our reinforcement learning overview.</p>
<figure>
<img src="Images/F21004.png" alt="F21004" width="738" height="267"/>
<figcaption><p><a id="figure21-4">Figure 21-4</a>: Step 2 of our reinforcement learning process. This step starts with the computation of the new state (far right).</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="607" id="Page_607"/>In this step, the environment processes the agent’s action to produce a new state and processes the information that follows from this change. The environment saves its new state in the state variables, so that they reflect the new environment when the agent next gets to choose an action. The environment also uses its new state to determine what actions will be available to the agent on its next move. The previous state and the available actions are entirely replaced by their new versions. Lastly, the environment provides a reward signal that tells the agent how “good” its last chosen action was. The meaning of “good” is completely dependent on what this whole system is doing. In a game, good actions are moves that lead to stronger positions or even victory. In an elevator scheduling system, good actions might be those that minimize wait times. </p>
<h3 id="h2-500723c21-0003">Step 3: The Agent Updates Itself</h3>
<p class="BodyFirst"><a href="#figure21-5" id="figureanchor21-5">Figure 21-5</a> shows step 3 of our reinforcement learning overview.  </p>
<figure>
<img src="Images/F21005.png" alt="F21005" width="450" height="299"/>
<figcaption><p><a id="figure21-5">Figure 21-5</a>: Step 3 of our reinforcement learning process, where the agent updates itself in response to the reward</p></figcaption>
</figure>
<p>In this step, the agent uses the reward value to update its private information and policy parameters so that the next time this situation comes around, it’s able to build on what it has learned from this choice. After step 3, the agent might wait quietly until the environment tells it that it’s time to take action again. Alternatively, it can immediately start planning for its next move. This is particularly useful for some real-time systems where the reward precedes the full calculation of the new state.</p>
<p>Rather than simply stashing each reward into its private information, an agent usually processes that reward in some way to extract as much value from it as possible. This might even involve changing the values for other actions. For example, if we have just won a game and received an ultimate reward, we probably want to add a little bit of that reward to each of the moves that led us to victory.</p>
<p>The goal of reinforcement learning is to discover ways to help the agent in this scenario learn from the feedback to choose actions that bring it the best possible rewards. Whether it’s winning a game, scheduling elevators, <span epub:type="pagebreak" title="608" id="Page_608"/>designing vaccines, or moving a robot, we want to create an agent that can <em>learn from experience</em> to become as good as possible at manipulating its environment to bring about positive rewards.</p>
<h3 id="h2-500723c21-0004">Back to the Big Picture</h3>
<p class="BodyFirst">Now that we’ve seen the overall approach, let’s look at some big-picture issues. When the agent updates its policy, it might have access to all the parameters of the state, or only some of them. If an agent gets to see the entire state, we say it has <em>full observability</em>, otherwise it has only <em>limited observability</em> (or <em>partial observability</em>). One reason we may give an agent only limited observability is that some parameters may be very expensive to compute, and we’re not sure if they’re relevant or not. So, we block the agent’s access to those parameters to see if doing so hurts the agent’s performance. If leaving them out does no harm, we can leave them off entirely from then on and save effort. Or we can only compute them and make them visible when they seem necessary. Another example of partial observability is if we’re teaching the system to play a card game such as poker. We don’t reveal to the system we’re teaching what cards are in its opponent’s hand.</p>
<p>As soon as we start thinking about using feedback to train agents in the way we’ve been discussing, we find ourselves facing two interesting problems. First, when we receive an ultimate reward (perhaps for winning or losing a game), we want to share some of that reward with every move we made along the way. Suppose we’re playing a game and make a winning move. That final move gets great feedback, but the intermediate steps were essential, and we should remember that they led to victory. That way if we see those intermediate boards again, we are more likely to select the move that leads to winning. Finding a way to share the ultimate reward this way is called the <em>credit assignment problem</em>. By the same token, if we lose, we’d want to let the moves that led us there take some of the blame, so we are less likely to select them again.</p>
<p>Second, suppose at some point the agent sees a situation (such as a game board) that it has seen before, and that at some earlier point, it tried a move that got a reasonably good score. But it hasn’t yet tried some of the other possible moves. Should it select the safe move with known returns, or risk something new that might either lead to failure or to even greater success? Somehow we need to decide, each time we pick an action, whether we want to take a risk with a new action and <em>explore</em> where it might lead, or play it safe with an action we’ve tried before and <em>exploit</em> what we’ve already learned. This is called the <em>explore or exploit dilemma</em>. Part of the task of designing a reinforcement learning system is thinking about how we want to balance these issues of the known and unknown, or guarantee and risk. </p>
<h3 id="h2-500723c21-0005">Understanding Rewards</h3>
<p class="BodyFirst">For the agent to perform as well as possible, it should be guided by a policy that leads the agent to pick the actions that deliver the highest rewards. <span epub:type="pagebreak" title="609" id="Page_609"/>Understanding the nature of rewards, and how to use them wisely, is time well spent. Let’s dig in.</p>
<p>We can distinguish between two categories of rewards: <em>immediate</em> and <em>long term</em>. Immediate rewards are the ones we’ve focused on so far. The environment delivers these back to the agent right after executing an action, as we saw in <a href="#figure21-2">Figure 21-2</a>. Long-term rewards are more general and refer to our overall objective, like winning a game.</p>
<p>We’d like to understand each immediate reward in the context of all the other rewards we get during a given game, or episode. There are lots of ways to interpret rewards and what they should mean to us. Let’s look at one popular approach called the <em>discounted future reward (DFR)</em>. This is a way to address the credit assignment problem, or make sure that all the actions that led us to success share in that ultimate victory.</p>
<p>To see how DFR works, we need to unwind the reward process a little bit. Let’s imagine that we’re an agent playing a game. When the game is done, we can line up the rewards we’ve collected for that game in a list, one after the other in the order they were received, along with the moves that earned those rewards. Adding up all the rewards gets us the <em>total reward </em>for that game, as in <a href="#figure21-6" id="figureanchor21-6">Figure 21-6</a>.</p>
<figure>
<img src="Images/F21006.png" alt="F21006" width="694" height="324"/>
<figcaption><p><a id="figure21-6">Figure 21-6</a>: The total reward associated with any episode is the sum of all the rewards that arrive from the first to the last move of the episode. </p></figcaption>
</figure>
<p>We can add up any piece of this list, such as the first five entries, or the last eight. Let’s start at move 5 and add up all the rewards from there up to the game’s end, as in <a href="#figure21-7" id="figureanchor21-7">Figure 21-7</a>.</p>
<p><a href="#figure21-7">Figure 21-7</a> shows us the <em>total future reward</em>, or <em>TFR</em>, associated with the fifth move of the game. It’s that part of the total reward that comes from the fifth move and all the moves that followed it. </p>
<p>The very first move of a game is special, because its total future reward is the same as the game’s total reward. Since our rewards so far are always zero or positive, each subsequent move’s TFR is equal to or less than the TFR of the move before it. </p>
<span epub:type="pagebreak" title="610" id="Page_610"/><figure>
<img src="Images/F21007.png" alt="F21007" width="780" height="283"/>
<figcaption><p><a id="figure21-7">Figure 21-7</a>: The total future reward for any move is the sum of the reward for that move and all other moves to the end of the episode. </p></figcaption>
</figure>
<p>The total future reward is a good description of how well a given move contributed to a game we just finished, but it’s not as good at predicting how useful that move might be in future games, even if they start with the exact same sequence of moves. This is because real environments are unpredictable. If we’re playing a multiplayer game, we can’t be sure that the other player (or players) will act the same way in the next game as they did in the previous game. If they make a different move, then that can change the trajectory of the game, and thus it can also change the rewards we earn. It can even change whether we win or lose. Even if we’re playing solitaire, we might be playing with a shuffled deck of cards, or a computer game with pseudorandom numbers, so we can’t be sure what’s going to come our way in the future, even if we play the exact same way we did in the past.</p>
<p>Immediate rewards are more reliable. We can imagine two types of immediate rewards. The first tells us the quality of the move we just made, <em>before</em> the environment responds. For example, in our game of tic-tac-toe, if the agent places an X in some cell, they can receive an <em>instant reward</em> describing how well the player is set up to win later on before the environment makes its move in return. This kind of reward is completely predictable. If we face the identical environment again later and make the same move, we get the same reward.</p>
<p>The second kind of reward tells us the quality of the move we just made <em>after</em> the environment responds, so the reward can be influenced by the environment’s move. This type of reward, which we might call the <em>resulting reward</em>, isn’t as predictable as the instant reward because the environment might respond in a different way each time we make the move. </p>
<p>Let’s compare the two. Suppose we’re training an agent-powered robot how to use a remote control to turn on a device. It might pick up the remote control, press the power button, and put the remote back down, doing the same thing 100 times in a row, earning high rewards. But all this time, the battery is draining, so the 101st time the agent repeats the process, the device won’t turn on. If the agent receives the instant reward for pressing the button, that is, the one that is computed and returned <em>before</em> the environment responds, the agent gets a large reward, because it <span epub:type="pagebreak" title="611" id="Page_611"/>did the right thing. On the other hand, the resulting reward, which is computed and returned <em>after</em> the environment responds, will be low or even 0, because the device failed to turn on.</p>
<p>From here on, we’ll be using the resulting reward when we refer to the immediate reward.</p>
<p>When something works 100 times in a row but then fails the 101st time, that’s a <em>surprise</em>.</p>
<p>It’s important to deal gracefully with surprises, because most environments are unpredictable. Generally speaking, each action we take is intended to bring about a result. So waiting to see that result, even if we can’t be certain about what will happen, is a big part of understanding if our action represented a good choice.</p>
<p>We say that real environments, with their unpredictable elements, are <em>stochastic</em>. By contrast, a perfectly predictable environment (such as a game based purely on logic) is <em>deterministic</em>. The amount of unpredictability (or <em>stochasticity</em>) can vary in amount. If the unpredictability is low (that is, the environment is largely deterministic), then we may feel pretty confident about saying that the rewards we just received are likely to be repeated, or very nearly so, in future games. With very high unpredictability (that is, in a largely stochastic environment), we have to assume that if we repeat the same actions, any predictions we make about future rewards should be considered little more than estimates.</p>
<p>We quantify our estimate of the stochasticity, or uncertainty, of the environment with a <em>discount factor</em>. This is a number between 0 and 1, usually written with the lowercase Greek letter <em>γ </em>(gamma). The value of <em>γ</em> that we select represents our confidence in the repeatability of the environment. If we think the environment is close to being deterministic and that we’ll get about the same reward for a given action every time, we set <em>γ</em> to a value near 1. If we think the environment is chaotic and unpredictable, we set <em>γ</em> to a value nearer to 0.</p>
<p>We need to somehow accommodate unexpected surprises into our learned rewards in a principled way. One way to do that is to create a modified version of the total future reward that accounts for how confident we are that the game will proceed in just the same way again. We generally attach high values of this modified TFR to moves we feel confident about, and lower values to the others.</p>
<p>We can use the discount factor to create a version of the total future reward called the discounted future reward (DFR). Rather than adding up all the rewards that come after an action, as we do for the TFR, we start with the immediate reward, and then we reduce the values of the subsequent rewards by multiplying them by <em>γ</em> one time for each step they are in our future. The reward for one step in the future is multiplied by <em>γ</em> once, the reward after that is multiplied by <em>γ</em> twice, and so on. This accounts for the fact that we consider future rewards increasingly less reliable. The technique is illustrated graphically in <a href="#figure21-8" id="figureanchor21-8">Figure 21-8</a>.</p>
<p>Notice that in <a href="#figure21-8">Figure 21-8</a> each successive value gets multiplied by <em>γ</em> one more time than the one before. These increased multiplications can have a significant effect on the amount by which each reward contributes to the sum.</p>
<span epub:type="pagebreak" title="612" id="Page_612"/><figure>
<img src="Images/F21008.png" alt="F21008" width="680" height="476"/>
<figcaption><p><a id="figure21-8">Figure 21-8</a>: The DFR is found by adding together the immediate reward, the next reward after multiplying it by gamma, the reward after that multiplied by gamma twice, and so on.</p></figcaption>
</figure>
<p>Let’s see this in action. We can consider the reward and the discounted future reward we’d get from our opening move in a game, using several values for <em>γ</em>. <a href="#figure21-9" id="figureanchor21-9">Figure 21-9</a> shows a set of immediate rewards for an imaginary game with 10 moves.</p>
<figure>
<img src="Images/F21009.png" alt="F21009" width="658" height="459"/>
<figcaption><p><a id="figure21-9">Figure 21-9</a>: Immediate rewards for a game with 10 moves. The game ended without a clear winner.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="613" id="Page_613"/>Applying different future discounts to these rewards following <a href="#figure21-8">Figure 21-8</a> gives us the curves of <a href="#figure21-10" id="figureanchor21-10">Figure 21-10</a>. Notice how quickly the rewards drop to 0 as the discount factor <em>γ</em> decreases. This means that we’re less sure of our predictions of the future.</p>
<figure>
<img src="Images/F21010.png" alt="F21010" width="675" height="505"/>
<figcaption><p><a id="figure21-10">Figure 21-10</a>: The rewards of <a href="#figure21-9">Figure 21-9</a> as discounted by different values of <em>γ</em> </p></figcaption>
</figure>
<p>If we add up the values of each curve in <a href="#figure21-10">Figure 21-10</a>, we get the discounted future reward for the first move for different values of <em>γ</em>. These DFRs are shown in <a href="#figure21-11" id="figureanchor21-11">Figure 21-11</a>. Notice that as we think of the future as being increasingly unpredictable (that is, <em>γ</em> gets smaller), the DFR also becomes smaller because we’re less confident of getting those future rewards.</p>
<p>When <em>γ</em> has a value near 1, the future rewards aren’t diminished much, so the DFR is close to the TFR. In other words, we’re saying that the total rewards we got for making this move are likely to be similar to the total rewards we’ll get if we play this move again.</p>
<p>But when <em>γ</em> has a value near 0, then the future rewards are scaled way down to the point where they practically don’t matter, and we’re left with just the immediate reward. In other words, we’re saying that we have little confidence that the game will continue again as it did this time, so the only reward we can be sure of is the immediate reward.</p>
<p>In many reinforcement learning scenarios, we often pick a value of <em>γ</em> around 0.8 or 0.9 to get started, and then adjust the value as we discover more about how stochastic our system is and how well our agent is learning.</p>
<span epub:type="pagebreak" title="614" id="Page_614"/><figure>
<img src="Images/F21011.png" alt="F21011" width="675" height="586"/>
<figcaption><p><a id="figure21-11">Figure 21-11</a>: The DFR from <a href="#figure21-10">Figure 21-10</a> for different values of <em>γ</em></p></figcaption>
</figure>
<p>So far, we’ve been discussing principles and ideas, but we still don’t have a specific algorithm for the agent to use when it picks an action. To develop such an algorithm, let’s start with a description of an environment. </p>
<h2 id="h1-500723c21-0004">Flippers</h2>
<p class="BodyFirst">In the following sections, we’re going to look at actual algorithms for learning a game. To keep our focus on the algorithms and not the game, let’s pare down tic-tac-toe into a new single-player game that we’ll call <em>Flippers</em>.</p>
<p>We play Flippers on a square grid of size three by three. Each cell holds a little tile that pivots around a bar, as in <a href="#figure21-12" id="figureanchor21-12">Figure 21-12</a>.</p>
<p>One side of each tile is blank, while the other side holds a dot. On each move, the player pushes one tile to flip it over. If it was showing a dot, the dot disappears, and vice versa.</p>
<p>The game begins with the tiles in a random state. Victory comes from having exactly three blue dots showing, arranged in either a vertical column or horizontal row, with all the other tiles showing blanks. This may not be the most intellectually demanding game ever invented, but it’ll help make our algorithms clear. </p>
<span epub:type="pagebreak" title="615" id="Page_615"/><figure>
<img src="Images/F21012.png" alt="F21012" width="448" height="369"/>
<figcaption><p><a id="figure21-12">Figure 21-12</a>: The board for the game of Flippers. Each tile is blank on one side and has a dot on the other. A move in the game consists of flipping (or rotating) one tile.</p></figcaption>
</figure>
<p>Starting from a random board, we want to get to victory in the smallest number of flips. Since diagonal lines don’t count as victory, there are six different boards that satisfy our conditions for winning: three with horizontal rows and three with vertical columns. </p>
<p><a href="#figure21-13" id="figureanchor21-13">Figure 21-13</a> shows an example game, along with our notation for showing the moves. We read the game left to right. Each board but the last shows the starting configuration for that move, with one cell highlighted in red. That’s the cell that is going to be flipped over.</p>
<figure>
<img src="Images/F21013.png" alt="F21013" width="688" height="170"/>
<figcaption><p><a id="figure21-13">Figure 21-13</a>: Playing a game of Flippers. (a) The initial board, showing three dots. The red square shows the tile we intend to flip for this move. (b) The resulting board is like part (a), but the tile in the upper-right has gone from blank to dot. Our move for this board is to flip the center tile. (c) through (e) show subsequent steps in game play. Board (e) is a winning board.</p></figcaption>
</figure>
<p>Now that we have a game to play, we can look at how to use reinforcement learning to win it.</p>
<h2 id="h1-500723c21-0005"><span epub:type="pagebreak" title="616" id="Page_616"/>L-Learning</h2>
<p class="BodyFirst">Let’s build a complete system for learning how to play Flippers. Although we will make this algorithm much better in the next section, this starting version is going to perform so badly that we call it <em>L-learning</em>, where L stands for “lousy.” Note that L-learning is a stepping-stone that we invented to help us get to something better and not a practical algorithm that appears in the literature. It is, after all, lousy.</p>
<h3 id="h2-500723c21-0006">The Basics</h3>
<p class="BodyFirst">To make things easy, we’re going to use a very simple reward system. Every move we make in Flippers gets an immediate reward of 0, except for the final move that wins the game. Because Flippers is such an easy game, every game can be won. To prove this, we can take any starting board and flip over all the tiles that are showing a dot, so that there are no dots showing. Then we can flip over three tiles in any row or column, and we’ve won. Thus, no game should take more than 12 moves at most.</p>
<p>Our goal is not simply to win, however, but to win in the smallest number of moves. The final, winning move gets a reward that depends on the length of the game. If it takes one move to win the game, the reward is 1. If it takes more moves, this final reward drops off quickly with the number of moves that were taken. The specific formula for this curve is less important than the fact that it drops off fast and is always getting smaller. <a href="#figure21-14" id="figureanchor21-14">Figure 21-14</a> shows a graph of our final reward versus game length curve.</p>
<figure>
<img src="Images/F21014.png" alt="F21014" width="675" height="494"/>
<figcaption><p><a id="figure21-14">Figure 21-14</a>: The reward for victory in Flippers starts at 1 for an immediate win but drops quickly with the number of moves required to win the game.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="617" id="Page_617"/>At the heart of our system is a grid of numbers that we call the <em>L-table</em>. Each row of the L-table represents one state of the board. Each column represents one of the nine actions we can make in response to that board. The content of each cell in the table is a single number, which we call an <em>L-value</em>. <a href="#figure21-15" id="figureanchor21-15">Figure 21-15</a> shows this schematically.</p>
<figure>
<img src="Images/F21015.png" alt="F21015" width="844" height="487"/>
<figcaption><p><a id="figure21-15">Figure 21-15</a>: The L-table contains one row for each of the 512 possible patterns of blanks and dots on a Flipper board and one column for each of the 9 possible actions. </p></figcaption>
</figure>
<p>This table is big, but not too big. The board has only 512 possible configurations, so we need 512 rows. Each row has 9 columns, for a total of 512 × 9 = 4,608 cells. We’re going to use the L-table to help us choose the highest-rewarding action in response to each board. To make that happen, we’re going to fill each cell in the table with a score: a number, based on experience, that tells us how good the corresponding move is. </p>
<p>We save values into the L-table as we learn how good moves are, and we read those values back to guide our choice of moves as we play. Before we start assigning values to the L-table, we initialize it with a 0 in every cell. As we play a game, we will keep a record of all the moves we’ve played. When the game is over, we will look back through our moves for the whole game, and determine a value for each. Then we will combine this value with the number already in that move’s cell to produce a new value for that move (we’ll get to the mechanics for this in a moment). The way we combine the old and new values is called the <em>update rule</em>.</p>
<p>As we play a game (either during the learning phase, or later for real), we pick an action by looking at the corresponding row for the board at the start of that move. We use a policy<b> </b>to tell us which of the actions in that row we want to select.</p>
<p><span epub:type="pagebreak" title="618" id="Page_618"/>Let’s make these steps concrete. First, after each game (or episode), we need to determine the score we want to assign to each action we played. Let’s use the total future reward, or TFR, that we discussed earlier. Recall that the TFR comes from lining up all our actions and their rewards, and then summing up all the rewards that came after that action.</p>
<p>While we play the game, every move along the way gets an immediate reward of 0, but the final move gets a positive reward based on the game’s length: the shorter the game, the larger the reward. This means that the TFR for each action we took along the way is the same as this final reward. </p>
<p>Second, let’s pick a simple update rule that says that after each game, the TFR we compute for each cell merely replaces whatever was in there before. In other words, the TFR for each action in this game becomes the new value in the cell at the intersection of the row of the board we were looking at when we took that action, and the column of the action we chose to take.</p>
<p>This simple update rule is good for getting familiar with how the L-learning system works. But because it doesn’t combine our new experience with what we’ve learned before, this rule is a big reason that this algorithm isn’t going to perform well.</p>
<p>Now that we have values in our L-table, we need a policy that tells us which move to play in response to a given configuration of the board. Let’s say that we choose the action corresponding to the largest L-value in the row. If multiple cells have the same maximum value, we pick one at random. <a href="#figure21-16" id="figureanchor21-16">Figure 21-16</a> shows this graphically.</p>
<figure>
<img src="Images/F21016.png" alt="F21016" width="844" height="289"/>
<figcaption><p><a id="figure21-16">Figure 21-16</a>: The policy step involves choosing one action in response to a board. </p></figcaption>
</figure>
<p>In <a href="#figure21-16">Figure 21-16</a>, we see a row of the L-table that lists the possible moves we can take in response to the board state shown at the far left. Each column holds the most recently computed TFR that resulted when that action was taken at that board state. Note that two columns hold 0, because we haven’t tried those actions yet. In L-learning, we choose the largest value. Here that means we flip the center-right tile.</p>
<h3 id="h2-500723c21-0007"><span epub:type="pagebreak" title="619" id="Page_619"/>The L-Learning Algorithm</h3>
<p class="BodyFirst">We now have all the steps required for L-learning. Let’s combine them into a functional, but lousy, reinforcement learning algorithm. We start our agent with a private memory that contains a 512 by 9 table filled with zeros, representing the L-table.</p>
<p>In the first move of the first game, the agent sees a board. It finds the row for that board in its L-table and scans the nine entries there for the move with the largest score. They’re all zero, so it picks one at random. This will happen frequently for quite a while because the agent will see lots of boards it has never seen before. The tile flips, the agent considers the new board, picks a new action, and so on, until it finally wins the game (the computer will produce a winning board eventually, even if the actions are selected entirely at random).</p>
<p>When the game ends, the agent wants to distribute the final reward among all the moves that got it to victory. To do so, while the game is played, the system will need to maintain a list of each move it plays, in order.</p>
<p>We’ll later find that list more useful if each entry saves more than just the selected move. Anticipating that need, let’s say that after each move, the agent retains a small bundle consisting of the starting board, the action the agent took, the immediate reward it received, and the board that resulted from that move. <a href="#figure21-17" id="figureanchor21-17">Figure 21-17</a> shows this visually. The agent saves these bundles in a list that starts empty at the start of the game and grows by one bundle after every move. </p>
<figure>
<img src="Images/F21017.png" alt="F21017" width="534" height="184"/>
<figcaption><p><a id="figure21-17">Figure 21-17</a>: Each time we make a move, we append a bundle of four values to the end of a growing list of bundles: the starting state, our chosen action, the reward we received, and the final state the environment returned to us after taking that action. </p></figcaption>
</figure>
<p>As <a href="#figure21-17">Figure 21-17</a> shows, we can save this bundle as a list of four numbers: the row number of the starting state, the column number of the action, the value of the reward, and the row number of the resulting state.</p>
<p>To make our first move, we look at the row of the L-table corresponding to our starting board, and the nine numbers we find along that row. Our policy will be to usually pick the largest value in the row, but sometimes pick one of the others for the sake of exploration. If all values are the same, as they are when we start out, we pick one at random.</p>
<p>The environment flips that tile for us, either making a dot appear or disappear. The environment then gives us back a reward and the new <span epub:type="pagebreak" title="620" id="Page_620"/>board. We make a little bundle to represent this move: the board we started with, the action we just took, the reward we got back, and the new state that resulted. We stick that bundle onto the end of our list of moves.</p>
<p>Because we’re playing solo, the environment isn’t going to make any moves on its own. As soon as it has sent us our feedback, the environment tells us to take a new action. So again, we look at the current board, find its row in the L-table, select the largest cell in that row, and report that as our action. We get back a reward and a new state, and we add a new bundle of the four items describing this move to our list.</p>
<p>This goes on until the game is over. In the final piece of feedback, we get our only nonzero reward. It’s the final reward based on the number of moves we played in the game, which drops off quickly, as we saw in <a href="#figure21-14">Figure 21-14</a>. With that final, nonzero reward, we know the game is over, so it’s time to learn from our experience.</p>
<p>We start by looking at our bundles from our list of moves. Conceptually, we line up our board states and resulting moves, along with their rewards, as in <a href="#figure21-18" id="figureanchor21-18">Figure 21-18</a>. One by one, we look at each move and find its TFR by adding up all the rewards that came after that move. In <a href="#figure21-18">Figure 21-18</a> the calculations aren’t very interesting, since all the immediate rewards except the last are zero. But it’s worth seeing the steps here, as we’ll have nonzero immediate rewards later on. </p>
<figure>
<img src="Images/F21018.png" alt="F21018" width="645" height="341"/>
<figcaption><p><a id="figure21-18">Figure 21-18</a>: Finding the TFR for each move. We add up the immediate reward for each move (shown directly underneath it) with the immediate rewards for all following moves. In our game where every immediate reward is zero except for the final reward, these sums are all the same.</p></figcaption>
</figure>
<p>We then use our simple update rule and the list of moves we made, and plunk each action’s TFR into the cell of the L-table corresponding to that action for that board, as shown in <a href="#figure21-19" id="figureanchor21-19">Figure 21-19</a>.</p>
<span epub:type="pagebreak" title="621" id="Page_621"/><figure>
<img src="Images/F21019.png" alt="F21019" width="688" height="513"/>
<figcaption><p><a id="figure21-19">Figure 21-19</a>: Updating our L-table with the new TFR for each action we took in the game. We find the row corresponding to the board we were looking at when we took each action and the column corresponding to the action we made. The new TFR replaces whatever was in that cell before.</p></figcaption>
</figure>
<p>If we want to learn some more, we go back up to the start of the process and play a new game. When we’re done, we compute a TFR value for each action we played and store that in its corresponding cell (overwriting whatever was there before). Note that we don’t reset the L-table after each game, though, so it gradually fills up with TFRs as we play more episodes.</p>
<p>When it’s time to stop training and start playing, we use the L-table to pick our moves. That is, at each move, we’re presented with a board, so we find that row of the table, pick the largest L-value in that row, and select the action corresponding to that column.</p>
<h3 id="h2-500723c21-0008">Testing Our Algorithm</h3>
<p class="BodyFirst">Let’s see how well our system works. Let’s start out by playing 3,000 episodes of Flippers from start to finish so the L-table can get filled up pretty well. <a href="#figure21-20" id="figureanchor21-20">Figure 21-20</a> shows a game of Flippers played from start to finish after these 3,000 episodes of training. It’s not a very nice result. There’s a simple two-move solution that any human would spot: flip the left-middle cell, and then the upper-left cell (or do it in the other order). Instead, our algorithm seems to meander randomly until it finally stumbles on a solution after six moves.</p>
<span epub:type="pagebreak" title="622" id="Page_622"/><figure>
<img src="Images/F21020.png" alt="F21020" width="752" height="758"/>
<figcaption><p><a id="figure21-20">Figure 21-20</a>: Playing a game of Flippers after 3,000 episodes of training with the L-table algorithm. Read the game left to right.</p></figcaption>
</figure>
<p>The arrangement shown in <a href="#figure21-20">Figure 21-20</a> shows rows of the L-table arranged as columns to better fit the page. Each column represents one board configuration (or state). The nine possible actions are shown in each row, highlighted in red. The thick black outline shows the action that the agent selected from that list, leading to the new board in the column to its right. The shaded cell shows the action taken. If the move causes a dot to appear, the move is shown as a solid red dot. If the move causes the dot to go away, it’s shown as an outlined red dot. The colored bar below each board shows its L-value from the table. Larger and greener bars correspond to larger L-values calculated with discounted future rewards.</p>
<p>Boards near the right have larger L-values than those near the left. That’s because those boards were sometimes the randomly chosen starting board for a game. If we picked a good move and won immediately, or in just a few moves, the final reward was large.</p>
<p><span epub:type="pagebreak" title="623" id="Page_623"/>Returning to this game, starting from the position on the far left, the algorithm’s first move was to flip the cell in the lower left, introducing a new dot. From that result, it then flipped the square in the middle of the leftmost column, again introducing a dot. From that position it then flipped the upper-left square, removing the dot that was there. The game continued in this way until it found a solution.</p>
<p>We’d expect our algorithm to improve with more training, and it does. <a href="#figure21-21" id="figureanchor21-21">Figure 21-21</a> shows the same game as <a href="#figure21-20">Figure 21-20</a> after doubling the length of the training run to 6,000 episodes.</p>
<figure>
<img src="Images/F21021.png" alt="F21021" width="373" height="725"/>
<figcaption><p><a id="figure21-21">Figure 21-21</a>: The same game as <a href="#figure21-20">Figure 21-20</a> after 6,000 episodes of training</p></figcaption>
</figure>
<p>This is very nice. The algorithm found the easy answer and went right for it.</p>
<p>We seem to have created a great algorithm for learning and playing. So why did we label everything with “L” for lousy? It seems to be working just fine.</p>
<p>It <em>is </em>just fine, as long as the environment remains completely predictable. Remember that earlier in this chapter we discussed unpredictable <span epub:type="pagebreak" title="624" id="Page_624"/>environments. In reality, most environments are unpredictable. Logic-based single-player games, such as the Flippers game we’ve been looking at, are one of the few activities that are completely deterministic. If our goal is to play only single-player games in completely deterministic environments where we are able to execute every intended move perfectly and the environment responds identically every time, then this algorithm isn’t so lousy. But such deterministic games and environments are rare. For example, as soon as there’s a second player, there’s uncertainty, and the game becomes unpredictable. In any situation in which the environment is not perfectly deterministic, the L-learning algorithm flounders.</p>
<p>Let’s see why, and then we will see how to fix it.</p>
<h3 id="h2-500723c21-0009">Handling Unpredictability</h3>
<p class="BodyFirst">Because we don’t have an opponent when playing Flippers on the computer, we have a completely deterministic system. Every time we make a move, we are guaranteed to get back the same result. But in the real world, even single-player activities can have unpredictable events. Video games throw random surprises at us, a lawnmower can hit a rock and jump to the side, or an internet connection can stutter and cause us to miss making the winning bid in an auction.</p>
<p>Since handling unpredictability is so important, let’s introduce some artificial randomness into Flippers and see how our L-learning algorithm responds. Our model of randomness takes the form of a big truck that drives by our playing area every now and then, shaking our board. Sometimes it’s enough to cause one or more random tiles to spontaneously flip over. Of course, we still want to play good games and win, even in the face of such surprises, but our L-learning system is helpless in the face of this kind of event.</p>
<p>It’s the combination of our policy and update rule that causes trouble. Remember that before we start learning, each row starts out with all zeros. When a training game is finally won, every action gets the same score, based on the length of the game, as we saw in <a href="#figure21-19">Figure 21-19</a>. As we continue to play our training games, the next time we come to that board, we pick the cell with the largest value.</p>
<p>Suppose we’re in the midst of a training game. We’re looking at a board that we once received as a starting board and we won it in two moves. The L-table values for each of those moves have large scores, so we select the high-scoring move, preparing to win on the next flip. But just after our first move, the big truck comes rumbling by, shaking our board and flipping a tile. Playing from this board forward, we end up requiring lots of moves before winning. This means that the TFR that ultimately comes from playing that action is less than if the truck hadn’t come by.</p>
<p>And here’s the problem: that smaller value overwrites the previous value in every cell that led to this long game. In other words, because of that event, every action we played sees its L-value lowered. In particular, that great starting move that led to victory in just one more move now has a low score. When we encounter this board again in a later game, we might find that one of the other cells has a larger value than the cell that formerly held the great move. <span epub:type="pagebreak" title="625" id="Page_625"/>The result is that this one-time random event causes us to stop making the best move we’ve found up to that point. We have “forgotten” that this was a great move, because a random event turned it into a bad move once. That low score makes it unlikely that we’ll ever choose that move again.</p>
<p>Let’s see this problem in action. <a href="#figure21-22" id="figureanchor21-22">Figure 21-22</a> shows an example where there are no unpredictable events. We start at the top with a board with three dots, and we find that the largest value in that row of the table is 0.6, corresponding to a flip of the center square. We make that move, and supposing the next move is also well-chosen, we have a victory in two moves, as shown in the center row. The reward of 0.7 replaces the 0.6 that was there for our first move, cementing that move’s status as the one to make. Everything went right.</p>
<figure>
<img src="Images/F21022.png" alt="F21022" width="844" height="507"/>
<figcaption><p><a id="figure21-22">Figure 21-22</a>: When there are no surprises, our algorithm works well. (a) The row of the L-table for the starting board. (b) The game plays out and is won in just a total of two moves. (c) The value of 0.7 overwrites the previous value for all table entries that led to this success.</p></figcaption>
</figure>
<p>In <a href="#figure21-23" id="figureanchor21-23">Figure 21-23</a> we introduce our rumbling truck. Just after we flip the center tile, the truck shakes the board and the bottom-right tile flips. This puts us on a whole new path. Let’s suppose the algorithm finally finds victory after four more moves. The total is five moves, and the reward of 0.44 is placed in every cell that led to this success.</p>
<p>This is terrible. In one quick stroke, we have “forgotten” our best move. In this example, two other actions now have better scores. The next time we come to this board, the cell with score 0.55 will be picked, which will not place us one move away from victory as before. In other words, our best move is now forgotten, and we’re going to always play a worse move.</p>
<span epub:type="pagebreak" title="626" id="Page_626"/><figure>
<img src="Images/F21023.png" alt="F21023" width="843" height="433"/>
<figcaption><p><a id="figure21-23">Figure 21-23</a>: (a) When a truck rumbles by, it flips the lower-right square, causing the game to take five moves to win. (b) The new reward of 0.44 overwrites the old value of 0.6. This cell is no longer the highest-scoring cell in the row.</p></figcaption>
</figure>
<p>Recall that we said that occasionally during training we’ll pick one of the cells in the row at random, just to explore what might happen. So someday we might make a new choice, or the truck might rumble by again and help us remember this cell, but that might not happen for a long time. And by the time the truck does come by and sets this move right again, others will have gone wrong. The L-table is almost always inferior to what it ought to be, and thus, on average, games powered by L-learning are longer and we get lower rewards. One surprise caused us to forget how to play this board well. </p>
<p>That’s why we called this algorithm lousy.</p>
<p>But all is not lost. We looked at this algorithm because the lousy version can be improved. Most of the algorithm is fine. We only need to fix how it fails in the face of unpredictability. From now on, we assume that when we play Flippers, that big truck may come thundering along, creating unpredictability in the form of occasionally flipping a random tile. In the next section, we’ll see how to handle this kind of unpredictable event gracefully and produce an improved learning algorithm that works well.</p>
<h2 id="h1-500723c21-0006">Q-Learning</h2>
<p class="BodyFirst">Without too much effort, we can upgrade L-learning to a much more effective algorithm that is commonly used today, called <em>Q-learning</em><b> </b>(the Q is for quality) (Watkins 1989; Eden, Knittel, and van Uffelen 2020). Q-learning <span epub:type="pagebreak" title="627" id="Page_627"/>looks a lot like L-learning, but, naturally, it instead fills up Q-tables with Q-values. The big improvement is that Q-learning performs well in stochastic, or unpredictable, environments.</p>
<p>To get from L-learning to Q-learning we make three upgrades: we improve how we compute new values for Q-table cells, how we update existing values, and the policy we use for choosing an action.</p>
<p>The Q-table algorithm starts with two important principles. First, we <em>expect</em> uncertainty in our results, so we build it in from the start. Second, we work out new Q-table values as we go, rather than waiting for the final reward. This second idea lets us work with games (or processes) that go on for a very long time, or perhaps never reach a conclusion (like scheduling elevators). By updating as we go, we’re able to develop our table of useful values even if we never get a final reward.</p>
<p>To make this work, we need to also upgrade the environment’s super simple rewarding process from the last section. Rather than always rewarding zero except for the final move, the environment will instead return immediate rewards that estimate the quality of each action as soon as it’s taken.</p>
<h3 id="h2-500723c21-0010">Q-Values and Updates</h3>
<p class="BodyFirst">Q-values are a way to approximate the total future reward even when we don’t know how things are going to end up. To find a Q-value, we add together the immediate reward, plus all the other rewards that are yet to come. So far, that’s nothing more than the definition of the total future reward. The change is that now we find the future rewards by using the reward from the next state.</p>
<p>In <a href="#figure21-17">Figure 21-17</a>, we saved four pieces of information for every move: the starting state, the action we chose, the reward we got, and the new state that action landed us in. We saved that new state so that we could use it now, where we will use it to compute the rest of the future rewards.</p>
<p>The key insight is to notice that our next move begins with that new state, and by following our policy we will always select the action whose cell has the greatest Q-value. If that cell’s Q-value is the total future reward for <em>that </em>action, then adding together that cell’s value with our immediate reward gives us the current cell’s total future reward. This works because our policy guarantees us that we always pick the cell with the largest Q-value for any given board state.</p>
<p>If multiple cells in the next state share the maximum value, then it doesn’t matter which one we pick when we get there. All we care about now is the total future reward that comes from the next action.</p>
<p><a href="#figure21-24" id="figureanchor21-24">Figure 21-24</a> shows this idea visually. Note that the value we compute in this step isn’t the final Q-value, but it’s almost there.</p>
<span epub:type="pagebreak" title="628" id="Page_628"/><figure>
<img src="Images/F21024.png" alt="F21024" width="844" height="318"/>
<figcaption><p><a id="figure21-24">Figure 21-24</a>: Part of the process for computing a new Q-value for a cell. The new value is the sum of two others. The first value is the immediate reward for taking the action that cell corresponds to, here 0.2. The second value is the largest Q-value of all the actions belonging to the new state, here 0.6.</p></figcaption>
</figure>
<p>The step that’s missing is where Q-learning accounts for randomness. Rather than use the value in the cell for our next action, we use the discounted value of that cell. Recall that this means we multiply it by our discount factor, a number from 0 to 1, often written as <em>γ</em> (gamma). As we discussed earlier, the smaller the value of <em>γ</em>, the less certain we are that unpredictable events in the future won’t change this value. <a href="#figure21-25" id="figureanchor21-25">Figure 21-25</a> shows the idea.</p>
<figure>
<img src="Images/F21025.png" alt="F21025" width="826" height="425"/>
<figcaption><p><a id="figure21-25">Figure 21-25</a>: To find the Q-value, we modify <a href="#figure21-24">Figure 21-24</a> to include the discount factor γ, which reduces the future rewards based on how confident we are that they won’t be changed by future, unpredictable events.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="629" id="Page_629"/>Note that the many multiplications in the discounted future reward shown in <a href="#figure21-8">Figure 21-8</a> are automatically handled by this scheme. The first multiplication is included here explicitly. The multiplication for the states beyond that are accounted for when the Q-values in the cells for the next state are evaluated.</p>
<p>Now that we’ve calculated a new value, how do we update the current value? We saw during L-learning that simply replacing the current value with the new one is a poor choice in the face of uncertainty. But we want to update the cell’s Q-value in some way, or we’ll never improve.</p>
<p>The Q-learning solution to this puzzle is to update the new cell’s value as a blend of the old and new values. The amount of blending is left up to us as a parameter that we specify. That is, the blend is controlled by a single number between 0 and 1, usually written as the lowercase Greek letter <em>α </em>(alpha). At the extreme value of <em>α</em> = 0, the value in the cell doesn’t change at all. At the other extreme value of <em>α</em> = 1, the new value replaces the old one, as in L-learning. Values of <em>α</em> between 0 and 1 blend, or mix, the two values, as shown in <a href="#figure21-26" id="figureanchor21-26">Figure 21-26</a>.</p>
<figure>
<img src="Images/F21026.png" alt="F21026" width="418" height="299"/>
<figcaption><p><a id="figure21-26">Figure 21-26</a>: The value of α lets us blend smoothly from the old value (when α = 0) to the new value (when α = 1), or any value in between.</p></figcaption>
</figure>
<p>The parameter <em>α</em> is called the<em> learning rate</em>, and it’s left up to us to set it. It’s unfortunate that this is the same term that’s used by the update step of backpropagation, but usually context makes it clear which type of “learning rate” we’re referring to. </p>
<p>In practice, we usually set α to a value close to 1, such as 0.9 or even 0.99. These values near 1 cause the new values to dominate the value stored in the cell. For instance, when <em>α</em> = 0.9, the new value stored in the cell is 10 percent of the old value, and 90 percent of the new value. But even a value of 0.99 is very different than 1, because remembering even 1 percent of the old value is often enough to make a difference.</p>
<p>Using our value for <em>α</em>, we run our system through some training and see how it does. Then we can adjust the value based on what we see and try <span epub:type="pagebreak" title="630" id="Page_630"/>again, repeating the process until we’ve found the value of <em>α </em>that seems to work best. We usually automate this search so we don’t have to do it ourselves.</p>
<p>The elephant in the room is that this whole argument has been based on having the correct Q-values in the next state, even before we get there. But where did they come from? And if we have the correct Q-values already, then why do any of this in the first place?</p>
<p>These are fair questions, and we will return to them after we look at the new policy rule.</p>
<h3 id="h2-500723c21-0011">Q-Learning Policy</h3>
<p class="BodyFirst">Recall that the policy rule tells us which action to select when we’re given a state of the environment. We use this policy while learning, and later, when playing actual games. The policy we used in L-learning was to usually select the action with the highest L-value in the row of the table corresponding to the current board. That makes sense, since we’ve learned that this is the action that brings us the highest rewards. But this policy doesn’t explicitly address the explore or exploit dilemma. In an unpredictable environment, the move that brings us the best rewards sometimes may not bring us the best reward at other times. And completely untried moves can be far better, if only we give them a chance.</p>
<p>Still, we don’t want to pick moves at random, because we do want to favor the ones that we know lead to high rewards. We just don’t want to do that every time. Q-learning picks a middle road. Instead of always picking the action with the highest Q-value, we<em> almost</em> always pick the action with the highest Q-value. The rest of the time we pick one of the other values. Let’s look at two popular policies for doing this.</p>
<p>The first approach we’ll look at is called <em>epsilon-greedy</em> or <em>epsilon-soft </em>(these refer to the Greek lowercase letter <em>ε</em>, epsilon, so they sometimes appear as <em>ε</em>-greedy and <em>ε</em>-soft). The algorithms are almost the same. We pick some number <em>ε</em> between 0 and 1, but usually it’s a small number quite close to 0, such as 0.01 or less.</p>
<p>Each time we’re at a row and ready to choose an action, we ask the system for a random number between 0 and 1, chosen from a uniform distribution. If the random number is greater than <em>ε</em>, then we proceed as usual and pick the action with the greatest Q-value in the row. But in that occasional case when the random number is less than <em>ε</em>, we select an action at random out of all the other actions in the row. In this way, we usually pick the most promising choice, but infrequently, we select one of the other actions and see where it leads us. <a href="#figure21-27" id="figureanchor21-27">Figure 21-27</a> shows this idea graphically.</p>
<p>The other policy we’ll look at is called <em>softmax</em>. This works in a way similar to the softmax layer that we discussed in Chapter 13. When we apply softmax to the Q-values in a row, they are transformed in a complex way so that they add up to 1. This lets us treat the resulting values as a discrete probability distribution, and then we randomly select one of entries according to those probabilities.</p>
<span epub:type="pagebreak" title="631" id="Page_631"/><figure>
<img src="Images/F21027.png" alt="F21027" width="694" height="361"/>
<figcaption><p><a id="figure21-27">Figure 21-27</a>: The epsilon-greedy policy</p></figcaption>
</figure>
<p>In this way, we usually get the action with the largest score. Infrequently, we get the value with the second-highest score. Even less frequently, we get the value with the third-highest score, and so on. <a href="#figure21-28" id="figureanchor21-28">Figure 21-28</a> illustrates the idea.</p>
<figure>
<img src="Images/F21028.png" alt="F21028" width="691" height="466"/>
<figcaption><p><a id="figure21-28">Figure 21-28</a>: The softmax policy for picking an action temporarily scales all the actions in the row so that they add up to 1.</p></figcaption>
</figure>
<p>An attractive quality of this scheme is that the probabilities of choosing each action always reflect the most current Q-values of all the actions associated with a given state. So as the values change over time, so too do the probabilities of picking the actions.</p>
<p><span epub:type="pagebreak" title="632" id="Page_632"/>The particular calculations carried out by softmax can sometimes lead to the system not settling down on a good set of Q-values. An alternative is the <em>mellowmax</em> policy, which uses slightly different math (Asadi and Littman 2017).</p>
<h3 id="h2-500723c21-0012">Putting It All Together</h3>
<p class="BodyFirst">We can summarize the Q-learning policy and update rule in a few words and a diagram. In words, when it’s time for a move, we use the current state to find the appropriate row of the Q-table. We then select an action from that row according to our policy (either epsilon-greedy or softmax). We take that action and get back a reward and a new state. Now we want to update our Q-value to reflect what we’ve learned from the reward. We look at the Q-values in that new state and select the largest one. We discount that by how much we think the environment is unpredictable, add it to the immediate reward we just got, and blend that new value with the current Q-value, producing a new Q-value for the action we just took, which we save.</p>
<p><a href="#figure21-29" id="figureanchor21-29">Figure 21-29</a> summarizes the process.</p>
<figure>
<img src="Images/F21029.png" alt="F21029" width="844" height="303"/>
<figcaption><p><a id="figure21-29">Figure 21-29</a>: The Q-learning policy and update procedure. (a) Choosing an action. (b) Finding a new Q-value for that action.</p></figcaption>
</figure>
<p>When we start a move, shown in <a href="#figure21-29">Figure 21-29</a>(a), we look at the Q-table row for the current state, and use our policy to pick an action, here shown in red. That action is communicated to the environment. The environment responds with a reward, and a new state. As in <a href="#figure21-29">Figure 21-29</a>(b), we find the row of the Q-table corresponding to the new state and select the largest reward there (this assumes that we’re going to pick the largest action when we get to that new state, which we know won’t always be the case. We will return to this issue soon). We discount this reward by multiplying it by <em>γ</em>, and then we add it to the immediate reward for this move, giving us a new value for the action we originally chose. We blend the old value and new value using <em>α</em>, and that new value is placed into the original action’s cell in the Q-table.</p>
<p><span epub:type="pagebreak" title="633" id="Page_633"/>The best values for the policy parameter <em>ε</em>, the learning rate <em>α</em>, and the discount factor <em>γ</em> have to be found by trial and error. These factors depend intimately on the specific nature of the task we’re performing, the nature of the environment, and the data we’re working with. Experience and intuition often give us good starting points, but nothing beats traditional trial-and-error to find the best values for any particular learning system. </p>
<h3 id="h2-500723c21-0013">The Elephant in the Room</h3>
<p class="BodyFirst">Earlier we promised to return to the problem that we needed to have accurate Q-values in order to evaluate the update rule, but those values themselves were computed by the update rule using the values that came after them, and so on. Each step seems to depend on the data from the following step. How can we use data that we haven’t created yet?</p>
<p>Here’s the beautiful, simple answer to that problem: we ignore it. Incredibly enough, we can initialize the Q-table with all zeros, and then start learning. In the beginning, the system makes moves erratically because there’s nothing in the Q-table to help it pick one cell over another. It picks one of the cells at random and plays that move. All of the actions in the resulting state are also zero, so the update rule, no matter what values we use for <em>α </em>and <em>γ</em>, keeps the cell’s score at zero.</p>
<p>Our system plays games that look chaotic and foolish, making terrible choices and missing obvious good moves. But eventually, the system stumbles onto a victory. That victory gets a reward of a positive number, and that reward updates the Q-value of the action that led to it. Sometime later, an action that led us to that action incorporates some of that great reward, because of the step in Q-learning that looks ahead to the next state. That ripple effect continues to slowly work backward through the system, as new games fall into the states that lead to states that previously led to victory.</p>
<p>Note that the information isn’t actually moving backward. Every game is played from beginning to end, and every update is made immediately after each move. The information seems to move backward because Q-learning involves the step of looking forward one move when evaluating the update rule. The score from the next<em> </em>move is able to influence the score for this one.</p>
<p>At some point, thanks to our policy that sometimes tries out new actions, every move eventually leads to a path to victory, and those values also influence earlier and earlier actions. Eventually the Q-table fills up with values that accurately predict the rewards of each action. Further playing serves to only improve the accuracy of those values. This process of settling into a consistent solution is called <em>convergence</em>. We say that the Q-learning algorithm <em>converges</em>.</p>
<p>We can prove mathematically that Q-learning converges (Melo 2020). This kind of proof guarantees that the Q-table gradually gets better. What we can’t say is how long that will take. The larger the table, and the more unpredictable the environment, the longer the training process requires. The speed of convergence also depends on the nature of the task the system is trying to learn, the feedback provided, and, of course, our chosen <span epub:type="pagebreak" title="634" id="Page_634"/>values for the policy variable <em>ε</em>, the learning rate <em>α,</em> and the discount factor <em>γ</em>. As always, there’s no substitute for trial-and-error experimentation to learn the specific idiosyncrasies of any particular system.</p>
<p>Note that the Q-learning algorithm very nicely addresses two of the problems we discussed earlier. The credit assignment problem asks us to make sure that the moves that lead up to a victory are rewarded, even when the environment isn’t providing that reward. The nature of the update rule takes care of this, propagating the rewards for successful moves backward from the final step that led to victory all the way back to the very first move. The algorithm also addresses the explore or exploit dilemma by using epsilon-greedy or softmax policies. They both favor choosing actions that have proven to be successful (exploitation), but they also sometimes try the other actions just to see what might come of them (exploration).</p>
<h3 id="h2-500723c21-0014">Q-learning in Action</h3>
<p class="BodyFirst">Let’s put Q-learning to work and see if it can learn how to play Flippers in an unpredictable environment. One way to measure the algorithm’s performance is to have the trained model play a large number of random games and see how long they take. The better the algorithm has gotten at finding good moves and eliminating bad ones, the fewer moves each game should require before reaching victory.</p>
<p>The longest well-played game is the one that starts out with all nine cells showing a dot. Then we have to flip six cells to get to a victory. So, we’d like to see our algorithm win every game in six moves or less.</p>
<p>To see the effect of training on the algorithm, let’s look at plots of the lengths of a large number of games for different amounts of training. Our plots show the results of playing games that start with each of the 512 possible patterns of dots and blanks, in an environment with a considerable degree of unpredictability. We played 10 games for each starting board, for a total of 5,120 games. We cut off any game that ran for more than 100 steps.</p>
<p>We set <em>α</em> to 0.95, so each cell retained just 5 percent of its old value when it was updated. This way we don’t completely lose what we’ve learned before, but we are expecting new values to be better than old ones, since they’ll be based on improved Q-table values when they pick the next move. To select moves, we used an epsilon-greedy policy with a relatively high <em>ε</em> of 0.1, encouraging the algorithm to seek out new moves 1 time out 10.</p>
<p>We introduced a lot of unpredictability by simulating our random truck coming by after each move with a probability of 1 in 10, flipping over a single random tile each time. To account for this, we set the discount factor <em>γ </em>to 0.2. This low value says we’re only 20 percent sure that the future will play out the same way each time because of the influence of those random events. We set this higher than the noise level we know the truck introduces (10 percent), because we expect that most well-played games will only be three or four moves long, so they are less likely to see a random event than a game of 10 or more moves.</p>
<p><span epub:type="pagebreak" title="635" id="Page_635"/>These values of <em>α</em>, <em>γ</em>, and <em>ε</em> are all basically informed guesses. In particular, <em>γ</em> was chosen based on our knowledge of how often random events would occur, which we rarely know ahead of time. In a real situation we’d experiment with our parameters to find what works best for this game and this amount of noise.</p>
<p><a href="#figure21-30" id="figureanchor21-30">Figure 21-30</a> shows the game lengths after training for just 300 games. The algorithm already found a lot of quick wins.</p>
<figure>
<img src="Images/F21030.png" alt="F21030" width="675" height="482"/>
<figcaption><p><a id="figure21-30">Figure 21-30</a>: The number of games that required from 0 to 40 moves to win (we played each of the 512 starting boards 10 times) using a Q-table that had been trained for 300 games</p></figcaption>
</figure>
<p>The “instant wins” are in the first column, corresponding to zero moves. These are games whose starting boards already have just three dots, arranged in a vertical column or horizontal row. Since there are six possible winning game configurations, and we ran through all the possible board configurations 10 times each, we started with a winning board 60 times.</p>
<p>Since no game in <a href="#figure21-30">Figure 21-30</a> hit our 100-move cutoff, we can see that the algorithm never fell into a long-lived loop. A loop might just be two states alternating forever, or a long string of them that wraps back around on itself. Loops are possible in Flippers, and there’s nothing in the basic Q-learning algorithm that explicitly prevents the system from getting into a loop.</p>
<p>We might say that the system “discovered” that loops don’t get to victory and thus don’t bring any rewards, so it learned to avoid them. If at some point it did return to a previously visited state, either by making that move <span epub:type="pagebreak" title="636" id="Page_636"/>or as the result of a randomly introduced flip, the relatively high value of <em>ε</em> meant it had a good chance of eventually picking a new action and thereby going off in a new direction.</p>
<p>Let’s raise the number of training games to 3,000, as in <a href="#figure21-31" id="figureanchor21-31">Figure 21-31</a>.</p>
<figure>
<img src="Images/F21031.png" alt="F21031" width="675" height="474"/>
<figcaption><p><a id="figure21-31">Figure 21-31</a>: The number of games of different lengths resulting from playing 5,120 games, based on a Q-table trained by playing 3,000 games</p></figcaption>
</figure>
<p>The algorithm has learned a lot. The longest game is now just 20 moves, with most games being won in 10 moves or less. It’s nice to see the denser clustering around four and five moves.</p>
<p>Let’s look at a typical game played after these 3,000 episodes of training. <a href="#figure21-32" id="figureanchor21-32">Figure 21-32</a> shows the game, played left to right. The algorithm took eight moves to win.</p>
<p><a href="#figure21-32">Figure 21-32</a> is not an encouraging result. Just by looking at the starting board, we can see at least four different ways to win this game in four moves. For example, flip the lower-left square and then flip the three dots in the middle and rightmost columns. But our algorithm seems to be flipping over tiles at random. It eventually stumbles onto a solution, but it’s definitely not an elegant result.</p>
<span epub:type="pagebreak" title="637" id="Page_637"/><figure>
<img src="Images/F21032.png" alt="F21032" width="675" height="681"/>
<figcaption><p><a id="figure21-32">Figure 21-32</a>: Playing a game of Flippers after training Q-learning for 3,000 episodes</p></figcaption>
</figure>
<p>If we train the algorithm for more episodes, we expect its performance to improve. After 3,000 more training episodes (for a total of 6,000), and looking at the number of games that required different numbers of moves, we get the results of <a href="#figure21-33" id="figureanchor21-33">Figure 21-33</a>.</p>
<p>Compared to our results in <a href="#figure21-31">Figure 21-31</a>, after 3,000 games of training, the longest game has decreased from 20 moves to 18, and the shorter games of just 3 and 4 steps have become more frequent.</p>
<p>This chart suggests that the algorithm is learning, but how is it actually performing when it plays a game? In fact, the algorithm has taken a huge jump in ability.</p>
<p><a href="#figure21-34" id="figureanchor21-34">Figure 21-34</a> shows the very same game as <a href="#figure21-32">Figure 21-32</a>, which required eight moves to win. Now it takes just four moves, which is the minimum number for this board (though there’s more than one way to achieve it).</p>
<span epub:type="pagebreak" title="638" id="Page_638"/><figure>
<img src="Images/F21033.png" alt="F21033" width="675" height="474"/>
<figcaption><p><a id="figure21-33">Figure 21-33</a>: The number of games that required a given number of moves to win our 5,120 games after training the Q-table with 6,000 games </p></figcaption>
</figure>
<p>Q-learning has done remarkably well even in this highly unpredictable learning environment, where a tile is flipped over at random after 10 percent of the moves. It weathered that unpredictability and managed to find ideal solutions for most games, even with only 6,000 training runs.</p>
<h2 id="h1-500723c21-0007">SARSA</h2>
<p class="BodyFirst">Q-learning does a great job, but it has a flaw that can reduce the accuracy of the Q-values that it relies on. It’s the problem we referred to when discussing <a href="#figure21-29">Figure 21-29</a>, when we noted that we were basing our future reward on the score of the most likely next action, even though that’s not necessarily the action that would be taken. In other words, the update rule <em>assumes</em> we’re going to pick the highest-scoring action on our next move, and its calculations of the new Q-value are based on that assumption. This isn’t an unreasonable assumption, because both our epsilon-greedy and softmax policies usually pick the most rewarding action. But the assumption is wrong when one of those policies chooses one of the other actions.</p>
<p>When our policy picks any action other than the one we used in the update rule, the calculation will have used the wrong data, and we end up with reduced accuracy in the new value that we compute for that action. Happily, we can fix that problem.</p>
<span epub:type="pagebreak" title="639" id="Page_639"/><figure>
<img src="Images/F21034.png" alt="F21034" width="675" height="681"/>
<figcaption><p><a id="figure21-34">Figure 21-34</a>: The game of <a href="#figure21-32">Figure 21-32</a>, solved more efficiently by Q-learning thanks to more training episodes</p></figcaption>
</figure>
<h3 id="h2-500723c21-0015">The Algorithm</h3>
<p class="BodyFirst">It would be nice to keep all the virtues of Q-learning, but avoid making the mistake of calculating a move’s Q-value by using the Q-value of the highest-scoring next action when there’s a chance we won’t actually select that action when we make our next move. We can do that by modifying Q-learning just a little, creating a new algorithm known as <em>SARSA</em> (Rummery and Niranjan 1994). This is an acronym for “state-action-reward-state-action.” The “SARS” part we’ve had covered ever since <a href="#figure21-17">Figure 21-17</a>, when we saved the starting state (S), action (A), reward (R), and resulting state (S). What’s new here is the extra action “A” at the end.</p>
<p>SARSA fixes the problem of choosing the wrong cell from the next state by choosing that next cell <em>with our policy </em>(rather than just selecting the <span epub:type="pagebreak" title="640" id="Page_640"/>biggest one), and <em>remembering </em>the choice of action (that’s the extra “A” at the end). Then when it’s time to make our new move, we select the action that we computed previously and saved.</p>
<p>In other words, we’ve moved the time when we apply our action-choosing policy. Instead of choosing our action at the start of a move, we choose it during the previous move and remember our choice. That lets us use the value of the action we really will use when building the new Q-value.</p>
<p>Those two changes (moving the action-choosing step and remembering the action we chose) are all that differentiate SARSA from Q-learning, but they can make a big difference in learning speed.</p>
<p>Let’s look at three successive moves using SARSA. The first move is shown in <a href="#figure21-35" id="figureanchor21-35">Figure 21-35</a>. Because this is the first move, we use our policy to pick an action for this move in <a href="#figure21-35">Figure 21-35</a>(a). This is the only time we do this. Once we have our chosen action, we use our policy to pick the action for move two. We get a reward from the environment, and update the Q-value for the action we just chose, in <a href="#figure21-35">Figure 21-35</a>(b).</p>
<figure>
<img src="Images/F21035.png" alt="F21035" width="848" height="427"/>
<figcaption><p><a id="figure21-35">Figure 21-35</a>: Using SARSA in the first move of our game. (a) We use our policy to pick the current action. (b) We also use our policy to pick our next action and update our current Q-value with the Q-value for that next action.</p></figcaption>
</figure>
<p>The second move is shown in <a href="#figure21-36" id="figureanchor21-36">Figure 21-36</a>. Now we use the action we picked for ourselves last time and then pick the action we’ll use in the third move once we get there.</p>
<span epub:type="pagebreak" title="641" id="Page_641"/><figure>
<img src="Images/F21036.png" alt="F21036" width="848" height="423"/>
<figcaption><p><a id="figure21-36">Figure 21-36</a>: The second move using SARSA. (a) We make the action we picked for ourselves last time. (b) We pick the next action, and use its Q-value to update the current action’s Q-value.</p></figcaption>
</figure>
<p>The third move is shown in <a href="#figure21-37" id="figureanchor21-37">Figure 21-37</a>. Here again we take the previously determined action and work out the action for the next, fourth move.</p>
<figure>
<img src="Images/F21037.png" alt="F21037" width="848" height="416"/>
<figcaption><p><a id="figure21-37">Figure 21-37</a>: The third move using SARSA. (a) We take the action we determined during the second move. (b) We choose an action for the fourth move, and use its Q-value to improve the current action’s Q-value.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="642" id="Page_642"/>Happily, we can prove that SARSA will also converge. As before, we can’t guarantee how long it will take, but it usually starts producing good results sooner than Q-learning and improves them quickly after that.</p>
<h3 id="h2-500723c21-0016">SARSA in Action</h3>
<p class="BodyFirst">Let’s see how well SARSA plays Flippers, using the same approach we took to Q-learning. <a href="#figure21-38" id="figureanchor21-38">Figure 21-38</a> shows the number of moves required by our 5,120 games after 3,000 training episodes using SARSA. For this plot and those following, we continue to use the same parameters as for the Q-learning plots: the learning rate <em>α</em> is 0.95, we introduce a random flip with a probability of 0.1 after every move, the discount factor <em>γ</em> is 0.2, and we pick moves with an epsilon-greedy policy with <em>ε</em> set to 0.1.</p>
<figure>
<img src="Images/F21038.png" alt="F21038" width="586" height="413"/>
<figcaption><p><a id="figure21-38">Figure 21-38</a>: The lengths of 5,120 games using SARSA after training with 3,000 games. Note that only a few games required more than the maximum of six moves.</p></figcaption>
</figure>
<p>This is looking great, with most values clustered around 4. The longest game is only 15 steps, with very few longer than 8.</p>
<p>Let’s look at a typical game. <a href="#figure21-39" id="figureanchor21-39">Figure 21-39</a> shows the game, played left to right. The algorithm needed seven moves to win. That’s not terrible, but we know it can be solved more quickly.</p>
<p>As always, more training should result in better performance. As before, let’s double our training to 6,000 episodes.</p>
<p><a href="#figure21-40" id="figureanchor21-40">Figure 21-40</a> shows the lengths of our 5,120 games after 6,000 training episodes.</p>
<span epub:type="pagebreak" title="643" id="Page_643"/><figure>
<img src="Images/F21039.png" alt="F21039" width="641" height="652"/>
<figcaption><p><a id="figure21-39">Figure 21-39</a>: Playing a game of Flippers after 3,000 episodes of training to SARSA</p></figcaption>
</figure>
<figure>
<img src="Images/F21040.png" alt="F21040" width="587" height="406"/>
<figcaption><p><a id="figure21-40">Figure 21-40</a>: The lengths of our 5,120 games using SARSA after training for 6,000 games. Note how much shorter most of the games have become, and that none of the games got caught in a loop.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="644" id="Page_644"/>The longest game has gone down from 15 to 14, which isn’t much to shout about, but the number of short games of lengths 3 and 4 is now even more pronounced. There weren’t many games that required more than 6 moves.</p>
<p><a href="#figure21-41" id="figureanchor21-41">Figure 21-41</a> shows the same game as <a href="#figure21-39">Figure 21-39</a>, which required 7 moves to win. Now it takes just 3 moves, which is the minimum for this board (though again, there’s more than one way to win with just 3 moves).</p>
<figure>
<img src="Images/F21041.png" alt="F21041" width="675" height="681"/>
<figcaption><p><a id="figure21-41">Figure 21-41</a>: The same game as <a href="#figure21-39">Figure 21-39</a>, after 3,000 more training episodes</p></figcaption>
</figure>
<h3 id="h2-500723c21-0017">Comparing Q-Learning and SARSA</h3>
<p class="BodyFirst">Let’s compare the Q-learning and SARSA algorithms. <a href="#figure21-42" id="figureanchor21-42">Figure 21-42</a> shows the lengths of all 5,120 possible games, after 6,000 games of training by Q-learning and SARSA. These results are slightly different from the previous plots because they were generated by new runs of the algorithm, so the random events were different.</p>
<p>They’re roughly comparable, but Q-learning produces a few games that are longer than SARSA’s maximum of 12.</p>
<span epub:type="pagebreak" title="645" id="Page_645"/><figure>
<img src="Images/F21042.png" alt="F21042" width="675" height="474"/>
<figcaption><p><a id="figure21-42">Figure 21-42</a>: Comparing game lengths after 6,000 training games for both Q and SARSA. SARSA’s longest game was 11 steps, while Q-learning went as high as 18.</p></figcaption>
</figure>
<p>More training helps. We’ve increased the training length by a factor of 10, for 60,000 games each. The results are shown in <a href="#figure21-43" id="figureanchor21-43">Figure 21-43</a>.</p>
<figure>
<img src="Images/f21043.png" alt="f21043" width="675" height="474"/>
<figcaption><p><a id="figure21-43">Figure 21-43</a>: The same training scenario as in <a href="#figure21-42">Figure 21-42</a>, but now we’ve trained for 60,000 games</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="646" id="Page_646"/>At this level of training, SARSA is doing an excellent job on Flippers, with almost all games coming in at 6 moves or less (very few games required 7 moves). Q-learning is faring slightly worse overall, needing up to 16 steps to solve some of its games, but it too is greatly concentrated in the region of 4 moves and under.</p>
<p>Another way to compare Q-learning and SARSA for this simple game is to plot the average game length after increasingly long training sessions. This gives us an idea of how effectively they’re learning to win the game. <a href="#figure21-44" id="figureanchor21-44">Figure 21-44</a> shows this for our Flippers game.</p>
<figure>
<img src="Images/f21044.png" alt="f21044" width="675" height="470"/>
<figcaption><p><a id="figure21-44">Figure 21-44</a>: The length of the average game for training sessions from 1 to 100,000 episodes (in increments of 1,000)</p></figcaption>
</figure>
<p>The trend here is easy to see. Both algorithms drop quickly and then level off, but after a noisy start, SARSA always performs better, ultimately saving almost a half move on every game (that is, in general, it plays one less move for every two games). By the time we reach 100,000 training games, both algorithms appear to have stopped improving. It seems likely that the Q-tables of each algorithm have settled down into stable states, changing a little bit over time due to the random flips introduced by the environment.</p>
<p>So, although Q-learning and SARSA can both do a great job of learning to play Flippers, SARSA’s games will generally be shorter.</p>
<h2 id="h1-500723c21-0008">The Big Picture</h2>
<p class="BodyFirst">Let’s step back and review the big picture of reinforcement learning. </p>
<p>There’s an environment and an agent. The environment provides the agent with two lists of numbers (the state variables and the available <span epub:type="pagebreak" title="647" id="Page_647"/>actions). Using its policy, the agent considers these two lists, along with whatever private information it has saved internally, to select one of the values from the list of actions, which it returns to the environment. In response, the environment gives the agent back a number (the reward) and two new lists.</p>
<p>Interpreting the lists as boards and moves was great because it lets us think of Q-learning in terms of learning to play a game. But the agent doesn’t know it’s in a game, or that there are rules, or really much of anything. It just knows that two lists of numbers come in, it picks a value from one of the lists, and then a reward value arrives in response. It’s remarkable that this little process can do much that’s interesting at all, but if we can find a way to describe our environment, and actions on that environment, using sets of numbers, and we can find even a crude way to distinguish a good action from a bad one, this algorithm can learn how to perform high-quality actions.</p>
<p>This worked for our simple game of Flippers, but how practical is all of this Q-table stuff in practice? In Flippers, there are nine squares and each can have a dot or not, so the game needs a Q-table with 512 rows and 9 columns, or 4,608 cells. In a game of tic-tac-toe, there are nine squares, and each can have one of three symbols: blank, X, or O. The Q-table for this game would need 20,000 rows and 9 columns, or 180,000 cells.</p>
<p>That’s big, but not ridiculously big for a modern computer. But what if we want a slightly more challenging game? Rather than play tic-tac-toe on a 3 by 3 board, suppose we played on a 4 by 4 board. There are a bit more than 43 million such boards, so our table would have 43 million rows and 9 columns, or a bit under 390 million cells. That’s getting pretty big, even for modern computers. Let’s increase it just one more modest step, and play tic-tac-toe on a 5 by 5 board. That hardly seems outrageous. Yet that board has almost 850 <em>billion</em> states. If we get a little ambitious and play on a 13 by 13 board, we find that the number of states is more than the number of atoms in the visible universe (Villanueva 2009). In fact, it’s roughly the number of atoms in one <em>billion</em> visible universes.</p>
<p>Storing the table for this game is not remotely practical, but it’s an entirely reasonable thing to want to do. More reasonably, we might want to play Go. The standard board for the game of Go is a grid of 19 by 19 intersections, and each intersection can be empty, have a black stone, or a white stone. This is like our tic-tac-toe board, but unfathomably bigger. We’d need a table whose rows would have labels requiring 173 digits. Such numbers are not just wildly impractical, they’re incomprehensible.</p>
<p>Yet this is the basic strategy that was used by the Deep Mind team to build AlphaGo, which famously beat a world champion human player (DeepMind 2020). They did it by combining reinforcement learning with deep learning. One of the key insights in this <em>deep reinforcement learning</em> approach was to eliminate explicit storage of the Q-table. We can think of the table as a function that takes a board state as input and returns a move number and Q-value as output. As we’ve seen, neural networks are great at learning how to predict things like this.</p>
<p><span epub:type="pagebreak" title="648" id="Page_648"/>We can build a deep learning system that takes the board input and predicts the Q-value we’d get for each move if we really did keep the table around. With enough training, this network can become accurate enough that we can abandon the Q-table and use just the network. Training a system like this can be challenging, but it can be done, with excellent results (Mnih et al. 2013; Matiisen 2015). Deep reinforcement learning has been applied to fields as diverse as video games, robotics, and even healthcare (François-Lavet et al. 2018). It’s also the central algorithm behind AlphaZero, arguably the best player of the game Go that has ever existed (Silver et al. 2017; Hassabis and Silver 2017; Craven and Page 2018).</p>
<p>Reinforcement learning has an advantage over supervised learning because it does not require a database that has been manually labeled, which is often a time-consuming and expensive process. On the other hand, it requires us to design an algorithm for creating rewards that guide an agent toward the desired behavior. In complex situations, this can be a difficult problem to solve.</p>
<p>This has necessarily been a high-level overview of a big topic. Much more information on reinforcement learning can be found in dedicated references (François-Lavet et al. 2018; Sutton and Baro 2018).</p>
<h2 id="h1-500723c21-0009">Summary</h2>
<p class="BodyFirst">In this chapter we took a look at some of the basic ideas in reinforcement learning, or RL. We saw that the basic idea is to break the world into an agent who acts, and an environment that encompasses everything else. The agent is given a list of options, and using a policy, it selects one. The environment executes that action, along with follow-on effects (which can include making a return move in a game, or carrying out a simulation or real-world action), and then returns to the agent a reward describing the quality of its chosen action. Typically the reward describes how well the agent has succeeded in improving the environment in some way.</p>
<p>We applied these ideas to the one-player game of Flippers with a simple algorithm that recorded the rewards in a table, and used a simple policy to select the move with the highest reward when possible. We saw that this didn’t handle the unpredictability of the real world very well, so we improved the method into the Q-learning algorithm with a better update rule and learning policy.</p>
<p>Then we improved that method again by prechoosing our next move, resulting in the SARSA algorithm. This learned to play Flippers even better.</p>
<p>In practice, a vast number of algorithms fall under the category of reinforcement learning, with more arriving all the time. It’s a vibrant field of research and development. </p>
<p>In the next chapter, we’ll look at a powerful method for training generators that can produce images, video, audio, text, and other kinds of data so well that we can’t reliably distinguish generated data from data in the training set.</p>
</section>
</div></body></html>