<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="83" id="Page_83"/>4</span><br/>
<span class="ChapterTitle">Bayes’ Rule</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">In Chapter 3 we discussed some performance measures based on probability. As we dig a little deeper into probability and its use in machine learning, we find that there are two fundamentally different schools of thought about how to approach the subject.</p>
<p>The approach that’s most commonly taught in schools is called the <em>frequentist method</em>. The other approach is called the <em>Bayesian method, </em>named for Thomas Bayes, who originally presented the idea in the 1700s. Although it’s less well known, the Bayesian method is popular in machine learning. There are many reasons for this, but one of the most important is that it gives us a way to explicitly identify and use our expectations about the system we’re measuring. </p>
<p>In this chapter we first look at the difference between frequentist and Bayesian methods. We then discuss the basics of Bayesian probability, covering enough of it to allow us to make sense of machine learning papers and documentation that are based on Bayesian ideas. We focus on Bayes’ Rule, also called Bayes’ Theorem, the cornerstone of Bayesian statistics. Even this <span epub:type="pagebreak" title="84" id="Page_84"/>single rule is a substantial topic, so we only address it in its broadest terms (Kruschke 2014). We come back to comparing Bayesian and frequentist approaches at the end of the chapter.</p>
<h2 id="h1-500723c04-0001">Frequentist and Bayesian Probability</h2>
<p class="BodyFirst">In mathematics, there’s almost always more than one way to think about a problem, or even a whole field. Sometimes the differences between approaches are subtle, and sometimes they’re dramatic. Probability definitely sits in the latter camp. There are at least two different philosophical approaches to probability, each with its strengths and weaknesses. The differences between the frequentist and Bayesian approaches have deep philosophical roots and are often expressed in the nuances of the mathematics and logic that are used to build their corresponding theories of probability (VanderPlas 2014). This makes it difficult to discuss the differences without getting into a wealth of detail. Despite being so different, the challenge of carefully describing the distinctions between these two approaches to probability has been called “especially slippery” (Genovese 2004).</p>
<p>Our approach here is to skip the complex arguments. Instead, we describe both approaches in general terms so that we can get a feeling for their different goals and processes without diving into the details. This helps set the stage conceptually for our discussion of Bayes’ Rule, which is the foundation of the Bayesian approach.</p>
<h3 id="h2-500723c04-0001">The Frequentist Approach</h3>
<p class="BodyFirst">Generally speaking, a <em>frequentist</em> is a person who distrusts any specific measurement or observation, considering it to be only an approximation of a true, underlying value. For instance, if we’re frequentists, and we want to know the height of a mountain, we assume that each measurement we take is likely to be at least a little too big or too small. At the heart of this attitude is the belief that a true answer already exists, and that it’s our job to find it. That is, the mountain has some exact, well-defined height, and if we work hard enough and take enough observations, we’ll be able to discover that value. </p>
<p>To find this true value, we combine a large number of observations. Even though we consider each measurement to probably be inexact, we also expect each measurement to be an approximation of the real value. If we take a large number of measurements, we say that the value that comes up most <em>frequently</em> is the one that’s most <em>probable</em>. This focus on the most-occurring value is what gives frequentism its name. The true value is found by combining a large number of measurements, with the most frequent values having the most influence (in some cases, we can merely take an average of all the measurements). </p>
<p>When probability is first discussed in schools, the frequentist approach is usually the one that’s presented because it’s easy to describe, and often fits well with common sense.</p>
<h3 id="h2-500723c04-0002"><span epub:type="pagebreak" title="85" id="Page_85"/>The Bayesian Approach</h3>
<p class="BodyFirst">Using the same broad brush, a <em>Bayesian </em>is a person who trusts every observation as an accurate measure of <em>something</em>, though it might be a slightly different something each time. The Bayesian attitude is that there’s no “true” value waiting to be found at the end of a process. Going back to our mountain example, a Bayesian would say that the true value of the height of the mountain is a meaningless idea. Instead, every measurement of the height of a mountain describes the distance from some point on the ground to some point near the top of the mountain, but they won’t be the identical two points every time. So even though every measurement has a different value, each one is an accurate measurement of something we could call the height of the mountain. Each careful measurement is just as true as the others—there’s not a single, definitive value out there, waiting for us to find it.</p>
<p>Instead, there’s only a range of possible heights for the mountain, each described by a probability. As we take more observations, that range of possibilities generally becomes more narrow, but it never shrinks to a single value. We can never state the height of the mountain as a number, but only as a range, where each value has its own probability. </p>
<h3 id="h2-500723c04-0003">Frequentists vs. Bayesians</h3>
<p class="BodyFirst">These two approaches to probability have led to an interesting social phenomenon. Some serious people working in probability believe that only the frequentist approach has any merit, and the Bayesian approach is a useless distraction. Other serious people believe exactly the other way around. Many people have less extreme, but still heartfelt, feelings on which approach should be considered the right way to think about probability. Of course, many people think that both approaches offer useful tools that are applicable in different situations. When we work with real data, our choice of how to think about probability can greatly influence what kinds of questions we can ask and answer (Stark and Freedman 2016).</p>
<p>A key feature of the Bayesian approach is that we explicitly identify our expectations before we start taking measurements. In our mountain example, we’d state up front about how high we expect the mountain to be. Some frequentists object to this, arguing that you should never enter an experiment with a preconceived expectation, or bias. Bayesians reply that bias is inevitable since it’s baked into the design of every experiment, influencing what we choose to measure, and how. They argue that it’s best to state those expectations clearly so that they can be examined and debated. Frequentists disagree and present counterarguments, Bayesians then present counter-counterarguments, and the debate goes on.</p>
<p>Let’s look at the two techniques in action by flipping a coin and asking if the coin is fair, so heads and tails come up about equally often, or if it’s weighted to come up one way or the other more frequently. Let’s start by looking at how a frequentist would address the question, and then we’ll see how a Bayesian would go about it.</p>
<h2 id="h1-500723c04-0002"><span epub:type="pagebreak" title="86" id="Page_86"/>Frequentist Coin Flipping</h2>
<p class="BodyFirst">People often use flipping (or tossing) coins as an example when discussing probability (Cthaeh 2016a; Cthaeh 2016b). Coin flipping is popular because it’s familiar to everyone, and each flip has only two possible outcomes: heads or tails (we’ll ignore weird cases like the coin landing on its side). Having only two outcomes makes the math simple enough that we can often work things out by hand. Though we won’t be doing any math beyond a few little examples, coin flipping is still a great way to see the underlying ideas, so we use that as our running example here.</p>
<p>We say that a <em>fair</em> coin is one that, on average, comes up heads half the time and tails the other half. A coin that isn’t fair we call a <em>rigged</em>, <em>weighted</em>,or <em>unfair</em> coin. To describe a rigged coin, we refer to its tendency to come up heads, or its <em>bias</em>. A coin with a bias of 0.1 comes up heads about 10 percent of the time, and a bias of 0.8 tells us to expect heads about 80 percent of the time. If a coin has a bias of 0.5, it comes up heads and tails equally often, and it is indistinguishable from a fair coin. We actually might say that a coin with a bias of 0.5, or 1/2, is the definition of a fair coin.</p>
<p>So, by taking lots of measurements (flips) and combining their results (heads or tails), we can hope to find the true answer (the coin’s bias). <a href="#figure4-1" id="figureanchor4-1">Figure 4-1</a> illustrates the frequentist’s approach to finding the bias of three different coins, using one row per coin. </p>
<figure>
<img src="Images/F04001.png" alt="F04001" width="694" height="486"/>
<figcaption><p><a id="figure4-1">Figure 4-1</a>: Top row: A coin with a bias of 0.2. Middle row: A coin with a bias of 0.5. Bottom row: A coin with a bias of 0.8.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="87" id="Page_87"/>On the left of each row in <a href="#figure4-1">Figure 4-1</a> we show 100 consecutive flips of the coin. On the right, we show a frequentist’s estimate of that coin’s bias after each flip. This is found by dividing the number of heads we’ve found up until then by the total number of flips. As usual, the frequentist looks at each measurement (here whether the coin is heads or tails) and considers it to be just one little approximation to the truth. By combining all these approximations (here with just a running average) we can converge on the single “true” value giving the bias of the coin. </p>
<h2 id="h1-500723c04-0003">Bayesian Coin Flipping</h2>
<p class="BodyFirst">Let’s consider how we might estimate a coin’s bias using a Bayesian point of view. To do so, let’s use a slightly more complicated situation that highlights how a Bayesian asks and answers questions.</p>
<h3 id="h2-500723c04-0004">A Motivating Example</h3>
<p class="BodyFirst">Let’s suppose that we have a friend who’s a deep-sea marine archaeologist. Her latest discovery is an ancient shipwreck that has, among its treasures, a box containing a marked board and a bag of two identical-looking coins. She thinks these were used for a betting game, and with her colleagues, she’s even reconstructed some of the rules.</p>
<p>The key element is that only one of the two seemingly identical coins is fair. The other coin is rigged and will come up heads two-thirds of the time (that is, it has a bias of 2/3). The difference between a bias of 1/2 and a bias of 2/3 isn’t big, but it’s enough to build a game around. The rigged coin has been cleverly made so that we can’t tell which coin is which by looking at them, or even by picking them up and casually feeling them. The game involves players flipping these coins and trying to figure out which coin is which, along with various forms of bluffing and betting along the way. When the game is over, the players figure out which coin is rigged and which is fair by spinning both coins on their edges. Because of its uneven weighting, the biased coin drops sooner than the fair coin.</p>
<p>Our archaeologist friend wants to explore the game further, but she needs to know the true identities of the coins. She asks us to help sort it out. She gives us two envelopes, marked Fair and Rigged, and our job is to put each coin in the appropriate envelope. We could use the spinning test to work out which coin is which, but let’s do it with probabilities instead, so we can get some experience with thinking this way. </p>
<p>Let’s start by picking a coin, flipping it once, seeing if it comes up heads or tails, and then finding out what we can do with that information. The first step is to select a coin. Since we can’t tell the two coins apart by looking at them, we have a 50 percent chance of picking up the fair coin, and the same chance for selecting the rigged coin. This choice sets up our big question: <em>Did we choose the fair coin?</em> Once we know which coin we’ve got, we can put it in its corresponding envelope and put the other coin in the other envelope. Let’s rephrase our question in terms of probabilities: <em>What is the <span epub:type="pagebreak" title="88" id="Page_88"/>probability that we picked the fair coin?</em> If we can be sure we have the fair coin, or be sure that we don’t, we’ll know everything we need to know.</p>
<p>So, we’ve got a question and we’ve got a coin. Let’s flip. Heads! The great thing about reasoning with probabilities is that with just this one flip, we can already make a valid, quantified statement about which coin we have.</p>
<h3 id="h2-500723c04-0005">Picturing the Coin Probabilities</h3>
<p class="BodyFirst">To draw the various probabilities involved in the coming discussion, let’s bring back our probability diagrams from Chapter 3. Let’s imagine a square wall at which we’ll throw darts, and that every dart has an equal probability of landing on every point on the wall. We can paint regions of the wall in different colors that correspond to different outcomes. For example, if one outcome has a 75 percent chance of happening, and the other has a 25 percent chance, we can paint three-fourths of the wall blue, and the remaining quarter pink. If we throw 100 darts, we’d expect about 75 of them to land in the blue region, and the rest to land in the pink region. </p>
<p>Our first step was to choose a coin. Since we can’t tell one coin from the other, the odds of choosing the fair coin are 50:50. To represent this, we can imagine splitting the wall into two regions of equal size. Let’s paint the fair region with flax (a kind of beige), and the rigged region with red, as in <a href="#figure4-2" id="figureanchor4-2">Figure 4-2</a>. When we throw a dart at the wall, the odds of it landing in the fair region are 50:50, corresponding to selecting the fair coin.</p>
<figure>
<img src="Images/F04002.png" alt="F04002" width="302" height="272"/>
<figcaption><p><a id="figure4-2">Figure 4-2</a>: When we pick one of the two coins at random, it’s the same as throwing a dart at a wall that’s been painted with two equal areas, one for the fair coin and one for the rigged coin.</p></figcaption>
</figure>
<p>Let’s paint the wall in a more informative way that tells us how likely we are to get heads or tails from our first flip. We know that the fair coin has a 50:50 chance of being heads or tails, so we can split the fair region into two equal pieces, one each for heads and tails, as in <a href="#figure4-3" id="figureanchor4-3">Figure 4-3</a>.</p>
<p>In <a href="#figure4-3">Figure 4-3</a> we also split the rigged side. Since we know from our friend that the rigged coin has a two-thirds chance of coming up heads, we assigned two-thirds of its area to heads and one-third to tails. <a href="#figure4-3">Figure 4-3</a> summarizes everything we know about our system. It tells us the likelihood of picking either coin (corresponding to landing in the yellow or red zones), <span epub:type="pagebreak" title="89" id="Page_89"/>and the likelihood of getting heads or tails in each situation (from the relative sizes of the heads and tails regions).</p>
<figure>
<img src="Images/F04003.png" alt="F04003" width="272" height="245"/>
<figcaption><p><a id="figure4-3">Figure 4-3</a>: We can split up the fair and rigged regions of the wall into heads and tails for each coin using the information we already know about how they are likely to come up when flipped.</p></figcaption>
</figure>
<p>If we throw a dart at a wall painted like <a href="#figure4-3">Figure 4-3</a>, our dart will land in a region corresponding to one of the coins and either heads or tails. But since we’ve already flipped the coin and observed heads, we know we landed in either Fair heads or Rigged heads.</p>
<p>Remember our question: What is the probability that we picked the fair coin? We can tighten that up by using the information that we got back heads. We’ll see later that the best way to phrase our question is in the form of a template that asks, “What is the probability that (something1) is true, given that (something2) is true?” In this case that becomes, “What is the probability that we have the fair coin, given that we saw heads?”</p>
<p>We can diagram this in pictures. It’s the area of the Fair heads region compared to the total area that could have given us heads, which is the sum of Fair heads and Rigged heads. <a href="#figure4-4" id="figureanchor4-4">Figure 4-4</a> shows this ratio.</p>
<figure>
<img src="Images/F04004.png" alt="F04004" width="297" height="353"/>
<figcaption><p><a id="figure4-4">Figure 4-4</a>: If the coin comes up heads, how likely is it that we had the fair coin? It’s the size of the region where a fair coin gives us heads divided by all the areas combined that would give us heads.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="90" id="Page_90"/>Let’s think about this picture for a moment. Since the Rigged heads area is larger than the Fair heads area, that makes it more likely that our result of “heads” came from landing in the rigged zone. In other words, now that we’ve seen that our coin came up heads, it’s a little more likely that it’s the rigged coin, just as a dart thrown at a wall painted as in <a href="#figure4-3">Figure 4-3</a> is more likely to land in the Rigged heads region than the Fair heads region.</p>
<p>Later on, we’re going to talk about “the ways something can happen,” or “all of the ways that something can come about.” This means if we’re looking for some property to be true, we account for all of the possible events that give us that result. In this case, the bottom half of <a href="#figure4-4">Figure 4-4</a> is the sum of all the ways we can get heads. In other words, we can receive heads from either the fair coin or from the rigged coin, so representing “all the ways to get heads” means combining these two possibilities.</p>
<h3 id="h2-500723c04-0006">Expressing Coin Flips as Probabilities</h3>
<p class="BodyFirst">Let’s rephrase <a href="#figure4-4">Figure 4-4</a> using probability terms. The probability of getting the fair coin <em>and </em>getting heads is P(H,F) (or equivalently P(F,H)). The probability of getting the rigged coin <em>and </em>getting heads is P(H,R).</p>
<p>Now we can interpret the ratio of areas in <a href="#figure4-4">Figure 4-4</a> as a probability statement. That diagram shows us the chance that our coin, which we know came up heads, is the fair coin. That’s P(F|H), which stands for “the probability that we have the fair coin given that we observed heads.” That is, this conditional probability is the answer to our question.</p>
<p>We can put this all together into <a href="#figure4-5" id="figureanchor4-5">Figure 4-5</a>.</p>
<figure>
<img src="Images/F04005.png" alt="F04005" width="485" height="205"/>
<figcaption><p><a id="figure4-5">Figure 4-5</a>: Translating <a href="#figure4-4">Figure 4-4</a> into the language of probability</p></figcaption>
</figure>
<p>Can we plug numbers into this diagram and come up with an actual probability? Sure, in this case we can, because the situation was contrived to be simple. But in general, we won’t know any of these joint probabilities, and they won’t be easy to find out.</p>
<p>Not to worry. All the boxes on the right of <a href="#figure4-5">Figure 4-5</a> are joint probabilities, and we saw in Chapter 3 that we can write any joint probability in two different and equivalent ways, each involving a simple probability and a conditional probability. Those terms are usually much easier for us to put numbers to. Those two approaches are repeated here as <a href="#figure4-6" id="figureanchor4-6">Figure 4-6</a> and <a href="#figure4-7" id="figureanchor4-7">Figure 4-7</a>.</p>
<span epub:type="pagebreak" title="91" id="Page_91"/><figure>
<img src="Images/F04006.png" alt="F04006" width="600" height="359"/>
<figcaption><p><a id="figure4-6">Figure 4-6</a>: We can write the joint probability of two events A and B as the conditional probability P(A|B) times the probability of B, given by P(B). </p></figcaption>
</figure>
<figure>
<img src="Images/F04007.png" alt="F04007" width="600" height="370"/>
<figcaption><p><a id="figure4-7">Figure 4-7</a>: We can find the joint probability of two events A and B as the conditional probability P(B|A) times the probability of A given by P(A). </p></figcaption>
</figure>
<p>Let’s write our expression for P(F|H) in <a href="#figure4-5">Figure 4-5</a> without the colored boxes, and then replace P(H,F) with the expression in <a href="#figure4-7">Figure 4-7</a>, which tells us that we can find the joint probability of P(H,F), or landing in heads <em>and </em>using the fair coin, by multiplying the chance of getting heads from a fair coin, P(H|F), with the chance of having the fair coin in the first place, P(F). This change is shown <a href="#figure4-8" id="figureanchor4-8">Figure 4-8</a>.</p>
<span epub:type="pagebreak" title="92" id="Page_92"/><figure>
<img src="Images/F04008.png" alt="F04008" width="420" height="251"/>
<figcaption><p><a id="figure4-8">Figure 4-8</a>: Our ratio of <a href="#figure4-5">Figure 4-5</a> represents P(F|H), the chance that we have the fair coin given that it came up heads. </p></figcaption>
</figure>
<p>Let’s do the same thing for the two other joint probabilities, replacing them by their expanded versions (the first of the two values is just P(H,F) again). <a href="#figure4-9" id="figureanchor4-9">Figure 4-9</a> shows the result.</p>
<figure>
<img src="Images/F04009.png" alt="F04009" width="595" height="272"/>
<figcaption><p><a id="figure4-9">Figure 4-9</a>: We can replace the other two joint probabilities in <a href="#figure4-8">Figure 4-8</a> with their expanded versions as well. </p></figcaption>
</figure>
<p>Since we can generally find numbers for all of the symbolic expressions in this expanded version, this is a useful way to find P(F|H).</p>
<p>Let’s use this expression to find the chance that we just flipped the fair coin. We need to assign a number to each term in <a href="#figure4-9">Figure 4-9</a>. P(F) is the probability that we picked the fair coin when we started. We’ve already seen that’s P(F)=1/2. P(R) is the probability that we picked the rigged coin when we started, which is also 1/2. P(H|F) is the probability of getting heads given that we chose the fair coin. By definition, that’s 1/2. P(H|R) is the probability of getting heads from the rigged coin. From what our archaeologist friend told us, that’s 2/3.</p>
<p>So now we have all the numbers we need to work out the probability that we have the fair coin, given that we just flipped it and got heads. <a href="#figure4-10" id="figureanchor4-10">Figure 4-10</a> shows plugging in the numbers and cranking through the steps (following math convention, we perform multiplications before additions—this lets us leave out some distracting parentheses).</p>
<span epub:type="pagebreak" title="93" id="Page_93"/><figure>
<img src="Images/F04010.png" alt="F04010" width="516" height="394"/>
<figcaption><p><a id="figure4-10">Figure 4-10</a>: Finding the probability that we picked the fair coin, given that we just saw heads</p></figcaption>
</figure>
<p>The result is 3/7 or about 0.43. This is kind of remarkable. It tells us that after <em>just one flip of the coin</em>, we can already say in a principled way that there’s only a 43 percent chance we have the fair coin, and therefore a 57 percent chance that we have the rigged coin. That’s a 14 percent difference, from one flip!</p>
<p>As an aside, consider that a frequentist wouldn’t dare to characterize the coin as fair or not after just one flip, while this Bayesian approach is already describing the coin with specific probabilities. </p>
<p>Returning to our first flip, let’s suppose we received tails instead. Now we’d like to find P(F|T), or the probability that we have the fair coin, given that we saw it come up tails. Recall that the bias is the probability of coming up heads, so the probability of coming up tails is 1 – bias. For the fair coin, the chance of it coming up tails, or P(T|F), is (1 – (1/2)) = 1/2. For the rigged coin, we know from our friend that the bias is 2/3, so P(T|R) is (1 – (2/3)) = 1/3. The chances of picking the fair and rigged coins, given by P(F) and P(R), are each 1/2, just as before. Let’s plug those values in and find P(F|T), the probability that we picked the fair coin given that it came up tails. <a href="#figure4-11" id="figureanchor4-11">Figure 4-11</a> shows the steps. The expression for P(F|T) is like the one for P(F|H) with the H’s and T’s reversed.</p>
<p>This is an even more dramatic answer, telling us it’s 60 percent likely that this result of tails means that we’re flipping the fair coin (and thus it’s 40 percent likely that we’re flipping the rigged coin). That’s a huge boost of confidence from just one flip! Note that the results aren’t symmetrical. If we get heads, we have a 43 percent probability of a fair coin, but if we get tails, we have a 60 percent probability of a fair coin.</p>
<p>We’ve seen that we can get a lot of information from one flip, but even 60 percent is far from being certain. Making more flips gives us a chance to find more refined probabilities, and we’ll see how to do that later in this chapter.</p>
<span epub:type="pagebreak" title="94" id="Page_94"/><figure>
<img src="Images/F04011.png" alt="F04011" width="525" height="392"/>
<figcaption><p><a id="figure4-11">Figure 4-11</a>: What if we got tails on our coin flip? </p></figcaption>
</figure>
<h3 id="h2-500723c04-0007">Bayes’ Rule</h3>
<p class="BodyFirst">Let’s find another way to write P(F|H). In <a href="#figure4-5">Figure 4-5</a>, <a href="#figure4-8">Figure 4-8</a>, and <a href="#figure4-9">Figure 4-9</a>, we saw several different ways to write the probability that we picked the fair coin given that we saw heads.</p>
<p>Let’s go back to the version in <a href="#figure4-8">Figure 4-8</a> (repeated at the top of <a href="#figure4-12" id="figureanchor4-12">Figure 4-12</a>). Note that the bottom part of the ratio, P(H,F) + P(H,R), combines the probabilities for all the possible ways we could have gotten heads (after all, it has to come from either the fair or rigged coin). If we were dealing with, say, 20 coins, then we’d have to write a sum of 20 joint probabilities, which would make for a very messy expression. We usually use a shortcut, and write these combined probabilities as simply P(H), or “the probability of getting heads.” This implicitly means the sum of all the ways we could have gotten heads. If we had 20 different coins, this would be the sum of the probability of each of those coins giving us heads. <a href="#figure4-12">Figure 4-12</a> shows this abbreviated notation.</p>
<figure>
<img src="Images/F04012.png" alt="F04012" width="348" height="215"/>
<figcaption><p><a id="figure4-12">Figure 4-12</a>: The last line of <a href="#figure4-8">Figure 4-8</a>, but we’ve replaced the bottom part of the ratio with the symbol P(H)</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="95" id="Page_95"/><a href="#figure4-13" id="figureanchor4-13">Figure 4-13</a> shows this most recent version all by itself. This is the famous <em>Bayes’ Rule</em> or <em>Bayes’ Theorem</em> that we mentioned earlier in the chapter. Here we’ve adopted the mathematician’s convention that two values placed side by side should be multiplied.</p>
<figure>
<img src="Images/F04013.png" alt="F04013" width="298" height="84"/>
<figcaption><p><a id="figure4-13">Figure 4-13</a>: Bayes’ Rule, or Bayes’ Theorem, as it’s usually written </p></figcaption>
</figure>
<p>Expressing this in words, we want to find P(F|H), the probability that we have a fair coin given that we just flipped it and saw heads. To determine that, we combine three pieces of information. First, P(H|F), or the probability that, if we do indeed have a fair coin, it will come up heads. We multiply that by P(F), the probability that we have a fair coin. As we’ve seen, this multiplication is just a more convenient way to evaluate P(H,F), or the probability that our coin is fair <em>and </em>that it came up heads. Lastly, we divide everything by P(H), or the probability that our coin will come up heads, <em>taking into account both the fair and rigged coins</em>. This is how likely we will be to get heads using <em>either</em> of these coins.</p>
<p>Bayes’ Theorem is usually written in the form of <a href="#figure4-13">Figure 4-13</a>, because it breaks things down into pieces that we can conveniently measure (the letters are often changed to better suit what’s being discussed). We just replace each term by its corresponding value and out pops the conditional probability that, given heads, we picked the fair coin. Remember that P(H) stands for the sum of the joint probabilities, as we saw in <a href="#figure4-12">Figure 4-12</a>.</p>
<p>This is why the questions we ask of Bayes’ Rule need to be in the form of a conditional probability: <em>What is the probability that (something1) is true, given that (something2) is true? </em>It’s because that’s what Bayes’ Rule provides us with.<em> </em>If we can’t express our problem in that form, then Bayes’ Rule isn’t the right tool for answering it.</p>
<h3 id="h2-500723c04-0008">Discussion of Bayes’ Rule</h3>
<p class="BodyFirst">Bayes’ Rule can be hard to remember because there are lots of letters floating around, and each one has to go in the right place. But the nice thing is that we can quickly re-derive the rule perfectly any time we need it.</p>
<p>Let’s write the joint probability of F and H in both forms (that is, P(F,H) and P(H,F)). We know that these are both the same thing: the probability of having a fair coin and getting heads. Replacing them with the expanded versions as we did in <a href="#figure4-8">Figure 4-8</a> gives us the second line of <a href="#figure4-14" id="figureanchor4-14">Figure 4-14</a>.</p>
<p>To get Bayes’ Rule, just divide each side by P(H), as shown in the third line. The result is the last line, which is Bayes’ Rule. This can be a handy way to re-create the rule if we need it and have forgotten it.</p>
<span epub:type="pagebreak" title="96" id="Page_96"/><figure>
<img src="Images/F04014.png" alt="F04014" width="351" height="321"/>
<figcaption><p><a id="figure4-14">Figure 4-14</a>: How to rediscover Bayes’ Rule if we forget, or a quick demonstration of why it’s true </p></figcaption>
</figure>
<p>Each of the four terms in Bayes’ Rule has a conventional name, summarized in <a href="#figure4-15" id="figureanchor4-15">Figure 4-15</a>.</p>
<figure>
<img src="Images/F04015.png" alt="F04015" width="371" height="170"/>
<figcaption><p><a id="figure4-15">Figure 4-15</a>: The four terms in Bayes’ Rule, with their names </p></figcaption>
</figure>
<p>In <a href="#figure4-15">Figure 4-15</a> we used the traditional letters A and B, which stand for any kinds of events and observations. With these letters, P(A) is our initial estimate for whether or not we have the fair coin. Because it’s the probability we use for “we chose the fair coin” before, or prior to, flipping the coin, we call P(A) the <em>prior probability</em>, or just the <em>prior</em>.</p>
<p>P(B) tells us the probability of getting the result we did, which in this case was that the coin came up heads. We call P(B) the <em>evidence</em>. This word can be misleading, since sometimes this word refers to something like a fingerprint at a crime scene. In our context, <em>evidence</em> is the probability that event B could have come about <em>by any means at all</em>. Remember that the evidence is the sum of the probabilities for every coin we might have chosen to come up heads.</p>
<p>The conditional probability P(B|A) tells us the likelihood<em> </em>of getting heads, assuming we have a fair coin. We call P(B|A) the <em>likelihood</em>.</p>
<p>Finally, the result of Bayes’ Rule tells us the probability that we picked the fair coin, given the observation of heads. Because P(A|B) is what we get at the end of the calculation, it’s called the <em>posterior probability</em>, or just the <em>posterior</em>.</p>
<p><span epub:type="pagebreak" title="97" id="Page_97"/>Earlier in this chapter we said that a virtue of the Bayesian approach is that it lets us explicitly identify our preconceptions and expectations. Now we can see that we do that by choosing our prior, P(A). In general, we know the likelihood P(B|A) and evidence P(B) from our experimental setup, but we’ll have to guess at the prior P(A). This can be a problem if we run the experiment only once, because if our estimate of the prior is wrong, the posterior will also be wrong. We’ll see later that if we can run an experiment multiple times (such as by flipping the coin more than once), then we can use Bayes’ Rule after each flip to refine our initial prior into a better and better description of P(A), giving us a more accurate value for what we really care about, the posterior P(B|A).  </p>
<p>We came up with a value for the prior pretty easily in our little coin-testing example, but in more complicated situations, choosing a good prior can be more complicated. Sometimes it comes down to a combination of experience, data, knowledge, and even just hunches about what the prior should be. Because there’s some subjective, or personal, aspect to our choice, picking a prior by ourselves is called <em>subjective Bayes</em>. On the other hand, sometimes we can use a rule or an algorithm to pick the prior for us. If we do so, that’s called <em>automatic Bayes</em> (Genovese 2004).</p>
<h2 id="h1-500723c04-0004">Bayes’ Rule and Confusion Matrices</h2>
<p class="BodyFirst">In Chapter 3 we looked at using the confusion matrix to help us properly understand the outcomes of a test. Let’s look at this idea again, but this time using Bayes’ Rule.</p>
<p>Rather than create some artificial, contrived example, let’s use something realistic and everyday. You’re the Captain of the Starship <em>Theseus</em>, on a mission into deep space to find rocky, uninhabited planets to mine for raw materials. You’ve just come across a promising rocky planet. It would be great to start mining it, but your orders are to never, ever mine a planet that has life on it. So the big question is this: Is there life on this planet?</p>
<p>In your experience, most of the life on these rocky worlds is just a little bacteria or bit of fungus, but life is life. As protocol dictates, you send down a probe to investigate. The probe lands and reports “no life.”</p>
<p>Because no probe is perfect, we must now ask the question, “What is the probability that the planet contains life, given that the probe detected nothing?” This question is in perfect form for using Bayes’ Rule. One condition (let’s call it L) is “life is present,” where a positive value means the planet has life on it, and a negative value means the planet doesn’t have life (so we can start mining). The other condition (we’ll call it D) is “detected life,” where positive means the probe detected life, and negative means it didn’t.</p>
<p>The situation we really want to avoid is mining on a planet that has life. That’s a false negative: the probe reported negative, but it shouldn’t have. This would be terrible, since we don’t want to interfere with, much less destroy, life in any form. False positives are less worrisome. Those are planets that are barren, but the probe thought it found signs of life. The <span epub:type="pagebreak" title="98" id="Page_98"/>only drawback there is that we fail to mine a planet we otherwise could have. There’s a financial loss, but that’s all.</p>
<p>The scientists who built our probes shared these same concerns, so they struggled hard to minimize the false negatives. They tried to keep down the false positives, too, but that wasn’t as critical. </p>
<p>In practice, some planets with life might not have life everywhere, so it’s possible that a probe could land in a life-free region of a populated planet, and detect nothing. For simplicity, let’s not worry about such situations, and say that any incorrect results (that is, missing life that’s there, or saying life is present when it’s not) will be due to the probe, and not the planet. </p>
<p>The probes they sent us out with have the performance shown in <a href="#figure4-16" id="figureanchor4-16">Figure 4-16</a>. To get these numbers, they sent their probes down onto 1,000 known planets of the type we’ll be wanting to mine, 101 of which were known to contain life. These values turn into our prior: out of every 1,000 planets, we expect life on 101 of them. </p>
<figure>
<img src="Images/F04016.png" alt="F04016" width="844" height="396"/>
<figcaption><p><a id="figure4-16">Figure 4-16</a>: The performance of our probes</p></figcaption>
</figure>
<p>The probe correctly reported that it found life (that is, a true positive) 100 times out 1,000. In other words, of the 101 planets with life, the probe missed life signs (a false negative) only once. </p>
<p>Out of the 899 empty planets, the probe correctly reported there was no life (a true negative) 869 times. Finally, it incorrectly reported finding life on a barren planet (a false positive) 30 times. All told, these aren’t bad numbers, since they’re skewed in favor of protecting life.</p>
<p>Using the letter D for “detected life” (the probe’s result), and the letter L for “life is present” (the reality on the ground), we can summarize these results in the confusion matrix of <a href="#figure4-17" id="figureanchor4-17">Figure 4-17</a>. For the marginal probabilities, we write not-D for the probe result “not detected-life” (that is, the probe said there was no life), and not-L for “not life-is-present” (that is, there really is no life on the planet).</p>
<span epub:type="pagebreak" title="99" id="Page_99"/><figure>
<img src="Images/F04017.png" alt="F04017" width="542" height="398"/>
<figcaption><p><a id="figure4-17">Figure 4-17</a>: The confusion matrix that summarizes <a href="#figure4-16">Figure 4-16</a>, demonstrating the performance of our life-detecting probe. The four marginal probabilities are shown in the right and bottom margins.</p></figcaption>
</figure>
<p><a href="#figure4-18" id="figureanchor4-18">Figure 4-18</a> gathers up the four marginal probabilities, plus two conditional probabilities that we’ll be using.</p>
<figure>
<img src="Images/F04018.png" alt="F04018" width="511" height="151"/>
<figcaption><p><a id="figure4-18">Figure 4-18</a>: Summaries of the four marginal probabilities and two conditional probabilities that we’ll be using based on the data in <a href="#figure4-17">Figure 4-17</a></p></figcaption>
</figure>
<p>To find P(D|L), or the probability that the probe reported life given that there really <em>is </em>life, we found the number of times the probe found life (100) and divided it by the number of planets where life was found (101). That is, we found TP / (TP + FN), which we saw in Chapter 3 is called the <em>recall</em>. The value of 100/101 is about 0.99.</p>
<p>To find P(not-D|L), we carried out the calculation the other way. It missed finding life once out of 101 planets. We found FN / (TP + FN), which we saw in Chapter 3 is called the <em>false negative rate</em>. It comes to 1/101, or about 0.01. (For more insight into the probe’s behavior, we can also use the definitions in Chapter 3 to find the probe’s accuracy<b> </b>as 969/1000, which is 0.969, and its precision as 100/130, which is about 0.77.)</p>
<p>Now we can answer our original question. The probability that there actually is life, given that our probe says there isn’t, is P(L|not-D). Using Bayes’ Rule, we plug in the numbers from either the previous paragraph or <a href="#figure4-18">Figure 4-18</a>, giving us <a href="#figure4-19" id="figureanchor4-19">Figure 4-19</a>.</p>
<span epub:type="pagebreak" title="100" id="Page_100"/><figure>
<img src="Images/F04019.png" alt="F04019" width="391" height="365"/>
<figcaption><p><a id="figure4-19">Figure 4-19</a>: Working out the probability that a planet has life, given that the probe reported that no life was detected</p></figcaption>
</figure>
<p>This is reassuring. The probability that there’s life on that planet, given that our probe said there wasn’t, is about 1 in 1,000. That’s a lot of confidence, but if want to be even more sure, we can send down more probes. We’ll see later how each successive probe can increase our confidence about whether there really is or isn’t any life down there.</p>
<p>Let’s switch things up, and suppose that the probe instead came back with a positive report, telling us that it <em>did</em> detect life. That would be a financial loss for us, so we’d like to be sure. How confident can we be that there really is life on that planet? To find that, we just use Bayes’ Rule again, but this time we work out P(L|D), the probability of life given that the probe detected life. Let’s work though the numbers in <a href="#figure4-20" id="figureanchor4-20">Figure 4-20</a>.</p>
<figure>
<img src="Images/F04020.png" alt="F04020" width="348" height="374"/>
<figcaption><p><a id="figure4-20">Figure 4-20</a>: Working out the probability that a planet has life, given that the probe reported it found signs of life </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="101" id="Page_101"/>Wow. If the probe says it found life, we can be about 77 percent confident that there really is life, just from this one probe. This is nowhere near the level of confidence we got from the negative report, but that’s because the probe was designed to have a greater chance of reporting a false positive than a false negative. Since we always want to err on the side of protecting life, these are good results overall.</p>
<p>As we mentioned earlier, we can send more probes to increase our confidence in either result, but we’ll never get to absolute certainty either way. At some point, either on probe 1 or probe 10 or probe 10,000, we’ll need to make a judgment call about whether to mine the planet or not.</p>
<p>Let’s now see how sending more probes can help us increase our confidence.</p>
<h2 id="h1-500723c04-0005">Repeating Bayes’ Rule</h2>
<p class="BodyFirst">In the preceding sections we saw how to use Bayes’ Rule to answer a question of the form <em>“What is the probability that (something1) is true, given that (something2) is true?”</em> We approached this question as a one-time event, plugging in what we know about the system and getting back a probability. </p>
<p>One event, or measurement, is not much to go on. Let’s return to our coin game with the two coins. Recall that one coin is fair, and one is rigged to come up heads more than half the time. We chose one of the two coins at random, flipped it, found that it came up heads, and we produced a probability that we had the fair coin. And that was the end of it.</p>
<p>But we can keep going. In this section let’s put Bayes’ Rule in the heart of a loop, where each new piece of data gives us a new posterior that we then use as the prior for the next observation. Over time, if the data is consistent, the prior should home in on the underlying probabilities we’re looking for.</p>
<p>Here’s the basic intuition, before we get into the details. We usually know the likelihood, P(B|A), and the evidence, P(B), from our experimental setup, so those are settled. But we rarely know the prior, P(A). We need a value for this, so we think about the problem and take our best guess. Since this completes all the values needed by Bayes’ Rule, we can plug them in and get the posterior, P(A|B). </p>
<p>Now comes the interesting step. The posterior tells us the probability of A given that event B happened, but <em>we know event B happened</em>. Whether it’s a coin coming up heads, or a probe finding life on a planet, we know that B happened since we chose to compute P(A|B), rather than, say, the probability of A given that B did not happen. Since we know that B <em>has</em> happened, P(A|B) is just P(A). </p>
<p>Let’s express this with another example to lock it down. Suppose event B is “the day is warm” and event A is “people are wearing sandals.” Suppose that it’s warm today. Then the probability that “people are wearing sandals, given that it’s warm” is equal to just “people are wearing sandals,” because we have already observed that it’s warm.</p>
<p>In other words, the posterior, P(A|B), becomes P(A), which is our prior! That’s the key insight. When we know that B has happened, then the output <span epub:type="pagebreak" title="102" id="Page_102"/>of Bayes’ Rule gives us a new estimate of P(A). So, Bayes’ Rule gives us a way to change and improve our expectations, or beliefs, about the system based on what experiments tell us. </p>
<p>To summarize, we guess at P(A). Then we run an experiment. Critically, we then choose to compute either P(A|B) or P(A|not-B) based on whether we saw event B or not, as we saw in Figures 4-19 and 4-20. This choice of which version of Bayes’ Rule to evaluate is the magic that makes the whole loop work. Our choice of P(A|B) or P(A|not-B), which is determined by which outcome we actually observed, adds new information into our process. That new information helps us refine our understanding of the system we’re learning about. So, having made this choice, we plug the numbers into the appropriate form of Bayes’ Rule, and produce a posterior. That becomes the new prior. With this new P(A), we run another experiment, using Bayes’ Rule again, compute either P(A|B) or P(A|not-B) based on whether B happened or not, and update our expectations again by using that result as our new P(A), or prior, for the next experiment, and so on. Over time, our belief, or expectation, about the probability of A, or P(A), gets gradually refined from a guess to an experimentally supported value. </p>
<p>Let’s package up this description into a loop, starting with a guess for the prior P(A) and then refining by running more experiments. </p>
<h3 id="h2-500723c04-0009">The Posterior-Prior Loop</h3>
<p class="BodyFirst">In <a href="#figure4-15">Figure 4-15</a> we gave names to the terms of Bayes’ Rule. These aren’t the only names that are used. We also refer to the events in Bayes’ Rule (which we’ve been calling A and B) in terms of a hypothesis and an observation (sometimes abbreviated Hyp and Obs). Our <em>hypothesis</em> states something whose truth we want to discover (for example, “we have the fair coin”). The <em>observation</em> is the experimental result (for example, “we got heads”). <a href="#figure4-21" id="figureanchor4-21">Figure 4-21</a> shows Bayes’ Rule with these labels.</p>
<figure>
<img src="Images/F04021.png" alt="F04021" width="794" height="83"/>
<figcaption><p><a id="figure4-21">Figure 4-21</a>: Writing Bayes’ Rule with descriptive labels for A and B</p></figcaption>
</figure>
<p>In our coin-flipping example, our hypothesis is “We have selected the fair coin.” We ran an experiment and got an observation, which was “The coin came up heads.” We combine our prior P(Hyp) with the likelihood P(Obs|Hyp) of the observation given that hypothesis to get the joint probability of both the observation and hypothesis being true. We then scale that by the evidence P(Obs), or the probability that the observation could have come about by any means. The result is the posterior P(Hyp|Obs), which tells us the probability that our hypothesis is true, given the observation.</p>
<p>As promised, let’s now wrap this in a loop. We compute the posterior, and then (because we know that the Observation has happened) we can use that as the prior when we repeat the experiment. The result is a new <span epub:type="pagebreak" title="103" id="Page_103"/>posterior, which we can use as our prior the next time, and so on. Each time around the loop, our prior gets a little more accurate at describing the system, thanks to the inclusion of the results of each previous experiment.</p>
<p>A drawing of this loop is shown in <a href="#figure4-22" id="figureanchor4-22">Figure 4-22</a>.</p>
<figure>
<img src="Images/F04022.png" alt="F04022" width="538" height="288"/>
<figcaption><p><a id="figure4-22">Figure 4-22</a>: Each time we have a new observation, we combine the evidence, the likelihood of that observation, and the prior to compute a posterior. That posterior is then used as the prior when a new observation is evaluated.</p></figcaption>
</figure>
<p>To recap, we start with a prior. This comes from analysis, experience, data, an algorithm, or just guesswork. Then we run an experiment (or make an observation) and start the loop. We combine the evidence, the likelihood of that observation, and the prior to select one of the two forms of Bayes’ Rule, with which we compute a posterior. This posterior becomes our new prior. Now, when another observation arrives, we enter the loop again, using our new prior.</p>
<p>The idea is that each time through the loop, our prior improves, from our initial guess toward a range of highly probable answers. The prior is improving because each time around the loop, the prior incorporates the latest observation, in addition to all the previous observations.</p>
<p>Let’s see this loop in action using our coin-flipping example.</p>
<h3 id="h2-500723c04-0010">The Bayes Loop in Action</h3>
<p class="BodyFirst">Recall our archaeologist friend and her two-coin problem. Let’s generalize it so we can try out a number of variations and explore how to use the Bayes’ Rule loop shown in <a href="#figure4-22">Figure 4-22</a> to answer questions.</p>
<p>Rather than having a single bag with a fair coin and a rigged coin, let’s suppose she found many such bags, where no two rigged coins had the same bias. Each bag is marked with the bias of its rigged coin (the bias is often written with the lowercase Greek θ [theta]). </p>
<p>She thinks that before players started a game, they’d agree on how biased they wanted the rigged coin to be. Then they’d pick the corresponding bag, and proceed as usual, picking out one of the two coins and then betting on which one had been picked.</p>
<p>Like them, we’ll first select a bag, and then a coin from the bag. Then we’ll determine the probability that we picked the fair coin. We can use the <span epub:type="pagebreak" title="104" id="Page_104"/>repeated form of Bayes’ Rule by flipping the coin many times, recording the heads and tails we get, and watching what Bayes’ Rule does with the observation, or result, of each flip when producing the posterior.</p>
<p>Suppose we make 30 flips. Even with so little data, we might see unusual events. For instance, we might have the fair coin and still get 25 heads and 5 tails. It’s very unlikely, but possible. It’s more likely that we’d get those results from a rigged coin with a high bias. Let’s see how Bayes’ Rule helps us decide which coin we have, based on multiple flips.</p>
<p>Let’s start by choosing the bag with a fair coin and a rigged coin with a bias of 0.2, which means we expect it to show 2 heads out of every 10 flips. Suppose we flip this coin 30 times, and only 20 percent (that is, 6) flips come up heads, so the other 24 flips come up tails. Do we have the fair coin or the rigged one? Since we’d expect 15 heads out of 30 flips from the fair coin, and 6 heads out of 30 flips from the rigged coin, getting 6 heads back seems like a good case for us having the rigged coin.</p>
<p><a href="#figure4-23" id="figureanchor4-23">Figure 4-23</a> shows the result of Bayes’ Rule after each flip. As before, the probability that we have the fair coin is shown in flax (or beige), while the probability of the rigged coin is shown in red. The two probabilities always add up to 1.</p>
<figure>
<img src="Images/F04023.png" alt="F04023" width="694" height="385"/>
<figcaption><p><a id="figure4-23">Figure 4-23</a>: The probability that we have the fair coin is shown in flax (beige) through successive flips. The letter below each bar is the observation that produced that bar.</p></figcaption>
</figure>
<p>To understand what this chart is telling us, look first at the letters along the bottom. These are either “H” or “T,” telling us the result of that flip. In this case, we have 24 tails, with 6 heads appearing here and there. Now consider the bars, starting at the left. The leftmost column shows which coin we think we have before we’ve flipped the coin at all, so the probabilities are both 0.5. After all, the chances are equal that we picked either coin, and we haven’t flipped it to gain any data. The bar to the right of that shows the result of the posterior of Bayes’ Rule after observing tails (T) on the first flip. Since the chance of getting tails from the fair coin is 0.5, but the chance <span epub:type="pagebreak" title="105" id="Page_105"/>of tails from the rigged coin is 0.8, getting tails suggests that it’s more likely we have the rigged coin. Continuing to the right, about 80 percent of the flips are tails. That’s what we’d expect from the rigged coin, so its probability quickly approaches 1. Note that the probability of our having the rigged coin dips about 2/3 of the way through the run when we get a bunch of heads close to one another, but then it goes back up with each new tail.</p>
<p>The height of the flax, or beige, block in each bar is the value of P(F), the probability that we selected the fair coin. After each flip, we use Bayes’ Rule to compute either P(F|H) or P(F|T) as appropriate. That becomes the new value of P(F), or our belief that we have the fair coin. We use that to compute the new posterior after the next flip. As we mentioned earlier, this choice is the key step that makes the whole loop work. After each experiment, we choose the version of Bayes’ Rule that returns P(F|H) or F(F|T) depending on what we observe. That choice is what enables us to use the posterior as a new prior, because it reflects what actually happened.</p>
<p>Toward the end, the probability that we have the fair coin is nearly 0. It never gets to exactly 0, because we can never be absolutely sure that this isn’t a fair coin with a wildly unusual flip pattern, so that option is always has at least a shred of probability.</p>
<p>In this example, the data we got back pretty clearly revealed that we had the rigged coin. Let’s keep this coin and do another run. Suppose we get even fewer heads on the next run, perhaps only three in total, making the case for the rigged coin even stronger. Running this through the loop produces the results in <a href="#figure4-24" id="figureanchor4-24">Figure 4-24</a>.</p>
<figure>
<img src="Images/F04024.png" alt="F04024" width="694" height="385"/>
<figcaption><p><a id="figure4-24">Figure 4-24</a>: We use the same coin with a bias of 0.2, but this time we happened to get only three heads in our 30 flips.</p></figcaption>
</figure>
<p>We get to about 90 percent confidence that we’ve got the rigged coin after just four flips. After 30 flips, the probability of a fair coin is again almost, but never quite, 0.</p>
<p>Suppose we do another run of 30 flips, and this time we happen to get 24 heads. This doesn’t match either coin very well. We’d expect the fair <span epub:type="pagebreak" title="106" id="Page_106"/>coin to give us 15 heads, but we’d expect only 6 heads from the rigged coin. Given just these two choices, the fair coin seems more likely. <a href="#figure4-25" id="figureanchor4-25">Figure 4-25</a> shows our results from Bayes’ Rule.</p>
<figure>
<img src="Images/F04025.png" alt="F04025" width="694" height="385"/>
<figcaption><p><a id="figure4-25">Figure 4-25</a>: We use the same coin with a bias of 0.2 but this time we happened to get 24 heads in our 30 flips.</p></figcaption>
</figure>
<p>Even though the fair coin should only come up heads about half the time, the rigged coin would come up heads only 20 percent of the time. All of those heads are unlikely from either coin, but they’re a lot <em>more </em>unlikely from the rigged coin, bolstering our confidence that we’ve got the fair coin.</p>
<p>We’ve seen three different flipping results for this coin, from a pattern of almost all tails to a pattern of almost all heads. Let’s generalize these results by flipping 10 coins with different biases. Let’s create 10 different flip patterns for each coin, with different ratios of heads to tails. We can use Bayes’ Rule on each pattern for each coin, creating 100 scenarios. The results are in <a href="#figure4-26" id="figureanchor4-26">Figure 4-26</a>, where each cell is a little bar chart like those in <a href="#figure4-23">Figure 4-23</a> through <a href="#figure4-25">Figure 4-25</a>. </p>
<p>Let’s start in the bottom left corner. At this location our value on the horizontal axis (labeled “Rigged coin bias”) is around 0.05. That means we’d expect this coin to come up heads about 1 time in 20. Our value on the vertical axis (labeled “Flip sequence bias”) is also about 0.05. This means we’re going to create an artificial sequence of observations, like we did earlier, where there’s a 1 in 20 chance that each observation will be heads. In this case, the number of heads in the pattern of 30 observations we created for this cell in the grid matches the number of heads we’d expect from the rigged coin, so our confidence that the coin is rigged (in red) grows quickly.</p>
<p>Let’s move up three cells. Since we haven’t moved horizontally, our horizontal axis value is still 0.05, so we’re flipping a coin that should come up heads 1 time in 20. But now the vertical axis is about 0.35, so we’re looking at a pattern where heads are significantly more frequent. With all of those <span epub:type="pagebreak" title="107" id="Page_107"/>heads, it seems more likely that we’re getting an unusual series of flips from a fair coin, than a <em>very</em> unusual series of flips from the rigged coin. Our confidence that the coin is fair grows stronger as the number of flips increases.</p>
<figure>
<img src="Images/F04026.png" alt="F04026" width="694" height="693"/>
<figcaption><p><a id="figure4-26">Figure 4-26</a>: Flipping a coin 30 times, and using repeated Bayes’ Rule applications to decide which coin we’re flipping. Each square reports our results for just one run of 30 random flips. Each row uses the same sequence of heads and tails.</p></figcaption>
</figure>
<p>Each cell can be understood in the same way. We invent a pattern of 30 heads and tails, where the relative proportion of heads is given by the vertical location, and we ask whether that pattern is more likely to come from a fair coin, or a coin with a probability of heads given by its horizontal location.</p>
<p>In the middle of the grid, where both values are close to 0.5, it’s almost impossible to tell. The rigged coin comes up heads almost as often as the fair coin, and the patterns of heads and tails are about evenly split, so we could be flipping either coin. The probabilities for both stick to around 0.5. But as we make patterns with fewer heads (the lower part of the figure) or more heads (the upper part), we can say how well that pattern matches <span epub:type="pagebreak" title="108" id="Page_108"/>a rigged coin with a low probability of coming up heads (the left side) or a high probability (the right side).</p>
<p>A series of 30 flips is revealing, but we can still get unusual surprises (like 25 heads from a fair coin). If we increase the number of flips to 1,000 in each plot, as in <a href="#figure4-27" id="figureanchor4-27">Figure 4-27</a>, such unusual sequences become rarer, and the patterns become clearer.</p>
<figure>
<img src="Images/F04027.png" alt="F04027" width="694" height="693"/>
<figcaption><p><a id="figure4-27">Figure 4-27</a>: This chart has the same setup as <a href="#figure4-26">Figure 4-26</a>, but now we flip each coin 1,000 times.</p></figcaption>
</figure>
<p>In the lower-left and upper right, the pattern of flips more closely matches the bias of the rigged coin than the fair coin, and Bayes’ Rule pushes the prior of “we selected the fair coin” toward 0. In the upper left and lower right, the flips more closely match the fair coin, and the prior moves toward 1. </p>
<p>The general lesson of <a href="#figure4-26">Figure 4-26</a> and <a href="#figure4-27">Figure 4-27</a> is that the more observations we make, the more certain we become that our hypothesis is either true or false. Each observation either increases or decreases our confidence. When the observations match up with our prior (“we have the fair <span epub:type="pagebreak" title="109" id="Page_109"/>coin”), our confidence in that prior grows. When the observations contradict that prior, our confidence decreases, and since there’s only one other alternative in this scenario (“we have the rigged coin”), that becomes more probable. Even when we have only a few observations, we can often gain a great deal of confidence early on.</p>
<h2 id="h1-500723c04-0006">Multiple Hypotheses</h2>
<p class="BodyFirst">We’ve seen how to use Bayes’ Rule to improve a hypothesis by combining it with an observation, perhaps repeatedly. But there’s nothing limiting us to testing just a single hypothesis. We’ve been making multiple hypotheses all along, actually. In just the last section, we explicitly saw the two hypotheses “this coin is fair” and “this coin is rigged” being updated simultaneously. Since we knew the two probabilities had to add up to 1, knowing either one of them revealed the other, so we only had to keep track of one of them.</p>
<p>But we could explicitly calculate both probabilities if we wanted. We’d just use two copies of Bayes’ Rule. Suppose that a flip comes up heads. Then we can independently compute the conditional probability that we have a fair coin, P(F|H), and the conditional probability that we have a rigged coin, P(R|H). This is shown in <a href="#figure4-28" id="figureanchor4-28">Figure 4-28</a>.</p>
<figure>
<img src="Images/F04028.png" alt="F04028" width="450" height="308"/>
<figcaption><p><a id="figure4-28">Figure 4-28</a>: Calculating probabilities for two hypotheses. Line (c) explicitly shows how to compute P(H) in both cases. </p></figcaption>
</figure>
<p>From <a href="#figure4-28">Figure 4-28</a>, we can see that the probabilities of the fair coin and the rigged coin are just their share of the two ways the coin can come up heads. If we want to track multiple hypotheses at once, we can use multiple copies of Bayes’ Rule in this way, updating them all after each new observation. </p>
<p>We can use this ability to help out our archaeologist friend again. She’s just found a chest with pieces for a new game, and again, the game uses bags to hold pairs of coins. Like before, the rigged coins in the bags have different biases. Because an extremely biased coin (that is, one that comes <span epub:type="pagebreak" title="110" id="Page_110"/>up heads much more often than tails, or vice versa) would be easier to spot, she thinks maybe there were levels of players, from newcomers to old hands. New players would play with coins that were highly biased, but as players became more skilled, they’d switch to rigged coins whose bias was closer and closer to 0.5. Those would be harder to detect, leading to longer games with riskier and more complicated betting.</p>
<p>Since our friend wants to know all about her discovery, she emptied out all the coins into one big box, and has asked us to find the bias of each coin. For the moment, let’s suppose that there are only five possible bias values—0, 0.25, 0.5, 0.75, and 1 (recall that a bias of 0.5 corresponds to a fair coin)—so we’ll create five hypotheses. We’ll number them 0 through 4, corresponding to the different bias values. Hypothesis 0 states, “This is the coin with bias 0,” Hypothesis 1 states, “This is the coin with bias 0.25,” and so on, up to Hypothesis 4, which states, “This is the coin with bias 1.”</p>
<p>Now we’ll pick up a coin at random from the box, flip it repeatedly, and try to determine which of these hypotheses is most likely. To get started, we need to cook up a prior for each hypothesis. Remember that this is going to get updated every time through the loop, so we only need a good starting guess. Since we don’t know anything about the coin we’ve selected, let’s say the chance of having picked each one is the same, so all five prior values have a 1 in 5 chance of being right, or 1 / 5 = 0.2.</p>
<p>Note that we could get fancier if we wanted. Each pair of coins has one that’s fair and one that’s rigged. Say we have 16 coins. If this is the case, then we have 8 fair coins, and 8 rigged coins (2 for each allowed value of the bias). The chance of picking a fair coin is then 8 / 16 = 0.5, whereas the chance of picking each rigged coin is 2 / 16 = 0.125. This is probably a better prior, since it uses more of the information we already know. Starting out with a better prior means that our loop will home in on the high-probability solutions more quickly. But one of the beauties of the Bayesian approach is that we can start with almost any prior that’s even roughly close, and, ultimately, we get the same results. For simplicity, let’s use the first prior, where every hypothesis has a value of 0.2. </p>
<p>The only thing left for us to specify is the likelihood for each coin. But we’ve already got those, because they <em>are </em>the bias. That is, if the coin has a bias of 0.2, then its likelihood of coming up heads is 0.2. That means the likelihood of tails is 1 − 0.2 = 0.8.</p>
<p>Hypothesis 0, which says, “We have the coin with bias 0.0,” has a likelihood of getting heads of 0, and tails of 1. Hypothesis 1, which says, “We have the coin with bias 0.2,” has a likelihood of getting heads of 0.2 (or 20 percent), and a likelihood of tails of 0.8 (or 80 percent). Our likelihoods are plotted in <a href="#figure4-29" id="figureanchor4-29">Figure 4-29</a>. </p>
<p>Since the coins themselves don’t change as we flip them and gather observations, the likelihoods don’t change, either. We’ll reuse these same likelihoods over and over again, each time we evaluate Bayes’ Rule after getting a new observation.</p>
<span epub:type="pagebreak" title="111" id="Page_111"/><figure>
<img src="Images/F04029.png" alt="F04029" width="694" height="367"/>
<figcaption><p><a id="figure4-29">Figure 4-29</a>: The likelihoods for getting heads or tails for each of our five hypotheses</p></figcaption>
</figure>
<p>Our goal is to flip our coin over and over and watch what happens to our five priors as they evolve. To show what’s happening at each flip, let’s draw the five prior values in red, and the five posterior values in blue. In <a href="#figure4-30" id="figureanchor4-30">Figure 4-30</a> we show the result of our first flip, which we’ll suppose came up heads. The five red bars, representing the prior for each of our five hypotheses, are all at 0.2. Since the coin came up heads, we multiply each prior by its corresponding likelihood from the left side of <a href="#figure4-29">Figure 4-29</a>, which gives us our likelihoods. After dividing by the sum of all five probabilities for getting heads, we get the posterior, or the output of Bayes’ Rule, shown in blue.</p>
<figure>
<img src="Images/F04030.png" alt="F04030" width="374" height="438"/>
<figcaption><p><a id="figure4-30">Figure 4-30</a>: We’re testing five hypotheses, which assert that our coin has a bias of 0, 0.25, 0.5, 0.75, and 1.0. We start with a prior of 0.2 (in red) for each hypothesis. After one flip of the coin, which came up heads, we compute the posterior (in blue).</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="112" id="Page_112"/>In <a href="#figure4-30">Figure 4-30</a>, each pair of bars shows the prior and posterior value for a single hypothesis. Right away we’ve ruled out Hypothesis 0, because that says that the coin never comes up heads, and we just got heads. The hypothesis that says this coin always comes up heads is the strongest so far, because we just saw heads.</p>
<p>Let’s now make a series of flips. We’ll use a list of 100 flips that contain 30 percent heads. That is, the flips correspond to the results we’d get from a coin with a bias of 0.3. None of our five hypotheses matches this exactly, but Hypothesis 1 comes the closest, representing a coin with bias 0.25. Let’s see how Bayes’ Rule performs. <a href="#figure4-31" id="figureanchor4-31">Figure 4-31</a> shows the results for the first 10 flips in the top two rows, and then takes bigger jumps in the bottom row.</p>
<figure>
<img src="Images/F04031.png" alt="F04031" width="694" height="406"/>
<figcaption><p><a id="figure4-31">Figure 4-31</a>: The evolution of our priors (red) and posteriors (blue) in response to a series of flips generated by a coin with bias 0.3. The number of the flip that was just evaluated is shown at the top of each graph.</p></figcaption>
</figure>
<p>As we can see in the first two lines, the posterior computed after each flip (in blue) becomes the prior for the next flip (in red). We can also see that after the first flip (heads), Hypothesis 0 at the far left dropped to a likelihood of 0, because that hypothesis was that we had a coin that would never come up heads. Then on the second flip, which happened to be tails, Hypothesis 4 went to 0, because that corresponded to a coin that always came up heads. That leaves just three hypotheses.</p>
<p>We can see how the three remaining options trade off the probabilities with each flip. As more flips come along, the number of heads comes closer to 30 percent, and Hypothesis 1 dominates. When we’ve reached 100 flips, the system has pretty much decided that Hypothesis 1 is the best one available, meaning that our coin is more likely to have a bias of 0.25 than any of the other choices.</p>
<p>If we can test 5 hypotheses, we can test 500. <a href="#figure4-32" id="figureanchor4-32">Figure 4-32</a> shows 500 hypotheses, each corresponding to a bias equally spaced from 0 to 1. We’ve <span epub:type="pagebreak" title="113" id="Page_113"/>added a fourth row showing many more flips. We’ve eliminated the vertical bars in these charts so we can more clearly see the values of all 500 hypotheses.</p>
<figure>
<img src="Images/F04032.png" alt="F04032" width="694" height="550"/>
<figcaption><p><a id="figure4-32">Figure 4-32</a>: The same situation as <a href="#figure4-31">Figure 4-31</a>, but now we’re evaluating 500 simultaneous hypotheses, each based on a coin with a slightly different bias </p></figcaption>
</figure>
<p>In this figure (as in the figures to come), we’re reusing the identical series of flip results that we used in <a href="#figure4-31">Figure 4-31</a>. And as we’d expect, the winning hypothesis is the one predicting a bias of about 0.3. But another interesting thing is happening here: the posteriors are taking on the form of a Gaussian. Recall from Chapter 2 that a Gaussian curve is the famous bell curve that’s flat except for a symmetrical bump. This is a typical feature for the priors that evolve from the mathematics of Bayes’ Rule. It’s just another of the many places in statistics and probability where a Gaussian curve often emerges from the data.</p>
<p>Notice that, as we said at the start of the chapter, Bayesian reasoning doesn’t zero in on one correct answer. Rather, it gradually assigns more probability to a smaller range of answers. The thinking is that any value in that range has its own probability of being the answer we seek. </p>
<p>If Bayes’ Rule seems to evolve so that the prior takes on the shape of a Gaussian, what would be the result if we <em>started </em>with priors that formed a Gaussian? Let’s do just that, but let’s make it even harder for the system by putting the mean of the prior’s bump (that is, its center) out at around 0.8. That says that our belief is that the coin we’re testing is most likely to have a bias of 0.8. This is far away from the value of 0.3 that we baked into our <span epub:type="pagebreak" title="114" id="Page_114"/>sequence of heads and tails. The probability at 0.3 that describes our coin starts out with a value of merely 0.004, so we’re asserting, through our prior, that the chance of this coin having a bias of 0.3 is 0.4 percent, or 4 out of 1,000. How does the system respond to a prior that is so wrong, that the correct answer has only this very slim chance?</p>
<p><a href="#figure4-33" id="figureanchor4-33">Figure 4-33</a> shows the result.</p>
<figure>
<img src="Images/F04033.png" alt="F04033" width="694" height="550"/>
<figcaption><p><a id="figure4-33">Figure 4-33</a>: This figure has the same setup as <a href="#figure4-32">Figure 4-32</a>, only now we’re starting with a set of priors formed in a Gaussian bump centered at 0.8.</p></figcaption>
</figure>
<p>Nice. Even with our poorly chosen prior, the system homed in on the proper bias of 0.3. It took a while, but it got there. </p>
<p>In <a href="#figure4-34" id="figureanchor4-34">Figure 4-34</a> we show the priors of <a href="#figure4-33">Figure 4-33</a> at 10 steps along the way in evaluating the first 3,000 flips, laid on top of one another rather than in sequence.   </p>
<p>Notice that we start out with a broad prior with a mean of 0.8, but as we flip more and gather more observations, the prior’s mean moves toward 0.3. The width of the bump also narrows, telling us the system is deciding that bias values far from the mean are less likely. The number of flips for each curve were chosen by hand so that the curves are spaced out roughly equally. Notice that as the system grows confident, by producing a narrow prior, the curve changes more slowly. In other words, the more certain the results, the more observations we need to make a big change to the posterior.</p>
<span epub:type="pagebreak" title="115" id="Page_115"/><figure>
<img src="Images/F04034.png" alt="F04034" width="694" height="472"/>
<figcaption><p><a id="figure4-34">Figure 4-34</a>: Some snapshots of the posteriors from the first 3,000 flips from <a href="#figure4-33">Figure 4-33</a>, overlaid on one another. The different colors show how many flips have elapsed, as given by the legend in the upper right. We can see the system giving more and more weight to the priors near 0.3, while reducing the probabilities elsewhere. The heights change in order to keep the area under each curve at 1.0.</p></figcaption>
</figure>
<p>We won’t get into the details, but with some math, we can carry our increasing number of possible biases (and thus the number of hypotheses) to its logical extreme, replacing our lists of values with continuous curves, like those suggested by <a href="#figure4-34">Figure 4-34</a>. The advantage is that we can then get as precise as we like, finding a bias for any value rather than just the closest one in a list.</p>
<h2 id="h1-500723c04-0007">Summary</h2>
<p class="BodyFirst">There are two broad camps in the field of probability: the frequentists and the Bayesians. The frequentist approach imagines that anything we choose to measure has an accurate, or true value. Each measurement is therefore only an approximation of that value. The Bayesian approach says that there is no single true value, only a range of possible values, each with its own probability. Each measurement is an accurate measure of something, but perhaps not what we want to measure.   </p>
<p>We spent most of this chapter working with the Bayesian approach. Bayesian probability is popular in deep learning, because it’s well suited to the nature of the kinds of problems we face and the kinds of questions we want to answer. The language of Bayesian probability is found in many of the papers, books, and documents of deep learning systems. At its core, it <span epub:type="pagebreak" title="116" id="Page_116"/>presents us with a set of tools for describing a measurement, not by looking for a single true number, but by finding a range of possible values for that measurement, each with its own probability.  </p>
<p>For example, if a deep learning system is helping someone write a text message by offering shortcuts for the next word, it usually shows several high-probability guesses, rather than a single best next word.</p>
<p>In the next chapter, we’ll look at some of the properties of curves and surfaces, which we’ll use to understand the types of errors our learning systems can make (and later, how to correct those errors).</p>
</section>
</div></body></html>