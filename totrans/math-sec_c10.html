<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
<head>
<title>Chapter 10: Computational Geometry for Facial Recognition</title>
<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:1ff3c234-c763-4a12-a0c7-4ddf7c732e40" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter chapter">
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_175" title="175"/>10</span><br/>
<span class="ChapterTitle">Computational Geometry for Facial Recognition</span></h1>
</header>
<figure class="opener">
<img alt="" src="image_fi/book_art/chapterart.png"/>
</figure>
<p class="ChapterIntro">Let’s leave behind the world of resource distribution and look at another field where computational geometry can help you: facial recognition. <em>Facial recognition</em> is the process of examining the features of a face and determining if it matches a previously seen face and, if so, to what degree. Most babies can recognize familiar faces by three to four months old, but unfortunately, just because babies can do it doesn’t mean it’s easy, at least not for computers. We’ll need to combine computational geometry and machine learning algorithms for our program to perform at a similar level.</p>
<p>We’ll begin this chapter with a brief look at facial recognition. We’ll then cover the main algorithm, and, as usual, put it to work in the proof of concept, which will cover loading, cleaning, and modeling data stored in a database of facial images. We’ll process each image in the database to extract the most important facial features for determining which person is <span epub:type="pagebreak" id="Page_176" title="176"/>in an image, and then we’ll use this information to build a model that can match a never-before-seen image of a face to the appropriate person. </p>
<p>To achieve this we’ll be relying on various machine learning (ML) tools. ML algorithms seek to identify or “learn” the relationship between input data and output values using two broad categories of solutions: unsupervised learning and supervised learning. </p>
<p>In <em>supervised learning</em> we provide the algorithm a set of input data along with the proper output we’d like the program to learn for that input (called its <em>class</em>). In our case, the input data will be a face’s geometry and the class we want to predict is the associated person’s name, which makes this a <em>discrete classification</em> problem. If the class value we wanted to predict were a continuous number (such as the price of a house), this would be called a <em>regression</em> problem (you may already be familiar with linear regression from statistics class). </p>
<p><em>Unsupervised learning</em> seeks to discover previously unknown relationships and largely deals with clustering data together based on different inputs to find interesting groupings. Because we don’t know the goal going in, unsupervised learning is also sometimes called <em>exploratory analysis</em>. We won’t be doing much unsupervised learning in this project, so I’ll leave it to you to dive into this topic more on your own. By the time you complete this chapter, you should feel comfortable working with image data, extracting geometric facial characteristics, and training supervised learning classifiers for your own facial recognition projects.</p>
<h2 id="h1-502567c10-0001">Uses of Facial Recognition in Security</h2>
<p class="BodyFirst">Today it isn’t hard to find examples of facial recognition being used in security. Facial recognition systems have become a widespread and generally accepted part of life. Industries that are already benefiting from the technology include retail stores, casinos, cell phone manufacturers, and law enforcement agencies. It wasn’t always this way, though. The core of the technology has been slowly progressing since the 1960s, when Woodrow Wilson Bledsoe invented a way for people to manually encode the geometry of a face using an electronic surface and a conductive stylus. Using the stylus, users would mark a set of standardized facial features like the bridge of the nose, eyebrows, and chin. The program would then measure the geometry between these points and shapes and create a geometric map of the input face. The data was then compared against a database of previously recorded traces to produce the name of the person the face was most closely matched to. Of course, this method was time consuming and prone to operator error. For the next 40 or so years, researchers kept improving on the algorithms by defining more standard points to measure and using image analysis techniques to automatically identify these key points in a picture of a face. </p>
<p>It wasn’t until the turn of the century that facial recognition started to move from science fiction and research labs to reality. One highly publicized case happened in January 2001, when the city of Tampa, Florida, used a facial recognition system to record and analyze the face of every attendee during Super Bowl XXXV in the hope of spotting criminals with warrants <span epub:type="pagebreak" id="Page_177" title="177"/>issued for their arrest. The program is credited with identifying a few petty criminals in attendance, but is largely considered a failure due to the high cost and large number of false positives. Worse, it prompted a large backlash from privacy advocacy groups including the Electronic Frontier Foundation and the American Civil Liberties Union.<sup class="endnote"><a href="b01.xhtml#c10-endnote-001" id="c10-noteref-001">1</a></sup> </p>
<p>The negative publicity did little to slow down the growth of the technology, though. Florida once again made the news for being one of the first states to adopt facial recognition technology as an accepted tool for police. In 2009, the Pinellas County Sheriff’s Office announced a program that allows officers to tap into the photo archives of the Florida Department of Highway Safety and Motor Vehicles. Within two years an estimated 170 deputies had been outfitted with cameras that could be immediately cross-checked against the faces in the database. Since then, the availability of cheap processing power, shrinking data storage costs, and the large number of facial data sets have allowed the technology to move from government programs into the security programmer’s toolbox. We’ll discuss more about the privacy and ethical concerns in the next section, “<span class="xref" itemid="xref_target_Ethics of Facial Recognition Research"><a href="#h1-502567c10-0002">Ethics of Facial Recognition Research</a></span>.” </p>
<p>Now you can build an effective facial recognition system for the cost of a decent camera (the $20 to $40 Raspberry Pi camera modules work great for this) and some cloud processing costs. I’m going to stay platform-agnostic with the concepts, but every major cloud service provider has some offering that allows you to translate the code we’ll write into a distributed scalable version (we’ll discuss cloud deployments more in <span class="xref" itemid="xref_target_Chapter 13"><a href="c13.xhtml">Chapter 13</a></span>).</p>
<p>The hardest part is collecting the database of images. For a good facial recognition data set, you need multiple pictures of the same person under different lighting conditions and with different facial poses. The features of the faces need to be distinguishable, so contrast is also important. We’ll discuss more about image quality in the section “<span class="xref" itemid="xref_target_Processing Image Data"><a href="#h2-502567c10-0003">Processing Image Data</a></span>” <span class="xref" itemid="xref_target_later in the chapter">later in the chapter</span>. For now, the takeaway is that the quality of the images in the data set will have a drastic impact on an ML algorithm’s ability to distinguish between faces. Old, blurry, grainy photos with low contrast will make the process much harder, if not impossible. </p>
<p>We’ll be using a facial recognition data set published by the computer science and engineering department at the University of Essex.<sup class="endnote"><a href="b01.xhtml#c10-endnote-002" id="c10-noteref-002">2</a></sup> One section of images in the data (labeled <em>faces94</em>) is fairly stationary. Researchers had the subjects sit at a fixed distance from the camera and asked them to speak while a sequence of images was taken. The speech introduces facial expression variation, which allows the underlying classification algorithm to understand how the facial shape changes for an individual and gives it a better chance of properly classifying an input image, even if the face is in a pose the algorithm hasn’t seen previously. The second part of the data set (labeled <em>faces95</em>) is more dynamic and introduces variation in scale, perspective, and lighting by asking the subject to take a step toward the camera as a set of 20 images was taken with a fixed-placement camera. The movement forward causes the head to appear larger in the later photos. It also changes the cast shadow and highlights on the face. Finally, the background for these images is a red curtain, which also introduces a degree <span epub:type="pagebreak" id="Page_178" title="178"/>of difficulty because the imperfect surface can make it challenging for the algorithm to detect the edges of features. Being able to properly classify faces despite all this variation will allow your program to operate more reliably in the wild, where you may not always be able to get a clean, stable image with a solid, still background.</p>
<h2 id="h1-502567c10-0002">Ethics of Facial Recognition Research</h2>
<p class="BodyFirst">Before we move on, let’s talk about the ethics of facial recognition research. There are many potential uses for, and abuses of, being able to automatically identify a person based on an image of their face. Facial recognition software can be used by first responders to identify victims after a disaster, or it can be used by dictators to identify political activists. As an analyst and developer, you’ll have to be very cautious when dealing with facial recognition projects in the wild. You never really know whose hands your software may end up in. </p>
<p>To start, you should become familiar with all the privacy laws that might apply to your situation. International regulations like the European Union’s General Data Protection Regulation consider biometric data such as facial analysis models <em>personally identifiable information</em>, which requires stricter security controls around its collection, processing, and storage. Facial recognition has even been banned from use by police forces in some US cities, like Boston, Massachusetts, due to the high error rate and potential for serious repercussions in the event of a mistaken identity. Knowing what laws and regulations apply to your project will help you navigate the other ethical questions more easily. The best way to avoid ethical and legal troubles when it comes to privacy is to gain informed consent from the people being included in the data. I strongly urge you to decline any project where you aren’t able or allowed to collect informed consent.</p>
<p>Aside from outright abuses, there are other ethical issues that are harder to spot. Racial and ethnic biases remain a key concern of developing facial recognition models. Facial recognition algorithms often achieve mean classification accuracy over 90 percent, but researchers have shown that this error rate doesn’t apply equally across all demographic groups. Several independent tests found the poorest accuracy for facial recognition was consistently for dark-skinned black females between the ages of 18 and 30. These unintentional biases are the result of technological and social choices made by the data collectors. Decisions like the type of camera lens and the locations where data was collected all have a subtle but definite impact on the overall representation of the population in the data. </p>
<p>I’ve included this facial recognition project here, despite these ethical concerns, because I think it presents an excellent learning opportunity. We’ll be using publicly available data that was gathered with the consent of the individuals and the knowledge that the images would be used for research like ours, which is perfectly ethical. The data set I selected is relatively small and represents a limited number of demographics. This could lead us to make falsely optimistic performance predictions, so we wouldn’t <span epub:type="pagebreak" id="Page_179" title="179"/>want to use it for developing any type of production system. It will serve as a good starting point, though. We can use it to illustrate the workflow and even test some parts of the algorithm. When you’re ready to develop a facial recognition system for a real project, you’ll have the knowledge and tools to collect a truly great data set that accurately reflects the diversity of humankind.</p>
<h2 id="h1-502567c10-0003">The Facial Recognition Algorithm</h2>
<p class="BodyFirst">Despite the fact that we’re using ML algorithms, the core of the facial recognition process remains remarkably similar to the one Bledsoe created more than 50 years ago. We’ll use a set of 68 facial points to create geometric maps for a database of faces, then use those maps to train an ML algorithm to compare an input face with previously seen faces and predict the closest match. Generally speaking, facial recognition is a <em>computer vision</em> problem: it deals with teaching computers to recognize information encoded in visual data, like pictures and video. This also falls into the broader category of <em>multiclass classification</em> problems, where the class to be predicted is from a set of three or more potential classifications. When a multiclass algorithm runs, it compares the input to the data recorded for each class and determines which class the input is most likely to belong to. We’re going to treat each individual person in the data set as a class we’re interested in learning to predict, and the input will be an image containing the face of a previously analyzed person. The algorithm we’ll use is a supervised learning algorithm; again, this means that when we train our algorithm, or model, it will have access to a list of the correct classifications, which it will use to correct previous mistakes and improve future predictions.</p>
<p>There’s a large number of potential classes (222 unique individuals in the final analysis) and a relatively small sample size for each class (approximately 20 images per individual), making our goal even more difficult. To counter this, we’ll collect a huge number of statistics for each image and let the algorithm decide which subset of these measurements allows for the best decision-making power. </p>
<h3 id="h2-502567c10-0001">Using Decision Tree Classifiers</h3>
<p class="BodyFirst">The classification is handled by an algorithm called a <em>random forest classifier</em>, which is an expanded version of a <em>decision tree classifier</em>. There are many benefits to decision tree algorithms: they are fast to train, can produce a human-readable model, and perform well for multiclass problems like facial recognition. Let’s examine a classic example as a way of illustrating how they work. Suppose we want to write a program that predicts if a person is likely to go out and play a round of golf on a given day, based on the weather. A decision tree would be a great choice for this type of problem because it will generate a list of rules we can use to examine the weather on any given day. Consider the decision tree in <a href="#figure10-1" id="figureanchor10-1">Figure 10-1</a>.</p>
<span epub:type="pagebreak" id="Page_180" title="180"/><figure>
<img alt="" class="" src="image_fi/502567c10/f10001.png"/>
<figcaption><p><a id="figure10-1">Figure 10-1</a>: An example of a golf decision tree</p></figcaption>
</figure>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	The code to generate <a href="#figure10-1">Figure 10-1</a> is in the <em>golf_decision_tree.ipynb</em> notebook. </p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>You can see in <a href="#figure10-1">Figure 10-1</a> that each branch represents a Boolean decision in the data (such as <code>Outlook_overcast &lt;= 0.5</code>). To read the tree, you start at the topmost box (called the <em>root node</em>) and follow the proper logical branches until you reach the bottommost box (called a <em>leaf</em> or <em>leaves</em>). Some statistics about the underlying data that generated the decision are listed below the Boolean decision. Each row (called an <em>instance</em> in data science parlance) is sent through the decision tree one at a time until it ends up at one of the leaves; along the way, the algorithm records how the data influenced the growth of the tree. The simplest statistic is <code>samples</code>. This is the total number of rows that reached this point in the decision tree during creation. </p>
<p>In <a href="#figure10-1">Figure 10-1</a> you can see that the root node received 14 samples of data. Since this is the root node, it processed every row in the data set, so this can also be interpreted as a summary of the data at the start of the algorithm. You can see the count of each class in the <code>values</code> statistic. For each possible class in the data, there’s an integer representing the number of rows within that particular class. In our example there are two potential <span epub:type="pagebreak" id="Page_181" title="181"/>classes, <code>Not_Play</code> and <code>Play</code>. Looking at the root node again, you can see the values <code>[5,9]</code>, which means five of those samples belonged to the <code>Not_Play</code> class and nine belonged to the <code>Play</code> class.</p>
<p>The <code>gini</code> statistic contains the <em>Gini impurity</em> coefficient, which you can think of as a measure of the distribution of classes that reach that particular node, called the <em>purity</em> of the node. Formally, the Gini impurity coefficient can be written as</p>
<figure class="graphic equation">
<img alt="" class="" src="image_fi/502567c10/m10001.png"/></figure>

<p class="BodyContinued">where <em>n</em> is the number of classes in the data and <em>p</em><sub><em>i</em></sub> is the probability of an instance being classified to the <em>i</em>th class. The resulting score for a node can range between 0 and 1. If all the instances in a node belong to a single class, then it is completely pure and will receive a Gini score of 0. A score of 1 means that the instance classes are randomly distributed and there’s no predictability. Scores between these two extremes denote some level of class purity, with lower scores being purer (and therefore better for decision-making purposes) than higher ones. The goal is to find pure leaf nodes that contain only one class of data (a Gini score of 0). That would mean the logic that led to that leaf was capable of making a perfect decision between classes. </p>
<p>To see an example of this, follow the <code>False</code> branch from the root node (down and to the right in <a href="#figure10-1">Figure 10-1</a>). You can tell this is a leaf node because there’s no Boolean expression at the top of the box and there are no branches extending from it. The <code>class</code> statistic shows the majority class for each node; when dealing with a leaf node like this, we can think of it as the probable class for data that reaches that point. With all that in mind, we can interpret this branch logically as “if the weather is overcast, predict the <code>Play</code> class” since we’re dealing with Boolean values (0 or 1) and the decision criteria is <code>Outlook_overcast &gt; 0.5</code>. The <code>samples</code> and count and the <code>values</code> statistic show that four samples reached this leaf node, all of which were of the <code>Play</code> class. </p>
<p>In <a href="#figure10-1">Figure 10-1</a>, none of the leaf nodes have more than a single class in them, making this a perfectly pure tree. Of course, more often than not, this isn’t the case, and one or two stragglers from other classes (called <em>outliers</em>) may show up in a leaf where the majority is of another class. In these cases, you can try to find additional data splits that would improve the purity of each leaf, but at some point you’ll have to accept the performance as “good enough” since it’s unlikely you’ll find a perfect split.</p>
<p>In the context of our facial recognition problem, we’ll classify an input face by converting it to geometric information and then sending that information through the decision tree until it lands at a leaf node, which will use the majority class to predict the likely subject who matches the input face. Although the Boolean decisions the algorithm makes are more complicated than <code>Outlook_overcast &gt; 0.5</code>, the principle remains the same. </p>
<p>The problem with a traditional decision tree is that it’s susceptible to the data’s initial conditions and configuration because it processes the samples in order. Rerunning the same decision tree algorithm on a shuffled version of the same data is likely to produce a significantly different tree <span epub:type="pagebreak" id="Page_182" title="182"/>each time. This means if you plan to use a decision tree, you’ll need to train several versions with different mixtures of data to make sure the performance is repeatable (in this context, <em>performance</em> refers to the accuracy of predicting each class). This led researchers to design random forests. A random forest algorithm repeatedly creates individual decision trees with semi-randomized starting data (called <em>bagging</em>). To classify a new data sample, the instance is sent down each generated tree and the resulting class predictions are tallied. Finally, the majority class from all the trees’ guesses is predicted as the most likely classification. </p>
<p>Having a large number of decision trees generated from different data mixtures will help ensure that the overall prediction is less susceptible to the starting condition of any given tree. We’ll discuss the random forest more once we start building the model in the proof of concept, but before we get there, we need to cover exactly how we’re going to collect the data we need. Let’s turn to converting a picture of a face into a set of geometric data. In the next section, we’ll discuss how to find important facial features and convert them into numeric representations.</p>
<h3 id="h2-502567c10-0002">Representing Facial Geometry</h3>
<p class="BodyFirst">The first step in defining our facial recognition application is to figure out how to divide an image of a face into measurable shapes. I mentioned before that we’ll use 68 points in an image to mark the features of the face. To save on development time and achieve our goal with less upfront coding, we’ll leverage a previously trained ML model, <code>shape_predictor_68_face_landmarks</code> (<a class="LinkURL" href="http://dlib.net">http://dlib.net</a>)<sup class="endnote"><a href="b01.xhtml#c10-endnote-003" id="c10-noteref-003">3</a></sup>, which identifies the points of interest in a frontal view of a human face. <a href="#figure10-2" id="figureanchor10-2">Figure 10-2</a> shows the 68 points laid approximately where they would fall on a face.</p>
<figure>
<img alt="" class="" src="image_fi/502567c10/f10002.png"/>
<figcaption><p><a id="figure10-2">Figure 10-2</a>: The points of interest on a face, generated by an algorithm (image source: <a class="LinkURL" href="https://i.stack.imgur.com/OBgDf.png">https://i.stack.imgur.com/OBgDf.png</a>)</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_183" title="183"/>The map of geometric features includes the jawline (points 0–16), left and right eyebrows (points 17–21 and 22–26, respectively), nose (points 27–30 for the bridge and 31–35 for the base), left and right eyes (points 36–41 and 42–47, respectively), and the mouth (points 48–59 for the exterior of the lips and 60–67 for the interior). When the algorithm receives an image of a face, it adjusts the locations of each point to try to match the positioning of the input face. We’ll use the adjusted locations of these points to create Shapely shapes representing the different features. We’ll then compute some geometric statistics about the face, such as the distance between the eyes, the length of the nose, and so on, to create a statistical representation of the face. <a href="#listing10-1" id="listinganchor10-1">Listing 10-1</a> shows how to load the model.</p>
<pre><code>import dlib
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor(
    "facial_model/shape_predictor_68_face_landmarks.dat"
)</code></pre>
<p class="CodeListingCaption"><a id="listing10-1">Listing 10-1</a>: Loading the facial landmark detector</p>
<p>The model is part of the dlib library, which wraps several C++ functions in Python goodness so you can leverage the speed of C++ for scientific computation and the friendly syntax of Python for everything else. The <code>get_front_face_detector</code> function returns a previously trained model for detecting faces in images based on a method known as <em>histogram of oriented gradients (HOG)</em>. The detector counts occurrences of gradient orientation in localized portions of an image, meaning it examines only a small box of pixels at a time; this is similar to the way a human might examine a picture with a magnifying glass to focus on detailed areas (except there’s no distortion of the pixels in this case). The output of the <code>get_front_face_detector</code> function is a list of <code>(</code><var>index</var><code>, </code><var>rectangle_coordinates</var><code>)</code> tuples, one for each detected face. We store this information in a variable named <code>detector</code>, which we’ll use to help the predictor focus in on the faces. The actual facial feature location is handled by a shape predictor that takes in an image region containing some object (in this case, a face) and outputs a set of point locations that define the pose of the object. To load the predictor, we tell dlib the path to the model we’re interested in, which, in this case, is <em>facial_model/shape_predictor_68_face_landmarks.dat</em>. </p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	There are many other models available from researchers you should look into as well, including ones that can detect faces from a three-quarter view or even detect other types of objects such as cats or dogs.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>With the face detector and landmark detector components defined, we’re ready to start processing images. It’s very important to the accuracy and reliability of your facial recognition system (and any other predictive algorithms, for that matter) that you process the test images exactly the same as the training images; otherwise, the difference in processing may <span epub:type="pagebreak" id="Page_184" title="184"/>corrupt the results in unpredictable ways. The next section will describe a modular bit of code we can use in the proof of concept to handle both the training data creation and the test image processing independent of the modeling functions.</p>
<h3 id="h2-502567c10-0003">Processing Image Data</h3>
<p class="BodyFirst">There are many possible ways to process image data so that predictive algorithms can build a model based on their information; they all share the same goal, however, of converting the data into a normalized format. The way you intend to predict faces has a lot of influence on what processing steps, if any, you should take before converting the picture into a feature set. For example, you may choose to preserve color information by creating a predictor for each of the three color channels (red, green, and blue) individually, in which case you wouldn’t want to convert the image to grayscale, as we do here. Other operations are fairly common regardless of the final processing plan. Operations such as cropping the image to the facial region or resizing the image help ensure that all samples are consistently scaled and the features are somewhat normalized in the end. <a href="#listing10-2" id="listinganchor10-2">Listing 10-2</a> shows the function for processing images in the <em>.jpeg</em> file format. </p>
<pre><code>import cv2
import imutils
def process_jpg(file_path):
    img = cv2.imread(file_path)
    image = imutils.resize(img, width=300)
    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</code></pre>
<p class="CodeListingCaption"><a id="listing10-2">Listing 10-2</a>: A function for processing a single <em>.jpeg</em> image</p>
<p>We start by defining the <code>process_jpg</code> function. The only parameter we need is the path to the <em>.jpeg</em> image, stored in <code>file_path</code>. We use the cv2 (short for <em>computer vision 2</em>) library’s <code>imread</code> function to read the file into a data array representing the pixel values for each color channel. Then we resize the image data using the <code>imutils.resize</code> function. We scale the images so they have a width of 300 pixels using the <code>width</code> parameter; the height of the image will be calculated based on this new width to avoid distorting the features. Finally, we convert the resized image data to grayscale using the <code>cv2.cvtColor</code> function and return the result. (I chose to convert the image data to grayscale since color information won’t help inform our geometric analysis.) </p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	The accompanying Jupyter notebook for this chapter also has a similar function to process <em>.gif</em> images, which you may come across in some facial recognition data sets. The imutils and cv2 libraries work well together for a large number of image processing and computer vision tasks, so I encourage you to dive into both libraries more and see what other preprocessing steps you might apply. </p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p><span epub:type="pagebreak" id="Page_185" title="185"/><a href="#figure10-3" id="figureanchor10-3">Figure 10-3</a> shows an example result. </p>
<figure>
<img alt="" class="" src="image_fi/502567c10/f10003.png"/>
<figcaption><p><a id="figure10-3">Figure 10-3</a>: Processed facial images ready for analysis</p></figcaption>
</figure>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	The code to generate <a href="#figure10-3">Figure 10-3</a> is in the 6th cell of the <em>Facial_Recognition_notebook1_Processing.ipynb</em> notebook.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>You can see that we get back grayscale images scaled to 300 pixels wide by 450 pixels tall. Note that the facial features aren’t distorted; this is because of the scaling method we used. We’ll use these two images to exemplify the rest of the process. Both images have decent contrast and the facial features are not obstructed (by things like sunglasses, hats, or heavy makeup), making them good candidates. Both also have some areas that will ultimately prove more difficult for the algorithm, as you’ll see.</p>
<p>Resizing the images and adjusting the color to grayscale before we continue processing them will allow us to get consistent, repeatable samples to work from, regardless of slight differences in picture quality, scale, and lighting. These steps are just the beginning. You should consider other image transformations you could apply (such as increasing the brightness or playing with the contrast values) to give the algorithm the best chance of success when analyzing the image. In the next section, we’ll take our processed images and begin the work of actually locating and analyzing the facial features.</p>
<h3 id="h2-502567c10-0004">Locating Facial Landmarks</h3>
<p class="BodyFirst">Now that we’ve defined our image processing step as a function, we can call it at the beginning of our facial landmark code to ensure that we’re working on a grayscale version of the resized image. Our next task, shown in <a href="#listing10-3" id="listinganchor10-3">Listing 10-3</a>, is to write the function that will locate the facial landmarks we’ll use to define the rest of the facial structure. </p>
<pre><code>from imutils import face_utils
def locate_landmarks(image_file):
  <span aria-label="annotation1" class="CodeAnnotationCode">❶</span> gray = process_jpg(image_file)
  <span aria-label="annotation2" class="CodeAnnotationCode">❷</span> clone = gray.copy()
  <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> rects = detector(clone, 1)
    feature_coordinates = {}
<span epub:type="pagebreak" id="Page_186" title="186"/>  <span aria-label="annotation4" class="CodeAnnotationCode">❹</span> for (i, rect) in enumerate(rects):
      <span aria-label="annotation5" class="CodeAnnotationCode">❺</span> shape = predictor(clone, rect)
      <span aria-label="annotation6" class="CodeAnnotationCode">❻</span> shape = shape_to_np(shape) # See PoC code
      <span aria-label="annotation7" class="CodeAnnotationCode">❼</span> for (part_name, (i, j)) in face_utils.FACIAL_LANDMARKS_IDXS.items():
            if len(rects) &gt;= 2:
                feature_coordinates[part_name] = []
            for x, y in shape[i:j]:
              <span aria-label="annotation8" class="CodeAnnotationCode">❽</span> feature_coordinates[part_name].append((x, y))
        face_points = []
        for n in feature_coordinates.keys():
          <span aria-label="annotation9" class="CodeAnnotationCode">❾</span> face_points += feature_coordinates[n]
      <span aria-label="annotation10" class="CodeAnnotationCode">❿</span> feature_coordinates[part_name] = face_points
    return feature_coordinates</code></pre>
<p class="CodeListingCaption"><a id="listing10-3">Listing 10-3</a>: Locating feature landmarks in an image</p>
<p>The <code>locate_landmarks</code> function takes in a filepath as its only argument. Then, we start by calling the <code>process_jpg</code> function from <a href="#listing10-2">Listing 10-2</a> to get the processed image from the file argument <span aria-label="annotation1" class="CodeAnnotation">❶</span>. I prefer to work on a copy of the image <span aria-label="annotation2" class="CodeAnnotation">❷</span> to avoid accidentally overwriting the source with any changes. Once we’ve copied the image, we locate the rectangular region for each face in the input using the <code>detector</code> we created in <a href="#listing10-1">Listing 10-1</a> <span aria-label="annotation3" class="CodeAnnotation">❸</span>. The result is a list of tuples containing the index of the face and the coordinates corresponding to the rectangular area that contains it. For the sample data there will only ever be one rectangle in the list, but you could extend the function to handle the case of multiple faces for a future project. </p>
<p>Next, we loop over the list of rectangles <span aria-label="annotation4" class="CodeAnnotation">❹</span> and send each to the <code>predictor</code> we also set up in <a href="#listing10-1">Listing 10-1</a> <span aria-label="annotation5" class="CodeAnnotation">❺</span>. The result is the list of point coordinates in the order shown in <a href="#figure10-1">Figure 10-1</a>. We use the function <code>shape_to_np</code> <span aria-label="annotation6" class="CodeAnnotation">❻</span>, which is a simple helper function to convert the shape’s (<em>x</em>, <em>y</em>) coordinates into a NumPy array.</p>
<pre><code>def shape_to_np(shape, dtype="int"):
    coords = np.zeros((68, 2), dtype=dtype)
    for i in range(0, 68):
        coords[i] = (shape.part(i).x, shape.part(i).y)
    return coords </code></pre>
<p>Here we create a NumPy array of zeros, named <code>coords</code>, to hold the 68 coordinate pairs. Next, we loop over all of the indices in the <code>shape.part</code> list. For each part we create a tuple from the <code>x</code> and <code>y</code> attributes and assign the tuple to the <code>coords</code> array at the same index. Once we’ve collected all the coordinate pairs, we return the resulting array of tuples.</p>
<p>Back in <a href="#listing10-3">Listing 10-3</a>, to make things easier going forward, we’ve built a dictionary that is keyed off the feature name and contains the list of points defining the feature. The <code>face_utils.FACIAL_LANDMARKS_IDXS.items</code> function <span aria-label="annotation7" class="CodeAnnotation">❼</span> returns a list of tuples that conveniently provides the feature name and a nested tuple that defines the start and end index in <code>shape</code> that correspond to the feature. We loop over each of these definitions and create a corresponding entry in the <code>feature_coordinates</code> dictionary <span aria-label="annotation8" class="CodeAnnotation">❽</span>. If there’s more than one <span epub:type="pagebreak" id="Page_187" title="187"/>face in the picture, the index number of the face will be appended to the feature name to keep them separated. </p>
<p>Next, we create a list of all the points in the face data and append it to a list <span aria-label="annotation9" class="CodeAnnotation">❾</span>; this will be used to calculate the convex hull of the face in the proof of concept. Formally speaking, a <em>convex hull</em> is the smallest convex polygon that encloses a set of points such that each point in the set lies within the polygon or on its perimeter. Remember from <span class="xref" itemid="xref_target_Chapter 7"><a href="c07.xhtml">Chapter 7</a></span> that a convex polygon is one where all its interior angles are less than 180 degrees. You can think of convex hulls as the result you’d get if you could stretch a rubber band around the outside of all the points in a shape. </p>
<p>Finally, we add the resulting list to the <code>feature_coordinates</code> dictionary <span aria-label="annotation10" class="CodeAnnotation">❿</span>. </p>
<p><a href="#figure10-4" id="figureanchor10-4">Figure 10-4</a> shows the results of running the algorithm against our two test images. </p>
<figure>
<img alt="" class="" src="image_fi/502567c10/f10004.png"/>
<figcaption><p><a id="figure10-4">Figure 10-4</a>: The results of landmark detection</p></figcaption>
</figure>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	The code to generate <a href="#figure10-4">Figure 10-4</a> is in the 9th cell of the <em>Facial_Recognition_notebook1_Processing.ipynb</em> notebook.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>The gray dots are located where the algorithm believes each feature of the face is placed. The black outline around the entire facial region represents the resulting convex hull. As you can see, the program did a decent job. For the most part, the dots correctly hit the landmarks we’re looking for. Where the program fell short is finding the jawline on the woman in the left image, because the contrast between her dark hair and the background is more defined than the difference between her jawline and the background. The measurements of her face are going to be skewed, which, unless they’re skewed the exact same way every time, will result in bad training data. </p>
<p>The algorithm did a much better job finding the man’s jawline, even though it’s hidden behind his beard. But the algorithm has failed to find the edges of his nose properly. It’s a fairly small difference from the actual position, however, and though it will impact a few measurements, the <span epub:type="pagebreak" id="Page_188" title="188"/>overall face shape remains in proportion and thus would generate usable training data. After a few months of doing these tests and examining the results, you’ll be able to look at an image and closely estimate how well the landmark detector will perform given the processing steps you define.</p>
<p>After calling the <code>locate_landmarks</code> function, you should get back the dictionary keyed by the feature name, which allows you to refer to the collection of points using a human-friendly name. For example, to create polygons representing the left and right eyes, you can use this code:</p>
<pre><code>from shapely import Polygon
leye = Polygon(feature_coordinates["left_eye"])
reye = Polygon(feature_coordinates["right_eye"])</code></pre>
<p class="BodyContinued">You can then use Shapely to measure the minimum distance between the shapes:</p>
<pre><code>dist = reye.distance(leye)</code></pre>
<p class="BodyContinued">Or you could measure the difference in area between the two eyes:</p>
<pre><code>diff = abs(reye.area – leye.area)</code></pre>
<p>There are many other potential characteristics you could use, but we’ll dive into those more when we start to build the training data set in the next section. Now that we’ve defined the functions we need, we can start building the proof of concept for our facial recognition system.</p>
<h2 class="HeadProject" id="h1-502567c10-0004"><span>The Proof of Concept: Developing a Facial Recognition System</span></h2>
<p class="BodyFirst">The proof of concept for this project is separated into two parts. The first part builds the training data from the set of facial images. Here, we’ll prepare each image for processing and define the statistics to collect from it. This is where computational geometry is going to help us. The image processing steps may take several minutes (or longer with much larger data sets), so it makes sense to compute this on its own and store the results to a file for processing later. This spares us from having to run expensive calculations over and over. It also makes adding new images to the data set easier. Rather than rebuilding the entire data set to retrain the model, we only need to process the new images before retraining. Be warned: trying to process all of these images in memory isn’t going to work. We’ll need to process the files one at a time to keep memory usage manageable. You’ll see how later in the chapter. </p>
<p>The second portion of the proof of concept defines the ML algorithm and trains it on the previously computed statistical data. We’ll test the algorithm multiple times using a cross-validation process called <em>leave one out (LOO)</em>. We’ll run the validation once for each class in the data, during which the LOO algorithm selects an image from that class to hold out from the training data (hence the name). The model is then trained on the rest of the data. After training the model, we’ll give it the selected <span epub:type="pagebreak" id="Page_189" title="189"/>image to classify, then total the results for each class to estimate overall performance. The major benefit of the LOO method of validation is that it provides the training algorithm with the most information because only a single instance is removed before training. Since we have a limited number of images for each face, we need to give the training algorithm the best chance possible to succeed.</p>
<h3 id="h2-502567c10-0005">Facial Statistics</h3>
<p class="BodyFirst">The first part of the proof of concept is in the <em>facial_recognition_poc_1.py</em> file. It covers the code you’ve seen up to now, but we’ll expand on it to create the final training data set. There are a number of ways you could approach collecting statistical information about the facial structure. My first attempts involved tessellating the face in different ways and measuring the predictive power of different sections. <a href="#figure10-5" id="figureanchor10-5">Figure 10-5</a> shows the method that scored the best, applied to the two example faces.</p>
<figure>
<img alt="" class="" src="image_fi/502567c10/f10005.png"/>
<figcaption><p><a id="figure10-5">Figure 10-5</a>: A result from automated facial tessellation </p></figcaption>
</figure>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	The code to generate <a href="#figure10-5">Figure 10-5</a> is in the 10th cell of the <em>Facial_Recognition_notebook1_Processing.ipynb</em> notebook. </p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>The tessellation treated the nose, eyes, and inner mouth portions as holes in the main facial polygon. The primary problem with this method turned out to be too much noise from similar triangles that didn’t contribute to the structural knowledge at all (for example, all of the triangles making up the chin); this created a large number of similar-valued variables across all the faces. Another problem with this method is that tessellation doesn’t include all the shapes of interest. For example, creating a triangle from the outer points of both eyes and the bottom of the chin would indicate the tilt of the entire head. Why am I bothering to tell you about this if it didn’t work? Because it’s important to realize that trial and error is necessary. Thinking about why a method failed can be more informative than considering why it works!</p>
<p><span epub:type="pagebreak" id="Page_190" title="190"/>During my research I found a paper from the Facial Identification Scientific Working Group (FISWG) that does an excellent job describing the standard facial statistics.<sup class="endnote"><a href="b01.xhtml#c10-endnote-004" id="c10-noteref-004">4</a></sup> Ultimately, I changed tactics from automatic tessellation to explicitly defining 62 measures taken mostly from the reference material. To help with defining the measurements in the code, you can create some variables that represent key points by name. For example:</p>
<pre><code>nose_btm = feature_coordinates["nose"][6]
bridge_top = feature_coordinates["nose"][0]
upper_lip_ctr = feature_coordinates["mouth"][3]
lower_lip_ctr = feature_coordinates["mouth"][9]
chin_ctr = feature_coordinates["jaw"][8]
r_temple = feature_coordinates["jaw"][0]
l_temple = feature_coordinates["jaw"][16]</code></pre>
<p>You can use these points to create the measurements in a way that allows you to return to the code months later and still understand which variables relate to which points of the face. <a href="#listing10-4" id="listinganchor10-4">Listing 10-4</a> shows a sample of the metrics from the proof of concept. </p>
<pre><code>from shapely import LineString
face_dict = {}
<span aria-label="annotation1" class="CodeAnnotationHang">❶</span> face_dict["tri_area"] = Polygon([r_temple, chin_ctr, l_temple]).area
<span aria-label="annotation2" class="CodeAnnotationHang">❷</span> face_dict["face_vert"] = LineString((chin_ctr, bridge_top)).length
face_dict["bow"] = LineString((upper_lip_ctr, nose_btm)).length
<span aria-label="annotation3" class="CodeAnnotationHang">❸</span> face_dict["bow_ratio"] = face_dict["face_vert"]/face_dict["bow"]</code></pre>
<p class="CodeListingCaption"><a id="listing10-4">Listing 10-4</a>: Defining the geometric statistics with Shapely</p>
<p>The three major types of statistics we collect are areas, distances, and ratios. Area metrics convert a set of facial points into a polygon object and then record the <code>area</code> property of the shape <span aria-label="annotation1" class="CodeAnnotation">❶</span>. Distance metrics create a <code>LineString</code> object from two or more points and then record the <code>length</code> property <span aria-label="annotation2" class="CodeAnnotation">❷</span>. Ratios are derived metrics that compare the values of two previously created metrics. For example, here we’ve compared the length of the line between the upper lip and bottom of the nose (colloquially called the Cupid’s bow) to the total vertical height of the face (measured from the chin to the top of the nose) <span aria-label="annotation3" class="CodeAnnotation">❸</span>. Ratios should compare only statistics of like types. The ratio of one area to another area or the length of one distance to another makes sense, but the ratio of a distance to an area doesn’t make much sense in this context.</p>
<p>On top of the 62 explicit metrics, I’ve included the x-coordinate and y-coordinate as separate features in the data for a total of 214 data points per image. We’ll let the model-building algorithm determine which of these features are most informative in the “<span class="xref" itemid="xref_target_Feature Engineering"><a href="#h2-502567c10-0008">Feature Engineering</a></span>” section of this chapter, but for now, defining more statistics means a higher chance of finding some meaningful ones.</p>
<h3 id="h2-502567c10-0006"><span epub:type="pagebreak" id="Page_191" title="191"/>Memory Management</h3>
<p class="BodyFirst">As mentioned previously, trying to process all of these images in memory isn’t feasible. Images contain a large amount of information, and trying to open a large number at the same time for processing will quickly fill your memory buffers. Instead, the proof of concept opens a single image at a time and computes all the statistics. It saves the data and immediately closes the image to move on to the next. <a href="#listing10-5" id="listinganchor10-5">Listing 10-5</a> shows the structure of the loop.</p>
<pre><code><span aria-label="annotation1" class="CodeAnnotationCode">❶</span> image_paths = get_image_files("faces95", image_paths)
face_collection = []
for image_file in image_paths:
  <span aria-label="annotation2" class="CodeAnnotationCode">❷</span> feature_coordinates = locate_landmarks(image_file)
  <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> if len(feature_coordinates.keys()) &lt; 8:
        continue
<var>    --snip--</var>
  <span aria-label="annotation4" class="CodeAnnotationCode">❹</span> face_series = pd.Series(face_dict)
    face_collection.append(face_series)
<span aria-label="annotation5" class="CodeAnnotationHang">❺</span> faces_df = pd.DataFrame(face_collection)
<span aria-label="annotation6" class="CodeAnnotationHang">❻</span> faces_df.to_csv("facial_geometry.csv")</code></pre>
<p class="CodeListingCaption"><a id="listing10-5">Listing 10-5</a>: Looping over image files for processing</p>
<p>The <code>get_image_files</code> function is another helper function in the proof of concept that recursively walks a directory structure collecting filenames that don’t end with <em>.txt</em> <span aria-label="annotation1" class="CodeAnnotation">❶</span>. Once we’ve done that, we loop over each filepath in the resulting list and pass it to the <code>locate_landmarks</code> function we defined in <a href="#listing10-3">Listing 10-3</a> <span aria-label="annotation2" class="CodeAnnotation">❷</span>. Some images in the data set aren’t clear enough for the landmark detector to find all the features. In these cases, the feature dictionary won’t have the proper number of keys, and we can skip any further processing <span aria-label="annotation3" class="CodeAnnotation">❸</span>. The snipped section is where we’ll add the code to create all of the facial statistics using the method shown in <a href="#listing10-4">Listing 10-4</a>. </p>
<p>Once all the data points are created, we convert the dictionary into a pandas <code>Series</code> object <span aria-label="annotation4" class="CodeAnnotation">❹</span> and append it to the collection of faces. After all the images are processed, we create a <code>DataFrame</code> from the list of <code>Series</code> objects <span aria-label="annotation5" class="CodeAnnotation">❺</span>. Finally, we save the results to a <em>.csv</em> file for later use <span aria-label="annotation6" class="CodeAnnotation">❻</span>. Running the script will conclude the first portion of the proof of concept by creating a data set derived from the geometric statistics. We’ll use this data to train the classifier we develop in the next two sections.</p>
<h3 id="h2-502567c10-0007">Data Loading</h3>
<p class="BodyFirst">At this point we’ve created our geometric data set and saved it to a <em>.csv</em>. The second stage of the proof of concept is located in the file <em>face_recognition_poc_2.py</em> and picks up with loading this previously created data into pandas. <a href="#listing10-6" id="listinganchor10-6">Listing 10-6</a> shows how to load the <em>facial_geometry.csv</em> and prepare the data for the association calculations.</p>
<pre><code>import pandas as pd
<span aria-label="annotation1" class="CodeAnnotationHang">❶</span> faces_df = pd.read_csv("facial_geometry.csv")
<span aria-label="annotation2" class="CodeAnnotationHang">❷</span> faces_df.drop(["Unnamed: 0", "file"], inplace=True, axis=1)
<span aria-label="annotation3" class="CodeAnnotationHang">❸</span> faces_df["category"] = faces_df["name"].astype("category")
<span epub:type="pagebreak" id="Page_192" title="192"/><span aria-label="annotation4" class="CodeAnnotationHang">❹</span> cat_columns = faces_df.select_dtypes(["category"]).columns
<span aria-label="annotation5" class="CodeAnnotationHang">❺</span> faces_df[cat_columns] = faces_df[cat_columns].apply(lambda x: x.cat.codes)
<span aria-label="annotation6" class="CodeAnnotationHang">❻</span> name_map = faces_df[["name", "category"]].set_index(["category"])</code></pre>
<p class="CodeListingCaption"><a id="listing10-6">Listing 10-6</a>: Preparing the <em>facial_geometry.csv</em> data for training</p>
<p>We start by calling the pandas <code>read_csv</code> function to get the data from the previous step <span aria-label="annotation1" class="CodeAnnotation">❶</span>. The creation of the file results in an <code>Unnamed</code> index row, which we drop to save memory <span aria-label="annotation2" class="CodeAnnotation">❷</span>. The next step is to define the <code>category</code> variable. The <code>name</code> field contains a randomly assigned fake name to make the data realistic while still preserving the data subjects’ privacy. This is the column we’re interested in building a model to predict, but pandas treats it as a text string by default, so we convert it to a categorical column using the <code>astype</code> function with the type <code>category</code> <span aria-label="annotation3" class="CodeAnnotation">❸</span>. We collect the name of all the categorical columns using the <code>select_dtypes</code> function. The result is a list of column names in <code>faces_df</code> that have the type <code>category</code> <span aria-label="annotation4" class="CodeAnnotation">❹</span>. Currently the <code>category</code> column should be the only result in the list, but it still makes it easier to reference all categorical variables this way should you choose to add more categorical information in the future. </p>
<p>Because pandas automatically assigns a numeric index to each category derived from a categorical column, we overwrite the content of the <code>category</code> column with that numeric index using the <code>apply</code> function <span aria-label="annotation5" class="CodeAnnotation">❺</span>. For convenience, we create a lookup table so we can convert category IDs to names or vice versa by copying the <code>name</code> and <code>category</code> columns from <code>faces_df</code> and saving them into another <code>DataFrame</code> object called <code>name_map</code> <span aria-label="annotation6" class="CodeAnnotation">❻</span>. </p>
<p>That wraps up our data loading code. At this point, we’ve loaded the previously created <em>facial_geometry.csv</em> file and converted the categories, which are the subjects’ names, into a format pandas can understand. The next step is to set aside a <em>true holdout set</em>, or the instances from the data that are never used during the feature engineering, training, and performance estimation phases. I chose three instances as the size of the holdout set so that the model training portion had plenty of data left to learn the model from. One major problem with ML occurs when researchers accidentally give the algorithm direct or indirect access to the answers for the test data; this biases the model, so it performs excellently on the test set but will likely fail horribly in practice. </p>
<p>As an example, suppose all subjects have 20 images except the three subjects that are held back for a true holdout set, which have only 19 images remaining in the data. If the algorithm had access to the count of images for each subject, it could narrow down the list to only those three subjects, even though this information isn’t likely to translate well in a production system where subjects will have different numbers of photos to work from. So, to make sure we haven’t tainted our result, the algorithm will hold back the three samples from different classes (one picture from three different people) to test the final model against. Using a true holdout set is equivalent to testing with three never-before-seen pictures, which is as close to testing on production requirements as you’re going to get. </p>
<p>In <a href="#listing10-7" id="listinganchor10-7">Listing 10-7</a> we create the three-instance holdout set.</p>
<pre><code><span epub:type="pagebreak" id="Page_193" title="193"/>from random import choice
real_test = {}
while len(real_test) &lt; 3:
  <span aria-label="annotation1" class="CodeAnnotationCode">❶</span> name = choice(list(faces_df["name"].unique()))
    if name not in real_test.keys():
      <span aria-label="annotation2" class="CodeAnnotationCode">❷</span> group = faces_df[faces_df["name"] == name]
      <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> real_test[name] = choice(group.index.to_list())
<span aria-label="annotation4" class="CodeAnnotationHang">❹</span> index_list = [r[1] for r in real_test.items()]
<span aria-label="annotation5" class="CodeAnnotationHang">❺</span> real_X = faces_df.iloc[index_list]
<span aria-label="annotation6" class="CodeAnnotationHang">❻</span> faces_df.drop(index_list, inplace=True)</code></pre>
<p class="CodeListingCaption"><a id="listing10-7">Listing 10-7</a>: Randomly selecting a true holdout data set</p>
<p>We use the <code>choice</code> function to randomly choose a subject name from a list of unique names <span aria-label="annotation1" class="CodeAnnotation">❶</span>. It would be better to do this by ID for real data, since the probability of two people in a corporate data set having the same name is fairly high. Luckily, we don’t have to worry about this in the sample data because the names were generated using faker (a library to randomly generate data that looks authentic), so the probability is much lower. If the selected subject is already in the holdout set, we continue to choose names randomly until we find one who isn’t. </p>
<p>Next, we gather all the instances for the randomly selected subject <span aria-label="annotation2" class="CodeAnnotation">❷</span>. We use the <code>choice</code> function once again to select a random instance index from the group of instances <span aria-label="annotation3" class="CodeAnnotation">❸</span>. The result is a dictionary keyed off the subject name with a value that indicates the randomly selected instance’s index. Once the indices have been collected, we use a list comprehension to collect all the indices from the dictionary <span aria-label="annotation4" class="CodeAnnotation">❹</span>. We copy the actual instance data from the <code>faces_df</code> object into a separate <code>DataFrame</code> <span aria-label="annotation5" class="CodeAnnotation">❺</span>. Finally, we remove the instances from <code>faces_df</code> so they won’t be used in the association matrix calculations <span aria-label="annotation6" class="CodeAnnotation">❻</span>. </p>
<h3 id="h2-502567c10-0008">Feature Engineering</h3>
<p class="BodyFirst">Now that the image processing is complete, it’s time to move on to the actual model training code. For this portion of the proof of concept, we’re going to apply feature engineering and ML to our previously generated facial data. Our goal is to produce a predictive model capable of identifying a subject based on a previously unprocessed image containing a face it has previously analyzed the geometry for (using the holdout set we created in <a href="#listing10-7">Listing 10-7</a>). To achieve our goal, we need to cut the excess noise from our data set so our algorithm can focus on the really informative measurements. That’s where feature engineering comes in. </p>
<p>One of the most important steps in any ML project, feature engineering involves mathematically analyzing the relationship between the different variables in the data and the class value we’re interested in predicting to determine which ones add the most useful information. Your ability to predict anything useful is directly tied to the quality and quantity of the data available. These days, a lack of data is hardly a problem. Quite the opposite: we usually have so much data about a topic that figuring out what’s really important to the outcome is nearly impossible for a human to do by hand. <span epub:type="pagebreak" id="Page_194" title="194"/>The important relationships can get drowned in the noise of useless data. To counter this problem, researchers use one or more feature engineering algorithms, which score the features in the data based on their contribution to some value we want to predict (in this case, the subject name associated with the face). We’re going to apply three steps to progressively whittle the features down to only those we’re confident are contributing to the model’s accuracy. </p>
<h4 id="h3-502567c10-0001">Association Matrix</h4>
<p class="BodyFirst">One popular method of scoring features is an <em>association matrix</em>, which determines which features in a large list of features are most correlated to one another. The result of running an association algorithm is an <em>n</em> × <em>n</em> matrix where <em>n</em> is the number of features in the data. Each cell contains the correlation score between the two features defined by the column and row. <a href="#figure10-6" id="figureanchor10-6">Figure 10-6</a> shows a portion of the association matrix for the facial data. </p>
<span epub:type="pagebreak" id="Page_195" title="195"/><figure>
<img alt="" class="" src="image_fi/502567c10/f10006.png"/>
<figcaption><p><a id="figure10-6">Figure 10-6</a>: A feature association matrix</p></figcaption>
</figure>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	The code to generate <a href="#figure10-6">Figure 10-6</a> is in the 15th cell of the <em>Facial_Recognition_notebook2_Modeling.ipynb</em> notebook.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>The correlation between a variable and itself will always be 1.0, so you can ignore those instances. What we’re most interested in are those features that have a high correlation with the feature we want to predict. Note that the correlation is taken as an absolute value to treat negative and positive correlation as equally important. Taking highly correlated variables together offers the best chance to properly predict the value of interest.</p>
<p><span epub:type="pagebreak" id="Page_196" title="196"/>The limitation to normal correlation approaches is that you must be correlating some continuous (real) numeric values. In this case, we want to measure the correlation of continuous variables with a discrete categorical variable (a subject’s name), so standard correlation measures won’t work. Instead the matrix was calculated using another correlation score known as Theil’s U, which handles categorical data, with code borrowed from a blog post by Shaked Zychlinski (<a class="LinkURL" href="https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9">https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9</a>). All of the functions are located in the <em>nominal.py</em> file with the author’s original descriptions of how they operate, so I’ll focus on how we integrate the <code>association</code> function into our facial recognition system. </p>
<p><a href="#listing10-8" id="listinganchor10-8">Listing 10-8</a> shows how we can calculate the association matrix.</p>
<pre><code>import nominal
assoc_matrix = associations(
    faces_df,
    nominal_columns=cat_columns,
    theil_u=True,
    return_results=True
)</code></pre>
<p class="CodeListingCaption"><a id="listing10-8">Listing 10-8</a>: Calculating the association matrix of the feature set</p>
<p>The <code>association</code> function from the <em>nominal.py</em> file takes in a <code>DataFrame</code> object to perform the calculation on; here, that’s <code>faces_df</code>. You need to pass in a list of nominal columns. This is where the <code>cat_columns</code> variable defined in <a href="#listing10-6">Listing 10-6</a> comes in handy; we won’t need to edit the code, even if we add more categorical information. To use Theil’s U as the calculation, we must set the <code>theil_u</code> parameter to <code>True</code>. By default, the <code>association</code> function just displays the results to the screen, but we want to use the data to programmatically select which features to use in the model, so we set the <code>return_results</code> parameter to <code>True</code> to also return the result as a matrix. </p>
<p>Now that we have an association score for each feature, we can collect the top predictors (those with high association scores with the <code>category</code> column) into a list so we can compare results with the next two feature engineering steps. An iterative approach may be to start with the 10 top-performing features and see if you can train a functional model. Continue increasing the number of features by 5 to 10 until you find the lowest number that produces a reliable model. Another approach (and the one I prefer) is to set a predictive threshold for the features you want to keep as follows:</p>
<pre><code>assoc_matrix = assoc_matrix[abs(assoc_matrix["name"]) &gt; 0.95]
key_features = [k for k in assoc_matrix["name"].index]</code></pre>
<p>Picking a point to cull features is as much an art as a science. After some trial and error, I found that I could keep features that scored higher than 0.95 and still get good performance from the final model. The <code>key_features</code> variable contains a list of the 19 column names that have an association score greater than 0.95 with regard to the <code>name</code> column; this includes the <code>name</code> and <code>category</code> columns, which have a score of 1.0, as I mentioned earlier. </p>
<p><span epub:type="pagebreak" id="Page_197" title="197"/>Still, the association matrix is just one indicator of predictive power. To be really sure we’re picking the best set of features, we’ll run another feature selection algorithm and compare the best performers from both to see which features are in both lists. Those features will have a very good chance to improve our prediction’s accuracy.</p>
<h4 id="h3-502567c10-0002">Mutual Information Classification </h4>
<p class="BodyFirst">If one measure of association is good, then two should be great, right? In this case we can apply a second method of ranking features to gain even more insight into which features will be most helpful. The <em>mutual information (MI)</em> score between two variables is a non-negative value that measures the dependency between the features. It is equal to 0 only when two random variables are completely independent. Higher values indicate higher dependency.<sup class="endnote"><a href="b01.xhtml#c10-endnote-005" id="c10-noteref-005">5</a></sup> <a href="#listing10-9" id="listinganchor10-9">Listing 10-9</a> shows how to calculate the MI using scikit-learn. </p>
<pre><code>from sklearn.feature_selection import mutual_info_classif
contributing = mutual_info_classif(
    faces_df.drop(["name", "category"], axis=1),
    faces_df["category"],
    discrete_features="auto",
    n_neighbors=7
)</code></pre>
<p class="CodeListingCaption"><a id="listing10-9">Listing 10-9</a>: Calculating the MI contribution for each feature</p>
<p>The first argument is the feature matrix we want to calculate the MI score for. To avoid tainting the data, we drop the <code>name</code> and <code>category</code> features from the set with an inline call to the <code>drop</code> function, which doesn’t remove the columns from the actual data, just the temporary feature list passed into the algorithm. The next argument is the list of instance categories that will be used internally to train a classifier; we pass <code>faces_df["category"]</code> as it contains the classes of the data we’re interested in finding the MI scores in relation to. You can set the <code>discrete_features</code> parameter to a list of feature labels to treat explicitly as discrete values, or you can let the algorithm attempt to detect the discrete features by setting it to <code>auto</code>, as we’ve done here. The results are calculated using a nearest-neighbor classifier, so once again a bit of trial and error can be necessary to find the right number of neighbors. An iterative method can help you find a good setting for this parameter. After a few test runs, I settled on seven neighbors. The result of the call to <code>mutual_info_classif</code> is a list of values in the same order as the columns in the <code>faces_df</code> data.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">Note</span></h2>
<p>	The Jupyter notebook has code showing how you can use scikit-learn to automate the process of iterating over model parameters using model selection to find the best configuration.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p><span epub:type="pagebreak" id="Page_198" title="198"/>As before, we can collect the top performers by selecting all the features with an MI score greater than 1. We’ll compare this to the list of key features generated from the association matrix to create an even smaller list of features that are highly informative when it comes to predicting the value in the <code>category</code> column. <a href="#listing10-10" id="listinganchor10-10">Listing 10-10</a> shows how.</p>
<pre><code>results = zip(
    faces_df.drop(["name", "category"], axis=1).columns,
    contributing
)
mi_scores = [f for f,v in results if v &gt;= 1]
reduced_features = [k for k in mi_scores if k in key_features]</code></pre>
<p class="CodeListingCaption"><a id="listing10-10">Listing 10-10</a>: Finding the overlap between best feature lists</p>
<p>The <code>zip</code> function combines the column names from the data with the results in the <code>contributing</code> variable we defined in <a href="#listing10-9">Listing 10-9</a> to create a list of tuples with the structure <code>(</code><var>column name</var><code>, </code><var>MI score</var><code>)</code>. We then use a list comprehension to filter the results into a list of column names that had a score greater than or equal to 1.0. Finally, we compare the column names in the <code>key_features</code> list to the columns in the <code>mi_scores</code> list. Any column that’s in both lists makes it to the <code>reduced_features</code> list, which represents those features that have scored well on both association and mutual information with respect to the categorical variable we want to predict. At this point there should be only nine columns left in the running, so we could stop here if we wanted. We’ve reduced the data set from more than 200 features down to just 9. In practice, you could probably get reliable performance modeling off of these, but I like to push things a bit—let’s see how extreme we can go. We’ll do one more feature engineering pass to see if we can concentrate the predictive power even more.</p>
<h4 id="h3-502567c10-0003">Correlation Ratio</h4>
<p class="BodyFirst">In statistics, the <em>correlation ratio</em> is a measure of the relationship between the statistical distribution within individual categories (in this case, geometric descriptions of photos for the same person) and the distribution across the whole population or sample (geometric descriptions of all photos in the data set). The measure is defined as the ratio of two standard deviations representing each variation, or the ratio of a feature’s variation within a class compared to its variance over the whole set. An ideal feature would have low variance within a single category but high variance between categories. </p>
<p>Intuitively, this is the same as saying we want features that are consistent for one person but differ between people. One example would be the distance between the outside points of the eyes. For a single person, we’d expect this measurement to be fairly consistent, but we’d expect measurements between two different people to produce a larger difference. </p>
<p><a href="#listing10-11" id="listinganchor10-11">Listing 10-11</a> shows how to gather the correlation ratio for the <code>faces_df</code> data.</p>
<pre><code>etas = {}
<span epub:type="pagebreak" id="Page_199" title="199"/>for feat in faces_df.columns.to_list():
    if feat not in ["category", "name"]:
      <span aria-label="annotation1" class="CodeAnnotationCode">❶</span> etas[feat] = correlation_ratio(faces_df[feat], faces_df["category"])
<span aria-label="annotation2" class="CodeAnnotationHang">❷</span> sorted_rank = sorted(etas.items(), key=lambda kv: kv[1])
reduced_key_features = []
for f in sorted_rank[-21:]:
  <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> if f[0] in reduced_features:
        reduced_key_features.append(f[0])</code></pre>
<p class="CodeListingCaption"><a id="listing10-11">Listing 10-11</a>: Calculating the correlation ratio for the features</p>
<p>The <code>correlation_ratio</code> function <span aria-label="annotation1" class="CodeAnnotation">❶</span> also resides in the <em>nominal.py</em> file. It takes in a <code>Series</code> object representing the feature we want to score, along with another <code>Series</code> representing the categorical feature. The value is in the range of real values between 0 and 1, where 0 means a category cannot be determined by the feature’s measurement, and 1 means a category can be determined with absolute certainty. We assign the resulting value to a dictionary keyed off the column name. Once all the features have been scored, we sort the dictionary using the <code>sorted</code> function. The result is a list of tuples with the structure <code>(</code><var>column name</var><code>, </code><var>value</var><code>)</code> sorted from worst performance to best <span aria-label="annotation2" class="CodeAnnotation">❷</span>. We loop over the last 21 entries and compare each column name to the <code>reduced_features</code> list. If a column is in both lists, it goes into the final feature set called <code>reduced_key_features</code> <span aria-label="annotation3" class="CodeAnnotation">❸</span>. The result is a list of the four most predictive features from the data:</p>
<pre><code>['outer_eyes', 'nose_area', 'face_horz', 'center_tri_area']</code></pre>
<p>Each of these features was in the list of top performers for all three feature selection methods, which is a strong indication of its ability to classify data. In the next step, we’ll use only these four features to train our models. In a production system, you could use this information to reduce the number of statistics you collect during the first phase. Clearly most of the measurements I defined in the first phase weren’t necessary to distinguish faces in the data, but in the beginning you rarely know what will be useful, so collecting a large number of data points and letting the algorithms do the work can reveal unexpected relationships. It’s very important to note these features are specific to this data set. You can’t do feature selection on one image set and expect those features to translate perfectly to every other image data set.</p>
<h3 id="h2-502567c10-0009">Model Training</h3>
<p class="BodyFirst">Now the data is finally ready for modeling. We’re going to create the reduced data set using the four features selected during feature engineering. We’ll then establish a null hypothesis by scoring a simple classifier on the data. Finally, we’ll build the real thing using the random forest classifier to perform the final classifications for the three images held out from the beginning. </p>
<h4 id="h3-502567c10-0004"><span epub:type="pagebreak" id="Page_200" title="200"/>Splitting the Data</h4>
<p class="BodyFirst">The first step is to split the data into training and testing sets. <a href="#listing10-12" id="listinganchor10-12">Listing 10-12</a> defines the two data sets as well as the object to handle the test data generation.</p>
<pre><code>from sklearn.model_selection import LeaveOneOut
X = faces_df[reduced_key_features]
y = faces_df["category"]
loo = LeaveOneOut()
splits = list(loo.split(X))</code></pre>
<p class="CodeListingCaption"><a id="listing10-12">Listing 10-12</a>: Creating the training and testing splits</p>
<p>Traditionally, the variable <code>X</code> is used to denote the test data (which doesn’t contain the classification information). Here we take only the subset of the features defined in <code>reduced_key_features</code> from the <code>faces_df</code> data to define <code>X</code>. We use the variable <code>y</code> to hold the corresponding class information from the <code>category</code> column. Finally, we use the <code>split</code> method from the <code>LeaveOneOut</code> class to create a set of <em>n</em> copies of the data in <code>X</code> (where <em>n </em>is the number of distinct classes). The result is a list of tuples with the form <code>(</code><var>training indexes</var><code>, </code><var>test indexes</var><code>)</code> called <code>splits</code>. In the LOO validation scheme, each split has a different image removed from the data set to test on, so the test indices will always contain a single instance ID and the training indices will have the rest. As I mentioned before, the LOO validation method gives the algorithm the most training information. It’s also closer to how the system would be used in production where an image of a subject’s face would be compared to a facial database.</p>
<h4 id="h3-502567c10-0005">Establishing a Baseline</h4>
<p class="BodyFirst">To establish a baseline score, we begin by modeling one or more <em>dummy classifiers</em>, which use very simple prediction methods (such as random guessing or guessing the majority class) to establish a worst-case performance score. <a href="#listing10-13" id="listinganchor10-13">Listing 10-13</a> shows the API for working with scikit-learn classifiers.</p>
<pre><code>from sklearn.dummy import DummyClassifier
from sklearn.model_selection import cross_val_score
<span aria-label="annotation1" class="CodeAnnotationHang">❶</span> dc = DummyClassifier(strategy="uniform")
scores = []
hits = 0
misses = 0
for train_index, test_index in splits:
  <span aria-label="annotation2" class="CodeAnnotationCode">❷</span> X_train, y_train, = X.iloc[train_index], y.iloc[train_index]
  <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> X_test, y_test, = X.iloc[test_index], y.iloc[test_index]  
  <span aria-label="annotation4" class="CodeAnnotationCode">❹</span> cvs = cross_val_score(dc, X_train, y_train, cv=4)
  <span aria-label="annotation5" class="CodeAnnotationCode">❺</span> score = sum(cvs) / 4 # Default cv value
    scores.append(score)
  <span aria-label="annotation6" class="CodeAnnotationCode">❻</span> dc.fit(X_train, y_train)
  <span aria-label="annotation7" class="CodeAnnotationCode">❼</span> y_pred = dc.predict(X_test)
  <span aria-label="annotation8" class="CodeAnnotationCode">❽</span> if y_test.values[0] == y_pred:
        hits += 1
    else:
        misses += 1
<span epub:type="pagebreak" id="Page_201" title="201"/><span aria-label="annotation9" class="CodeAnnotationHang">❾</span> print((sum(scores) / len(scores))*100)
<span aria-label="annotation10" class="CodeAnnotationHang">❿</span> print((hits / (hits+misses))*100)</code></pre>
<p class="CodeListingCaption"><a id="listing10-13">Listing 10-13</a>: Training a baseline dummy classifier</p>
<p>We use the <code>DummyClassifier</code> class from scikit-learn (which has the same API as the actual classifiers) to define the baseline model. Passing the <code>uniform</code> argument <span aria-label="annotation1" class="CodeAnnotation">❶</span> creates a model that will randomly guess the class from the set of possible classes. We loop over the splits defined previously to create the training and testing instances from the corresponding index lists. The <code>X_train</code> and <code>y_train</code> variables <span aria-label="annotation2" class="CodeAnnotation">❷</span> are used to train the model (if the model is one that requires training), while the <code>X_test</code> and <code>y_test</code> variables <span aria-label="annotation3" class="CodeAnnotation">❸</span> will be used to score the resulting model. </p>
<p>We use the <code>cross_val_score</code> function <span aria-label="annotation4" class="CodeAnnotation">❹</span> to get an a priori performance estimate for the split. The function takes a classifier object and the two parts of the training data set. The <code>cv</code> parameter sets the number of folds that will be used to validate the model. The default is five folds, but the data includes a class that has only four images, so if we don’t set this to <code>4</code>, Python will print a bunch of warnings. The function performs the four-fold cross-validation using the classifier object passed in. To get the average score across all four folds, we take the sum of the scores and divide it by the number of folds <span aria-label="annotation5" class="CodeAnnotation">❺</span>. We save the average score into a list so we can analyze it after the loop completes. </p>
<p>Next, we fit the <code>DummyClassifier</code> object with the training data <span aria-label="annotation6" class="CodeAnnotation">❻</span>. <em>Fitting</em> a classifier is the proper terminology for training the model on the data. Nothing happens internally when we do this for our dummy classifier, but it’s the proper workflow when we go to use a more sophisticated classifier in the next step, so it’s best to stick to convention. </p>
<p>Finally, we use the fitted classifier to predict the outcome from the <code>y_test</code> data set <span aria-label="annotation7" class="CodeAnnotation">❼</span>, which is the one image held out by the LOO algorithm. The result of the <code>predict</code> function is the class the algorithm thinks the data belongs to. We compare the predicted class to the actual class <span aria-label="annotation8" class="CodeAnnotation">❽</span> and increment the hit or missed count accordingly. </p>
<p>Once the loop completes, we calculate the average cross-validation score by summing the <code>scores</code> list and dividing by the length of the list <span aria-label="annotation9" class="CodeAnnotation">❾</span>. The result is the average of average scores for cross-validation, which is a decent indicator of real-world performance. In my tests the <code>DummyClassifier</code> scored about 0.4 percent accuracy. To validate the performance estimate, we also calculate the ratio of hits to misses <span aria-label="annotation10" class="CodeAnnotation">❿</span>. During my testing, the actual score was just under 0.6 percent (26 hits out of 4,457 chances). This result serves as the null hypothesis for our conclusion going forward (if you aren’t familiar with hypothesis testing, check out this article: <a class="LinkURL" href="https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing">https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing</a>). If our actual classifier can do better than 0.6 percent correct classification, then the model is having some impact on the correctness beyond random coincidence.</p>
<h4 id="h3-502567c10-0006"><span epub:type="pagebreak" id="Page_202" title="202"/>Implementing the Random Forest</h4>
<p class="BodyFirst">Now it’s time to implement the random forest classifier. Since the API is shared between all scikit-learn classifiers, the code remains largely unchanged from the dummy classifier code in <a href="#listing10-13">Listing 10-13</a>. <a href="#listing10-14" id="listinganchor10-14">Listing 10-14</a> shows the changes.</p>
<pre><code>from sklearn.ensemble import RandomForestClassifier
from random import randint
<span aria-label="annotation1" class="CodeAnnotationHang">❶</span> rfc = RandomForestClassifier(
    n_estimators=100, min_samples_split=5, min_samples_leaf=3
)
<span aria-label="annotation2" class="CodeAnnotationHang">❷</span> for i in range(50):
    split_i = randint(0, len(splits))
  <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> while split_i in chose:
        split_i = randint(0, len(splits))
    chose.append(split_i)
<var>--snip--</var></code></pre>
<p class="CodeListingCaption"><a id="listing10-14">Listing 10-14</a>: A decision tree algorithm definition</p>
<p>Rather than defining a dummy classifier, we use the <code>RandomForestClassifier</code> class from scikit-learn’s <code>ensemble</code> module <span aria-label="annotation1" class="CodeAnnotation">❶</span>. <em>Ensemble</em> classifiers use multiple classifiers internally and aggregate the predictions into a single prediction. In this case, each internal classifier is a random tree trained off a random sampling of the input data—hence the random <em>forest</em>. </p>
<p>The <code>n_estimators</code> parameter defines the number of internal classifiers to train. The <code>min_samples_split</code> parameter defines the minimum number of instances used to train the internal classifiers. The <code>min_samples_leaf</code> parameter tells the random trees the minimum number of samples to consider a valid leaf node. Setting this parameter to higher values will start to automatically prune less useful logic branches from the resulting decision trees. If you look back at the decision tree in <a href="#figure10-1">Figure 10-1</a>, you can see that the leaf nodes at the very bottom of the tree have only one sample each. Since the data set was small to begin with, that’s fine, but if the number of samples at a leaf node is low when there’s an abundance of data, it most likely means that logic branch doesn’t add as much information as other leaves with more samples covered. You can once again use manual or automated parameter tuning to find an optimal configuration here. </p>
<p>The second change is to use the <code>randint</code> function to randomly select splits <span aria-label="annotation2" class="CodeAnnotation">❷</span> rather than iterating over all of them in order. I chose 50 random splits for no special reason; I encourage you to find a number that works better. We use a <code>while</code> loop to ensure the split hasn’t already been used to train a random forest <span aria-label="annotation3" class="CodeAnnotation">❸</span>. </p>
<p>From this point on, the code is identical to the previous dummy classification code (you can compare cells 23 and 25 in the <em>Facial_Recognition_notebook2_Modeling.ipynb</em> notebook to see this). Just be sure to rename the <code>dc</code> object references to <code>rfc</code>. And with that, the code should be ready to run. The result of my test gave a five-fold cross-fold estimated performance of 76.5 percent and a test set performance of 72 percent (36 hits out of 50 chances). It may not seem like 72 percent is all that great, but <span epub:type="pagebreak" id="Page_203" title="203"/>considering the fact that we’re predicting the correct outcome using only four geometric features, it’s pretty impressive! </p>
<h4 id="h3-502567c10-0007">Testing the Holdout Images</h4>
<p class="BodyFirst">For the final validation of the model, we’re going to give it the three true holdout images and see if it can predict the correct subject. Remember that these images haven’t been used by any portion of the code up to now, so they’re completely new to the model. Given the previous result of 72 percent accuracy, it would be fair to guess the result should be two or three correct out of the three possible. <a href="#listing10-15" id="listinganchor10-15">Listing 10-15</a> shows how we run the trained classifier on the holdout images.</p>
<pre><code>real_y = real_X["category"]
test_X = real_X[reduced_key_features]
rfc = RandomForestClassifier(
    n_estimators=100, min_samples_split=5, min_samples_leaf=3, random_state=42
)
rfc.fit(X, y)
y_pred = rfc.predict(test_X)
print(list(zip(y_pred, real_y)))</code></pre>
<p class="CodeListingCaption"><a id="listing10-15">Listing 10-15</a>: Testing with the true holdout data</p>
<p>The <code>category</code> column from the <code>real_X</code> data set defines the proper classes we want predicted. We define the <code>test_X</code> data by taking the <code>reduced_key_features</code> subset from the <code>real_X</code> data set. Then we create the <code>RandomForestClassifier</code> object exactly as before. When we fit the model, we use the entire data set not in the true holdout set. Then we call the <code>predict</code> function on the <code>test_X</code> data. The result is a list of the class indices that should match the three indices in the <code>real_y</code> list. To compare the two easily, we can use the <code>zip</code> function to combine the two lists and print it out. </p>
<p>Here’s the result from my test while writing this:</p>
<pre><code>[(25, 25), (122, 122), (174, 174)]</code></pre>
<p>A perfect score! It’s important to realize that, due to the stochastic nature of the algorithm, your results will change between runs. The dummy classifier in my tests ranged from 0.2 percent to 0.6 percent, while the <code>RandomForestClassifier</code> regularly scores above 72 percent. If you get an odd result, such as zero correct classifications from the holdout set, try rerunning the code. With an expected precision of 72 percent, there’s still a 28 percent chance one holdout image will be misclassified, a 7.8 percent chance that two will be misclassified (0.28^2 = 0.078), and a 2.2 percent chance of the algorithm misclassifying all three holdout images (0.28^3 = 0.022). </p>
<p>At this point we’ve proven our concept is viable. You can refine the process to improve the accuracy and reliability of the algorithm, but we’ve proven that we could in fact use computational geometry to produce a functional facial recognition system. Clearly, the result is no fluke, given the dreadful performance of the baseline classifier, so we’ve achieved our goal of properly predicting three subjects from previously unseen images. </p>
<p><span epub:type="pagebreak" id="Page_204" title="204"/>In addition to the accuracy, you should also consider adding a measure of confidence in the prediction. What we’ve done by predicting a single class is called a <em>hard classification</em>. The problem with it is that you’ll always get back some prediction, regardless of whether there’s any reason to believe it’s accurate. You could instead opt for a <em>soft classifier</em>, which predicts the likelihood that a test instance belongs to any given class in the data. After fitting the classifier, you can use the scikit-learn <code>predict_proba</code> function instead of the standard <code>predict</code> to get back the list of probability-like scores. By examining these scores, you can get a sense of how “sure” the algorithm is in a prediction. You can calibrate the random forest classifier to get better probability scores and set a threshold confidence level to accept or reject a classification. You can see an example of using probability predictions in the <em>Facial_Recognition_notebook2_Modeling.ipynb</em> notebook in the section labeled “Soft Prediction.” </p>
<p>The final step in our proof of concept is to save our work for future use. In the next section, we’ll cover saving and reloading the trained model in a way that’s suitable for deployment in modern production environments.</p>
<h3 id="h2-502567c10-0010">Model Persistence</h3>
<p class="BodyFirst">It wouldn’t be practical if you had to retrain the model every time you wanted to classify an image of a face. Training a model like this from any realistic facial database will take hours on a single machine. Luckily, we can store the results of the trained model and then load that saved state into one or more processing applications without having to train them on the data directly.</p>
<p>The scikit-learn documentation on model persistence (<a class="LinkURL" href="https://scikit-learn.org/stable/model_persistence.html">https://scikit-learn.org/stable/model_persistence.html</a>) recommends using a library called joblib to handle storing the data in a <em>pickled</em> format. As you might know, pickle is Python’s most popular data serialization and storage library and is capable of storing complex Python objects to a file on disk. The joblib library includes two functions, <code>dump</code> and <code>load</code>, which are convenience wrappers around the pickle library. The following code uses the <code>joblib.dump</code> function to save the trained model to a file named <em>trained_facial_model.pkl</em>:</p>
<pre><code>import joblib
joblib.dump(rfc, "trained_facial_model.pkl")</code></pre>
<p>Now we can load the previously trained model into another program by simply calling the <code>joblib.load</code> function:</p>
<pre><code>import joblib
loaded_model = joblib.load("trained_facial_model.pkl")</code></pre>
<p>From this point, you can treat the <code>loaded_model</code> object the same way you would treat the previously trained <code>rfc</code> model. If you run the <code>type</code> function on the <code>loaded_model</code> object, you’ll see <code>&lt;class 'sklearn.ensemble._forest.RandomForestClassifier'&gt;</code>.</p>
<p>One of the major benefits to saving a trained model as a pickled object is that we separate the training of the model from the use of the model. <span epub:type="pagebreak" id="Page_205" title="205"/>When we need to incorporate new data into our model (such as images of a new person’s face), we can rerun the model training code without interrupting any running analysis. From the point the training process completes, all future runs of the code that loads the model can use the updated model, allowing for seamless updates. </p>
<p>Finally, a note on security: it’s well known that unpickling a specially crafted malicious object can result in arbitrary code execution. Since the <code>joblib.load</code> function just wraps the <code>pickle.load</code> function, the same is true for it as well. To reduce your risk, you should never unpickle or load an object from an untrusted source. When you develop an application that loads pickled models in production, you need to make sure you add some type of data integrity check <em>before</em> you unpickle the model.</p>
<h2 id="h1-502567c10-0005">Summary</h2>
<p class="BodyFirst">In this chapter we’ve explored all the steps necessary to develop a functional facial recognition system using our knowledge of computational geometry and a healthy dose of machine learning principles. With some tweaks and further testing, you can definitely improve the system’s performance. By combining the geometric information used here with other nongeometric analysis—such as color palette histograms, wavelet transformations, or Eigenfaces—you can get the performance well above 95 percent. Some researchers have reported accuracy as high as 99.96 percent (0.04 percent error rate, as of 2020).<sup class="endnote"><a href="b01.xhtml#c10-endnote-006" id="c10-noteref-006">6</a></sup> This was under optimal conditions on a test set developed by the US National Institute of Standards and Technology (NIST) for scoring vendors of facial recognition systems. </p>
<p>Improving the accuracy and confidence thresholds is very important when you plan to apply these facial recognition systems in a security context. Misclassifying a face that isn’t in the data as one that is—in other words, a false positive—could lead to accidental authentication of something very sensitive. Another potential security risk researchers have proven under test conditions is the use of photos or specially designed masks to bypass facial recognition systems.<sup class="endnote"><a href="b01.xhtml#c10-endnote-007" id="c10-noteref-007">7</a></sup> Clearly there’s still a lot of open area for research and improvement in this field. To continue developing the system, you can combine all of the elements into a processing pipeline using a platform like TensorFlow or Spark to distribute the computational load across many computers and begin to fix (or attack) some of these problems yourself. You can also translate the principles into other image classification domains, such as fingerprint analysis or software failure analysis.</p>
<p>This ends our adventure through computational geometry. I hope the last three projects have shown you how flexible a tool it can be. Oftentimes, translating all or part of a problem into a geometric representation can help you understand the essence better. There’s a vast amount of theory left for you to explore on your own. With the foundations you’ve picked up here, the rest will be much easier to understand and apply in meaningful security applications.</p>
<p><span epub:type="pagebreak" id="Page_206" title="206"/>We’ll revisit some computational geometry in the next part of the book, “<span class="xref" itemid="xref_target_The Art Gallery Problem"><a href="p03.xhtml">The Art Gallery Problem</a></span>,” where we’ll use it to help analyze the shapes of rooms and the locations of resources, much in the same way we applied Voronoi tessellation to analyze the distribution of Portland’s fire stations. Let’s turn there now!</p>
</section>
</body>
</html>