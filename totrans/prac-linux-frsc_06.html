<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch06"><span epub:type="pagebreak" id="page_145"/><strong><span class="big">6</span><br/>RECONSTRUCTING SYSTEM BOOT AND INITIALIZATION</strong></h2>&#13;
<div class="image1"><img src="Images/common01.jpg" alt="Image" width="190" height="189"/></div>&#13;
<p class="noindent">This chapter covers the forensic analysis of the Linux system boot and initialization process. We’ll examine the early boot stages where the BIOS or UEFI firmware pass control to the bootloader, the loading and executing of the kernel, and systemd initialization of a running system. Also included here is analysis of power management activities like sleep and hibernation, and the final shutdown process of the system.</p>&#13;
<h3 class="h3" id="ch00lev1_27"><strong>Analysis of Bootloaders</strong></h3>&#13;
<p class="noindent">Traditional PCs used a BIOS (basic input/output system) chip to run code from the first sector of a disk to boot the computer. This first sector is called the <em>master boot record (MBR)</em>, and it initiates the process of loading the operating system kernel and other components into memory for execution. Modern PCs use the <em>unified extensible firmware interface (UEFI)</em> to run EFI binary program files from a FAT filesystem in the EFI system partition. These UEFI-specific <span epub:type="pagebreak" id="page_146"/>programs are run directly by the firmware and manage the process of loading and executing the operating system. This section describes forensic artifacts from these early boot stages of a Linux system that may be interesting for an investigator.</p>&#13;
<p class="indent">PC-based Linux systems booting with BIOS or UEFI use software called a <em>bootloader</em> to start up. The bootloader is responsible for loading the Linux kernel and other components into memory, choosing the right kernel parameters, and executing the kernel. Non-PC systems may have a completely different boot process. For example, the Raspberry Pi doesn’t use BIOS or UEFI, but has its own bootloading mechanism,<sup><a id="ch06foot01" href="footnotes.xhtml#ch06foot_01">1</a></sup> which is also described here.</p>&#13;
<p class="indent">Modern Linux PCs overwhelmingly use the <em>GRand Unified Bootloader (GRUB)</em> system for booting. GRUB replaced the older, more basic loader called LILO (LInux LOader). This section focuses primarily on MBR and UEFI booting with GRUB. I’ll cover Raspberry Pi booting and briefly describe other bootloaders later in this chapter.</p>&#13;
<p class="indent">From a forensics perspective, we might identify or extract a number of artifacts when analyzing the bootloader process, such as:</p>&#13;
<ul>&#13;
<li class="noindent">The installed bootloader</li>&#13;
<li class="noindent">Evidence of booting more than one operating system</li>&#13;
<li class="noindent">Evidence of multiple Linux kernels previously installed</li>&#13;
<li class="noindent">Timestamps of boot files</li>&#13;
<li class="noindent">UUIDs of partitions and filesystems</li>&#13;
<li class="noindent">Parameters passed to the kernel on boot</li>&#13;
<li class="noindent">The root filesystem location</li>&#13;
<li class="noindent">The hibernation image location</li>&#13;
<li class="noindent">Bootloader password hashes</li>&#13;
<li class="noindent">EFI system partition contents</li>&#13;
<li class="noindent">Unusual bootloader binaries (for possible malware analysis)</li>&#13;
</ul>&#13;
<p class="indent"><a href="ch03.xhtml">Chapter 3</a> covered the analysis of partition tables, and even though the bootloader and partition tables are closely related, I’ve chosen to cover them separately. A comprehensive analysis of bootloader executable code is beyond the scope of this book. Analyzing maliciously modified bootloaders involves malware reverse engineering, binary code decompilation and disassembly, and execution debugging or tracing of code blocks. This topic alone could easily fill an entire book, so here I include only the extraction of bootloader components and data to be analyzed. The analysis of BIOS settings and EFI variables are operating system independent and are mentioned only briefly.</p>&#13;
<h4 class="h4" id="ch00lev2_73"><span epub:type="pagebreak" id="page_147"/><strong><em>BIOS/MBR GRUB Booting</em></strong></h4>&#13;
<p class="noindent">Booting with an MBR is considered legacy, but it’s still used (often for small virtual machines). Modern UEFI mainboards support MBR boots using the <em>compatibility support module (CSM)</em>.<sup><a id="ch06foot02" href="footnotes.xhtml#ch06foot_02">2</a></sup> Checking the PC’s BIOS/firmware settings will indicate whether CSM booting is enabled.</p>&#13;
<p class="indent"><a href="ch06.xhtml#ch06fig01">Figure 6-1</a> shows the diagram for Linux GRUB using the MBR.</p>&#13;
<div class="image"><img id="ch06fig01" src="Images/ch06fig01.jpg" alt="Image" width="452" height="650"/></div>&#13;
<p class="figcap"><em>Figure 6-1: GRUB MBR boot data flow</em></p>&#13;
<p class="indent">The BIOS reads the first sector of a drive and executes the code if the last two bytes of sector zero are 0x55 and 0xAA.<sup><a id="ch06foot03" href="footnotes.xhtml#ch06foot_03">3</a></sup> This signature indicates that it is an MBR. The 64 bytes just before the signature are reserved for a DOS partition table consisting of four entries of 16 bytes each. The first 446 bytes of an MBR contain executable binary code (written in assembly language) that is loaded into memory by the BIOS and executed. When you install or update the GRUB MBR, the <em>boot.img</em> file is written to sector zero <span epub:type="pagebreak" id="page_148"/>(after being modified to the requirements of the system) and is used as the initial bootloader code.<sup><a id="ch06foot04" href="footnotes.xhtml#ch06foot_04">4</a></sup></p>&#13;
<p class="indent">GRUB’s MBR contains several searchable strings shown here together with their hexadecimal representation:</p>&#13;
<pre>47 52 55 42 20 00 47 65 6f 6d 00 48 61 72 64 20 GRUB .Geom.Hard&#13;
44 69 73 6b 00 52 65 61 64 00 20 45 72 72 6f 72 Disk.Read. Error</pre>&#13;
<p class="noindent">The <code>grub-install</code> program runs <code>grub-bios-setup</code> to write the MBR. The 512-byte boot sector (<em>boot.img</em>) can be extracted using <code>dd</code> or with a hex editor that supports exporting the sector.</p>&#13;
<p class="indent">The code in sector zero is responsible for loading the next stage of the bootloader code and executing it. This subsequent code is also read directly from sectors on the disk; however, it is much larger (tens of kilobytes), giving it the functionality to understand partitions and filesystems, and read files. GRUB version 2 calls this stage the <em>core.img</em>, and it’s assembled from <em>*.img</em> files and modules in the <em>grub/</em> directory. This image is created with <code>grub-mkimage</code> and written directly to the drive sectors when GRUB is installed or updated. The first sector of <em>core.img</em> is stored in the MBR at byte offset 92 (0x5c) and is 8 bytes long (stored in little-endian form on Intel). In DOS-partitioned drives, the <em>core.img</em> code is typically located in the area between the MBR (from sector 1) and the start of the first partition (usually sector 63 or 2048). If this “MBR gap” is not available, the <em>core.img</em> can be stored elsewhere on the drive and read using a specified list of sectors. The first sector of <em>core.img</em> contains several searchable strings shown in the following example together with their hexadecimal representation:</p>&#13;
<pre>6C 6F 61 64 69 6E 67 00 2E 00 0D 0A 00 47 65  loading......Ge&#13;
6F 6D 00 52 65 61 64 00 20 45 72 72 6F 72 00  om.Read. Error.</pre>&#13;
<p class="indent">The <code>grub-install</code> program runs <code>grub-mkimage</code> to create and write the <em>core .img</em> to the drive. The size of the <em>core.img</em> and the list of sectors used (“block list” in the documentation) are specified in the initial sector of <em>core.img</em> (called <em>diskboot.img</em>). The <em>core.img</em> sectors can be extracted using <code>dd</code> or with a hex editor that supports exporting by sector.<sup><a id="ch06foot05" href="footnotes.xhtml#ch06foot_05">5</a></sup> The <em>core.img</em> code finds and reads the <em>grub.conf</em> file, loads additional GRUB modules, provides the menu system, and performs other GRUB tasks.</p>&#13;
<h4 class="h4" id="ch00lev2_74"><strong><em>UEFI GRUB Booting</em></strong></h4>&#13;
<p class="noindent">The BIOS/MBR boot process was introduced in the early 1980s with the original IBM PC. Around 20 years later, Intel developed a new more advanced firmware and boot system for PCs. This evolved into the UEFI standard that defines a modern interface between hardware and operating system. It includes a more scalable partitioning scheme called <em>GPT</em>, a file-based <span epub:type="pagebreak" id="page_149"/>boot partition (instead of a sector-based mechanism) called the <em>EFI System Partition (ESP)</em>, and many other modern features.</p>&#13;
<p class="indent">To prevent accidental partition data loss on GPT-partitioned drives, a <em>protective MBR</em> is installed on sector zero that defines a single maximal DOS partition with a type 0xEE, indicating the drive is using GPT partitions. (The GPT partitioning scheme is discussed in <a href="ch03.xhtml">Chapter 3</a>.)</p>&#13;
<p class="indent">The firmware’s increased sophistication helped reduce the complexity of the bootloading process. Unlike MBR, EFI booting does not require writing code blocks directly to raw sectors on a drive. Executable code can be placed in regular files and simply copied to expected locations on a normal FAT filesystem (the ESP).</p>&#13;
<p class="indent">A Linux distribution can specify a path in the ESP for a file, such as <em>EFI/Linux/grubx64.efi</em>. If this file is not found (or the EFI variable is not set), the default file is located at <em>EFI/BOOT/BOOT64.EFI</em>. This file combines the functionality of both the <em>boot.img</em> and <em>core.img</em> files described in the preceding subsection. <a href="ch06.xhtml#ch06fig02">Figure 6-2</a> is a diagram of Linux GRUB using UEFI.</p>&#13;
<div class="image"><img id="ch06fig02" src="Images/ch06fig02.jpg" alt="Image" width="451" height="612"/></div>&#13;
<p class="figcap"><em>Figure 6-2: Grub UEFI boot data flow</em></p>&#13;
<p class="indent">A mainboard with UEFI support contains more interesting forensic evidence than traditional BIOS/MBR mainboards. The firmware contains persistent EFI variables, including information about current and previously installed operating systems, boot order, secure boot information, asset and <span epub:type="pagebreak" id="page_150"/>inventory tags, and more (it’s generic and can be used to store any variables). Extracting and analyzing EFI variables from a mainboard’s NVRAM variables is beyond the scope of this book. GRUB detects whether a system is booting with UEFI or MBR and can install on both as appropriate.</p>&#13;
<p class="indent">From a forensics perspective, it’s important to identify and analyze suspicious binaries found in the ESP partition. ESP has been used for both exploitation and as a forensic technique for extracting memory. WikiLeaks has published leaked documents related to EFI and UEFI from Vault 7: CIA Hacking Tools Revealed (<em><a href="https://wikileaks.org/ciav7p1/cms/page_26968080.html">https://wikileaks.org/ciav7p1/cms/page_26968080.html</a></em>). Academic research work has been done to describe the use of UEFI binaries for dumping memory images (<em><a href="https://www.diva-portal.org/smash/get/diva2:830892/FULLTEXT01.pdf">https://www.diva-portal.org/smash/get/diva2:830892/FULLTEXT01.pdf</a></em>).</p>&#13;
<h4 class="h4" id="ch00lev2_75"><strong><em>GRUB Configuration</em></strong></h4>&#13;
<p class="noindent">The GRUB differences between MBR and UEFI are primarily found in the installation process (writing sectors for MBR versus copying files and setting EFI variables for UEFI). However, the configuration between the two is very similar.</p>&#13;
<p class="indent">The configuration revolves around the <em>grub.conf</em> file, which is stored in different places depending on the distribution. Here are several typical locations where the <em>grub.conf</em> might be found:</p>&#13;
<ul>&#13;
<li class="noindent"><em>/boot/grub/grub.cfg</em></li>&#13;
<li class="noindent"><em>/boot/grub2/grub.cfg</em></li>&#13;
<li class="noindent"><em>EFI/fedora/grub.cfg</em> (on the UEFI FAT filesystem)</li>&#13;
</ul>&#13;
<p class="noindent">Sometimes a Linux system will have a separate small filesystem mounted on <em>/boot/</em> where the GRUB configuration files are saved.</p>&#13;
<p class="indent">The <em>grub.cfg</em> file is not usually modified by hand, but rather generated from the <code>grub-mkconfig</code> script (<code>update-grub</code> on some systems). These scripts read configuration variables from the <em>/etc/default/grub</em> file and include helper scripts from the <em>/etc/grub.d/</em> directory. The files <em>/etc/grub.d/40_custom</em> and <em>/boot/grub/custom.cfg</em> (if they exist) are intended for additional customization.</p>&#13;
<p class="indent">The files mentioned here may contain changes and customization made by a system administrator and should be analyzed during a forensic examination. The following is a sample <em>/etc/default/grub</em> file:</p>&#13;
<pre>...&#13;
GRUB_DEFAULT=0&#13;
GRUB_TIMEOUT_STYLE=hidden&#13;
GRUB_TIMEOUT=0&#13;
GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian`&#13;
GRUB_CMDLINE_LINUX_DEFAULT="quiet splash"&#13;
GRUB_CMDLINE_LINUX=""&#13;
...</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_151"/>The <em>/usr/bin/grub-mkconfig</em> shell script<sup><a id="ch06foot06" href="footnotes.xhtml#ch06foot_06">6</a></sup> contains all the variables that can be defined (look for the <code>GRUB_*</code> lines inside the script). The <code>GRUB_CMDLINE_*</code> variables are interesting because they contain information passed to the kernel. The other variables are processed by the helper scripts. On some systems, like Fedora and SUSE, <em>/etc/sysconfig/grub</em> may be symbolically linked (symlinked) to <em>/etc/default/grub</em>.</p>&#13;
<p class="indent">The resulting <em>grub.cfg</em> file consists of multiple sections generated from each of the helper scripts. GRUB has a built-in scripting language used to parse more complex <em>grub.cfg</em> files and provide an elaborate menu and submenu interface for a user to choose boot options. Here is an example of the menu options found in a sample <em>grub.cfg</em> file:</p>&#13;
<pre>menuentry 'Arch Linux (on /dev/nvme0n1p3)'&#13;
submenu 'Advanced options for Arch Linux (on /dev/nvme0n1p3)&#13;
...&#13;
menuentry 'Linux Mint 20 Ulyana (20) (on /dev/nvme0n1p4)'&#13;
submenu 'Advanced options for Linux Mint 20 Ulyana (20) (on /dev/nvme0n1p4)'&#13;
...&#13;
menuentry 'System setup'&#13;
...</pre>&#13;
<p class="indent">During a forensic examination, the <code>menuentry</code> and <code>submenu</code> lines will potentially reveal other operating systems, past versions of other operating systems, and other setup/diagnostic options. For each of the menu options, the parameters passed to the kernel are defined, including current and past root UUIDs and the location of hibernation images (<code>resume=</code>). These are of interest in a Linux forensic examination because they provide a reconstruction of OS installation activity on the drive.</p>&#13;
<p class="indent">Historically, Linux users would dual-boot their machines into different operating systems, but it is becoming more common to use virtual machines inside one host operating system. As a result, not all installed operating systems will be detected by the GRUB configuration scripts and visible in the <em>grub.cfg</em> file.</p>&#13;
<p class="indent">In addition to loading the kernel and initramfs binary images (described in the next section), GRUB can also load CPU firmware updates (from the same directory), which are typically <em>ucode.img</em> for Intel and <em>amd-ucode.img</em> for AMD.</p>&#13;
<p class="indent">In some cases, a GRUB password may be found. If this password is only to control access during boot, it won’t affect our ability to image or analyze the system in a forensic context. The following example (as generated by SUSE scripts) shows a password-protected <em>grub.cfg</em> entry:</p>&#13;
<pre>### BEGIN /etc/grub.d/42_password ###&#13;
# File created by YaST and next YaST run probably overwrite it&#13;
set superusers="root"&#13;
password_pbkdf2 root grub.pbkdf2.sha512.10000.0E73D41624AB768497C079CA5856E5334A&#13;
<span epub:type="pagebreak" id="page_152"/>40A539FE3926A8830A2F604C78B9A1BD2C7E2C399E0F782D3FE7304E5C9C6798D49FBCC1E1A89EFE&#13;
881A46C04F2E.34ACCF04562ADDBD26781CA0B4DD9F3C75AE085B3F7937CFEA5FCC4928F10A382DF&#13;
7A285FD05CAEA283F33C1AA47AF0AFDF1BF5AA5E2CE87B0F9DF82778276F&#13;
export superusers&#13;
set unrestricted_menu="y"&#13;
export unrestricted_menu&#13;
### END /etc/grub.d/42_password ###</pre>&#13;
<p class="indent">Another feature of GRUB is the ability to request a password to unlock a LUKS-encrypted root filesystem during the bootloading process (see the section on LUKS encryption in <a href="ch03.xhtml">Chapter 3</a>).</p>&#13;
<p class="indent">You can find the grub scripting language used in <em>grub.cfg</em>, file formats, design details, and much more in the online manual (<em><a href="https://www.gnu.org/software/grub/manual/grub/">https://www.gnu.org/software/grub/manual/grub/</a></em>).</p>&#13;
<h4 class="h4" id="ch00lev2_76"><strong><em>Other Bootloaders</em></strong></h4>&#13;
<p class="noindent">SYSLINUX is a bootloader designed to boot from a DOS/Windows filesystem making it easier for new Linux users to install Linux or test a live system. It is also sometimes used for booting Linux rescue images. A SYSLINUX image can be identified by the existence of the <em>LDLINUX.SYS</em> file in the root directory. In addition, a <em>syslinux.cfg</em> configuration file may be located in the root (/) directory or the <em>/boot/</em> or <em>/syslinux/</em> subdirectories. This file determines how SYSLINUX behaves and may include (using the <code>INCLUDE</code> configuration parameter) other configuration files. These files contain information like menu options, the location of the kernel image and initial ramdisk, the kernel command line, and other defined variables.</p>&#13;
<p class="indent">SYSLINUX files are located on a FAT filesystem that can be analyzed with regular filesystem forensic tools. Within the same software project, the ISOLINUX, EXTLINUX, and PXELINUX variants are also available for booting from optical discs, Linux filesystems, and network booting with PXE (using DHCP and TFTP). See the project’s website (<em><a href="https://www.syslinux.org/">https://www.syslinux.org/</a></em>) for more information.</p>&#13;
<p class="indent">The systemd developers created an alternative UEFI bootloader and manager called systemd-boot (formerly known as Gummiboot), which was designed to provide a simple menu system, basic configuration files, and other features. One characteristic of systemd-boot is the expectation that the kernel and initial ramdisk images reside in the EFI system partition. The mainboard’s NVRAM stores a number of systemd-boot-related EFI variables. The UEFI firmware executes <code>systemd-bootx64.efi</code>, an EFI binary that looks for the default configuration file <em>loader/loader.conf</em>. Further configuration for booting multiple operating systems is found in <em>loader/entries/*</em> (typically one directory per operating system boot option). From a digital forensics perspective, the entire bootloading process and files are all contained within a single FAT filesystem that can be analyzed using common FAT filesystem forensic tools to identify timestamps and evidence of deleted files. For more information, see the systemd-boot(7) man page and the Boot Loader Specification document (<em><a href="https://systemd.io/BOOT_LOADER_SPECIFICATION/">https://systemd.io/BOOT_LOADER_SPECIFICATION/</a></em>).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_153"/>Diskless systems may use the Preboot eXecution Environment (PXE) to boot the operating system over the network. Here the mainboard firmware makes DHCP requests to the local network segment and then fetches the bootloader, kernel, and initramfs. The root filesystem is then mounted via NFS or other network file-sharing protocol. A netbooting machine might still have a local drive for caching or swap, which can be analyzed. If no physical drive is installed, all forensic evidence (operating system filesystem tree, home directories, and so on) will reside on the PXE server.</p>&#13;
<p class="indent">The Raspberry Pi does not use MBR, UEFI, or even GRUB for booting, relying instead on its own multistage boot process.<sup><a id="ch06foot07" href="footnotes.xhtml#ch06foot_07">7</a></sup> The first stage of the bootloader is code in the ROM, which loads the second stage <em>bootcode.bin</em> file (this file is stored in the EEPROM of Raspberry Pi 4 models). The third stage (<em>start*.elf</em> ) is a binary firmware image that finds and starts the kernel. Potentially interesting artifacts are the user configurable settings in several files in the <em>/boot/</em> directory. The <em>cmdline.txt</em> file specifies parameters that are passed to the kernel. The <em>settings.conf</em> file specifies the parameters for the bootloaders to configure the Raspberry Pi during startup. A <em>wpa_supplicant.conf</em> file that contains a Wi-Fi network and password may also exist. If an <em>ssh</em> or <em>ssh.txt</em> file existed during the first boot, a systemd unit (<em>/lib/systemd/system/sshswitch.service</em>) would enable SSH and remove the file. These are documented at the official Raspberry Pi website (<em><a href="https://www.raspberrypi.org/documentation/">https://www.raspberrypi.org/documentation/</a></em>).</p>&#13;
<p class="indent">It is also worth mentioning Linux containers and how they boot. Because containers are started from within a running Linux host system and share the same kernel as the host, they don’t need a bootloader. A Linux system can be booted in a container with a separate filesystem tree using commands provided by the container manager (LXC, systemd-nspawn, and so on). Forensic analysis here may involve the examination of both the hosting system and the container’s file tree.</p>&#13;
<h3 class="h3" id="ch00lev1_28"><strong>Analysis of Kernel Initialization</strong></h3>&#13;
<p class="noindent">The Linux kernel is modular and configurable. Kernel modules can be built into the kernel at compile time, dynamically loaded at boot or during operation, or manually loaded by the user. The configuration of the core kernel and modules can be done during boot, when loading a module (<code>modprobe</code>) or manually by the user. In this section, I describe how to identify which modules were loaded and how the kernel is configured.</p>&#13;
<p class="indent">The modules loaded and the configured state of the kernel change dynamically during operation and are visible only while the machine is running. Postmortem forensic analysis must be done through induction or inference because we can’t observe the running kernel (unless we have a memory image). This section focuses on the modules and configuration defined at boot time and attempts to find traces of other changes during operation.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_154"/>In a forensic context, knowing the kernel’s configuration and loaded modules helps us reconstruct the state of the machine under analysis, which helps us answer various questions and identify the following:</p>&#13;
<ul>&#13;
<li class="noindent">Non-default kernel modules loaded</li>&#13;
<li class="noindent">Default kernel modules prevented from being loaded</li>&#13;
<li class="noindent">Kernel configuration explicitly defined or changed</li>&#13;
<li class="noindent">Explicit changes manually made by a system administrator</li>&#13;
<li class="noindent">Changes introduced by malicious actors</li>&#13;
</ul>&#13;
<p class="indent">We are especially interested in the modules and configuration that deviate from the defaults of the distribution or installed software packages. If we can identify non-default, explicit, or deliberate activity, we can try to determine why and how these changes happened.</p>&#13;
<h4 class="h4" id="ch00lev2_77"><strong><em>Kernel Command Line and Runtime Parameters</em></strong></h4>&#13;
<p class="noindent">The kernel is just a program, albeit a unique and special one. Like most programs, it can be started with parameters to provide some initial configuration. These parameters, sometimes called the <em>kernel command line</em>, are provided by the bootloader and passed to the kernel at boot time.</p>&#13;
<p class="indent">The kernel command line parameters configure several parts of the system during boot, including the following:</p>&#13;
<ul>&#13;
<li class="noindent">Core kernel parameters</li>&#13;
<li class="noindent">Parameters for modules built in to the kernel</li>&#13;
<li class="noindent">Init system parameters (<code>systemd pid 1</code>)</li>&#13;
</ul>&#13;
<p class="indent">The kernel understands multiple parameters that allow it to configure itself when executed. Built-in kernel modules can be configured using a dot (<code>.</code>) separating the module name and the module parameter; for example, <code>libata.allow_tpm=1</code>. Parameters specified for loadable modules may be handled by the startup scripts and units of the init process. Parameters that the kernel is unable to understand are passed on to the init system, either as command parameters or as environment variables.</p>&#13;
<p class="indent">On a running system, the command line is found in <em>/proc/cmdline</em>; however, for a postmortem investigation, we must find evidence in persistent storage. Because the bootloader passes the command line to the kernel, the parameters are likely stored in the bootloader configuration (which we covered in the previous section).</p>&#13;
<p class="indent">For the GRUB bootloader, the kernel command parameters are typically found in the <em>/boot/grub/grub.cfg</em> file (some distros use a <em>grub2</em> directory). Look for a line (possibly indented) that starts with <code>linux</code> followed by the path to a kernel image. The parameters are listed after the kernel image filename, such as the following:</p>&#13;
<pre><span epub:type="pagebreak" id="page_155"/>linux /boot/vmlinuz-linux root=UUID=da292e26-3001-4961-86a4-ab79f38ed237&#13;
rw resume=UUID=327edf54-00e6-46fb-b08d-00250972d02a libata.allow_tpm=1&#13;
intel_iommu=on net.ifnames=0</pre>&#13;
<p class="indent">In this example, the root filesystem is defined (<code>root=UUID=...</code>), the hibernate partition is defined (<code>resume=UUID=...</code>), a parameter for the built-in <code>libata</code> module is configured (<code>libata.allow_tpm=1</code>), a core kernel parameter is configured (<code>intel_iommu=on</code>), and network configuration is passed on to systemd init (<code>net.ifnames=0</code>).</p>&#13;
<p class="indent">As mentioned earlier, the <em>grub.cfg</em> file is typically generated with scripts. These scripts read the <em>/etc/default/grub</em> file for additional kernel parameters defined in <code>GRUB_CMDLINE_*</code> variables. For systemd-boot, the kernel parameters are defined in the <em>loader/entries/*</em> files. On Raspberry Pi systems, the user-configurable kernel command line is stored in <em>/boot/cmdline.txt</em> (the boot process may add additional parameters before starting the kernel). The kernel-command-line(7) man page describes additional parameters that are interpreted by the systemd initialization process.</p>&#13;
<p class="indent">Potentially interesting forensic artifacts on the kernel command line are:</p>&#13;
<ul>&#13;
<li class="noindent">The name and location of the kernel image</li>&#13;
<li class="noindent">The location (and possible UUID) of the root filesystem (<code>root=</code>)</li>&#13;
<li class="noindent">The location of a potential hibernation memory dump (<code>resume=</code>)</li>&#13;
<li class="noindent">The configuration of modules to be loaded (<code>module.parameter=</code>)</li>&#13;
<li class="noindent">Possible alternative init<sup><a id="ch06foot08" href="footnotes.xhtml#ch06foot_08">8</a></sup> program (<code>init=</code>)</li>&#13;
<li class="noindent">Other kernel configuration indicating the use of certain hardware</li>&#13;
<li class="noindent">Possible indicators of manipulation or abuse</li>&#13;
</ul>&#13;
<p class="indent">Understanding the kernel command line gives the investigator a more complete understanding of the Linux system under examination. See the bootparam(7) man page and the Linux kernel documentation (<em><a href="https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html">https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html</a></em>) for a list of commands and further information.</p>&#13;
<h4 class="h4" id="ch00lev2_78"><strong><em>Kernel Modules</em></strong></h4>&#13;
<p class="noindent">Modules add kernel functionality to manage filesystems, network protocols, hardware devices, and other kernel subsystems. Modules can be statically built in to the kernel at compile time or dynamically added to a running kernel.</p>&#13;
<p class="indent">To list the modules statically compiled into the kernel, we can view the <em>/lib/modules/*/modules.builtin</em> file for the installed kernel:</p>&#13;
<pre>$ <span class="codestrong1">cat /lib/modules/5.7.7-arch1-1/modules.builtin</span>&#13;
kernel/arch/x86/platform/intel/iosf_mbi.ko&#13;
<span epub:type="pagebreak" id="page_156"/>kernel/kernel/configs.ko&#13;
kernel/mm/zswap.ko&#13;
...</pre>&#13;
<p class="noindent">Because these modules are static on the filesystem, they are easy to identify and examine in a postmortem forensic analysis. There may also be multiple kernels installed that can be compared to one another as well as with the original files in the distribution’s release.</p>&#13;
<p class="indent">Modules inserted and removed dynamically can be identified from the boot configuration and available logs. To determine the modules loaded at boot time, we can examine configuration files in several places.</p>&#13;
<p class="indent">The systemd initialization process provides the <code>systemd-modules-load</code> <code>.service</code> to load kernel modules during boot. A local user (or system administrator) can explicitly load modules at boot by placing configuration files in <em>/etc/modules-load.d/*.conf</em>. Software packages that provide their own configuration to load modules explicitly can be found in <em>/usr/lib/modules -load.d/*.conf</em>. Here is an example of a configuration file to load modules for the CUPS printing system:</p>&#13;
<pre>$ <span class="codestrong1">cat /etc/modules-load.d/cups-filters.conf</span>&#13;
# Parallel printer driver modules loading for cups&#13;
# LOAD_LP_MODULE was 'yes' in /etc/default/cups&#13;
lp&#13;
ppdev&#13;
parport_pc</pre>&#13;
<p class="noindent">See the systemd-modules-load(8) and modules-load.d(5) man pages for more information.</p>&#13;
<p class="indent">There are other places to look for evidence of kernel module loading/ unloading activity. Some distributions (Debian-based, for example) may have an <em>/etc/modules</em> file containing a list of additional modules to be loaded at boot time. The shell history files (for both root and non-root users possibly using <code>sudo</code>) can be searched for evidence of commands, such as <code>modprobe</code>, <code>insmod</code>, or <code>rmmod</code> to identify modules inserted or removed by a user. The kernel command line may be used to load modules during the early boot process (by systemd). These command line options are <code>modules_load=&lt;modulename&gt;</code> or <code>rd.modules_load=&lt;modulename&gt;</code>; the latter refers to the initial RAM disk (rd).</p>&#13;
<p class="indent">Inserting and removing modules in the kernel may or may not generate log entries. The amount of logging is up to the module’s developer. For example, the <code>i2c_dev</code> driver prints nothing when removed from the kernel, and prints only minimal information when inserted. Here’s the log entry in dmesg:</p>&#13;
<pre>[13343.511222] i2c /dev entries driver</pre>&#13;
<p class="noindent">If kernel module log information is generated (via the kernel ring buffer), it will typically be passed to dmesg, syslog, or the systemd journal. See <a href="ch05.xhtml">Chapter 5</a> for more information about examining kernel messages.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_157"/>During a forensic examination, these module configuration files and directories should be reviewed for unusual or unexplained kernel modules. In particular, modules that deviate from the distribution and software package defaults should be examined.</p>&#13;
<h4 class="h4" id="ch00lev2_79"><strong><em>Kernel Parameters</em></strong></h4>&#13;
<p class="noindent">The initial kernel configuration is set during system startup, with dynamic reconfiguration occurring later based on the needs of the system over time. Some examples of dynamically changing configuration might include adding, removing, or modifying hardware; changing network settings; mounting filesystems; and so on. Even the hostname is a kernel configuration setting that is set during system boot. Forensic analysis here involves reconstructing the kernel’s configuration at boot time and determining changes that happened over time during system operation. In particular, we’re interested in configuration that deviates from normal defaults, possibly introduced by the user or a malicious actor.</p>&#13;
<p class="indent">Kernel parameters can also be specified manually at runtime. On a running system, the system administrator can read and write kernel parameters with the <code>sysctl</code> command or by redirecting text to/from the appropriate pseudo-files in the <em>/proc/sys/</em> directory. In a postmortem forensic investigation, we can search for evidence of the <code>sysctl</code> command in the shell history files or in logs indicating that <code>sysctl</code> was used with privilege escalation. The following example shows a non-privileged user (Sam) setting a kernel parameter with the sysctl <code>-w</code> flag:</p>&#13;
<pre>Dez 09 16:21:54 pc1 sudo[100924]: sam : TTY=pts/4 ; PWD=/ ; USER=root ;&#13;
COMMAND=/usr/bin/sysctl -w net.ipv6.conf.all.forwarding=1</pre>&#13;
<p class="noindent">This user enabled IPv6 packet forwarding. If an organization focused only on managing IPv4 security, this action could be a malicious attempt to bypass network controls or reduce the chances of detection.</p>&#13;
<p class="indent">Kernel parameters can also be set at boot time by adding them to configuration files. These follow the typical Linux convention of a configuration file in <em>/etc/</em> and directories for additional configuration files and are located in the following:</p>&#13;
<ul>&#13;
<li class="noindent"><em>/etc/sysctl.conf</em></li>&#13;
<li class="noindent"><em>/etc/sysctl.d/*.conf</em></li>&#13;
<li class="noindent"><em>/usr/lib/sysctl.d/*.conf</em></li>&#13;
</ul>&#13;
<p class="noindent">A system administrator will typically make changes to <em>sysctl.conf</em> or create files in the <em>/etc/sysctl.d/</em> directory. Installed packages requiring kernel configuration may also place configuration files in the <em>/usr/lib/sysctl.d/</em> directory.</p>&#13;
<p class="indent">During a forensic investigation, files and directories providing sysctl configuration should be reviewed for unusual or unexplained kernel settings. Custom modifications and deviations from the distribution defaults can be found by comparing them to the original files. The creation and last <span epub:type="pagebreak" id="page_158"/>modified timestamp on the files is a potential indicator of when the changes took place. Manual kernel setting changes may provide additional insight into an investigation (for example, changes could indicate the manual installation of a particular hardware device at some point in the past).</p>&#13;
<p class="indent">See the sysctl(8), sysctl.conf(5), and sysctl.d(5) man pages for more information about sysctl.</p>&#13;
<h4 class="h4" id="ch00lev2_80"><strong><em>Analyzing initrd and initramfs</em></strong></h4>&#13;
<p class="noindent">The kernel binary executable is typically called vmlinuz<sup><a id="ch06foot09" href="footnotes.xhtml#ch06foot_09">9</a></sup> and is usually found in the <em>/boot/</em> directory. It may also be a symlink to a filename with version information (for example, <em>vmlinuz-5.4.0-21-generic</em>). You will typically find a companion file called <em>initrd</em> or <em>initramfs</em> (sometimes with the <em>*.img</em> extension). These files may also be symlinks to filenames with version information (for example, <em>initrd.img-5.4.0-21-generic</em> or <em>initramfs-5.4-x86_64.img</em>).</p>&#13;
<p class="indent">The <em>initrd</em> and <em>initramfs</em> files solve a chicken-or-egg problem when the kernel boots. The kernel needs various files, utilities, and modules to mount the root filesystem, but those items are located on the root filesystem that can’t be mounted yet. To solve this problem, the bootloader loads a temporary minimal root filesystem containing all the required files into memory and provides it to the kernel as a RAM disk. This is called the <em>initial RAM disk</em>, and it comes in two forms: initrd and initramfs (see the initrd(4) man page for more information). The initial RAM disk file is created with scripts, usually run by bootloader tools (mkinitramfs, mkinitcpio, or dracut) during installation or when the kernel is changed or upgraded.</p>&#13;
<p class="indent">The kernel runs the init program found inside the initramfs (parameters can be passed on the kernel command line), and the initial setup begins. Some distros use busybox<sup><a id="ch06foot010" href="footnotes.xhtml#ch06foot_010">10</a></sup> as the init program within the initramfs. Others, often dracut-based,<sup><a id="ch06foot011" href="footnotes.xhtml#ch06foot_011">11</a></sup> use systemd init. When finished, there is a switch to the main root filesystem and execution is passed to the main init system to begin the full system startup.</p>&#13;
<p class="indent">From a forensics perspective, the contents of the initial RAM disks may contain interesting information about the system and the boot process, such as the following:</p>&#13;
<ul>&#13;
<li class="noindent">Possible file timestamps (though some systems set files to the Unix epoch, January 1, 1970)</li>&#13;
<li class="noindent">List of executables and kernel modules</li>&#13;
<li class="noindent">Configuration files (like <em>/etc/fstab</em>)</li>&#13;
<li class="noindent">Scripts (startup, custom, and so on)</li>&#13;
<li class="noindent">Information about RAID configuration</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_159"/>Information about encrypted filesystems</li>&#13;
<li class="noindent">Kiosk and IoT device custom startup</li>&#13;
</ul>&#13;
<p class="noindent">For cases involving encrypted filesystems, the initial RAM disk may be the only unencrypted data available to analyze. There could also be information about the decryption process and location of keys.</p>&#13;
<p class="indent">If commercial forensic tools cannot access the contents of initial RAM disk files, an investigator can copy the file to a similar Linux distribution and use Linux commands to perform the analysis.</p>&#13;
<p class="indent">For example, listing the contents of an Arch Linux <em>initramfs</em> file using <code>lsinitcpio</code>, looks like this:</p>&#13;
<pre>$ <span class="codestrong1">lsinitcpio -v initramfs-linux.img</span>&#13;
lrwxrwxrwx   0 root   root        7 Jan 1 1970 bin -&gt; usr/bin&#13;
-rw-r--r--   0 root   root     2515 Jan 1 1970 buildconfig&#13;
-rw-r--r--   0 root   root       82 Jan 1 1970 config&#13;
drwxr-xr-x   0 root   root        0 Jan 1 1970 dev/&#13;
drwxr-xr-x   0 root   root        0 Jan 1 1970 etc/&#13;
-rw-r--r--   0 root   root        0 Jan 1 1970 etc/fstab&#13;
-rw-r--r--   0 root   root        0 Jan 1 1970 etc/initrd-release&#13;
...</pre>&#13;
<p class="noindent">The <code>lsinitcpio</code> command also provides a useful analysis summary with the <code>-a</code> flag.</p>&#13;
<p class="indent">Listing the contents of a Debian <em>initrd</em> file using <code>lsinitramfs</code> looks like the following:</p>&#13;
<pre>$ <span class="codestrong1">lsinitramfs -l initrd.img-4.19.0-9-amd64</span>&#13;
drwxr-xr-x   1 root   root        0 Jun 1 08:41 .&#13;
lrwxrwxrwx   1 root   root        7 Jun 1 08:41 bin -&gt; usr/bin&#13;
drwxr-xr-x   1 root   root        0 Jun 1 08:41 conf&#13;
-rw-r--r--   1 root   root       16 Jun 1 08:41 conf/arch.conf&#13;
drwxr-xr-x   1 root   root        0 Jun 1 08:41 conf/conf.d&#13;
-rw-r--r--   1 root   root       49 May 2 2019 conf/conf.d/resume&#13;
-rw-r--r--   1 root   root     1269 Feb 6 2019 conf/initramfs.conf&#13;
drwxr-xr-x   1 root   root        0 Jun 1 08:41 etc&#13;
-rw-r--r--   1 root   root        0 Jun 1 08:41 etc/fstab&#13;
...</pre>&#13;
<p class="noindent">Fedora and SUSE have a similar tool called <code>lsinitrd</code> to list the contents of the initial RAM disk files.</p>&#13;
<p class="indent">After listing the contents of the files, it may be useful to extract files for further analysis. One easy way to do this is to extract everything into a separate directory using the <code>unmkinitramfs</code> or <code>lsinitcpio</code> tools, depending on the Linux distribution. Here is an example of extracting an <em>initrd</em> file on a Debian system:</p>&#13;
<pre>$ <span class="codestrong1">unmkinitramfs -v initrd.img-5.4.0-0.bpo.4-amd64 evidence/</span>&#13;
...&#13;
<span epub:type="pagebreak" id="page_160"/>bin&#13;
conf&#13;
conf/arch.conf&#13;
conf/conf.d&#13;
conf/initramfs.conf&#13;
conf/modules&#13;
cryptroot&#13;
cryptroot/crypttab&#13;
...&#13;
$ <span class="codestrong1">ls evidence/</span>&#13;
bin   cryptroot/ init lib32 libx32 sbin     usr/&#13;
conf/ etc/       lib  lib64 run/   scripts/ var/</pre>&#13;
<p class="indent">On an Arch system, the same <code>lsinitcpio</code> command can be used, but with the <code>-x</code> flag:</p>&#13;
<pre>$ <span class="codestrong1">lsinitcpio -v -x initramfs-linux.img</span></pre>&#13;
<p class="noindent">In these examples, <code>unmkinitramfs</code> and <code>lsinitcpio</code> will extract the contents into the current directory and thus expect to have write permission. For a postmortem examination, the file being analyzed can be copied to a separate analysis system.</p>&#13;
<p class="indent">It should be possible to analyze these files with regular commercial forensic tools without a Linux system. The files are typically compressed CPIO archives using gzip or zstd. The file can be decompressed first and then handled as a normal CPIO archive (a standard Unix format, similar to tar). These two examples list an <em>initramfs</em>’s contents by piping from a compression program (<code>gunzip</code> or <code>zstcat</code>) into the <code>cpio</code> program:</p>&#13;
<pre>$ <span class="codestrong1">gunzip -c initramfs-linux.img | cpio -itv</span>&#13;
$ <span class="codestrong1">zstdcat initramfs-linux.img | cpio -itv</span></pre>&#13;
<p class="noindent">Removing the <code>t</code> flag from the <code>cpio</code> flags will extract the contents into the current directory.</p>&#13;
<p class="indent">The bootloader can also load CPU microcode updates in a similar way to <em>initrd</em> files. These may also be packaged as CPIO files (but not compressed), and the contents can be listed with the <code>cpio</code> command. Two examples with Intel and AMD processors are shown here:</p>&#13;
<pre>$ <span class="codestrong1">cpio -itv &lt; intel-ucode.img</span>&#13;
drwxr-xr-x  2 root   root         0 Apr 27 14:00 kernel&#13;
drwxr-xr-x  2 root   root         0 Apr 27 14:00 kernel/x86&#13;
drwxr-xr-x  2 root   root         0 Apr 27 14:00 kernel/x86/microcode&#13;
drwxr-xr-x  2 root   root         0 Apr 27 14:00 kernel/x86/microcode/.enuineIntel&#13;
.align.0123456789abc&#13;
-rw-r--r--  1 root   root   3160064 Apr 27 14:00 kernel/x86/microcode/GenuineIntel.bin&#13;
6174 blocks&#13;
...&#13;
<span epub:type="pagebreak" id="page_161"/>$ <span class="codestrong1">cpio -itv &lt; amd-ucode.img</span>&#13;
-rw-r--r--  0 root   root     30546 May 27 10:27 kernel/x86/microcode/AuthenticAMD.bin&#13;
61 blocks</pre>&#13;
<p class="noindent">The timestamps in these files may vary. They can be from the original packaging process or from the local install process.</p>&#13;
<p class="indent">Some <em>initramfs</em> files (Red Hat, for example) contain a single archive for firmware and initramfs (appended to each other). To extract the second one, use the <code>skipcpio</code> tool from the dracut software package.</p>&#13;
<p class="indent">The Raspberry Pi operates differently and doesn’t need an initial RAM disk. Because the hardware is standard, Raspberry Pi developers can create a specific kernel with all the necessary drivers.</p>&#13;
<h3 class="h3" id="ch00lev1_29"><strong>Analysis of Systemd</strong></h3>&#13;
<p class="noindent">From a digital forensics perspective, we want to understand what the system was doing during startup, how it appears in a fully booted target state, and what activity has taken place over time. In particular, we are reconstructing configuration and activity that deviates from the default distro behavior. This includes configuration explicitly created by a system administrator, installed software packages, or possibly a malicious process or attacker.</p>&#13;
<p class="indent">The most common Linux initialization system is systemd. Since its original announcement in 2010, systemd has been adopted by every major Linux distribution, replacing the traditional Unix sysvinit and other distro-specific alternatives like Upstart from Ubuntu. Systemd is fundamentally different from traditional Unix and Linux init systems, and its introduction was not without controversy.</p>&#13;
<p class="indent">This section focuses on the systemd system initialization process. When performing a postmortem forensic analysis, we want to reconstruct essentially the same information provided by systemd commands on a running system (like <code>systemctl</code>, for example), which we can do by examining the systemd files and directories on the filesystem.</p>&#13;
<p class="indent">Systemd is very well documented. The systemd.index(7) man page has a list of all the systemd man pages (more than 350). For forensic investigators unfamiliar with Linux, these man pages are the best and most authoritative source of information on systemd.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>Warning: systemd makes extensive use of symlinks. If you mount a suspect Linux filesystem on your examination Linux machine, the symlinks may point to your own installation and not the suspect drive. Make sure you are analyzing the right files on the suspect filesystem during a forensic examination.</em></p>&#13;
</div>&#13;
<h4 class="h4" id="ch00lev2_81"><strong><em>Systemd Unit Files</em></strong></h4>&#13;
<p class="noindent">Systemd uses configuration files to initialize the system and manage services. This is a fundamental change from traditional Unix and Linux init systems that used shell scripts to achieve similar goals.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_162"/>Systemd uses the concept of <em>units</em> to control how a system is started or services are run. Units have associated text files called <em>unit configuration files</em>. Unit file content is organized into sections, with each section containing directives or options that are set by the system administrator, package maintainer, or distro vendor. Unit files are not only used for system startup, but also for operational maintenance (start, stop, restart, reload, and so on) and system shutdown. More information can be found in the systemd(1) and bootup(7) man pages.</p>&#13;
<p class="indent">The following list shows systemd’s 11 different unit types, listed with the objects they control and the man page describing the unit file:</p>&#13;
<div class="bqparan">&#13;
<p class="noindentin"><strong>Service</strong> For programs or daemons; systemd.service(5)</p>&#13;
<p class="noindentin"><strong>Socket</strong> For IPC and sockets; systemd.socket(5)</p>&#13;
<p class="noindentin"><strong>Target</strong> Groups of units; systemd.target(5)</p>&#13;
<p class="noindentin"><strong>Device</strong> For kernel devices; systemd.device(5)</p>&#13;
<p class="noindentin"><strong>Mount</strong> Filesystem mount points; systemd.mount(5)</p>&#13;
<p class="noindentin"><strong>Automount</strong> Filesystem on-demand mounting; systemd.automount(5)</p>&#13;
<p class="noindentin"><strong>Timer</strong> Time-based unit activation; systemd.timer(5)</p>&#13;
<p class="noindentin"><strong>Swap</strong> Swap partitions or files; systemd.swap(5)</p>&#13;
<p class="noindentin"><strong>Path</strong> Unit activation based on file changes; systemd.path(5)</p>&#13;
<p class="noindentin"><strong>Slice</strong> Units grouped for resource management; systemd.slice(5)</p>&#13;
<p class="noindentin"><strong>Scope</strong> Units grouped by process parent; systemd.scope(5)</p>&#13;
</div>&#13;
<p class="indent">Unit files are normal text files with a filename describing the unit and extension matching the type (<em>httpd.service</em> or <em>syslog.socket</em>, for example). A unit may also have an associated <em>*.d</em> directory containing <em>*.conf</em> files that provide additional configuration.</p>&#13;
<p class="indent">Unit files can have <code>[Unit]</code> and <code>[Install]</code> sections with options that describe the unit’s basic behavior and provide generic unit settings (see the systemd.unit(5) man page). All unit files except <em>target</em> and <em>device</em> have a self-titled section name with additional options that are specific to that unit type. For example, <em>service</em> has a <code>[Service]</code> section, <em>socket</em> has <code>[Socket]</code>, and so on. The <em>service</em>, <em>socket</em>, <em>swap</em>, and <em>mount</em> units have additional options that specify paths, users, groups, permissions, and other options relevant to the execution environment (see the systemd.exec(5) man page). The <em>service</em>, <em>socket</em>, <em>swap</em>, <em>mount</em>, and <em>scope</em> units have additional kill options that describe how processes belonging to a unit are terminated (see the systemd.kill(5) man page). The <em>slice</em>, <em>scope</em>, <em>service</em>, <em>socket</em>, <em>mount</em>, and <em>swap</em> units have additional resource control options that specify CPU and memory usage, IP network access control,<sup><a id="ch06foot012" href="footnotes.xhtml#ch06foot_012">12</a></sup> and other limits (see the systemd.resource-control(5) man page). All available systemd options, variables, and directives (more than 5,000!) are listed together on the systemd.directives(7) man page. When <span epub:type="pagebreak" id="page_163"/>examining unit files, this index should provide you with the documentation needed to understand the individual options.</p>&#13;
<p class="indent">The following example is a typical service unit file. It was installed from the xorg-xdm package provided by the distro and provides a graphical login screen:</p>&#13;
<pre>$ <span class="codestrong1">cat /usr/lib/systemd/system/xdm.service</span>&#13;
[Unit]&#13;
Description=X-Window Display Manager&#13;
After=systemd-user-sessions.service&#13;
&#13;
[Service]&#13;
ExecStart=/usr/bin/xdm -nodaemon&#13;
Type=notify&#13;
NotifyAccess=all&#13;
&#13;
[Install]&#13;
Alias=display-manager.service</pre>&#13;
<p class="noindent">The <code>[Unit]</code> section provides a description and dependency information. The <code>[Service]</code> section defines the command to run and other options described in the systemd.service(5) man page. The <code>[Install]</code> section provides information needed to enable or disable the unit.</p>&#13;
<p class="indent">Systemd can operate as a <em>system</em> instance (during init and system operation) or as a <em>user</em> instance (during a user login session). Users can create and manage their own systemd unit files. System administrators with privileged access can manage the systemd system unit files. When forensically examining a Linux system, you need to know where to look for unit files. These are created and saved in several common locations.</p>&#13;
<p class="indent">Unit files installed by a distro’s packaging system are located in the <em>/usr/lib/systemd/system/</em> directory (some distros may use <em>/lib/systemd/system/</em>). Unit files installed by a system administrator or those created during system configuration are typically installed in <em>/etc/systemd/system/</em>. Files created by the system administrator in the <em>/etc/systemd/system/</em> directory take precedence over those in the <em>/usr/lib/systemd/system/</em> directory. Unit files that are not part of any installed package are interesting because they were explicitly added by an administrator or potentially malicious privileged process.</p>&#13;
<p class="indent">User unit files can be created by the distro’s packaging system, a system administrator, or the users themselves. The distro’s user unit files are found in the <em>/usr/lib/systemd/user/</em> directory, and the system administrator’s user unit files are found in the <em>/etc/systemd/user/</em> directory. Users may place their own unit files in <em>~/.config/systemd/user/</em> of their home directory. User unit files are used during a user’s login session.</p>&#13;
<p class="indent">From a forensics perspective, a user’s own unit files are interesting, as they could have been created from a running program, explicitly by hand, or from malicious activity targeting the user. See the systemd.unit(5) man page for a full list of where systemd searches for unit files.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_164"/>If a unit file is empty (zero bytes) or symlinked to <em>/dev/null</em>, it is considered to be <em>masked</em>, which means it cannot be started or enabled. On a running system, unit directories can be found in the <em>/run/systemd/</em> pseudo-directory; however, they exist only in the running system’s memory, so they won’t be available during a postmortem forensic examination.</p>&#13;
<h4 class="h4" id="ch00lev2_82"><strong><em>Systemd Initialization Process</em></strong></h4>&#13;
<p class="noindent">When the kernel has started and mounted the root filesystem, it looks for the init program (typically symlinked to <em>/lib/systemd/systemd</em>) to initialize the system’s userspace. When systemd starts, it reads the <em>/etc/systemd/system.conf</em> file to configure itself. This file provides various options to change how systemd behaves. Here is part of a <em>system.conf</em> file:</p>&#13;
<pre>[Manager]&#13;
#LogLevel=info&#13;
#LogTarget=journal-or-kmsg&#13;
#LogColor=yes&#13;
#LogLocation=no&#13;
#LogTime=no&#13;
#DumpCore=yes&#13;
#ShowStatus=yes&#13;
#CrashChangeVT=no&#13;
#CrashShell=no&#13;
#CrashReboot=no&#13;
#CtrlAltDelBurstAction=reboot-force&#13;
...</pre>&#13;
<p class="indent">The default file lists all the compile time default entries, but they’re commented out (using the <code>#</code>). A system administrator may deviate from these defaults by modifying or adding entries. This file configures logging, crashing, various limits, accounting, and other settings. See the systemd-system.conf(5) man page for more information.</p>&#13;
<p class="indent">When other systemd daemons start (or reload), they also read various <em>/etc/systemd/*.conf</em> configuration files. Some examples of these files are listed here by their man page:</p>&#13;
<ul>&#13;
<li class="noindent">systemd-user.conf(5)</li>&#13;
<li class="noindent">logind.conf(5)</li>&#13;
<li class="noindent">journald.conf(5)</li>&#13;
<li class="noindent">journal-remote.conf(5)</li>&#13;
<li class="noindent">journal-upload.conf(5)</li>&#13;
<li class="noindent">systemd-sleep.conf(5)</li>&#13;
<li class="noindent">timesyncd.conf(5)</li>&#13;
<li class="noindent">homed.conf(5)</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_165"/>coredump.conf(5)</li>&#13;
<li class="noindent">resolved.conf(5)</li>&#13;
</ul>&#13;
<p class="indent">The systemd.syntax(7) man page calls these <em>daemon config files</em>, which shouldn’t be confused with unit files. Typically, these config files (including <em>system.conf</em> ) will also have a list of default options, which are commented out (with <code>#</code>). In a forensic examination, look for <code>*.conf</code> entries that have been uncommented or added. These indicate explicit changes made by the system owner.</p>&#13;
<p class="indent">Traditional Unix and Linux systems have <em>run levels</em>, where a system can be brought up into different states of operation (single user, multiuser, and so on). Systemd has a similar concept called <em>targets</em>. A target is reached when a defined group of units have successfully become active. The primary purpose of targets is to manage dependencies.</p>&#13;
<p class="indent">When systemd boots, it starts all the units needed to achieve the default target state. The default target is the <em>default.target</em> unit file, which is usually a symlink to another target such as <em>graphical.target</em> or <em>multi-user.target</em>. Some common target states that Linux systems have include:</p>&#13;
<div class="bqparan">&#13;
<p class="noindentin"><strong><em>rescue.target</em></strong> Single-user mode, for sysadmins, no users, minimal services</p>&#13;
<p class="noindentin"><strong><em>sysinit.target</em></strong> <strong>and</strong> <strong><em>basic.target</em></strong> Set up swap, local mount points, sockets, timers, and so on</p>&#13;
<p class="noindentin"><strong><em>multi-user.target</em></strong> A fully booted system without the graphical interface (typical for servers)</p>&#13;
<p class="noindentin"><strong><em>graphical.target</em></strong> A fully booted graphical system</p>&#13;
<p class="noindentin"><strong><em>default.target</em></strong> The default, usually a symbolic link to multiuser or graphical targets</p>&#13;
<p class="noindentin"><strong><em>shutdown.target</em></strong> Cleanly brings the system down</p>&#13;
</div>&#13;
<p class="indent">The systemd standard targets are described in the systemd.special(7) and bootup(7) man pages. The traditional Unix-style boot is described in the boot(7) man page. The default target can be overridden by explicitly providing another target name on the kernel command line (<code>systemd.unit=</code>).</p>&#13;
<p class="indent">Unit files contain information about dependency relationships to other unit files or targets. These are defined in the <code>[Unit]</code> and <code>[Install]</code> sections. During startup, the <code>[Unit]</code> section defines the dependencies and how a unit behaves if those dependencies have failed. The following list shows some common dependency options:</p>&#13;
<div class="bqparan">&#13;
<table>&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:80%"/>&#13;
</colgroup>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top"><span class="codestrong">Wants=</span></td>&#13;
<td style="vertical-align: top">Other units wanted by this unit (continue if they failed)</td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><span class="codestrong">Requires=</span></td>&#13;
<td style="vertical-align: top">Other units required by this unit (fail if they failed)</td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><span class="codestrong">Requisite=</span></td>&#13;
<td style="vertical-align: top">Fail if other units are not already active</td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><span class="codestrong">Before=</span></td>&#13;
<td style="vertical-align: top">This unit must be activated before these others</td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><span class="codestrong">After=</span></td>&#13;
<td style="vertical-align: top">This unit must be activated after these others</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
</div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_166"/>An alternative to the <code>Wants=</code> and <code>Requires=</code> options is to place unit files or symlinks to unit files in the <em>*.wants/</em> or <em>*.requires/</em> directories.</p>&#13;
<p class="indent">Starting with the <em>default.target</em> unit file, it is possible to work backward and build a list of all started unit files based on the <code>Requires=</code> and <code>Wants=</code> configuration entries or <em>*.wants/</em> and <em>*.requires/</em> directories. This approach requires an exhaustive manual examination, which may be necessary in some investigations. If you want to assess only what services have been created or enabled by the system administrator under normal circumstances, analyze the <em>/etc/systemd/system/</em> directory for the existence of unit files (or symlinks to unit files).</p>&#13;
<p class="indent">Options in the <code>[Install]</code> section of a unit file are used to enable or disable a unit with the <code>systemctl</code> command. This section is not used by systemd during startup. The <code>[Install]</code> dependencies can be defined with <code>WantedBy=</code> or <code>RequiredBy=</code> options.</p>&#13;
<h4 class="h4" id="ch00lev2_83"><strong><em>Systemd Services and Daemons</em></strong></h4>&#13;
<p class="noindent">A <em>daemon</em> (pronounced either dee-men or day-mon) originates from Unix and describes a process running in the background. Systemd starts daemons using a <em>*.service</em> unit file that includes a <code>[Service]</code> section to configure how the daemon is started. Daemons can also be started on demand using various forms of activation (described in the next section). The words <em>service</em> and <em>daemon</em> are often used interchangeably, but in the context of systemd, there are differences. A systemd service is more abstract, can start one or more daemons, and has different service types.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>Starting and stopping a service is not the same as enabling and disabling a service. If a service is</em> enabled<em>, it will automatically start at boot time. If</em> disabled<em>, it will not start at boot time. Services can be started and stopped by a system administrator during system operation, independent of the enabled/disabled state. A</em> masked <em>service can’t be started or enabled.</em></p>&#13;
</div>&#13;
<p class="indent">Daemons under systemd are slightly different from traditional Unix daemons because their terminal output (<code>stdout</code> and <code>stderr</code>) is captured by the systemd journal. See <em><a href="https://www.freedesktop.org/software/systemd/man/daemon.html">https://www.freedesktop.org/software/systemd/man/daemon.html</a></em> for a detailed comparison between systemd and traditional daemons.</p>&#13;
<p class="indent">This example unit file (<code>sshd.service</code>) manages the secure shell daemon:</p>&#13;
<pre>[Unit]&#13;
Description=OpenSSH Daemon&#13;
Wants=sshdgenkeys.service&#13;
After=sshdgenkeys.service&#13;
After=network.target&#13;
&#13;
[Service]&#13;
ExecStart=/usr/bin/sshd -D&#13;
ExecReload=/bin/kill -HUP $MAINPID&#13;
&#13;
<span epub:type="pagebreak" id="page_167"/>KillMode=process&#13;
Restart=always&#13;
&#13;
[Install]&#13;
WantedBy=multi-user.target</pre>&#13;
<p class="noindent">This file describes how to start, stop, and reload the daemon, and also when it should be started.</p>&#13;
<p class="indent">On a live system, units can be active or inactive (that is, started or stopped), and their status can be checked with the <code>systemctl status</code> command. On a forensic image, we can determine only whether a unit is enabled or disabled at startup (obviously, nothing is active on a dead system). When a system administrator explicitly enables a service, a symlink is created in <em>/etc/systemd/ system/</em> or in a <em>*.target.wants/</em> directory. Examining all the symlinks in these directories will indicate which services are started for each target.</p>&#13;
<p class="indent">In the example <em>sshd.service</em> unit file in the preceding code block, we can determine that the secure shell daemon is enabled by observing the symlink created in the multi-user target’s <em>*.wants/</em> directory:</p>&#13;
<pre>$ <span class="codestrong1">stat /etc/systemd/system/multi-user.target.wants/sshd.service</span>&#13;
  File: /etc/systemd/system/multi-user.target.wants/sshd.service -&gt;&#13;
  /usr/lib/systemd/system/sshd.service&#13;
  Size: 36         Blocks: 0          IO Block: 4096  symbolic link&#13;
Device: 802h/2050dInode: 135639164  Links: 1&#13;
Access: (0777/lrwxrwxrwx) Uid: (  0/  root)  Gid: (  0/  root)&#13;
Access: 2020-08-09 08:06:41.733942733 +0200&#13;
Modify: 2020-08-09 08:06:41.670613053 +0200&#13;
Change: 2020-08-09 08:06:41.670613053 +0200&#13;
 Birth: 2020-08-09 08:06:41.670613053 +0200</pre>&#13;
<p class="noindent">We can also see from the timestamps when the symlink was created, indicating when the service was last enabled. The timestamps on the original file <em>/usr/lib/systemd/system/sshd.service</em> indicate when the service file was last installed or upgraded.</p>&#13;
<p class="indent">The starting and stopping of services is logged. The following example shows the secure shell daemon being stopped and started (restarted):</p>&#13;
<pre>Aug 09 09:05:15 pc1 systemd[1]: Stopping OpenSSH Daemon...&#13;
   Subject: A stop job for unit sshd.service has begun execution&#13;
...&#13;
   A stop job for unit sshd.service has begun execution.&#13;
Aug 09 09:05:15 pc1 systemd[1]: sshd.service: Succeeded.&#13;
   Subject: Unit succeeded&#13;
...&#13;
   The unit sshd.service has successfully entered the 'dead' state.&#13;
Aug 09 09:05:15 pc1 systemd[1]: Stopped OpenSSH Daemon.&#13;
   Subject: A stop job for unit sshd.service has finished&#13;
...&#13;
   <span epub:type="pagebreak" id="page_168"/>A stop job for unit sshd.service has finished.&#13;
...&#13;
Aug 09 09:05:15 pc1 systemd[1]: Started OpenSSH Daemon.&#13;
   Subject: A start job for unit sshd.service has finished successfully&#13;
...&#13;
   A start job for unit sshd.service has finished successfully.&#13;
...&#13;
   The job identifier is 14262.&#13;
Aug 09 09:05:15 pc1 sshd[18405]: Server listening on 0.0.0.0 port 22.&#13;
Aug 09 09:05:15 pc1 sshd[18405]: Server listening on :: port 22.</pre>&#13;
<p class="indent">The systemd journal does not log information about enabling or disabling services aside from a simple <code>systemd[1]: Reloading</code> message. An examination of the file timestamps on the symlink will determine when services were enabled. If services were enabled with <code>systemctl</code>, the timestamps should correlate with the systemd reloading log entry.</p>&#13;
<h4 class="h4" id="ch00lev2_84"><strong><em>Activation and On-Demand Services</em></strong></h4>&#13;
<p class="noindent">The concept behind on-demand services is simply that a background process or daemon is not started until the moment it is needed. Services and daemons can be triggered in various ways, including by D-Bus, socket, path, and device activation. Service activation can be used in a system context or be specific to individual users. Activation is typically logged and can be examined in a forensic investigation.</p>&#13;
<h5 class="h5"><strong>Socket Activation</strong></h5>&#13;
<p class="noindent">Socket activation is the starting of services based on incoming FIFO, IPC, or network connection attempts. Traditional Unix-style activation used a daemon called inetd (or the xinetd alternative) to listen on multiple incoming TCP and UDP ports and start the appropriate daemon when a network connection was attempted. Today, systemd’s <em>*.socket</em> unit files provide the same functionality. In the following example, PipeWire<sup><a id="ch06foot013" href="footnotes.xhtml#ch06foot_013">13</a></sup> is configured to be socket activated if a user needs it:</p>&#13;
<pre>$ <span class="codestrong1">cat /usr/lib/systemd/user/pipewire.socket</span>&#13;
[Unit]&#13;
Description=Multimedia System&#13;
&#13;
[Socket]&#13;
...&#13;
ListenStream=%t/pipewire-0&#13;
...</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_169"/>Here the user’s runtime directory (<code>%t</code>) is selected as the location of the <code>pipewire-0</code> listening pipe. If it is accessed, a service with the same name is activated:</p>&#13;
<pre>$ <span class="codestrong1">cat /usr/lib/systemd/user/pipewire.service</span>&#13;
[Unit]&#13;
Description=Multimedia Service&#13;
...&#13;
Requires=pipewire.socket&#13;
&#13;
[Service]&#13;
Type=simple&#13;
ExecStart=/usr/bin/pipewire&#13;
...</pre>&#13;
<p class="noindent">The <code>ExecStart</code> option then runs the <code>pipewire</code> program. Notice how two unit files are used, one for the socket activation and one for the actual service. See the systemd.socket(5) man page for more information, and see <a href="ch08.xhtml">Chapter 8</a> for network service examples.</p>&#13;
<h5 class="h5"><strong>D-Bus Activation</strong></h5>&#13;
<p class="noindent">The D-Bus<sup><a id="ch06foot014" href="footnotes.xhtml#ch06foot_014">14</a></sup> is both a library and daemon (<code>dbus-daemon</code>) that facilitates communication between processes. The D-Bus daemon can run as a system-wide instance or as part of a user login session. Several common directories are associated with D-Bus configuration that can be examined on a suspect drive image:</p>&#13;
<div class="bqparan">&#13;
<p class="noindentin"><strong><em>/usr/share/dbus-1/</em></strong> Package default configuration</p>&#13;
<p class="noindentin"><strong><em>/etc/dbus-1/</em></strong> Sysadmin-specified configuration</p>&#13;
<p class="noindentin"><strong><em>~</em></strong><strong><em>/.local/share/dbus-1/</em></strong> User-specified configuration</p>&#13;
</div>&#13;
<p class="indent">These directories (if they exist) may contain system and session configuration files, XML definition files, and service files specifying activation details.</p>&#13;
<p class="indent">The <code>dbus-daemon</code> manages D-Bus activity, activates services on request, and logs activity to the systemd journal. Once a D-Bus service is requested, the service is activated either directly or via systemd. See the dbus-daemon(1) man page for more information.</p>&#13;
<p class="indent">The logging of D-Bus activation shows several items that are interesting in reconstructing past events. In this example, a D-Bus request is made to activate the PolicyKit service:</p>&#13;
<pre>   Aug 08 09:41:03 pc1  <span class="ent">➊</span> dbus-daemon[305]: [system] Activating via  <span class="ent">➋</span> systemd:&#13;
<span class="ent">➌</span> service name='org.freedesktop.PolicyKit1' unit='polkit.service'&#13;
   requested by ':1.3' (uid=0 pid=310 comm="/usr/lib/systemd/systemd-logind  <span class="ent">➍</span> ")&#13;
   ...&#13;
   <span epub:type="pagebreak" id="page_170"/>Aug 08 09:41:03 pc1 dbus-daemon[305]: [system] Successfully activated&#13;
   service 'org.freedesktop.PolicyKit1'</pre>&#13;
<p class="noindent">Here, the D-Bus daemon (shown with its PID) <span class="ent">➊</span> generates the log and asks systemd <span class="ent">➋</span> to start the policykit service <span class="ent">➌</span>. The originator of the activation request is also logged <span class="ent">➍</span> (<code>systemd-logind</code> in this case).</p>&#13;
<p class="indent">Services that are D-Bus aware may also shut down after a period of inactivity. In this example, the GeoClue service is started by D-Bus activation, and the service terminates itself after 60 seconds of inactivity:</p>&#13;
<pre>Mar 21 19:42:41 pc1 dbus-daemon[347]: [system] Activating via systemd: service&#13;
name='org.freedesktop.GeoClue2' unit='geoclue.service' requested by ':1.137'&#13;
(uid=1000 pid=2163 comm="/usr/bin/gnome-shell ")&#13;
...&#13;
Mar 21 19:43:41 pc1 geoclue[2242]: Service not used for 60 seconds. Shutting down..&#13;
Mar 21 19:43:41 pc1 systemd[1]: geoclue.service: Succeeded.</pre>&#13;
<h5 class="h5"><strong>Path-Based Activation</strong></h5>&#13;
<p class="noindent">Path-based activation uses a kernel feature called inotify that allows the monitoring of files and directories. The <em>*.path</em> unit files define which files to monitor (see the systemd.path(5) man page). A <em>*.service</em> file with the same name is activated when the path unit file’s conditions are met. In this example, a <em>canary.txt</em> file is monitored to detect possible ransomware. The canary file, path unit, and service unit are shown here:</p>&#13;
<pre>$ <span class="codestrong1">cat /home/sam/canary.txt</span>&#13;
If this file is encrypted by Ransomware, I will know!&#13;
&#13;
$ <span class="codestrong1">cat /home/sam/.config/systemd/user/canary.path</span>&#13;
[Unit]&#13;
Description=Ransomware Canary File Monitoring&#13;
&#13;
[Path]&#13;
PathModified=/home/sam/canary.txt&#13;
&#13;
$ <span class="codestrong1">cat /home/sam/.config/systemd/user/canary.service</span>&#13;
[Unit]&#13;
Description=Ransomware Canary File Service&#13;
&#13;
[Service]&#13;
Type=simple&#13;
ExecStart=logger "The canary.txt file changed!"</pre>&#13;
<p class="indent">Two unit files, <em>canary.path</em> and <em>canary.service</em>, are located in the user’s <em>~/.config/systemd/user/</em> directory and define the path-activated service. If the file is modified, the service is started and the command executed, which is shown in the journal:</p>&#13;
<pre><span epub:type="pagebreak" id="page_171"/>&#13;
Dec 13 10:14:39 pc1 systemd[13161]: Started Ransomware Canary File Service.&#13;
Dec 13 10:14:39 pc1 sam[415374]: The canary.txt file changed!&#13;
Dec 13 10:14:39 pc1 systemd[13161]: canary.service: Succeeded.</pre>&#13;
<p class="noindent">Here, the logs show the canary service starting, executing (the logger command output), and finishing (<code>Succeeded</code>). A user must be logged in for their own unit files to be active.</p>&#13;
<h5 class="h5"><strong>Device Activation</strong></h5>&#13;
<p class="noindent">Device activation uses the udev dynamic device management system (the <code>systemd-udevd</code> daemon). The appearance of new devices observed by the kernel can be configured to activate service unit files. The <em>*.device</em> unit files described in the systemd.device(5) man page are created dynamically on a running kernel and aren’t available during a postmortem forensic examination. However, we can still examine systemd device activation configured in the udev rule files and the journal. For example, a rule file (<em>60-gpsd.rules</em>) defines a systemd service to run when a particular GPS device (pl2303) is plugged in:</p>&#13;
<pre>$ <span class="codestrong1">cat /usr/lib/udev/rules.d/60-gpsd.rules</span>&#13;
...&#13;
ATTRS{idVendor}=="067b", ATTRS{idProduct}=="2303", SYMLINK+="gps%n",&#13;
TAG+="systemd" <span class="ent">➊</span>, ENV{SYSTEMD_WANTS}="gpsdctl@%k.service" <span class="ent">➋</span> &#13;
...&#13;
$ <span class="codestrong1">cat /usr/lib/systemd/system/gpsdctl@.service</span> <span class="ent">➌</span> &#13;
[Unit]&#13;
Description=Manage %I for GPS daemon&#13;
...&#13;
[Service]&#13;
Type=oneshot&#13;
...&#13;
RemainAfterExit=yes&#13;
ExecStart=/bin/sh -c "[ \"$USBAUTO\" = true ] &amp;&amp; /usr/sbin/gpsdctl add /dev/%I || :"&#13;
ExecStop=/bin/sh -c "[ \"$USBAUTO\" = true ] &amp;&amp; /usr/sbin/gpsdctl remove /dev/%I || :"&#13;
...</pre>&#13;
<p class="noindent">In this example, the udev rule is tagged with systemd <span class="ent">➊</span> and the <code>SYSTEMD_WANTS</code> <span class="ent">➋</span> environment variable specifies the <code>gpsdctl@.service</code> template with <code>%k</code> representing the kernel name of the device (it will become <code>ttyUSB0</code>). The service template file <span class="ent">➌</span> describes how and what program to run. The journal shows the insertion of the device and subsequent activation:</p>&#13;
<pre>Dec 13 11:10:55 pc1 kernel: pl2303 1-1.2:1.0: pl2303 converter detected&#13;
Dec 13 11:10:55 pc1 kernel: usb 1-1.2: pl2303 converter now attached to ttyUSB0&#13;
Dec 13 11:10:55 pc1 systemd[1]: Created slice system-gpsdctl.slice.&#13;
Dec 13 11:10:55 pc1 systemd[1]: Starting Manage ttyUSB0 for GPS daemon...&#13;
Dec 13 11:10:55 pc1 gpsdctl[22671]: gpsd_control(action=add, arg=/dev/ttyUSB0)&#13;
<span epub:type="pagebreak" id="page_172"/>Dec 13 11:10:55 pc1 gpsdctl[22671]: reached a running gpsd&#13;
Dec 13 11:10:55 pc1 systemd[1]: Started Manage ttyUSB0 for GPS daemon.</pre>&#13;
<p class="noindent">The kernel detects the device as <code>ttyUSB0</code>, and the systemd unit is activated and runs the <code>gpsdctl</code> commands with the device name. The systemd.device(5), udev(7), and systemd-udevd(8) man pages have more information.</p>&#13;
<p class="indent">In a forensic examination, these activation logs may be useful to help reconstruct past device activity. In addition, investigators should analyze the logs immediately before and after activation to see whether anything related or suspicious can be found.</p>&#13;
<h4 class="h4" id="ch00lev2_85"><strong><em>Scheduled Commands and Timers</em></strong></h4>&#13;
<p class="noindent">Every modern operating system allows scheduling of programs to run in the future, either once or on a repeating basis. On Linux systems, scheduling is done with traditional Unix-style <code>at</code> and <code>cron</code> jobs, or with systemd timers. From a forensics perspective, we want to answer several questions:</p>&#13;
<ul>&#13;
<li class="noindent">What jobs are currently scheduled?</li>&#13;
<li class="noindent">When are they scheduled to execute?</li>&#13;
<li class="noindent">When was the job created?</li>&#13;
<li class="noindent">Who created the job?</li>&#13;
<li class="noindent">What is scheduled to be executed?</li>&#13;
<li class="noindent">What other jobs have been run in the past?</li>&#13;
</ul>&#13;
<p class="noindent">Log entries and files found in the <em>/var/spool/</em> directory often reveal more information to help answer these questions.</p>&#13;
<h5 class="h5"><strong>at</strong></h5>&#13;
<p class="noindent">The <code>at</code> program is used to create jobs that are run once at a specific time by the <code>atd</code> daemon. One example of malicious activity using <code>at</code> jobs is to execute a logic bomb at some point in the future. A scheduled <code>at</code> job is identified by a file located in the <em>/var/spool/at/</em> or <em>/var/spool/cron/atjobs/</em> directory; for example:</p>&#13;
<pre># <span class="codestrong1">ls -l /var/spool/cron/atjobs</span>&#13;
total 8&#13;
-rwx------ 1 sam daemon 5832 Dec 11 06:32 a000080198df05&#13;
...</pre>&#13;
<p class="noindent">Here, the filename encodes information about the job. The first character is the queue state (<code>a</code> is pending and <code>=</code> is executing), the next five characters are the job number (in hexadecimal), and the last eight characters are the number of minutes since the epoch, January 1, 1970 (also in hexadecimal).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_173"/>Converting the last eight characters into decimal and multiplying by 60 will reveal the timestamp (in seconds) of pending execution.</p>&#13;
<p class="indent">The job file is a script created by the <code>at</code> command that contains information about how to run the program, where to email the output, environment variables, and the contents of the user’s script. Here is an example of an <code>at</code> job shell script header:</p>&#13;
<pre># <span class="codestrong1">cat /var/spool/cron/atjobs/a000080198df05</span>&#13;
#!/bin/sh&#13;
# atrun uid=1000 gid=1000&#13;
# mail sam 0&#13;
...</pre>&#13;
<p class="indent">The header information is embedded in the shell script using comments. The owner of the <code>at</code> job can be determined from the filesystem ownership or the uid comments in the shell job’s header. The job’s filesystem creation timestamp indicates when the user submitted the job. A hidden file <em>.SEQ</em> contains the number of the last job run on the system. A spool directory (<em>/var/spool/at/spool/</em> or <em>/var/spool/cron/atspool/</em>) saves the output of running jobs into email messages that are sent to the owner on completion. Investigators can check email logs and mailboxes for <code>at</code> job output email (for example, <code>Subject: Output from your job 27</code>). The timestamps of these emails will indicate when the job completed. Once an <code>at</code> job is completed, the spool files are deleted. The execution and completion of the <code>at</code> job may appear in the journal:</p>&#13;
<pre>Dec 11 07:06:00 pc1 atd[5512]: pam_unix(atd:session): session opened for user sam&#13;
by (uid=1)&#13;
...&#13;
Dec 11 07:12:00 pc1 atd[5512]: pam_unix(atd:session): session closed for user sam</pre>&#13;
<p class="indent">The submission of an at job is not logged, but it might be found in the user’s shell history. Shell histories can be searched for the existence of the at command being run.</p>&#13;
<h5 class="h5"><strong>cron</strong></h5>&#13;
<p class="noindent">The cron system is traditionally configured in the <em>/etc/crontab</em> file. The file format consists of one line per scheduled job. Each line begins with fields specifying the minute, hour, day of month, month of year, and day of week. If a field contains an asterisk (*), the command is run every time (every hour, every day, and so on). The last two fields specify the user under which to run the job as well as the command to be executed. The following is a sample <em>crontab</em> file with some helpful comments.</p>&#13;
<pre><span epub:type="pagebreak" id="page_174"/># Example of job definition:&#13;
# .---------------- minute (0 - 59)&#13;
# | .------------- hour (0 - 23)&#13;
# | | .---------- day of month (1 - 31)&#13;
# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...&#13;
# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat&#13;
# | | | | |&#13;
# * * * * * user-name command to be executed&#13;
&#13;
59 23 * * * root /root/script/backup.sh&#13;
...</pre>&#13;
<p class="noindent">In this example, every day at one minute before midnight, a backup script starts running as root.</p>&#13;
<p class="indent">Most Linux distros have a crontab and also run hourly, daily, weekly, and monthly scripts that are stored in various directories:</p>&#13;
<pre>$ <span class="codestrong1">ls -1d /etc/cron*</span> &#13;
/etc/cron.d/&#13;
/etc/cron.daily/&#13;
/etc/cron.hourly/&#13;
/etc/cron.monthly/&#13;
/etc/crontab&#13;
/etc/cron.weekly/</pre>&#13;
<p class="noindent">Installed packages can place files in these directories for periodic tasks. Individual users may also have <em>crontab</em> files in the <em>/var/spool/cron/</em> directory. The format is almost the same as <em>/etc/crontab</em>, but without the username field because the filename indicates the name of the user.</p>&#13;
<p class="indent">A forensic investigator can examine the <em>crontab</em> files and directories for signs of malicious scheduled activity (exfiltrating data, deleting files, and so on).</p>&#13;
<h5 class="h5"><strong>Systemd Timers</strong></h5>&#13;
<p class="noindent">Systemd timers are starting to replace cron on modern Linux systems. Timers are systemd unit files that specify when and how corresponding unit files (with the same name but different extensions) are activated. This is also a form of activation as discussed in the previous section, but it is timer based. Timers have a <em>*.timer</em> extension and are normal systemd units with an additional <code>[Timer]</code> section, as illustrated in this example:</p>&#13;
<pre>$ <span class="codestrong1">cat /usr/lib/systemd/system/logrotate.timer</span>&#13;
[Unit]&#13;
Description=Daily rotation of log files&#13;
Documentation=man:logrotate(8) man:logrotate.conf(5)&#13;
&#13;
[Timer]&#13;
OnCalendar=daily&#13;
<span epub:type="pagebreak" id="page_175"/>AccuracySec=1h&#13;
Persistent=true&#13;
&#13;
[Install]&#13;
WantedBy=timers.target</pre>&#13;
<p class="indent">The <em>logrotate.timer</em> unit specifies that the <em>logrotate.service</em> unit be activated every day. The <em>logrotate.service</em> unit file contains the information about how to run the <code>logrotate</code> program. Timer execution information is logged in the journal with the <code>Description=</code> string, as shown here:</p>&#13;
<pre>Jul 22 08:56:01 pc1 systemd[1]: Started Daily rotation of log files.</pre>&#13;
<p class="indent">Timers are typically found in the same locations as other systemd unit files installed by software packages or by system administrators. Users can also create timers in their own home directories (<em>./config/systemd/user/*.timer</em>), but the timers will not remain active after logout.<sup><a id="ch06foot015" href="footnotes.xhtml#ch06foot_015">15</a></sup> See the systemd.timer(5) man page for more information. Systemd provides a flexible notation for specifying time periods used in the <code>OnCalendar=</code> directive. The systemd.time(7) man page has more details.</p>&#13;
<h3 class="h3" id="ch00lev1_30"><strong>Power and Physical Environment Analysis</strong></h3>&#13;
<p class="noindent">The Linux kernel interacts directly with hardware that is part of the physical environment. Changes to this physical environment may leave digital traces in the logs that are interesting to forensic investigators. These digital traces may provide useful information about electrical power or temperature or indicate the physical proximity of people near the computer.</p>&#13;
<h4 class="h4" id="ch00lev2_86"><strong><em>Power and Physical Environment Analysis</em></strong></h4>&#13;
<p class="noindent">Most server installations have backup power with uninterruptible power supply (UPS) devices. These devices contain batteries able to provide power continuity during an outage. They usually have a serial or USB cable connected to a server responsible for taking action (clean shutdown, notification, and so on) when power fails. In a Linux environment, a daemon listens for alerts from the UPS. Common UPS software packages include PowerPanel/Cyber-Power with the <code>pwrstatd</code> daemon, Network UPS Tools (NUT) with the <code>upsd</code> daemon, and the <code>apcupsd</code> daemon.</p>&#13;
<p class="indent">This example shows a server losing and then regaining power:</p>&#13;
<pre>Aug 09 14:45:06 pc1 apcupsd[1810]: Power failure.&#13;
Aug 09 14:45:12 pc1 apcupsd[1810]: Running on UPS batteries.&#13;
...&#13;
Aug 09 14:45:47 pc1 apcupsd[1810]: Mains returned. No longer on UPS batteries.&#13;
Aug 09 14:45:47 pc1 apcupsd[1810]: Power is back. UPS running on mains.</pre>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_176"/>These logs may be useful in enterprise computing environments where accidental failure or intentional sabotage are being investigated.</p>&#13;
<p class="indent">Log messages related to laptop power may come from several sources (or not at all), depending on the Linux distro and the configuration. An ACPI daemon (<code>acpid</code>) could be running and logging to syslog, systemd or the window environment may be reacting to ACPI messages and taking actions, and there may be other daemons configured to react to ACPI changes. Linux may not fully support the implemented ACPI interface of some hardware, and certain error messages may appear. For example, in this log, the laptop noticed a change when the power cable was unplugged, but didn’t recognize what it was:</p>&#13;
<pre>Aug 09 15:51:09 pc1 kernel: acpi INT3400:00: Unsupported event [0x86]</pre>&#13;
<p class="noindent">This usually happens with a buggy or unsupported ACPI BIOS.</p>&#13;
<p class="indent">Temperature issues may result from being in a high temperature environment, blocked ventilation, fan failure, explicit overclocking by the owner, or other factors. Depending on how the system was installed and configured, the logs may have traces of temperature readings.</p>&#13;
<p class="indent">The ACPI interface may provide some temperature information, the <code>lm_sensors</code> software package provides temperature information, and other temperature programs may be plug-ins for a graphical environment. Enterprise systems may run monitoring software like Icinga/Nagios that checks and reports temperature. Daemons like <code>thermald</code> also log temperature information. Daemons like <code>hddtemp</code> read Self-Monitoring Analysis and Reporting Technology (SMART) data on drives to monitor the temperature (and log thresholds).</p>&#13;
<p class="indent">In some cases, the kernel detects temperature changes. This example shows the system reacting to high load on a CPU and changing its speed:</p>&#13;
<pre>Feb 02 15:10:12 pc1 kernel: mce: CPU2: Package temperature above threshold,&#13;
cpu clock throttled (total events = 1)&#13;
...&#13;
Feb 02 15:10:12 pc1 kernel: mce: CPU2: Core temperature/speed normal</pre>&#13;
<p class="indent">Reactions to hitting temperature thresholds depend on the software configured and may include reporting to a sysadmin, logging, slowing down a device, shutting down a device, or even shutting down the entire system. Depending on the context of an investigation, temperature indicators may be of forensic interest. Examples of this include correlating potential high CPU activity from an unexpected process or changes in the physical environment in which the machine is located.</p>&#13;
<h4 class="h4" id="ch00lev2_87"><strong><em>Sleep, Shutdown, and Reboot Evidence</em></strong></h4>&#13;
<p class="noindent">Depending on the investigation, knowing when a computer was online, offline, suspended, or rebooted can be important for building a forensic timeline. For example, knowing when a computer was suspended may conflict <span epub:type="pagebreak" id="page_177"/>with someone’s claim that a machine was online and working, or the unplanned reboot of a server could be the result of malicious activity. The state of a machine can be deduced from a timeline analysis and also determined from log analysis.</p>&#13;
<p class="indent">The ACPI specification defines multiple sleep states (“S” states) for a computer and the Linux kernel implements variations of these sleep states (<em><a href="https://www.kernel.org/doc/html/latest/admin-guide/pm/sleep-states.html">https://www.kernel.org/doc/html/latest/admin-guide/pm/sleep-states.html</a></em>). Each state listed here provides an increasing level of power savings through various methods:</p>&#13;
<div class="bqparan">&#13;
<p class="noindentin"><strong>Suspend-to-Idle (S0 Idle)</strong> Freeze userspace, devices in low power, CPU idle</p>&#13;
<p class="noindentin"><strong>Standby (S1)</strong> In addition to S0 Idle, non-boot CPUs offline, low-level system functions suspended</p>&#13;
<p class="noindentin"><strong>Suspend-to-Ram (S3)</strong> RAM has power; other hardware is off or in low power mode</p>&#13;
<p class="noindentin"><strong>Hibernation (S4 or S5)</strong> RAM is suspended to disk and system is powered off</p>&#13;
</div>&#13;
<p class="indent">The ACPI specification also defines S0 as normal operation and S5 as powered off. Under Linux, these states are changed by explicit user requests, idle timeouts, or low-battery threshold conditions.</p>&#13;
<p class="indent">Many of these sleep changes can be seen in the logs when systemd manages the suspension process:</p>&#13;
<pre>Dec 09 11:16:02 pc1 systemd[1]: Starting Suspend...&#13;
Dec 09 11:16:02 pc1 systemd-sleep[3469]: Suspending system...&#13;
...&#13;
Dec 09 11:17:14 pc1 systemd-sleep[3469]: System resumed.&#13;
Dec 09 11:17:14 pc1 systemd[1]: Finished Suspend.</pre>&#13;
<p class="indent">In some cases, individual daemons aware of the changes may also log messages about going to sleep or waking up.</p>&#13;
<p class="indent">The hibernation process suspends everything to disk and shuts the system down (analysis of this hibernation area is described in <a href="ch03.xhtml">Chapter 3</a>), which can be observed in the logs:</p>&#13;
<pre>Dec 09 11:26:17 pc1 systemd[1]: Starting Hibernate...&#13;
Dec 09 11:26:18 pc1 systemd-sleep[431447]: Suspending system...&#13;
...&#13;
Dec 09 11:29:08 pc1 kernel: PM: hibernation: Creating image:&#13;
Dec 09 11:29:08 pc1 kernel: PM: hibernation: Need to copy 1037587 pages&#13;
...&#13;
Dec 09 11:29:08 pc1 kernel: PM: Restoring platform NVS memory&#13;
Dec 09 11:29:07 pc1 systemd-sleep[431447]: System resumed.&#13;
Dec 09 11:29:08 pc1 systemd[1]: Finished Hibernate.</pre>&#13;
<p class="noindent">This example shows how systemd begins the hibernate process and then hands it over to the kernel to finish writing memory to disk. On resume, the <span epub:type="pagebreak" id="page_178"/>kernel reads memory back from disk and hands it back over to systemd to complete the wakeup.</p>&#13;
<p class="indent">Systemd manages both the initialization and shutdown of a Linux system and logs the activity to the journal. Downtime from a halt or power-off depends on the system administrator. The shutdown and bootup times can be deduced from a filesystem timeline analysis, but the information should also be available in various logs.</p>&#13;
<p class="indent">Rebooting a Linux system causes a clean shutdown and immediately restarts the system. A reboot is initiated by systemd and shown in the logs:</p>&#13;
<pre>Dec 09 08:22:48 pc1 systemd-logind[806]: System is rebooting.&#13;
Dec 09 08:22:50 pc1 systemd[1]: Finished Reboot.&#13;
Dec 09 08:22:50 pc1 systemd[1]: Shutting down.</pre>&#13;
<p class="noindent">The downtime from a reboot is limited to the time needed to shut down fully and then fully restart.</p>&#13;
<p class="indent">Halting a Linux system performs a clean shutdown and then halts the kernel, but without rebooting or powering off. The initiation of a halt process can be observed in the logs:</p>&#13;
<pre>Dec 09 12:32:27 pc1 systemd[1]: Starting Halt...&#13;
Dec 09 12:32:27 pc1 systemd[1]: Shutting down.</pre>&#13;
<p class="noindent">The final kernel logs are shown on the console (but not in the journal, as systemd logging is already stopped).</p>&#13;
<p class="indent">The power-off of a Linux system begins the same way as a reboot or halt, but the hardware is instructed to power off after the Linux shutdown is complete. A power-off can be observed in the logs:</p>&#13;
<pre>Dec 09 12:38:48 pc1 systemd[1]: Finished Power-Off.&#13;
Dec 09 12:38:48 pc1 systemd[1]: Shutting down.</pre>&#13;
<p class="noindent">Rebooting, halting, and powering off a system have similar shutdown processes. The only difference is what happens after kernel execution stops.</p>&#13;
<p class="indent">The journal keeps a list of boot periods, which you can view by copying the journal file(s) to an analysis machine and running <span class="codestrong">journalctl</span> with the <span class="codestrong">--list-boots</span> flag:</p>&#13;
<pre># <span class="codestrong1">journalctl --file system.journal --list-boots</span>&#13;
...&#13;
-4 cf247b03cd98423aa9bbae8a76c77819 Tue 2020-12-08 22:42:58 CET-Wed 2020-12-09 08:22:50 CET&#13;
-3 9c54f2c047054312a0411fd6f27bbbea Wed 2020-12-09 09:10:39 CET-Wed 2020-12-09 12:29:56 CET&#13;
-2 956e2dc4d6e1469dba8ea7fa4e6046f9 Wed 2020-12-09 12:30:54 CET-Wed 2020-12-09 12:32:27 CET&#13;
-1 5571c913a76543fdb4123b1b026e8619 Wed 2020-12-09 12:33:36 CET-Wed 2020-12-09 12:38:48 CET&#13;
 0 a494edde3eba43309957be06f20485ef Wed 2020-12-09 12:39:30 CET-Wed 2020-12-09 13:01:32 CET</pre>&#13;
<p class="noindent">This command produces a list of each boot period from start to end. Other logs, such as <em>lastlog</em> and <em>wtmp</em>, will also log reboots and shutdowns. Daemons may log shutdown information showing that they are terminating themselves due to a pending shutdown.</p>&#13;
<h4 class="h4" id="ch00lev2_88"><span epub:type="pagebreak" id="page_179"/><strong><em>Human Proximity Indicators</em></strong></h4>&#13;
<p class="noindent">Determining whether a person was within physical proximity of a computer is often useful in investigations. Although Linux has flexible remote access capabilities, with secure shell and remote desktop, investigators can still determine when some activity was likely done by a person sitting at (or near) the computer or performing some interaction with the local hardware. I call these <em>human proximity indicators</em>.</p>&#13;
<h5 class="h5"><strong>Laptop Lids</strong></h5>&#13;
<p class="noindent">One human proximity indicator is interaction with a laptop lid. If a lid was opened or closed, someone likely made physical contact with the machine to do it. Knowing the difference between a lid opening and a lid closing is also interesting, as it may indicate an intention to start working or stop working at a certain point in time.</p>&#13;
<p class="indent">Laptop lid activity is logged in the systemd journal. The following example shows a laptop lid being closed and then opened:</p>&#13;
<pre>Aug 09 13:35:54 pc1 systemd-logind[394]: Lid closed.&#13;
Aug 09 13:35:54 pc1 systemd-logind[394]: Suspending...&#13;
...&#13;
Aug 09 13:36:03 pc1 systemd-logind[394]: Lid opened.</pre>&#13;
<p class="indent">Typically, closing a laptop lid will trigger a screen-locking program, and when the lid is opened, authentication is required. Successful authentication and continued user activity (as observed from the timeline and other indicators) suggests that the machine’s owner was nearby at that time.</p>&#13;
<h5 class="h5"><strong>Power Cables</strong></h5>&#13;
<p class="noindent">The power cable on a laptop can also be interesting from an investigative perspective. If a laptop power cable was physically unplugged or plugged in, it may leave traces in the logs. Unless there was a power outage, this indicates that someone was in physical proximity of the laptop. Many laptop systems use the <code>upowerd</code> daemon for power management. This daemon keeps several logs of power-related events, including a history of battery charging/discharging states, times, and power consumption.</p>&#13;
<p class="indent">The <em>/var/lib/upower/</em> directory contains the power historical data reported via ACPI<sup><a id="ch06foot016" href="footnotes.xhtml#ch06foot_016">16</a></sup> from battery-operated peripherals and laptop batteries. A battery has four history files (* is a string identifying the battery):</p>&#13;
<div class="bqparan">&#13;
<p class="noindentin"><strong><em>history-charge-*.dat</em></strong> Log of percentage charged</p>&#13;
<p class="noindentin"><strong><em>history-rate-*.dat</em></strong> Log of energy consumption rate (in watts)</p>&#13;
<p class="noindentin"><strong><em>history-time-empty-*.dat</em></strong> When unplugged, log of time (in seconds) until empty</p>&#13;
<p class="noindentin"><strong><em>history-time-full-*.dat</em></strong> When charging, log of time (in seconds) until full</p>&#13;
</div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_180"/>There are three charging states found in the logs that may be interesting in a forensic investigation:</p>&#13;
<div class="bqparan">&#13;
<p class="noindentin"><strong>Charging</strong> Battery is being charged; cable is plugged in</p>&#13;
<p class="noindentin"><strong>Discharging</strong> Battery is discharging; cable is unplugged</p>&#13;
<p class="noindentin"><strong>Fully charged</strong> Battery is charged to its maximum; cable attached</p>&#13;
</div>&#13;
<p class="indent">For a list of all the supported charging states, see the project documentation (<em><a href="https://upower.freedesktop.org/docs/">https://upower.freedesktop.org/docs/</a></em>).</p>&#13;
<p class="indent">The charging and discharging of the battery correlates to the plugged and unplugged state of the power cable. Changes to this state are logged with a timestamp and shown in this example:</p>&#13;
<pre>$ <span class="codestrong1">cat /var/lib/upower/history-rate-5B10W13932-51-4642.dat</span>&#13;
...&#13;
1616087523     7.466  discharging&#13;
1616087643     7.443  discharging&#13;
1616087660     7.515  charging&#13;
1616087660     7.443  charging&#13;
...&#13;
1616240940     3.049  charging&#13;
1616241060     2.804  charging&#13;
1616241085     3.364  fully-charged&#13;
1616259826     1.302  discharging&#13;
1616259947     7.046  discharging&#13;
...</pre>&#13;
<p class="noindent">Here, the charging history contains timestamps (Unix epoch), power consumption, and the charging state. In a forensic examination, the transitions between <code>charging</code>, <code>discharging</code>, and <code>fully-charged</code> may indicate when a power cable was physically plugged in or unplugged (or a power outage occurred). These state transitions may be observed in one or more of the four <em>upower</em> history files.</p>&#13;
<h5 class="h5"><strong>Ethernet Cables</strong></h5>&#13;
<p class="noindent">An Ethernet cable link status can also be interesting from an investigative perspective. In server environments, if an Ethernet cable is physically plugged in or unplugged from a machine, the kernel will notice and log the information:</p>&#13;
<pre>Dec 09 07:08:39 pc1 kernel: igb 0000:00:14.1 eth1: igb: eth1 NIC Link is Down&#13;
...&#13;
Dec 09 07:08:43 pc1 kernel: igb 0000:00:14.1 eth1: igb: eth1 NIC Link is Up&#13;
1000 Mbps Full Duplex, Flow Control: RX/TX</pre>&#13;
<p class="indent">This activity may include unused Ethernet ports suddenly becoming active or configured interfaces suddenly going down. These actions can indicate human proximity (people plugging in and unplugging cables), but they can also indicate other infrastructure situations, such as a switch <span epub:type="pagebreak" id="page_181"/>going down, an administrator disabling a port, a severed cable, or the machine itself deactivating a port (with the <code>ip link set</code> command, for example). Possible malicious reasons for unexpected Ethernet port activity may include disruption, creating a side channel for data exfiltration, bypassing perimeter security, or performing some other unauthorized network activity.</p>&#13;
<h5 class="h5"><strong>Plugged-In Peripheral Devices and Removable Media</strong></h5>&#13;
<p class="noindent">Another indicator of a person’s physical proximity is the record of USB devices being plugged in or removed from a machine. <a href="ch11.xhtml">Chapter 11</a> discusses the detection of attached USB devices, but the following example shows a physically attached (and later removed) USB thumb drive:</p>&#13;
<pre>Aug 09 15:29:43 pc1 kernel: usb 1-1: New USB device found, idVendor=0951,&#13;
idProduct=1665, bcdDevice= 1.00&#13;
...&#13;
Aug 09 15:29:43 pc1 kernel: usb 1-1: Product: DataTraveler 2.0&#13;
Aug 09 15:29:43 pc1 kernel: usb 1-1: Manufacturer: Kingston&#13;
Aug 09 15:29:43 pc1 kernel: usb 1-1: SerialNumber: 08606E6D418ABDC087172926&#13;
...&#13;
Aug 09 15:53:16 pc1 kernel: usb 1-1: USB disconnect, device number 9</pre>&#13;
<p class="indent">It is also possible to determine the physical plug used to attach the USB device by examining the bus and port numbers (for example, to determine whether the activity happened in front of or behind a PC).</p>&#13;
<p class="indent">Other indicators of human proximity include the insertion or removal of physical removable media (CD-ROM, tape, SD card, and so on). Depending on the media and drive, this action may leave traces in the logs indicating that a person was present to perform the action.</p>&#13;
<h5 class="h5"><strong>Console Logins and Other Indicators</strong></h5>&#13;
<p class="noindent">Logging in to a machine from the physical console (local keyboard, screen, and so on) is the most obvious example of human proximity. If a login session is bound to a systemd “seat” (which is not the case with remote access like SSH), it indicates a local physical login. The <code>last</code> log output (described in <a href="ch10.xhtml">Chapter 10</a>) provides a history of local and remote logins.</p>&#13;
<p class="indent">A login to a local physical console will use a <code>tty</code>, whereas a remote SSH session will use a pseudoterminal (<code>pts</code>). The following example is from the <code>last</code> output showing logins from user Sam:</p>&#13;
<pre>sam   pts/3    10.0.1.10    Fri Nov 20 15:13 - 20:08 (04:55)&#13;
sam   tty7     :0           Fri Nov 20 13:52 - 20:08 (06:16)</pre>&#13;
<p class="noindent">Here <code>tty7</code> represents the local physical device where a login was made (<code>:0</code> is the X11 server), and <code>pts/3</code> shows a remote login (from the given IP address).</p>&#13;
<p class="indent">When a physical keyboard/video/mouse (KVM) device is attached to a PC and accessed remotely, physical proximity can’t be determined (unless the KVM device retains its own logs).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_182"/>Other indicators of human proximity are physical key presses on a locally attached keyboard.<sup><a id="ch06foot017" href="footnotes.xhtml#ch06foot_017">17</a></sup> These are not typically logged, but certain keys (power, brightness, function keys, and so on) may be associated with an action performed by the operating system. Logs may exist depending on the key or the daemon configured to take action. Some of these keyboard actions may also trigger scripts or programs that leave traces in the logs when run, such as shown here:</p>&#13;
<pre>Dec 09 09:30:23 pc1 systemd-logind[812]: Power key pressed.</pre>&#13;
<p class="noindent">In this example, the power button was pressed on a computer, triggering a suspend action. The physical button press is logged, indicating that someone was in proximity of the computer.</p>&#13;
<p class="indent">The use of fingerprint readers for biometric authentication can also help determine human proximity. If a person scanned in a fingerprint on a local fingerprint reader, it’s an indicator that they were in physical contact with the system at a particular point in time. The advantage here is the combined determination of proximity together with biometric identification of the person. More information about Linux fingerprint authentication is explained in <a href="ch10.xhtml">Chapter 10</a>.</p>&#13;
<p class="indent">The absence of human proximity indicators does not mean nobody was near the computer. Also, just knowing that a person was in physical proximity of a computer and performing some action does not identify that person. This must be deduced from corroborating timestamps from other logs or the filesystem (or even logs from remote servers). If a laptop lid was opened and passwords were subsequently entered to log in or unlock a physical system, those actions point to anyone with knowledge of the password, not necessarily the user observed in the logs (in other words, the password may have been stolen or known by someone else).</p>&#13;
<h3 class="h3" id="ch00lev1_31"><strong>Summary</strong></h3>&#13;
<p class="noindent">In this chapter, you have learned how a Linux system boots, runs, and shuts down. You have seen examples of systemd unit files and more examples of logs that we can use to reconstruct past events. You have also been introduced to the concept of human proximity indicators and Linux power management. This chapter provides the background knowledge an investigator needs to analyze the system layer activity of a Linux machine.</p>&#13;
</div></body></html>