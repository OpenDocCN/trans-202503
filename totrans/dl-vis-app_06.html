<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="117" id="Page_117"/>5</span><br/>
<span class="ChapterTitle">Curves and Surfaces</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">In machine learning, we frequently work with various kinds of curves and surfaces. Two of the most important properties of these objects are called the <em>derivative</em> and the <em>gradient</em>. They describe the shape of a curve or surface, and thus which directions to move in order to climb uphill or slide downhill. These ideas are at the heart of how deep systems learn. Knowing about the derivative and gradient is key to understanding backpropagation (the topic of Chapter 14), and thus knowing how to build and train successful networks.</p>
<p>As usual, we’ll skip the equations, and instead focus on building intuition for what these two terms describe. You can find mathematical depth and rigor on everything we touch on here in most books on modern multivariable calculus and in more approachable form on many online websites (Apostol 1991; Berkey 1992; 3Blue 2020). </p>
<h2 id="h1-500723c05-0001"><span epub:type="pagebreak" title="118" id="Page_118"/>The Nature of Functions</h2>
<p class="BodyFirst">As just mentioned, in machine learning, we often deal with various kinds of curves. Most often, these are plots of mathematical <em>functions</em>. We usually think of functions in terms of an input<b> </b>and an output. When we’re dealing with a curve in two dimensions (2D), the input is expressed by selecting a location on the horizontal axis of a graph. The output is the value of the curve directly above that point. In this scenario, we provide one number as input, and get back one number as output.</p>
<p>When we have two inputs, we move into the world of three dimensions. Here, our function is a surface, like a sheet fluttering in the wind. Our input is a point on the ground below the sheet, and the output is the height of the sheet directly above that point. In this situation, we provide two numbers as input (to identify a point on the ground) and again get back a single output.</p>
<p>These ideas can be generalized, so functions can accept any number of input values, also called <em>arguments</em>, and can provide multiple output values, sometimes called <em>returned values</em>, or simply <em>returns</em>. We can think of a function as a machine that converts inputs to outputs: one or more numbers go in, and one or more numbers come out. As long as we don’t deliberately introduce randomness, the system is <em>deterministic</em>: every time we give a particular function the same inputs, we get back the same outputs.</p>
<p>In this book we’re going to use curves and surfaces in a few ways. One of the most important ways, and the focus of this chapter, is to determine how to move along them in order to get back larger or smaller outputs. The technique we use for that process requires that our functions satisfy a few conditions. We’ll illustrate those conditions with curves, but the ideas extend to surfaces and more complex shapes as well.</p>
<p>We want our curves to be <em>continuous</em>, meaning that we can draw them with a single stroke of a pen or pencil, without ever lifting it from the page. We also want our curves to be <em>smooth</em>, so that they have no sharp corners (called <em>cusps</em>). <a href="#figure5-1" id="figureanchor5-1">Figure 5-1</a> shows a curve that has both of these forbidden features.</p>
<figure>
<img src="Images/f05001.png" alt="f05001" width="344" height="275"/>
<figcaption><p><a id="figure5-1">Figure 5-1</a>: The circle encloses a cusp, and the dashed arrow shows a discontinuity, or jump.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="119" id="Page_119"/>We also want our curves to be <em>single-valued</em>. In 2D, this means that for each horizontal position on the page, if we draw a vertical line at that point, the line crosses the curve only once, so only a single value corresponds to that horizontal position. In other words, if we follow the curve with our eyes from left to right (or right to left), it never reverses direction on itself. A curve that violates this condition is shown in <a href="#figure5-2" id="figureanchor5-2">Figure 5-2</a>.</p>
<figure>
<img src="Images/f05002.png" alt="f05002" width="341" height="272"/>
<figcaption><p><a id="figure5-2">Figure 5-2</a>: Inside the purple zones, the curve has multiple values in the vertical direction.</p></figcaption>
</figure>
<p>From now on, let’s assume that all of our curves meet these rules (that is, they’re smooth, continuous, and single-valued). This is a safe assumption because we’re usually going to deliberately choose curves that have these properties.</p>
<h2 id="h1-500723c05-0002">The Derivative</h2>
<p class="BodyFirst">One of the most important aspects of a curve is called its <em>derivative</em>. The derivative tells us a lot about the shape of a curve at any point along it. In this section, we look at some core ideas that lead us to the derivative. </p>
<h3 id="h2-500723c05-0001">Maximums and Minimums</h3>
<p class="BodyFirst">A vital part of training in deep learning involves minimizing the system’s error. We usually do this by imagining the error as a curve and then searching for the smallest value of that curve. </p>
<p>The more general problem is finding the smallest or largest value of a curve anywhere<em> </em>along its entire length, as illustrated in <a href="#figure5-3" id="figureanchor5-3">Figure 5-3</a>. If these are the largest and smallest values for the whole curve (and not just the part we happen to be looking at), we call these points the <em>global minimum</em> and <em>global maximum</em>.</p>
<span epub:type="pagebreak" title="120" id="Page_120"/><figure>
<img src="Images/f05003.png" alt="f05003" width="335" height="266"/>
<figcaption><p><a id="figure5-3">Figure 5-3</a>: The global maximum (brown circle), and the global minimum<b> </b>(orange square) of a curve</p></figcaption>
</figure>
<p>Sometimes we want only these largest and smallest values, but other times we want to know <em>where</em> on the curve these points are located. Sometimes finding these values can be difficult. For example, if the curve goes on forever in both directions, how can we be sure we found the very smallest or largest values? Or if the curve repeats, as it does in <a href="#figure5-4" id="figureanchor5-4">Figure 5-4</a>, which of the high (or low) points should we pick as the location of <em>the</em> global maximum or minimum?</p>
<figure>
<img src="Images/f05004.png" alt="f05004" width="335" height="266"/>
<figcaption><p><a id="figure5-4">Figure 5-4</a>: When a curve repeats forever, we can have infinitely many points that we could use as the location of the maximum (brown circles) or minimum (orange squares).</p></figcaption>
</figure>
<p>To get around these problems, let’s think of maximum and minimum values in the <em>neighborhood</em> of a given point. To describe this, consider the following little thought experiment. Starting from some point on the curve, let’s travel to the left until the curve changes direction. If the values start increasing as we move left, we continue as long as they increase, but as soon as they start to decrease, we stop. We follow the same logic if the values are decreasing as we move to the left, stopping when they start to increase. We do the same thought experiment again, starting at the same point, but this time, we move to the right. This gives us three interesting points: our starting point, and the two points where we stopped when moving left and right.</p>
<p><span epub:type="pagebreak" title="121" id="Page_121"/>The smallest value out of these three points is the <em>local minimum</em> for our starting point, and the largest value of the three points is the <em>local maximum</em> for our starting point. <a href="#figure5-5" id="figureanchor5-5">Figure 5-5</a> shows the idea.</p>
<figure>
<img src="Images/f05005.png" alt="f05005" width="335" height="266"/>
<figcaption><p><a id="figure5-5">Figure 5-5</a>: For the point in black, the brown circle and orange box respectively show that point’s local maximum and minimum. </p></figcaption>
</figure>
<p>In <a href="#figure5-5">Figure 5-5</a> we moved left until we got the point we then marked with a circle, and we moved right until we reached the point we marked with a square. The local maximum is given by the largest value of these three points, which, in this case, is the center of the brown circle. The local minimum is given by the smallest value of these three points, which, in this case, is the center of the orange square.</p>
<p>If the curve zooms off to positive or negative infinity, things get more complicated. In this book, we always assume that we can find a local minimum and maximum for any point on any curve we want.</p>
<p>Note that there is only one global maximum and only one global minimum for any given curve, but there can be many local maximums and minimums (sometimes called <em>maxima </em>and <em>minima</em>) for any given curve or surface, since they depend on the point we’re considering. <a href="#figure5-6" id="figureanchor5-6">Figure 5-6</a> shows this idea visually.</p>
<figure>
<img src="Images/f05006.png" alt="f05006" width="335" height="267"/>
<figcaption><p><a id="figure5-6">Figure 5-6</a>: The influences of these local maximums and minimums are shown by their corresponding colored region. </p></figcaption>
</figure>
<h3 id="h2-500723c05-0002"><span epub:type="pagebreak" title="122" id="Page_122"/>Tangent Lines</h3>
<p class="BodyFirst">The next step in our road to the derivative involves an idea called a <em>tangent line</em>. To illustrate the idea, we’ve marked up a two-dimensional curve in <a href="#figure5-7" id="figureanchor5-7">Figure 5-7</a>.</p>
<figure>
<img src="Images/f05007.png" alt="f05007" width="335" height="266"/>
<figcaption><p><a id="figure5-7">Figure 5-7</a>: Some points on this curve are marked with dots. The tangent line at each of those points is drawn in black.</p></figcaption>
</figure>
<p>At each point on the curve, we can draw a line whose slope is given by the shape of the curve at that point. This is the tangent line. We can think of this as a line that just grazes the curve at that point. If we imagine ourselves traveling along the curve, the tangent line tells us where we’re looking (as well as where we’d be looking if we had eyes in the back of our heads). Tangent lines are useful to us because they are horizontal at every local maximum and local minimum. One way to find a curve’s maximum and minimum values is to find points on the curve where the tangent is horizontal (as <a href="#figure5-7">Figure 5-7</a> shows, the tangent is also horizontal where the curve is horizontally flat, but we’ll ignore that for now).</p>
<p>Here’s one way to find the tangent line. Let’s pick a point, which we’ll call the <em>target point</em>. We can move an equal distance along the curve to the left and right of the target point, draw dots there, and draw a line connecting those two dots, as in <a href="#figure5-8" id="figureanchor5-8">Figure 5-8</a>.</p>
<figure>
<img src="Images/f05008.png" alt="f05008" width="694" height="151"/>
<figcaption><p><a id="figure5-8">Figure 5-8</a>: To find the tangent line at a given point, we can look at a pair of points at equal distances along the curve around that point and draw a line between them. </p></figcaption>
</figure>
<p>Now let’s pull the two dots in toward the target point at the same speed, keeping each on the curve. At the very last instant before they merge, the line that passes through them is the tangent line. We say that this line is <em><span epub:type="pagebreak" title="123" id="Page_123"/>tangent to the curve</em>, meaning that it just touches it. It’s the best straight line that describes the curve at that point. The ancient Greeks called the tangent line the <em>kissing line.</em></p>
<p>We can measure the <em>slope</em> of the tangent line we constructed in <a href="#figure5-8">Figure 5-8</a>. The slope is just a single number that tells us the angle that the line forms with respect to a horizontal line. A horizontal line has a slope of 0. If we rotate the line counterclockwise, the value takes on increasingly positive values. If we rotate the line counterclockwise, the slope takes on increasingly negative values. When a line becomes exactly vertical, its slope is said to be infinite. </p>
<p>And now we’ve come to the derivative! It’s just another name for the slope. Every point on a curve has its own derivative, because every point’s tangent line has its own slope.</p>
<p><a href="#figure5-9" id="figureanchor5-9">Figure 5-9</a> shows why we created the rules before that said our curves need to be continuous, smooth, and single-valued. Those rules guarantee that we can always find a tangent line, and thus a derivative, for every point on the curve.</p>
<figure>
<img src="Images/f05009.png" alt="f05009" width="844" height="376"/>
<figcaption><p><a id="figure5-9">Figure 5-9</a>: Top row: Curves with issues. Bottom row: Problems finding the derivative, shown in blue.</p></figcaption>
</figure>
<p>In <a href="#figure5-9">Figure 5-9</a>(a), the curve isn’t continuous, so the two different curve ends have different derivatives above our chosen point (marked with a square). The problem is that we don’t know which derivative to pick, so we avoid the question by not allowing discontinuities in the first place. In <a href="#figure5-9">Figure 5-9</a>(b), the curve isn’t smooth, so the slopes are different as we arrive at the cusp from the left and right. Again, we don’t know which one to pick, so we won’t work with curves that have cusps. In <a href="#figure5-9">Figure 5-9</a>(c), the curve isn’t single-valued. We have more than one point on the curve to choose from, each with its own derivative, and once again we don’t know which one to pick. <a href="#figure5-9">Figure 5-9</a>(d) shows that if a curve ever becomes perfectly vertical, that also violates our single-value rule. Worse, the tangent line is perfectly vertical, which means it has an infinite slope. Handling infinite values can make <span epub:type="pagebreak" title="124" id="Page_124"/>simple algorithms messy and complicated. So, we sidestep this problem, just like the others, and say that we won’t use curves that can become vertical, and thus we never need to worry about infinite derivatives. By requiring our curves to be continuous, smooth, and single-valued, we can be sure that they can never create one of these situations.</p>
<p>We said before that a curve is a graphical version of a function: we provide an input value, conventionally along the horizontal X axis, and then look up (or down) to find the y value of the curve at that x. That y value is the output of the function, as shown in <a href="#figure5-10" id="figureanchor5-10">Figure 5-10</a>.</p>
<figure>
<img src="Images/f05010.png" alt="f05010" width="345" height="305"/>
<figcaption><p><a id="figure5-10">Figure 5-10</a>: A curve in two dimensions. Values of x increase as we move right, and values of y increase as we move up.</p></figcaption>
</figure>
<p>As we move to the right from some point (that is, as x increases), we can ask if the curve is giving us values of y that are increasing, decreasing, or not changing at all. We say that if y increases as x increases, the tangent line has a <em>positive slope</em>. If y decreases with an increasing x, we say the tangent line has a <em>negative slope</em>. The more extreme the slope (that is, the closer it gets to vertical), the more positive or negative it becomes. This is just another way to state the relationship of the angle of the slope relative to a horizontal line. <a href="#figure5-11" id="figureanchor5-11">Figure 5-11</a> shows the idea.</p>
<figure>
<img src="Images/f05011.png" alt="f05011" width="345" height="274"/>
<figcaption><p><a id="figure5-11">Figure 5-11</a>: Marking the tangent lines from <a href="#figure5-7">Figure 5-7</a> by whether they have a positive slope (+), negative slope (–), or are flat (0)</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="125" id="Page_125"/>Notice that there are points in <a href="#figure5-11">Figure 5-11</a> that aren’t hills and valleys but still have a slope of 0. We only find slopes of 0 at the tops of hills, the bottoms of valleys, and plateaus like these.</p>
<h3 id="h2-500723c05-0003">Finding Minimums and Maximums with Derivatives</h3>
<p class="BodyFirst">Let’s see how to use the derivative to drive an algorithm that finds a local minimum or maximum at a point.</p>
<p>Given a point on a curve, we first find its derivative. If we want to move along the curve so that the y values increase, we move in the direction of the <em>sign</em> of the derivative. That is, if the derivative is positive, then moving in the positive direction along the X axis, or to the right, takes us to larger values. In the same way, if the derivative is negative, then to find smaller values of y, we move left. <a href="#figure5-12" id="figureanchor5-12">Figure 5-12</a> shows the idea.</p>
<figure>
<img src="Images/f05012.png" alt="f05012" width="345" height="289"/>
<figcaption><p><a id="figure5-12">Figure 5-12</a>: The derivative at a point tells us which way to move to find larger or smaller values of the curve. </p></figcaption>
</figure>
<p>We can gather up both cases and say that to find the local maximum near some point, we find the derivative at that point and take a small step along the X axis in the direction of the sign of the derivative. Then we find the derivative there and take another small step. We repeat this process over and over until we reach a point where the derivative is 0. <a href="#figure5-13" id="figureanchor5-13">Figure 5-13</a> shows this in action, starting from the rightmost point.</p>
<figure>
<img src="Images/f05013.png" alt="f05013" width="344" height="183"/>
<figcaption><p><a id="figure5-13">Figure 5-13</a>: Using the derivative to find the local maximum at a point</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="126" id="Page_126"/>At our starting point, the rightmost, we get a somewhat large, negative derivative, so we take a big step to the left. The second derivative is a bit smaller (that is, the slope is still negative, but a little less so), so we take a smaller step to the left. A third, smaller step takes us to the local maximum, where the tangent line is horizontal, so the derivative is 0. To make this algorithm practical, we’d have to address some details, such as the size of the steps we take and how to avoid overshooting the maximum, but right now we’re just after the conceptual picture.</p>
<p>To find a local minimum, we do the same thing, but we move along X in the direction given by the <em>opposite</em> of the derivative’s sign, as in <a href="#figure5-14" id="figureanchor5-14">Figure 5-14</a>. Here we start at the leftmost point and find it has a negative derivative, so we keep moving right until we find a derivative of 0.</p>
<figure>
<img src="Images/f05014.png" alt="f05014" width="344" height="176"/>
<figcaption><p><a id="figure5-14">Figure 5-14</a>: Using the derivative to find the local minimum at a point</p></figcaption>
</figure>
<p>Finding local maximums and minimums is a core numerical technique used throughout machine learning, and it relies on our being able to find the derivative at every point on the curve we’re following. Our three curve conditions of smoothness, continuity, and being single-valued were chosen specifically so that we can always find a single, finite derivative at every point on our curve, which means that we can rely on this curve-following technique to find local minimums and maximums.</p>
<p>In machine learning, most of our curves obey these rules most of the time. If we happen to be using a curve that doesn’t, and we can’t compute the tangent or derivative at some point, there are mathematical techniques that usually (though not always) automatically finesse the problem so we can carry on.</p>
<p>We mentioned earlier that the derivative is also 0 where the curve itself flattens out. This can trick our algorithm into thinking it’s found a maximum or minimum. In Chapter 15, we’ll see a technique called <em>momentum</em> that can help us avoid getting fooled in this way and continue on our search for a real maximum or minimum. </p>
<h2 id="h1-500723c05-0003">The Gradient</h2>
<p class="BodyFirst">The <em>gradient</em> is the generalization of the derivative into three dimensions, or four dimensions, or <em>any</em> number of dimensions beyond that. With the gradient, we can find the minimums and maximums for surfaces in these higher dimensional spaces. Let’s see how this works.</p>
<h3 id="h2-500723c05-0004"><span epub:type="pagebreak" title="127" id="Page_127"/>Water, Gravity, and the Gradient</h3>
<p class="BodyFirst">Imagine that we’re in a big room, and above us is a billowing sheet of fabric that rises and falls without any creases or tears, as in <a href="#figure5-15" id="figureanchor5-15">Figure 5-15</a>.</p>
<figure>
<img src="Images/f05015.png" alt="f05015" width="260" height="244"/>
<figcaption><p><a id="figure5-15">Figure 5-15</a>: A sheet of smooth fabric without creases or tears</p></figcaption>
</figure>
<p>The surface of this fabric naturally satisfies the rules we required of our curves before: it’s both smooth<em> </em>and continuous<em> </em>because it’s a single piece of fabric, and it’s single-valued<em> </em>because the fabric never curls over on itself (like a crashing wave). In other words, from any point on the floor below it, there is just one piece of the surface above it, and we can measure its height above the floor.</p>
<p>Now let’s imagine that we can freeze the fabric at a particular moment. If we climb up onto the fabric and walk around on it, it will feel like we’re hiking on a landscape<em> </em>of mountains, plateaus, and valleys.</p>
<p>Suppose that the fabric is dense enough that water can’t pass through it. As we stand in one spot, let’s pour some water onto the fabric at our feet. The water, naturally, flows downhill. In fact, the water follows the path that takes it downhill in the fastest possible way, because it’s being pulled downward by gravity. At every point, it effectively searches the local neighborhood and moves in the direction that takes it downhill the fastest, as shown in <a href="#figure5-16" id="figureanchor5-16">Figure 5-16</a>.</p>
<figure>
<img src="Images/f05016.png" alt="f05016" width="600" height="309"/>
<figcaption><p><a id="figure5-16">Figure 5-16</a>: Left: Dripping water onto the surface. Right: A drop of water exploring multiple points in its local neighborhood (yellow) to find the one that is the most downhill. </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="128" id="Page_128"/>Out of all the ways to move, the water always follows the steepest route downhill. The direction followed by the water is called the direction of <em>maximum descent</em>. The opposite direction, in which we climb upward as fast as possible, is the direction of <em>maximum ascent</em>.<em> </em></p>
<p>The direction of maximum ascent is the same as the gradient. If we want to descend, we follow the <em>negative of the gradient</em>, or just the <em>negative gradient</em>. A hiker trying to reach the highest mountaintop as quickly as possible follows the gradient. A stream of flowing water flowing downhill as quickly as possible follows the negative gradient.</p>
<p>Now that we know the direction of maximum ascent, we can also find its <em>magnitude</em>, or strength, or size. That’s simply how quickly we’re going uphill. If we’re going up a gentle slope, the magnitude of our ascent is a small number. If we’re climbing up a steep grade, it’s a bigger number.</p>
<h3 id="h2-500723c05-0005">Finding Maximums and Minimums with Gradients</h3>
<p class="BodyFirst">We can use the gradient to find the local maximum in three dimensions (3D), just as we used the derivative in two dimensions (2D). In other words, if we’re on a landscape and we want to climb to the highest peak around, we need only follow the gradient by always moving in the direction of the gradient associated with the point under our feet as we climb.</p>
<p>If we instead want to descend to the lowest point around, we can follow the negative gradient and always walk in the direction exactly opposite<em> </em>the gradient associated with each point under our feet as we descend. Essentially, we’re acting like a drop of water, moving downhill in the fastest way possible. <a href="#figure5-17" id="figureanchor5-17">Figure 5-17</a> shows this step-by-step process in action.</p>
<figure>
<img src="Images/f05017.png" alt="f05017" width="255" height="240"/>
<figcaption><p><a id="figure5-17">Figure 5-17</a>: To get downhill, we can repeatedly find the negative gradient and take a small step in that direction. </p></figcaption>
</figure>
<p>Suppose that we’re at the very top of a hill, as in <a href="#figure5-18" id="figureanchor5-18">Figure 5-18</a>. This is a local maximum (and maybe the global maximum). Here, there is no uphill direction to go in. If we were to zoom in on the very top of the hill, we’d find the nearby surface is flat. Because there’s no way to go up, our maximum rate of ascent is 0, and the magnitude of the gradient is 0. There’s no gradient at all! We sometimes say the gradient has <em>vanished</em>, or that we have a <em>zero gradient</em>.</p>
<span epub:type="pagebreak" title="129" id="Page_129"/><figure>
<img src="Images/f05018.png" alt="f05018" width="540" height="245"/>
<figcaption><p><a id="figure5-18">Figure 5-18</a>: At the very top of a hill, there is no uphill. Left: The hill. Right: Our location at the very top of the hill. </p></figcaption>
</figure>
<p>When the gradient vanishes, as at the top of a hill, the negative gradient goes away, too.</p>
<p>What if we’re at the bottom of a bowl-shaped valley, as in <a href="#figure5-19" id="figureanchor5-19">Figure 5-19</a>? This is a local minimum (and maybe the global minimum).</p>
<figure>
<img src="Images/f05019.png" alt="f05019" width="540" height="229"/>
<figcaption><p><a id="figure5-19">Figure 5-19</a>: At the very bottom of a bowl, every move we make is uphill. Left: A bowl. Right: A point at the bottom of the bowl. </p></figcaption>
</figure>
<p>At the very bottom of the bowl, every direction seems to go up. But if we zoom way in, we’d see that the bottom of the bowl is flat. Again, the gradient has vanished.</p>
<p>What if we’re not on a hilltop or in a valley or on the side of a slope but just on a flat plain, or <em>plateau</em>, as in <a href="#figure5-20" id="figureanchor5-20">Figure 5-20</a>?</p>
<figure>
<img src="Images/f05020.png" alt="f05020" width="540" height="143"/>
<figcaption><p><a id="figure5-20">Figure 5-20</a>: A flat surface, plain, or plateau. Left: The plateau. Right: The point on the plain is mainly on the plane. This point has no gradient.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="130" id="Page_130"/>Just like being on the hilltop, there’s nowhere to go up or down. When we’re on a plateau, we again have no gradient at all.</p>
<h3 id="h2-500723c05-0006">Saddle Points</h3>
<p class="BodyFirst">So far, we’ve seen local minimums, maximums, and flat regions, just as we saw in 2D. But in 3D, there’s a completely new type of feature. In one direction, we’re in the bottom of a valley, while in the other direction, we’re at the top of a hill. In the local neighborhood of such a point, the surface looks like the saddle that horse riders use. Naturally enough, this kind of shape is called a <em>saddle</em>. An example saddle is shown in <a href="#figure5-21" id="figureanchor5-21">Figure 5-21</a>.</p>
<figure>
<img src="Images/f05021.png" alt="f05021" width="540" height="232"/>
<figcaption><p><a id="figure5-21">Figure 5-21</a>: A saddle goes upward in one direction and downward in another. Left: A saddle. Right: A point on the saddle. </p></figcaption>
</figure>
<p>If we’re in the middle of the saddle, as in <a href="#figure5-21">Figure 5-21</a>, then it’s like being at a hilltop and valley at the same time. And just like those places, the local neighborhood looks like a plateau, so there’s no gradient. But if we move just a little bit in one direction or another, we’ll find a little bit of curvature, and then the gradient reemerges to show us the direction of maximum ascent from that spot. </p>
<p>When we train a deep learning algorithm, we usually want to find the least amount of error. Thinking of the error as a surface, the best scenario is when we can find the bottom of a bowl. But if we find ourselves at the top of a hill, or on a saddle, or on a plateau, we say that we’ve become <em>stuck</em> at these places. We know we’re not at a minimum, but the gradient has vanished, so we have no idea which way to go in order to move downward.</p>
<p>Happily, modern algorithms offer a variety of automatic techniques to get us unstuck. But sometimes they fail, and unless we can introduce a major change, such as providing additional training data, our algorithm stays stuck, unable to move to a lower value of the surface. In practical terms, this means that the algorithm simply stops learning, and its output stops improving.  </p>
<p>We’ll see later that we can watch our learning progress by measuring its error. If the error stops improving before our results are acceptable, we can change the algorithm just a little so that it takes a different path when learning, and sidesteps that particular spot of zero gradient.</p>
<h2 id="h1-500723c05-0004"><span epub:type="pagebreak" title="131" id="Page_131"/>Summary</h2>
<p class="BodyFirst">In this chapter, we looked at some ways to find the minimums and maximums of curves. When we train a deep learning system, we adjust it to minimize the system’s overall error. If we think of the error as a surface in a many-dimensional space, we’re looking for a minimum on that surface. To find that minimum error, we find the steepest downhill direction, given by the negative gradient. We then change the network so that the error moves in that direction. In essence, the gradient tells us how to change the network so that the overall error of the system is reduced.</p>
<p>In later chapters, we’ll see how we actually use this idea in practice to teach our deep learning systems to get better and better at their jobs.</p>
<p>For now, let’s turn to a little bit of information theory, which will help us better understand the nature of errors and how to interpret them.</p>
</section>
</div></body></html>