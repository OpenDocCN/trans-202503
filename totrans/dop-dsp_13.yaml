- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Troubleshooting Hosts
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: Engineers spend a lot of time trying to figure out why something isn’t working
    as intended. Instrumentation, tracing, and monitoring play big roles in determining
    the health of a host or application, but sometimes, observability is not enough.
    There will be times when you’ll need to roll up your sleeves and figure out why
    something is broken and how to fix it. In other words, you’ll be troubleshooting
    and debugging. *Troubleshooting* is the process of analyzing the system and rooting
    out potential causes of trouble. *Debugging*, on the other hand, is the process
    of discovering the cause of trouble and possibly implementing steps to remedy
    it. The differences are subtle, and in fact, you can think of debugging as a subset
    of troubleshooting. Most of what you’ll do in this chapter is considered troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll explore common performance problems and issues you may
    encounter on a Linux host. You’ll look at symptoms, commands you can use to diagnose
    various potential problems, and the next steps to take after troubleshooting.
    By the end of this chapter, you’ll have expanded your command line arsenal and
    sleuthing skills to troubleshoot common issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Troubleshooting and Debugging: A Primer'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Troubleshooting and debugging is an art, not an exact science. Rarely will you
    see a big neon sign with an arrow pointing to the exact issue. Most of the time,
    you’ll find a trail of breadcrumbs that leads you from clue to clue. You may have
    to crawl through the weeds to find those crumbs, and you may want to pull out
    your hair before you find what you’re looking for. But diagnosing a broken system
    can be very rewarding, and figuring out an issue that’s plaguing your customers
    or haunting a coworker can feel amazing.
  prefs: []
  type: TYPE_NORMAL
- en: 'But even an artist needs a method, and having a standard set of steps and techniques
    to follow whenever you are investigating an issue is a great way to start. So
    here are some tips to keep in mind when venturing forth to confront those fickle
    beasts we call hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: Start simple. When troubleshooting a problem, it can be tempting to jump to
    conclusions and assume it’s the worst-case scenario. Instead, be methodical and
    build upon the knowledge you have gained. The problem is usually human error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a mental model. Understanding what the system’s role is and how it interacts
    with other systems will help you troubleshoot faster. You will find yourself spending
    less time worrying about architecture and more time working on the issue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take your time developing a theory. You may want to latch on to the first clue
    you find, but it’s always worth checking to see if the breadcrumb trail leads
    any farther. Come up with a test to validate your theory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have consistent tools across hosts. Make sure your hosts were built with the
    same tooling. There is nothing worse than logging in to a host and finding out
    it is not like the others. Tool consistency is one of the benefits of building
    your hosts with automation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep a journal. Keep a high-level account of problems, symptoms, and fixes so
    you don’t forget important details about an issue. Your future self will thank
    you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Know when to ask for help. If your business depends on solving an issue but
    you are struggling to find the cause, it is best to send up a flare. Someone with
    more experience can usually help, and someday, you will pay that knowledge forward
    or maybe even return the favor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scenario: High Load Average'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linux has a metric called *load average* that provides an idea of how busy a
    host is. The load average takes into account data like CPU and I/O when calculating
    this number. The load of a system is displayed in 1-minute, 5-minute, and 15-minute
    averages. At first glance, any high number in an average might seem like a problem.
    But troubleshooting a high load average can be tricky because a high load doesn’t
    always indicate that your host is in a degraded state. A busy host can have a
    high load but still respond to requests and commands without issue. It’s like
    when two people have the same temperature, but one person is awake and functioning
    in a normal capacity and the other is bedridden and lethargic. Each host and workload
    is different, so you first need to identify what a normal range for your host
    looks like. A good rule of thumb is if the load average is larger than the CPU
    core count, you may have processes waiting and causing latency or performance
    degradation. When investigating this scenario, a good first step is to identify
    the high load and try to locate any process that could be causing it.
  prefs: []
  type: TYPE_NORMAL
- en: uptime
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Enter the `uptime` command to display how long a host has been running, the
    number of logged-in users, and the system load. It reports the load in 1-minute,
    5-minute, and 15-minute averages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This four-core CPU host has been `up` for `47 days` and `31 minutes`, and `2
    users` are currently logged in. The 1-minute `load average` is `8.05`. The 5-minute
    `load average` is `1.01`, which means the pressure on the system has been increasing
    during somewhere between 1 and 5 minutes of runtime. You know this because the
    15-minute `load average` is `0.00` (no load at that time). If the numbers were
    reversed, with the 15-minute load showing the higher number and the 1-minute load
    at zero, you could infer that the spike in load is not ongoing and happened around
    15 minutes ago. Since this load seems to be increasing and has been climbing for
    more than 5 minutes, and since it is greater than the CPU core count, it may be
    worth investigating why.
  prefs: []
  type: TYPE_NORMAL
- en: top
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `top` command displays information about a system and the processes running
    on that host. It provides details like CPU percentage, load average, memory, and
    process information. Execute the `top` command to launch an interactive real-time
    dashboard showing system information, as shown in [Figure 10-1](#figure10-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of running the top command, showing all the system information
    in several columns with a summary at the top](image_fi/502482c10/f10001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-1: The `top` command output on a mostly idle host'
  prefs: []
  type: TYPE_NORMAL
- en: By default, `top` sorts all the processes by `CPU` percentage. The first row
    contains the process using the most `CPU` percentage at that given poll cycle.
    The display refreshes (polls) every 3.0 seconds, so you’ll want to view `top`
    for a few cycles before settling on a process or any data that might be or indicate
    the cause of the high load.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet is from a `top` report where a process is using 120 percent
    CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The key columns are `PID`, `RES`, `%CPU`, `%MEM`, and `COMMAND`. (Others are
    omitted here for readability.) The `fail2ban-server` command (in the `COMMAND`
    column) is using 120.3 percent CPU and is consuming around `177,740`KB of memory,
    as shown in the `RES` column. This process is using around `1.8` percent of the
    total memory (`%MEM`) available on the host. Taking everything into account, it
    would be a good idea for you to investigate process `3048` to determine why it
    is using so much CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Next Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a scenario with a high load average, you’ll want to dig down further into
    the offending process. Perhaps this application is misconfigured, hung, or busy
    waiting on external resources (like a disk or an HTTP call). Maybe the host is
    undersized for its use case. If it’s a cloud-based instance, perhaps there aren’t
    enough CPU cores or disk IOPS. Also, check whether the host is experiencing increased
    traffic during this time, as that could indicate an intermittent spike. You can
    also use tools like `vmstat`, `strace`, and `lsof` to discover more about a process’s
    interaction with the system. (You’ll learn more details about those tools in later
    sections.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario: High Memory Usage'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Temporary spikes in traffic, performance-related issues, or an application with
    a memory leak can cause memory to be consumed at a high rate. The first step in
    investigating high memory usage is to make sure the host is really running low
    on memory. Linux likes to use all the memory for caches and buffers, so it can
    appear that free memory is low. But the Linux kernel can reallocate that cached
    memory elsewhere if needed. The `free`, `vmstat`, and `ps` commands can help identify
    how much memory is being used and what process may be the culprit.
  prefs: []
  type: TYPE_NORMAL
- en: free
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `free` command provides a quick sanity check on system memory by displaying
    used and available memory at the time it is run. Pass the `-h` and `-m` flags
    to instruct the `free` command to show all output fields in human-readable (`-h`)
    format using the *mebibyte* unit (`-m`) of measure. In *human-readable format*,
    data appears in familiar units like *mebibyte* or *gibibyte* instead of bytes.
    The following example shows a host that’s low on available memory. Enter the following
    command to display memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The system contains `981Mi` of `total` memory, and `838Mi` of memory is being
    `used`, with `95Mi` `free`. The `buff/cache` column contains information from
    data that has been read off disk and the metadata associated with it. This is
    used for fast retrieval if you need to access it again, which is why Linux tries
    to use all the system memory it can instead of letting it sit idle. A Linux host
    will swap data out of memory and write it to disk if a system is running low on
    memory. As you can imagine, using disk as memory is much slower than using actual
    RAM. If the `free` column for `Swap` is ever low, your system may be performing
    slower than it normally can. In this example, the system is swapping to disk only
    a little (`141Mi`), which can be normal.
  prefs: []
  type: TYPE_NORMAL
- en: The `used` and `free` columns can be misleading on a Linux host. Linux likes
    to use every bit of RAM on a system, so it may appear at a quick glance that a
    host is low on memory. Or, as in this case, it can appear that there is more memory
    than actually is available. Here, the `free` column shows `95Mi`, but according
    to the `available` column, only `43Mi` is left. When using the `free` command
    to display system memory, pay attention to the `available` column as a barometer
    of actual memory available to the system and new processes.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at how little memory is available in this example, it’s safe to say
    this host has a memory shortage. Having roughly `43Mi` out of 1Gi left on a system
    can cause stability issues and stop new processes from being created. It can also
    force the Linux kernel to invoke the out of memory manager (OOM) and select a
    process to kill, which can and will cause unexpected behavior.
  prefs: []
  type: TYPE_NORMAL
- en: vmstat
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `vmstat` command provides useful information about processes, memory, IO,
    disks, and CPU activity. It can report this data over a period of time, which
    is an upgrade over the `free` command and makes trends much easier to spot. You’ll
    pass two parameters to the `vmstat` command: `delay`, which specifies the time
    delay between each of the polling counts, and `count`, which specifies the number
    of times `vmstat` will fetch data until it quits. For this example, you will poll
    the data five times with a one-second delay between each poll. Enter the following
    command to poll the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `vmstat` report is divided into multiple categories: `procs`, `memory`,
    `swap`, `io`, `system`, and `cpu`. Each category contains like columns. The first
    row of data is an average of each statistic since the last boot time. Since you
    are hunting for high memory usage, you’ll focus only on the `memory` and `swap`
    sections from the `vmstat` output.'
  prefs: []
  type: TYPE_NORMAL
- en: The `swpd` column of the `memory` section shows the total swap space used; in
    this case, it’s around 54Mi (`54,392`Ki). Next comes the `free` column. According
    to `vmstat`, the free memory has fluctuated between 71,000Ki and 74,000Ki in the
    polling snapshot. This does not mean you have only 71,000Ki of memory available;
    it’s an estimate because of the free-able cache and buffers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the `swap` section are two columns: `si` (swapped in) and `so` (swapped
    out). The `si` and `so` columns indicate you are paging memory to and from the
    disk. At one point, you were swapping memory from the disk at about `104`KiB per
    second. As mentioned previously, a little swapping can be okay, but being low
    on free memory plus swapping usually indicates a memory bottleneck.'
  prefs: []
  type: TYPE_NORMAL
- en: The `r` and `b` columns under `procs` can provide good indications of possible
    bottlenecks. The `r` column is the number of running (or waiting-to-run) processes.
    A high number here can indicate a CPU bottleneck. The `b` column is the number
    of processes in an uninterruptable sleep. If the number in the `b` column is high,
    it can be a good signal that there are processes waiting on resources like disk
    or network IO.
  prefs: []
  type: TYPE_NORMAL
- en: ps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If memory usage is high on the host, you’ll want to check all the running processes
    to find where the memory is being used. The `ps` command provides a snapshot of
    the current processes on a host. You’ll use some flags to narrow down the results
    and show only the top-10 hosts sorted by most memory. Enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `-efly` and `--sort=-rss` flags are used to show all the processes in a
    long format. The `RSS` (resident set size) column shows the amount of non-swappable
    physical memory a process uses (in kilobytes), in descending numerical order.
    You pipe those results to the `head` command, which displays only 10 by default.
    The `CMD` column shows the command that belongs to each process. In this example,
    the `memory-hog` command is using around 890MB (`890,652`KB) of physical memory,
    according to the `RSS` column. Considering that this host has only 1Gi of total
    memory, that application is hogging all the memory.
  prefs: []
  type: TYPE_NORMAL
- en: Next Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The steps you’ll take to resolve a high-memory-usage issue like this will depend
    on risk factors for your system and/or users. If you’re dealing with a production
    system, you’ll want to tread lightly and check the logs, traces, and metrics to
    determine when and where the problem started. If this were a new behavior on a
    production system, rolling back `memory-hog` to a previous version would be a
    great first step. (Any time you can recover quickly in production is a win.) Once
    you have remediated the issue in production, do a performance profile in a different
    environment and dig through the clues to figure out why and where the memory is
    being used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario: High iowait'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A host that is spending too much time waiting for disk I/O is said to have a
    condition called *high iowait*. The way to measure iowait is to check the percentage
    of time that CPUs are idle because the system has unfinished disk I/O requests
    that are blocking processes from doing other work. Significant iowait usually
    results in a host having an increased load and possibly higher reported CPU usage
    than it normally would. To put it another way, if your CPU is waiting for the
    disk to respond, it has less time to service other requests from other parts of
    the system. One cause of high iowait might be an aging, slow, or failing disk.
    Another culprit could be an application that is performing heavy disk reads and
    writes. If you are in a virtualized environment, slow network-attached storage
    is most likely where your congestion lies.
  prefs: []
  type: TYPE_NORMAL
- en: All systems will have some iowait, and modern CPUs are faster than storage.
    High iowait by itself, however, is not enough to signal a problem. Some systems
    with high iowait can perform without issues, while others will show significant
    signs of a bottleneck. The goal is to identify issues that are accompanied by
    high iowait. There’s no bright line with normal iowait on one side and high iowait
    on the other, so I have set the threshold for high iowait at anything over 30
    percent that is sustained over a significant period.
  prefs: []
  type: TYPE_NORMAL
- en: Two command line tools, `iostat` and `iotop`, will help you troubleshoot a host
    with high iowait.
  prefs: []
  type: TYPE_NORMAL
- en: iostat
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `iostat` command line tool reports CPU and I/O stats for devices, so it’s
    a great tool to help you determine whether your system is experiencing any iowait.
    If `iostat` is not installed by default, use your package manager to install the
    sysstat package.
  prefs: []
  type: TYPE_NORMAL
- en: 'As I mentioned previously, having some iowait is normal. You are looking for
    abnormal behavior, so you’ll want to poll the system over a period of time to
    get a better view of the problem, like you did with the `vmstat` command. For
    this example, enter the command below to poll for statistics every second for
    a total of 20 times. The command and output should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first report `iostat` prints is from the last time the host was booted.
    Since that data is not relevant to your current troubleshooting scenario, I’ve
    omitted it here, along with multiple columns from the `Device` output. The `-xz`
    flag shows only active devices using an extended stat format. The `w/s` column
    shows that the `vda` device is executing a lot of write requests per second (`1179.00`).
    The `CPU` is waiting on outstanding disk requests around `66.67%` of the time
    (`%iowait`). Finally, as further proof that this disk is quite busy, the `%util`
    (percent utilization) column shows `100%`.
  prefs: []
  type: TYPE_NORMAL
- en: You can conclude that the host is suffering from high iowait that is sustained
    and not just intermittent. More importantly, you know that the iowait is occurring
    on the device named `vda`. From here, it is worth trying to find a process that
    could be the cause of the increased iowait. You can do that with the `iotop` command,
    which you’ll explore next.
  prefs: []
  type: TYPE_NORMAL
- en: iotop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `iotop` command displays I/O usage in a `top`-like format. Not only does
    it provide an overview of I/O on the host, but it lets you drill down to the process
    level to locate any processes that might be causing a lot of disk I/O. Most distributions
    don’t include `iotop` by default, so use your package manager to install it.
  prefs: []
  type: TYPE_NORMAL
- en: 'When running `iotop`, you’ll want to limit the output to show only active processes
    that are performing I/O, using a batch mode that polls constantly to keep the
    output concise and reveal any possible I/O patterns. This command requires elevated
    permissions, so you’ll need to run it with `sudo` or as a privileged user. Enter
    the command below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `-oPab` flags make `iotop` show only processes performing I/O with accumulative
    stats in a batch mode. In this example, the `heavy-io` command is at `83.26%`,
    according to the `IO` column. The `PID` column reports the process ID, which in
    this case is `88576`. No other processes in your report are using a lot of I/O,
    so it’s safe to assume that the `heavy-io` process is part of the reason for the
    high iowait.
  prefs: []
  type: TYPE_NORMAL
- en: Next Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After checking the stats and finding the process ID that is causing high iowait,
    you might want to explore what this application is used for. If you have the source
    code or configuration files, look for more clues by checking any disk operations
    or files the process has access to. Another cause for high iowait could be that
    your VM is in a cloud provider and you do not have enough provisioned I/O operations
    for your disk. Check the disk metrics to confirm and adjust the number to compensate
    the load. If all else fails, use tools like `lsof`to examine what files are open,
    `strace` to trace any system calls the process is making, or `dmesg` for any hardware
    kernel errors. (We’ll discuss `lsof`, `strace`, and `dmesg` later in this chapter.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario: Hostname Resolution Failure'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditionally, when a service needs to connect to another service, it uses Domain
    Name System (DNS) to look up the IP address to send it a request. *DNS is a directory
    for host IP address mappings. It allows us to use names like google.com or nostarch.com
    without needing to know those hosts’ exact IP addresses. Humans are far better
    at remembering names than IP addresses like 142.250.72.78 or 104.20.208.3\. Imagine
    if you had to find a store by trying to remember its latitude and longitude coordinates
    without using GPS instead of just remembering it’s at 123 Main Street. You would
    get lost . . . a lot.*
  prefs: []
  type: TYPE_NORMAL
- en: '*For this scenario, say you have an application that is trying to connect to
    a Postgres database in your local environment. The application starts emitting
    errors in the logs that look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It appears that the application can’t resolve the DNS record for *db.smith.lab*.
    There can be multiple reasons for the failure in name resolution. We’ll explore
    a few tools to help troubleshoot this error. Before that, though, you really need
    to understand how your host uses DNS.
  prefs: []
  type: TYPE_NORMAL
- en: resolv.conf
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first place to start investigating DNS issues on any Linux host is the
    */etc/resolv.conf* file that provides information on what DNS servers to query
    and any special options needed (like timeout or security). The following is a
    *resolv.conf* file from a typical Ubuntu host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The file contains several comments describing `systemd-resolved`, and most importantly,
    it notes that you shouldn’t edit it. This file is controlled by the `systemd-resolved`
    service provided by *systemd*, and it will overwrite the file next time the host
    or service restarts. After the comments, the second line from the bottom contains
    the `nameserver` keyword and the IP address of the DNS server to query. On this
    Ubuntu host, the `nameserver` is set to `127.0.0.53`, which means any DNS requests
    will be sent to this address. If the local `resolver` does not know the answer
    to the query, the `resolver` will forward the request to an upstream `DNS server`.
  prefs: []
  type: TYPE_NORMAL
- en: The DNS upstream servers are usually set when you receive an IP address lease
    from a DHCP server. These upstream DNS servers can be internal servers that handle
    all your requests, or they can be any of the many public servers that the internet
    uses. For example, Cloudflare hosts public DNS servers at 1.1.1.1\. There are
    quite a few public DNS servers around the globe.
  prefs: []
  type: TYPE_NORMAL
- en: The last line in the file modifies some specific resolver attributes using the
    `options` keyword. In this example, the `edns0` and `trust-ad` options are set.
    The `edns0` option enables expanded features to the DNS protocol. See RFC 2671
    ([https://tools.ietf.org/html/rfc2671/](https://tools.ietf.org/html/rfc2671/))
    for more details. The `trust-ad`, or authenticated data (AD) bit, option will
    include the authenticated data on all outbound DNS queries and preserve the authenticated
    data in the response. This will allow the client and server to validate the exchange
    between each other. This option is a part of a larger set of extensions that add
    security to DNS. See [https://www.dnssec.net/](https://www.dnssec.net/) for more
    information.
  prefs: []
  type: TYPE_NORMAL
- en: resolvectl
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this example host’s *resolv.conf*, the DNS server is set to `127.0.0.53`,
    which is a local resolver that proxies any DNS request it does not know about.
    Each DNS server typically will have an upstream server that it forwards unknown
    requests to. Since you are using `systemd-resolver`, you can use a tool called
    `resolvectl` to interact with your local resolver. If this command line application
    is missing, you can install it via your package manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll want to know where your local DNS resolver (`127.0.0.53`) sends unknown
    requests. This might help you figure out why *db.smith.lab* resolution is failing.
    To see what DNS servers the resolver points to upstream, enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The results show the downstream DNS server is set to `10.0.2.3` for interface
    `enp0s3`, which is the default interface and route on this host. Your setup and
    interface might be different. When any application on this host tries to connect
    to *db.smith.lab*, it first sends a DNS request to `127.0.0.53`, asking what IP
    address the hostname resolves to. The local resolver first looks for the answer
    locally. If the mapping is there, the results are returned immediately. However,
    if the answer is unknown, the resolver forwards the request to the upstream DNS
    server at IP `10.0.2.3`. Now, if the DNS server at `10.0.2.3` knows the answer
    for *db.smith.lab*, it will return a response to the local resolver, which in
    turn will respond to the user. If it doesn’t know the answer, the upstream server
    will forward that request to its upstream server until it reaches the authoritative
    server for the domain it’s looking for.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know the IP address of your local resolver and upstream DNS server,
    you can query both to look for clues.
  prefs: []
  type: TYPE_NORMAL
- en: dig
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `dig` command line tool queries DNS servers and displays the results. This
    is extremely handy when you are troubleshooting DNS issues or need to fetch an
    IP address for a host. All you need to do is pass `dig` the hostname, and the
    response will provide information about the query and server that is responding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try querying the local resolver for the IP address of *db.smith.lab*. Enter
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `status` field 1 lets us know whether the query was successful. A successful
    query would have a status of `NOERROR`. In this example, the status is set to
    `SERVFAIL`, showing that no answer could be given. This makes sense, as the local
    DNS does not know where to find *db.smith.lab*. The `QUESTION SECTION` displays
    the query that was sent to the DNS server. In this case, the query is for the
    A record for *db.smith.lab* 2. (An *A record* is a type of DNS record that maps
    a domain to an IP address.) The `SERVER` section tells us which DNS server was
    contacted to make the query. In this example, it’s the local resolver (`127.0.0.53`)
    3, as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test your upstream server, you can instruct `dig` to talk to a specific
    DNS server instead of the local one. This will let you verify whether DNS resolution
    is failing locally or upstream. To do this, enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `@10.0.2.3` parameter makes `dig` skip the local DNS and query the upstream
    host directly. The results, however, are the same, and you received a `SERVFAIL`
    for the status. This means the upstream server couldn’t provide an answer for
    the hostname. You know you queried the correct server, because the `SERVER` section
    now states `10.0.2.3` instead of `127.0.0.53`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be safe, you should try one more query to make sure the local and upstream
    DNS servers are working correctly. First, you’ll query for a DNS record that you
    are positive will return a response. This will let you verify whether DNS is broken
    for any domains, not just *db.smith.lab*. Enter the following command to query
    the A record for google.com:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The status is `NOERROR`, and you received the A record of `142.250.72.78` in
    the `ANSWER SECTION`. This means the DNS server can resolve another hostname without
    error, but for some reason, it doesn’t know about the *db.smith.lab* A record.
    Note that when there is an error or no answer to be given, the `ANSWER SECTION`
    is omitted from the results.
  prefs: []
  type: TYPE_NORMAL
- en: Next Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If there are resolution issues with a given hostname and DNS is functioning
    correctly and can resolve other hostnames, then the issues might stem from a DNS
    resolver that is missing the information that maps the hostname to an IP address.
    If your DNS is hosted on a service like Amazon Route53, make sure the record has
    not been removed by configuration management software or due to human error. If
    you manage the DNS server locally, you can look to see if the A record is present.
    If it is not, perhaps the configuration contains some syntax error preventing
    the record from being served, or perhaps the DNS server needs to be restarted
    to read in its new records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario: Out of Disk Space'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will run out of disk space eventually. When this happens, you need to find
    out what is using all the space. The culprit could be anything from a misbehaving
    application to uncapped logfiles to a buildup of Docker images. To find the source
    of the problem, you’ll first need to figure out which drive and filesystem are
    low on space. Once you locate those pieces, you will be able to search for files
    on the disk that may be using a lot of space.
  prefs: []
  type: TYPE_NORMAL
- en: df
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `df` command displays the free disk space on all the mounted filesystems
    on a host. It has multiple options, but the `-h` flag (for human-readable) is
    probably all you’ll need. To see the free space on the mounted filesystems, enter
    the following command in a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this example, device */dev/vda1* is using `100%` of its `25G` of disk space.
    The filesystem is mounted at */*, which is the root directory. If your host has
    multiple mounted disks, they’ll be visible in the output as well.
  prefs: []
  type: TYPE_NORMAL
- en: find
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `find` command searches the filesystem for directories and files, and you
    can filter it to narrow down the search by looking for files that match only certain
    criteria or a specific directory. You can also locate files by their sizes on
    disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'In your example, since you know the *root* filesystem is out of space after
    running the `df` command, you should direct `find` to search there. You’ll execute
    the `find` command and search the *root* filesystem, looking for any files over
    `100M`. You’ll sort them by size and display the top 10 with the `head` command.
    This could take a while, depending on the number of files on your drive. Enter
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: For each file located that is more than 100M, you’ll execute (`-exec` flag)
    the `du -ah` command to fetch the file size on disk in human-readable format.
    The results, with file size, are sorted with largest files first. Then, the first
    10 results are displayed.
  prefs: []
  type: TYPE_NORMAL
- en: This output shows a file named *php7.2-fpm.log* that is located under */var/log*
    and is `10G` in size. Also, a Docker container log located in */var/lib/docker/containers*
    is using `5G` of space. Together, these files are taking up 15GB of space on your
    disk. Usually, application logs like these should rotate and not become so large.
    The fact that both files are so big should trip your Spidey sense that something
    is not right here.
  prefs: []
  type: TYPE_NORMAL
- en: With more breadcrumbs to follow, check to see what process, if any, is using
    the *php7.2-fpm.log* file before you form a hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: lsof
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use the `lsof` command to list open files on a host. Files on a Linux host can
    be regular files, directories, or sockets, to name just a few. You can search
    for files owned by a particular process or by a specific user.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll use `lsof`, which requires elevated privileges, to find the process
    writing to the */var/log/php7.2-fpm.log* file. Enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You must pass the full path to the file you are interested in. In this case,
    it’s the logfile. The `php-fpm7` command with the `PID 23496` owns the logfile
    in question. The file descriptor is `2w`, which means the file’s descriptor is
    `2` and the file is opened for write access (`w`). The `TYPE` of file is `REG`
    (regular), representing a typical ASCII text file.
  prefs: []
  type: TYPE_NORMAL
- en: Next Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When your free disk space is low and you have tracked down a file that is contributing
    to the lack of space, you have a couple of options to remedy the situation. Since
    this logfile is currently being used, truncating or deleting it out from under
    the `php-fpm7` process isn’t wise. Doing so could cause the process to die or
    stop writing logs completely. Instead, you can start by looking at the log output
    to see whether there are any telling errors or the application log level is perhaps
    stuck on `debug`. Also, there might be some correlation between this logfile and
    the fact that a Docker container log is large. Perhaps this process is running
    inside that container. Check the contents of the container log as well for any
    visible errors. On a housecleaning note, you should always make sure the host
    is set up to use the `logrotate` command to compress and rotate logfiles on a
    schedule. This can keep your logfiles from growing unbound and eating up your
    disk space. The `logrotate` configuration files are located in the */etc/logrotate.d**/*
    directory on Ubuntu systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario: Connection Refused'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes, services refuse connections and do not leave an obvious reason why.
    For example, say you have an internal API that is reporting a high error rate,
    and say other services that use this API are throwing a lot of errors as well.
    The errors in the application logs would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: It appears users are receiving a `Connection refused` error when trying to connect
    to the API server. You know the Docker container is up and running, or you would
    have gotten an alert that it was down. To troubleshoot this, you’ll use a few
    commands that will help you identify any network- or configuration-related issues.
  prefs: []
  type: TYPE_NORMAL
- en: curl
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Anytime you need to check whether a web server is responding to requests or
    just want to fetch some data or a file, turn to the `curl` command. For this example,
    you’ll want to verify that an endpoint is down for everyone and that there is
    not just a routing issue on other hosts. The API server should respond with an
    `HTTP 200` status if it is functioning properly. To double-check that the API
    server is refusing connections, you could use `curl` by entering the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The output shows you are getting a `Connection refused` error as well. This
    usually means the host is not listening on your port or a firewall is rejecting
    packets. Regardless of the reason, something is breaking your API requests.
  prefs: []
  type: TYPE_NORMAL
- en: ss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `ss` (socket statistics) command is used to dump socket information on
    a host. For your troubleshooting scenario, you’ll use it to see whether any application
    on the host is bound (or listening) to requests on `port 8080`. Enter the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `-l` flag shows all the listening sockets on the host. The `-n` flag instructs
    `ss` not to resolve any service names like HTTP or SSH, and the `-p` flag shows
    the process that’s using the socket. For `ss` to determine which process owns
    the socket, `sudo` or elevated permissions are required. I truncated the beginning
    of the output line for readability, but the important part shows that the `docker-proxy`
    process is listening on all interfaces for port 8080 (`0.0.0.0:8080`). Next, you
    can verify that the requests destined for *api.smith.lab* are making it all the
    way to the host, where it lives.
  prefs: []
  type: TYPE_NORMAL
- en: tcpdump
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One way to verify network traffic on a host is with the `tcpdump` command, which
    has many options and can capture traffic on one or all interfaces. It can even
    write out the network capture into a file for later analysis. Not only is `tcpdump`
    great for troubleshooting network issues, but you can use it for security auditing
    as well. For your example, you’ll use it to capture network traffic intended for
    the *api.smith.lab* host on port 8080\. This will let you know whether traffic
    being sent to that host is reaching its target, and it will hopefully shed some
    light on why you are getting the `Connection refused` error message.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the host where the API application is running, enter the following command
    in a terminal. This will start the network packet capture on all interfaces for
    any TCP packet headed for port 8080 (note that elevated privileges are needed
    to listen on a network interface):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `-n` flag makes sure you do not try to resolve any host or port names. The
    `-i` flag tells `tcpdump` the network interface on which to listen. In this case,
    the term `any` is specified and means “Listen on all interfaces.” You want to
    capture all packets destined for port 8080 since there might be numerous network
    interfaces on this host. The final `tcp port 8080` parameter states that you want
    only TCP packets that have port 8080 in them. These will include packets from
    both the client and the server.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s focus on the parts of the output that help with the `Connection refused`
    error problem. On the first line, the `IP` section shows that something from source
    `IP` `192.168.50.26` is trying to connect to `192.168.50.4` on port `8080`. The
    `>` (greater-than) sign tells us the direction of the communication from one IP
    to another. The `Flags` being set show the types of network packets being sent.
    The first packet has an `S` (synchronize) flag. Anytime a client wants to establish
    a connection to another host, it sends the synchronize packet. In the next packet,
    host `192.168.50.4` responds to `192.168.50.26` with a reset (`R`) packet. A reset
    packet is usually sent when there is an unrecoverable error and the server wants
    the client to terminate the connection immediately. Undeterred by the “Get off
    my lawn!” reset packet, the client tries again with another synchronize packet,
    which in turn causes server `192.168.50.4` to send another reset packet back to
    `192.168.50.26`. The client at `192.168.50.26` finally takes a hint, and the connection
    is closed.
  prefs: []
  type: TYPE_NORMAL
- en: The flags show this connection isn’t normal. A normal TCP connection starts
    off with a `SYN` packet from the client, followed by a `SYN-ACK` packet from the
    server. Once that packet is received, the client sends back an `ACK` packet to
    the server, acknowledging the last packet. This is referred to as a *three-way
    handshake*. See [Figure 10-2](#figure10-2) for details.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing one client server on the left with the IP 192.168.50.26 passing
    packets back and forth with the server at 192.168.50.4 on the right](image_fi/502482c10/f10002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-2: TCP three-way handshake'
  prefs: []
  type: TYPE_NORMAL
- en: You clearly do not see any other packets (except resets) being sent from the
    server. The reset packets will cause the connecting clients to report that the
    connection is being refused. The good news is you verified that connections are
    making it all the way to server. The bad news is you still do not know why you
    are being refused.
  prefs: []
  type: TYPE_NORMAL
- en: Next Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, you know the service is listening on port 8080\. You verified
    this with the `ss` command. You also know traffic is making it all the way to
    the server, according to your network capture with `tcpdump`.
  prefs: []
  type: TYPE_NORMAL
- en: The next places to look are the Docker container and the application configuration.
    It is possible `docker-proxy` is having issues and not forwarding the traffic
    to the container running the API. Another possibility is that the container was
    started with incorrect internal port mappings. You know the external port, 8080,
    is mapped correctly, since it is listening for connections. But it’s possible
    the mapped internal port is misconfigured. You can check both of these scenarios
    by looking at Docker’s system logs for proxy errors, or by running `docker ps`
    `<container id>` or `docker inspect` `<container_id>` to check the port mappings.
  prefs: []
  type: TYPE_NORMAL
- en: Searching Logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In almost every troubleshooting scenario, you’ll most likely need to check logs.
    System and application logs hold a wealth of information you can view from the
    command line. Modern Linux distributions use `systemd`, which has a log-collection
    mechanism called the *journal* that pulls in log events from multiple sources
    like *syslog*, *auth.log*, and *kern.log*. This lets you view and search logs
    in a single stream. As a troubleshooting archaeologist, you should know where
    logs are located and how to view and parse them.
  prefs: []
  type: TYPE_NORMAL
- en: Common Logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most system and application logs on a Linux host are stored in the */var/log*
    directory. The most common logs on a host that will aid in troubleshooting are
    *syslog*, *auth.log*, *kern.log*, and *dmesg*. Depending on your Linux distribution,
    the names of the logfiles may be different.
  prefs: []
  type: TYPE_NORMAL
- en: /var/log/syslog
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The *syslog* file contains general global system messages for the Linux OS.
    Here is an example of a log line for `systemd`, stating that the logs are finished
    rotating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The line begins with a timestamp, followed by the host it is on (`box`) and
    the process (`systemd[1]`) that is reporting the log event. The last part of the
    line is the text message. This structured line format, also called *syslog*, is
    the default protocol for logging on a Linux host.
  prefs: []
  type: TYPE_NORMAL
- en: /var/log/auth.log
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The *auth.log* file contains information regarding authorization and authentication
    events. This makes it a great place to investigate user logins and brute-force
    attacks, or to track a user’s `sudo` commands. Here is an example of an *auth.log*
    message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This message shows a failed login attempt over SSH for the user *aiden*, from
    the IP address `192.168.1.133`.
  prefs: []
  type: TYPE_NORMAL
- en: /var/log/kern.log
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The *kern.log* is a good place to look for Linux kernel messages, such as hardware
    issues or general information related to the Linux kernel. The following log line
    shows the Linux out of memory manager (OOM) in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Process `20371` was killed by the `Out of memory` manager because the system
    was running low on memory.
  prefs: []
  type: TYPE_NORMAL
- en: /var/log/dmesg
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The *dmesg* log contains bootup messages from the host since last boot time.
    These messages can be anything from a USB device being recognized to a possible
    SYN packet flood attack. This sample log line from *dmesg* shows a `Network driver`
    being loaded into the kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The *dmesg* log has its own command line application, `dmesg`, to view the kernel
    ring buffer in real time. The `dmesg` command prints information, just like the
    *dmesg* log, but it can show information after bootup as well. You can also use
    it to troubleshoot multiple scenarios, such as port exhaustion, hardware failures,
    and OOM.
  prefs: []
  type: TYPE_NORMAL
- en: Common journalctl Commands
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On a host that is using `systemd`, all of these common logs are stored in a
    single binary stream called a journal, which is orchestrated by the `journald`
    daemon. You can access the journal with the `journalctl` command line application.
    The journal is a handy troubleshooting tool because you can use it to view and
    search multiple logs at the same time. The `journalctl` command mimics many other
    logging commands you’ve discussed in this book, such as `tail`, minikube `minikube
    kubectl -- logs` and `docker logs`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say you want to review the logs, with the newest lines first. Enter the `sudo`
    command and pass the `-r` flag (reverse) to `journalctl` to view all logs in that
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This output shows log lines for all services, with newest lines first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, view logs during a certain time frame with the `--since` flag. Enter
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This output shows the logs that have a timestamp starting `2 hours ago` up till
    the current time, when the command is run. With the `-r` flag, the newest logs
    are displayed first.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can filter logs based on a `systemd` service name. For example, to view
    all the logs that were written by the SSH service, enter the following command
    to pass the `-u` (unit) flag to `journalctl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The output shows log lines for SSH pertaining to a login `session`, in reverse
    order.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also display log lines that match a specific log level, like info or
    error. Choose the priority level (`-p`) by using keywords like `info`, `err`,
    `debug`, or `crit`. The following is the same command as above but with the `-p
    err` flag to show only error logs from the SSH daemon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The output shows an `error` log line where the *root* user reached the maximum
    failed login attempts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Narrowing down logs to a specific time frame or showing log lines that match
    a given log level is great, but what if you want to find a specific message in
    the journal stream? The pattern-matching flag (`-g`) in `journalctl` can match
    any message using a regular expression. The following example searches the SSH
    logs for the `session opened` message. Enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here, SSH sessions for two different users (*vagrant* and *x7b7*) are filtered
    out.
  prefs: []
  type: TYPE_NORMAL
- en: The `journalctl` tool is helpful when you want to view many logs at once, but
    you’ll also encounter logs that are not captured in the journal system.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing Logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parsing logs is a key troubleshooting skill. In addition to `journalctl`, you
    can parse and traverse logs with the `grep` and `awk` commands. The `grep` command
    is used to search for patterns in text or a file. The `awk` command is a scripting
    language tool that can filter text, but it also has more advanced features like
    built-in functions for math and time.
  prefs: []
  type: TYPE_NORMAL
- en: grep
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `grep` command allows you to search for a pattern quickly. For example,
    to use `grep` to find any occurrences of the IP address 10.0.2.33 in */var/log/syslog*,
    pass `grep` the search pattern and the file to search by entering this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This command returned two log lines for the postfix daemon containing the `10.0.2.33`
    IP address.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find users trying to execute the `sudo` command who don’t have permission,
    search */var/log/auth.log* using `grep` by entering the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The search pattern `"user NOT in sudoers``"` indicates an unauthorized `sudo`
    attempt violation. This search returns one match showing that the user *akira*
    tried to read the contents of the */etc/passwd* file but was denied.
  prefs: []
  type: TYPE_NORMAL
- en: Taking it one step further, it would be helpful to check the *auth.log* to see
    what else this user was doing around the same time. To get extra log lines with
    `grep`, use the `-A` flag to grab a given number of lines after the matched lines
    or use the `-B` flag to fetch a given number of lines before the matched results.
    You can also use the `-C` flag to fetch before and after the match, simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you should grab the five log lines before the log line alerting to the
    `sudo` violation for the user *akira*. This will help you get an idea of what
    else might have been going on around that time in the log. Enter the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The first five lines show the user *akira* logging in over SSH 1. Within five
    seconds of logging in (`17:37:35` to `17:37:40`), the user *akira* tried to read
    the contents of the */etc/passwd* file 2. Without the extra context, it might
    be tempting to overlook this action, but after seeing the user’s behavior upon
    logging in, grabbing additional lines around a match can provide more insight.
  prefs: []
  type: TYPE_NORMAL
- en: awk
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `awk` command can search for specific patterns like `grep` does, but it
    can also filter out information from any column. For this example, you should
    grab all the source IP addresses from the requests in */var/log/nginx/access.log*.
    This log contains all the requests to a website proxied by Nginx. The source IP
    address is usually the first column in the log line, unless you have modified
    Nginx’s default logging format. You’ll use `awk`’s `print` function and pass the
    `$1` argument so it prints only the first column. By default, `awk` splits columns
    on whitespace. Enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows only two IP addresses. Clearly, it’s not a busy web server,
    but the output doesn’t show the whole log line as do the previous `grep` examples.
    You can parse the text and display the column of your choosing with the `awk`
    command. Each column in the log line is given a unique column number. For example,
    to see only the date timestamps (fourth column) in the *access.log*, pass `$4`
    to the `print` function. If you want to return more than the one column, pass
    multiple column numbers to the `print` function, separating each from the next
    with a comma, like this: `''{print $1,$4}''`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll use `awk` to search for all the HTTP 500 response code, which is usually
    in the ninth column (`$9`) in the Nginx *access.log* file. Enter the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Inside the parentheses, the tilde (`~`) is a field number that tells `awk` to
    apply the search pattern only to a specific column. In this case, you want to
    search in the ninth column for anything matching 500\. The command returned a
    single result for a `GET` request that responded with an HTTP `500`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can change the search pattern to suit your needs. For instance, if you
    want to search the logs for any unauthorized HTTP requests, change the pattern
    of `/500/` to `/401/`. To expand on this even further, you can change the search
    pattern from `/500/` to `/404/` and add a requirement that any 404 responses must
    be from an HTTP POST method. You do this by adding an `if` conditional block to
    `awk`. To search for any lines that match those criteria, enter the following
    in a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The search pattern is like the previous one. Match the value at column `$9`
    to the number `404`. Then pass an `if` block that states, “If the line from the
    column `$9` match contains the word `POST` anywhere in it, print that whole log
    line.” The result shows an HTTP `POST` to the */login* path that returned an HTTP
    `404`.
  prefs: []
  type: TYPE_NORMAL
- en: Probing Processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, you won’t encounter many symptoms when investigating issues on a
    host. The health stats may look okay, the logs may show nothing interesting .
    . . but something will still not be right. Maybe a scheduled job didn’t execute
    cleanly, or an application appears to be hung. One way to dig deeper is to investigate
    the running process on the host.
  prefs: []
  type: TYPE_NORMAL
- en: strace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `strace` command traces system calls and signals, allowing you to attach
    to a process and gain valuable knowledge in real time. Your application uses system
    calls to ask the Linux kernel to perform tasks like opening a network socket,
    reading and writing a file, or creating a child process. You should use the `strace`
    command to troubleshoot a process that looks for issues in these calls, or when
    you need an overview of what a process is doing. Note that the `strace` command
    needs *root* privileges since it is attaching to another process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many system calls are available, but here are a few for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '`open()` Create or open files.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`read()` Read from a file descriptor.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`write()` Write to a file.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`connect()` Open a connection.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`futex()` Wait or wake up threads when a condition becomes true (blocking lock).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, you should trace a process. The following command attaches to the running
    process `19419`, which is the Greeting web server from Chapter 4 and prints out
    any system calls that are happening when the trace begins:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `-s` flag sets the message output size of `128` bytes. The `-p` flag tells
    `strace` which PID to attach to (in this case, it’s `19419`). I cherry-picked
    some system calls from the output to make it easier to follow. The `accept4` system
    call creates a new connection from IP address `172.28.128.1` and returns file
    descriptor `9`. The `recvfrom` system call receives an HTTP `GET` request from
    a socket with file descriptor `9`. The first `sendto` system call sends an HTTP
    header response from the web server back over the socket. The following `sendto`
    system call transmits the body of the HTTP `GET` response back to the socket as
    well. The `write` system call writes what appears to be a *syslog* line to file
    descriptor `1`. Finally, the `close` system call is executed, closing the previous
    socket file descriptor `9`, which closes the network connection. You have captured
    the transaction between an HTTP client and an HTTP server for a `GET` request.
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine you’re trying to investigate an issue but are lacking context on
    a process. You have exhausted other means, like log spelunking and metric watching.
    Everything seems in order, but your application is still not behaving correctly.
    You can use the summary flag (`-c`) for `strace` to get an overview of what system
    calls the process is using. It will output a running count of what system calls
    are being executed, how long each one is taking, and any errors that those calls
    return. Once you run the command, it will pause in the foreground while it collects
    data, and it won’t display the results until you press CTRL-C. The longer you
    let it run, the more data you will accumulate.
  prefs: []
  type: TYPE_NORMAL
- en: The `strace` command has numerous flags and options to use for tracing. You
    can use the follow (`-f`) flag to follow any new processes created (forked) from
    the parent. You can use the syscall (`-e`) flag when you want to track only specific
    system calls. You can use the summarize (`-c`) flag when you want an overall view
    of the system calls, timings, and errors. Finally, the output (`-o`) flag can
    be extremely useful for storing the trace output to a file so you can review and
    parse it later.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, enter the following command to fetch a summary for process ID
    `28485`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The `% time` column shows the percentage of time each call made up during the
    trace capture. In this example, the process spent most of its trace time 1 (before
    the trace was stopped) in the `sendto` system call. The `calls` column shows how
    many times the system call was executed. In this case, `getpeername` 3 was executed
    the most (50 times). The `getpeername` call returns the IP address of the peer
    connected over the socket. During the trace, process `28485` counted six errors
    2 when calling the `openat` system call. You can use this call to open a file
    by its specified path name.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should run `strace` again to focus on the errors for the `openat` system
    call. Enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that process `28485` is trying to open the */var/log/telnet-server.log*
    file. The call is returning `-1`, which means the file does not exist. This matches
    the error output from the earlier summary. As you can see, being able to peer
    down into a running process and understand what it is doing at the system call
    level can be invaluable.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the scenarios described here reflect issues you will encounter throughout
    your career. Experience and repetition will help you build muscle memory for making
    quick work of these issues. My goal in describing these scenarios has been to
    show you how to use deductive reasoning to follow clues to find causes.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned about helpful forensic tools like `top`, `lsof`,
    `tcpdump`, `iostat`, and `vmstat`, which will help you diagnose symptoms. You
    also learned how to parse common logfiles using tools like `journalctl`, `grep`,
    and `awk`. All the tools and tactics discussed here should aid you the next time
    you find yourself trying to investigate problems.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes Part III, which has been on monitoring and troubleshooting. You
    now can monitor and alert on any application you deploy to Kubernetes. You have
    also gotten a troubleshooting primer to help you investigate common problems that
    arise when managing hosts and software.*
  prefs: []
  type: TYPE_NORMAL
