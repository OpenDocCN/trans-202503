- en: '**10'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**10'
- en: 'A BOUNDARY APPROACH: SUPPORT VECTOR MACHINES**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 边界方法：支持向量机（SVM）**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/common.jpg)'
- en: Support vector machines (SVMs), together with neural networks (NNs), are arguably
    the two most “purist” of ML methods, motivated originally by artificial intelligence—that
    is, nonstatistical concepts. We’ll cover SVMs in this chapter and NNs in the next.
    SVMs are best known for classification applications. They can be used in regression
    settings as well, but we will focus on classification.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM），与神经网络（NN）一起，被认为是最“纯粹”的机器学习方法之一，最初受到人工智能的启发——即非统计概念。我们将在本章讨论SVM，在下一章讨论神经网络。SVM最为人熟知的是其分类应用。虽然它们也可以用于回归问题，但我们将重点讨论分类。
- en: Keep in mind this chapter will be a tad more mathematical than the others. Staying
    true to the nonmath spirit of the book, though, equations will be kept to the
    absolute minimum. SVM is such a powerful, generally usable method that understanding
    a bit of math here is an excellent investment of time. Even reading the documentation
    of SVM software requires some understanding of the structural underpinnings of
    the method.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，本章的数学内容会比其他章节稍微多一些。然而，为了保持书籍非数学化的精神，方程式会保持在最低限度。SVM是一个非常强大、通用的方法，理解一些数学内容是时间的绝佳投资。即使是阅读SVM软件的文档，也需要对该方法的结构基础有所理解。
- en: 10.1 Motivation
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1 动机
- en: Everything about SVM involves boundary lines separating one class from another.
    To motivate that, we will first do a boundary analysis using the logistic model
    and then later bring in SVM. It will be important to keep in mind that throughout
    this section, we are simply exploring, to motivate SVM.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的所有内容都涉及将一个类与另一个类分开的边界线。为了引出这个概念，我们将首先使用逻辑回归模型进行边界分析，然后再引入SVM。在本节中，重要的是要记住，我们只是为了引出SVM而进行探索。
- en: '***10.1.1 Example: The Forest Cover Dataset***'
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***10.1.1 示例：森林覆盖数据集***'
- en: Let’s revisit the forest cover data from [Section 5.4](ch05.xhtml#ch05lev4).
    Here we will construct a motivational graph, so we will need to look at only a
    small subset of the data. First, to avoid the “black screen problem,” in which
    we have so many points that the graph becomes an amorphous mess, we will graph
    a random subset of just 500 data points. Second, to keep things to a visualizable
    two dimensions, we will use just two features.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下[第5.4节](ch05.xhtml#ch05lev4)中的森林覆盖数据。在这里，我们将构建一个激励图，因此只需要查看数据的一个小子集。首先，为了避免“黑屏问题”（即数据点太多，图形变得杂乱无章），我们将绘制一个随机选择的500个数据点的子集。其次，为了保持数据的二维可视化，我们只使用两个特征。
- en: 'The `qeML` package includes a dataset `forest500`, consisting of a random 500
    rows of the original data. Now, what about the columns? We could try the Feature
    Ordering by Conditional Independence (FOCI) approach from [Section 4.5.1](ch04.xhtml#ch04lev5sec1):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`qeML`包包含一个数据集`forest500`，它由原始数据的500行随机抽取而来。那么，列呢？我们可以尝试使用[第4.5.1节](ch04.xhtml#ch04lev5sec1)中的条件独立特征排序（FOCI）方法：'
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We might run the function again, as there is some randomness involved, but
    let’s go with the above for our example and, as always, get acquainted with the
    data:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会再次运行该函数，因为其中涉及一些随机性，但我们以上述内容为例，像往常一样，先熟悉一下数据：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As seen above, there are seven cover types, meaning this is a multiclass problem.
    Here we’ll look at a two-class version, in which we wish to predict whether we
    have cover type 1 versus all others. The `regtools::toSubFactor()` function is
    handy for that kind of thing.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，数据中有七种封面类型，这意味着这是一个多类问题。这里我们将考虑一个二分类版本，在该版本中，我们希望预测封面类型1与其他所有类型的区别。`regtools::toSubFactor()`函数非常适合这种情况。
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s see what the data looks like:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据的样子：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This code generates the plot shown in [Figure 10-1](ch10.xhtml#ch10fig01).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码生成了[图 10-1](ch10.xhtml#ch10fig01)中所示的图表。
- en: '![Image](../images/ch10fig01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch10fig01.jpg)'
- en: '*Figure 10-1: Forest cover data*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-1：森林覆盖数据*'
- en: We want to plot columns 1 and 2, hence the expression `f500[,c(1,2)]`. But we
    want to visually distinguish between the two classes, say, using squares and plus
    signs as symbols. In base-R graphics, plotting symbols are specified via the `pch`
    (point character) argument, and it turns out that the numerical codes were 0 and
    3.^([1](footnote.xhtml#ch10fn1)) The squares are the cover type 1 points, and
    pluses are non−type 1.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要绘制第1列和第2列的数据，因此使用表达式`f500[,c(1,2)]`。但我们希望在视觉上区分两类，比如使用方形和加号作为符号。在基础R图形中，绘图符号通过`pch`（点字符）参数来指定，事实证明数字编码分别是0和3。^([1](footnote.xhtml#ch10fn1))
    方形代表封面类型1的点，而加号代表非类型1的点。
- en: There seems to be no sharp tendency in the graph (that is, no trend of separation
    of the two groups). We see squares and pluses all over the graph. However, the
    plus signs seem to fall to the left and more on the upward side, versus more to
    the right for the squares.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表中似乎没有明显的趋势（即没有两组分离的趋势）。我们看到图表中到处都是方块和加号。然而，加号似乎更多集中在左侧和上方，而方块则更多集中在右侧。
- en: We would like to draw a line in [Figure 10-1](ch10.xhtml#ch10fig01), such that
    most pluses are on one side of the line and most squares are on the other side.
    The reader can take a sneak peak at [Figure 10-2](ch10.xhtml#ch10fig02) to see
    where we are headed. But where did the line come from? Actually, one can use a
    logit model here. This should not be too surprising, as you will recall that the
    logit model has a linear form at its core.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在[图10-1](ch10.xhtml#ch10fig01)中绘制一条线，使得大多数加号位于线的一侧，大多数方块位于另一侧。读者可以偷看一下[图10-2](ch10.xhtml#ch10fig02)来了解我们要走的方向。那么，这条线是从哪里来的呢？实际上，可以在这里使用logit模型。这应该不会太令人惊讶，因为你会记得logit模型在其核心具有线性形式。
- en: 'Here is how such a line can be drawn. Let’s fit the model:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何绘制这样一条线的步骤。让我们拟合模型：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Normally, in prediction contexts we are not interested in the estimated logistic
    model coefficients ![Image](../images/unch08equ07.jpg) . Here, however, we will
    want to draw a separating line in [Figure 10-1](ch10.xhtml#ch10fig01) using these
    coefficients. How do we obtain them from the output object `w`?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在预测上下文中我们不关心估计的逻辑模型系数 ![Image](../images/unch08equ07.jpg)。然而，在这里，我们希望使用这些系数在[图10-1](ch10.xhtml#ch10fig01)中绘制分隔线。我们如何从输出对象`w`中获取这些系数呢？
- en: Recall from [Section 8.9.3](ch08.xhtml#ch08lev9sec3) that multiclass applications
    of the logit model use either a One vs. All (OVA) or an All vs. All (AVA) approach;
    `qeLogit()` uses OVA. It thus runs one logit model for each class, placing the
    `glm()` outputs in the `glmOuts` component of the `qeLogit()` output.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第8.9.3节](ch08.xhtml#ch08lev9sec3)，logit模型的多类应用使用的是“一个对所有”（OVA）或“所有对所有”（AVA）方法；`qeLogit()`使用的是OVA。因此，它对每个类运行一个logit模型，并将`glm()`的输出放在`qeLogit()`输出的`glmOuts`组件中。
- en: However, the two-class model is a little different. To avoid essentially running
    the same model twice—for the forest cover data, type 1 versus non−type 1—`qeLogit()`
    runs it just once. In other words, we’ll look at `w$glmOuts[[1]]`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，二分类模型稍微不同。为了避免实际上运行相同的模型两次——对于森林覆盖数据，类型1与非类型1——`qeLogit()`只运行一次。换句话说，我们将查看`w$glmOuts[[1]]`。
- en: 'To obtain the coefficients, we turn to `coef()`, yet another generic function
    like the ones we’ve seen before, such as `print()` and `plot()`. This function
    extracts the estimated coefficients:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取系数，我们使用`coef()`，这是另一个通用函数，就像我们之前看到的`print()`和`plot()`一样。这个函数提取估计的系数：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now recall again that the logistic model routes the linear model into the logistic
    function, *ℓ*(*t*) = 1/(1 + *e*^(−*t*)); the placeholder *t* is set to the linear
    form. The above output gives the estimated probability of type-1 cover for a location
    having feature values of *v*1 and *v*6 as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在再回忆一下，逻辑模型将线性模型转换为逻辑函数，*ℓ*(*t*) = 1/(1 + *e*^(−*t*))；占位符*t*被设置为线性形式。上述输出给出了一个位置在具有特征值*v*1和*v*6的情况下，类型1覆盖的估计概率：
- en: '![Image](../images/ch10equ01.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10equ01.jpg)'
- en: Say we guess the location to have type-1 cover or not depending on whether the
    estimated probability in [Equation 10.1](ch10.xhtml#ch10equ01) is greater than
    0.5 or not. Setting that equation to 0.5, things look formidable at first but
    become simple when we note the fact that *e*⁰ = 1\. In other words, if the exponent
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们根据[方程10.1](ch10.xhtml#ch10equ01)中的估计概率是否大于0.5来猜测该位置是否有类型1覆盖。将该方程设置为0.5，开始时看起来很复杂，但当我们注意到*e*⁰
    = 1时，事情就变得简单了。换句话说，如果指数
- en: '![Image](../images/ch10equ02.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10equ02.jpg)'
- en: then the right-hand side of [Equation 10.1](ch10.xhtml#ch10equ01) is equal to
    0.5, which is just what we want, a straight line forming the decision boundary.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，[方程10.1](ch10.xhtml#ch10equ01)的右侧等于0.5，这正是我们想要的，一个形成决策边界的直线。
- en: 'So the line in [Equation 10.2](ch10.xhtml#ch10equ02) forms the boundary between
    predicting type-1 or non-type-1 cover. That is the equation of a straight line,
    which is plotted in [Figure 10-2](ch10.xhtml#ch10fig02). We superimposed that
    line onto [Figure 10-1](ch10.xhtml#ch10fig01) by using R’s `abline()` function,
    which plays exactly the role implied by the name—that is, adding a line to an
    existing plot:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，[公式10.2](ch10.xhtml#ch10equ02)中的线形成了区分预测类型1或非类型1封面的边界。那就是一条直线的方程，绘制在[图10-2](ch10.xhtml#ch10fig02)中。我们通过使用R的`abline()`函数将这条线叠加到[图10-1](ch10.xhtml#ch10fig01)上，`abline()`函数的作用正如其名——即在现有图形中添加一条直线：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The result is shown in [Figure 10-2](ch10.xhtml#ch10fig02). It happens to be
    almost vertical, which is not surprising since the coefficient of `V6` is so small,
    but no matter. Data points to the right of the line are predicted to be of type-1
    cover, with a non-type-1 prediction for those to the left.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如[图10-2](ch10.xhtml#ch10fig02)所示。它恰好几乎是垂直的，这并不奇怪，因为`V6`的系数非常小，但这无关紧要。线右侧的数据点被预测为类型1封面，左侧的数据点则预测为非类型1封面。
- en: '![Image](../images/ch10fig02.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10fig02.jpg)'
- en: '*Figure 10-2: Forest cover data with a logistic boundary line*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-2：带有logistic边界线的森林覆盖数据*'
- en: Clearly, quite a few data points are misclassified—that is, plus signs to the
    right of the line and squares to the left. We probably could reduce the number
    of misclassified points by increasing the number of features we use— we had only
    *p* = 2 features here—but there still would be some misclassified points.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，有相当多的数据点被误分类——也就是说，右边的加号和左边的方块。我们可能通过增加使用的特征数量来减少误分类的点——这里我们只有*p* = 2个特征——但仍然会有一些误分类的点。
- en: 'This motivates the basic goal of SVM:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这激发了SVM的基本目标：
- en: We wish to find a line that separates our classes well and then use that line
    to predict new cases in the future by determining which side of the line they
    fall on. Our line typically will not fully separate our classes, so we will have
    some misclassification errors, just as with any ML method. But hopefully a carefully
    chosen line will serve us well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望找到一条能够很好地区分类别的直线，然后通过判断新样本落在直线的哪一侧来预测未来的案例。我们的直线通常不会完全将类别分开，所以我们会有一些误分类错误，就像任何机器学习方法一样。但希望精心选择的直线能够为我们提供良好的结果。
- en: With *p* = 3 features, the line becomes a plane in three dimensions, difficult
    to visualize, and if we have more than three features, it is impossible to visualize.
    But by always keeping in mind the two-feature case and its geometric interpretation,
    we will have the intuition to use SVM effectively.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当*p* = 3个特征时，线变成了三维空间中的一个平面，难以直观地理解，如果特征超过三个，那就完全无法可视化。但通过始终牢记二特征情况下的几何解释，我们将能够直观地使用SVM。
- en: 'One more point before getting into the details: Why not simply use the above
    logit scheme to create our boundary line? What might be the advantage of using
    an SVM-produced line? The answer is that logit is very confining. It specifies
    a particular form for the regression function, involving the exponential function
    and so on as in [Equation 10.1](ch10.xhtml#ch10equ01), and though this might be
    a good assumption in some applications, it might not be so in others.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 进入细节之前，还有一个问题：为什么不直接使用上述的logit方案来创建我们的边界线呢？使用SVM生成的线有什么优势吗？答案是，logit方法非常局限。它规定了回归函数的特定形式，涉及指数函数等内容，如[公式10.1](ch10.xhtml#ch10equ01)所示，虽然在某些应用中这可能是一个合理的假设，但在其他情况下则不一定适用。
- en: By contrast, aside from the implicit assumption that the best interclass boundary
    is a straight line rather than some other curve (even this condition can be dropped,
    as we will see later), SVM makes no assumptions, so it is more flexible and may
    produce a better fit (just as is the case for k-NN, random forests, and so on,
    which make even fewer assumptions).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，除了隐含假设最佳的类间边界是直线而不是其他曲线（即使这个条件可以被放宽，稍后我们会看到），SVM没有做出任何假设，因此它更具灵活性，可能会产生更好的拟合效果（就像k-NN、随机森林等方法一样，这些方法做出的假设更少）。
- en: 10.2 Lines, Planes, and Hyperplanes
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2 直线、平面与超平面
- en: Let’s explore that geometric view a bit further.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步探讨一下这个几何视角。
- en: The derivation above always holds with a logistic model; prediction of *Y* =
    1 versus *Y* = 0 will always boil down to computing a linear function of the features.
    If we have *p* = 2 (that is, two features, such as *v*1 and *v* 6), the boundary
    between predicting *Y* = 1 and *Y* = 0 is a straight line of the form
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ03.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: as we saw in [Equation 10.2](ch10.xhtml#ch10equ02). If *p* = 3, say, adding
    the *v* 8 feature, the boundary takes the form of a plane, with the form
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ04.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: As noted above, this is hard to visualize. As also noted, for *p* > 3, we can’t
    visualize the setting at all. But we are still working with a linear form in the
    features, whose behavior is like that of a line or plane. Since it is planelike,
    we call it a *hyperplane*. For technical accuracy, we’ll use the acronym LPH (line/plane/hyperplane)
    rather than merely saying “line,” but readers should always think in terms of
    lines in the *p* = 2 case to guide their intuition.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Math Notation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Central to any discussion of SVM—including reading the documentation for SVM
    software—is the “dot product” notation. Though the name and math formula may sound
    intimidating, it’s just a way of stating things more succinctly. We first discuss
    how to change much of the SVM notation to vector form and then introduce dot products.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '***10.3.1 Vector Expressions***'
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As can be seen from [Equations 10.3](ch10.xhtml#ch10equ03) and [10.4](ch10.xhtml#ch10equ04),
    an LPH can be represented by its coefficient vector—for example, (*c*[1], *c*[2],
    *c*[3], *c*[4]) in [Equation 10.4](ch10.xhtml#ch10equ04). It’s customary in SVM
    to write things in terms of equality to 0, so, for instance, we rewrite [Equation
    10.4](ch10.xhtml#ch10equ04) as
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ05.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: 'and set:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ06.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: The vector *w* and the number *w*[0] compose our description of the LPH.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we would summarize [Equation 10.2](ch10.xhtml#ch10equ02) by writing:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ07.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: '***10.3.2 Dot Products***'
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The underlying theory of SVM makes heavy use of calculus and linear algebra.
    As noted, such mathematics is far beyond the scope of this book. However, it will
    be productive and easy to use some notation from that subject— just notation and
    nothing conceptual other than a little algebra.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Our goal will be to come up with a simple, compact way to determine on which
    side of the boundary line or LPH a new case falls so that we may easily predict
    its class.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'The *dot product* between two vectors *u* = (*u*[1], . . . , *u**[m]*) and
    *v* = (*v*[1], . . . , *v**[m]*) is simply a sum of products:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ08.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: 'For example, let’s take the dot product of *w* in [Equation 10.7](ch10.xhtml#ch10equ07)
    with the vector (1,−4):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ09.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: 'It is helpful to recast [Equation 10.1](ch10.xhtml#ch10equ01) in our new dot
    product notation:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ10.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: 'Note some algebraic properties:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '*e*⁰ = 1'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*e**^t* > 1 for *t* > 0'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*e**^t* > 1，当*t* > 0时'
- en: '*e*^(−*t*) < 1 for *t* > 0'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*e*^(−*t*) < 1，当*t* > 0时'
- en: '![Image](../images/unch10equ01.jpg)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![Image](../images/unch10equ01.jpg)'
- en: Thus, in [Equation 10.10](ch10.xhtml#ch10equ10),
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在[方程 10.10](ch10.xhtml#ch10equ10)中，
- en: '![Image](../images/ch10equ11.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10equ11.jpg)'
- en: and
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![Image](../images/ch10equ12.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10equ12.jpg)'
- en: So, faced with a new case to predict, we simply look at the sign—positive and
    negative—of *w* • (*v*1, *v*6) + *w*[0]. If positive, the new case is more likely
    (probability more than 0.5) to have cover type 1, while in the negative case,
    that probability is less than 0.5\. In other words, we predict cover type 1 for
    the new case if *w* • (*v*1, *v*6) + *w*[0] > 0, and otherwise predict non−type
    1.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当面对一个新的预测案例时，我们只需要查看 *w* • (*v*1, *v*6) + *w*[0] 的符号——正负。如果是正数，那么这个新案例更有可能（概率大于0.5）属于封面类型1，而如果是负数，那这个概率小于0.5。换句话说，如果
    *w* • (*v*1, *v*6) + *w*[0] > 0，我们预测该新案例为封面类型1，否则预测为非类型1。
- en: 'This again is saying that we guess cover type 1 if our new case to be predicted
    falls to the right of the line or non−type 1 if it is on the left. And our SVM
    boundary is the vector *x* that makes:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这再次表明，如果我们要预测的新案例位于线的右侧，我们猜测它是封面类型1；如果位于左侧，则猜测它是非类型1。而我们的SVM边界是使以下式子成立的向量 *x*：
- en: '![Image](../images/ch10equ13.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10equ13.jpg)'
- en: Again, this is just notation, but in math, it is often the case that convenient
    notation helps clarify things. Drawing lines is visualizable for applications
    with *p* = 2 features, but drawing planes is hard to visualize if *p* = 3, and
    if *p* > 3, visualization is impossible. What’s nice about the dot product notation
    is that we know which way to guess the class of a new case by simply noting whether
    *w* • (*v*1, *v*6) + *w*[0] is positive or negative.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这只是符号表示，但在数学中，便捷的符号往往有助于澄清问题。对于 *p* = 2 个特征的应用，画线是可以可视化的，但如果 *p* = 3，则画平面就很难可视化；如果
    *p* > 3，则无法可视化。使用点积符号的好处是，我们只需注意 *w* • (*v*1, *v*6) + *w*[0] 是否为正或负，就能知道该如何预测新案例的类别。
- en: By the way, SVM theorists also like to code the two classes as *Y* = +1 and
    *Y* = −1 rather than 1 or 0, as is standard in statistics. We’ll continue to use
    the latter coding in general but will turn to the former in this chapter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，SVM的理论学者也喜欢将两个类别编码为*Y* = +1和*Y* = −1，而不是像统计学中标准的那样编码为1或0。我们通常会继续使用后者的编码，但在本章中会转而使用前者。
- en: '***10.3.3 SVM as a Parametric Model***'
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***10.3.3 SVM作为一个参数模型***'
- en: We have referred to linear and logistic models as *parametric* in that the regression
    function is modeled as being determined by a finite number of values *β*[0], *β*[1],
    . . . , *β**[p]* . This is in contrast to, for example, k-NN methods, which make
    no assumptions regarding the form of the regression function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们曾提到线性和逻辑模型是*参数化的*，因为回归函数被建模为由有限数量的值 *β*[0], *β*[1], . . . , *β**[p]* 确定。这与例如k-NN方法相对立，后者并不对回归函数的形式做出任何假设。
- en: One implication of [Equation 10.13](ch10.xhtml#ch10equ13) is that SVM too is
    a parametric model. Instead of assuming a parametric form for the regression function,
    here we assume a parametric form for the boundary line between the two classes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 10.13](ch10.xhtml#ch10equ13)的一个含义是，SVM也是一个参数模型。与其假设回归函数的参数形式，这里我们假设两个类别之间边界线的参数形式。'
- en: '10.4 SVM: The Basic Ideas—Separable Case'
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4 SVM：基本思想——可分离情况
- en: As noted earlier, the line we drew in [Figure 10-2](ch10.xhtml#ch10fig02) did
    not cleanly separate the two classes. There were plus signs and squares on both
    sides of the line. This is typical, not only for logit-produced lines but also
    for lines created by SVM—our focus here. However, the SVM method is easier to
    explain if we first consider datasets for which the two classes are cleanly separable,
    so most books begin with that case, as we will here.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们在[图 10-2](ch10.xhtml#ch10fig02)中绘制的那条线并没有完全分开两个类别。线的两边都有加号和方框。这是典型的情况，不仅适用于逻辑回归产生的线，也适用于SVM——我们在此关注的内容。然而，如果我们首先考虑两个类别可以干净地分开的数据集，那么SVM方法更容易解释，因此大多数书籍都会从这个情况开始，正如我们将在这里所做的那样。
- en: For clarity, we will continue to focus on the two-class case, as with the type-1/non-type-1
    cover example above. And again, we will continue looking at the case of *p* =
    2 features, where the LPH is a line.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，我们将继续专注于两类问题，就像上面提到的类型1/非类型1封面例子一样。再者，我们将继续讨论 *p* = 2 特征的情况，其中LPH是一条直线。
- en: Note that all our references to “the data” here will be in terms of the training
    data. We find a boundary line for the training data and then predict future cases
    according to that line. Similarly, when we speak of separability of “the data,”
    we mean the training data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在此处提到的“数据”将指训练数据。我们为训练数据找到边界线，然后根据该线预测未来的案例。类似地，当我们谈论“数据”的可分性时，我们指的是训练数据。
- en: '***10.4.1 Example: The Anderson Iris Dataset***'
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***10.4.1 示例：安德森鸢尾花数据集***'
- en: 'Edgar Anderson’s data on iris flowers, included in R, has been the subject
    of countless examples in books, websites, and so on. There are three classes:
    *setosa*, *versicolor*, and *virginica*.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 埃德加·安德森关于鸢尾花的数据，包含在R中，已经成为书籍、网站等中无数示例的主题。数据包含三类：*setosa*、*versicolor*和*virginica*。
- en: 'This dataset is included in R:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含在R中：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that what we do in this section will be mainly for motivating the subsequent
    material. For day-to-day SVM computation, you’ll use `qeSVM()`. So, in the example
    here, in which we do some non-SVM analysis for motivational purposes, we will
    omit some of the code and algebra.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本节中的内容主要是为了激发后续材料的动机。对于日常的 SVM 计算，您将使用 `qeSVM()`。因此，在这里的示例中，我们进行一些非 SVM
    分析以激发动机，我们将省略部分代码和代数。
- en: As mentioned, for this example, we’d like data in which the two classes are
    cleanly separated by a straight line. This is the case if we take our two iris
    classes as setosa and nonsetosa, with the features taken to be the `Sepal.Length`
    and `Petal.Width` columns.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，对于这个示例，我们希望数据中两个类别能够被一条直线清晰分隔。如果我们将两个鸢尾花类别取为 setosa 和非 setosa，并选择 `Sepal.Length`
    和 `Petal.Width` 列作为特征，那么情况就是这样。
- en: Let’s first plot the data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先绘制数据。
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This produces the graph in [Figure 10-3](ch10.xhtml#ch10fig03).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生[图 10-3](ch10.xhtml#ch10fig03)中的图形。
- en: '![Image](../images/ch10fig03.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10fig03.jpg)'
- en: '*Figure 10-3: Setosa and nonsetosa*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-3：Setosa 和非 Setosa*'
- en: One can easily draw a line between the two classes—in fact, many lines. But,
    which line is optimal?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 可以很容易地在两个类别之间画一条线——实际上，可以画许多条线。但，哪一条线是最优的呢？
- en: '***10.4.2 Optimizing Criterion***'
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***10.4.2 优化准则***'
- en: 'Choosing our boundary line amounts to choosing the coefficient vector *w* and
    the *w*[0] term. In other words, the situation is similar to that of the linear
    and generalized linear models, where we choose the estimated coefficients ![Image](../images/unch08equ07.jpg).
    (Actually, *w* and *w*[0] are estimates as well, but to avoid clutter, we do not
    use the hat notation.) Now recall that with a linear model, the way we choose
    our coefficients ![Image](../images/unch08equ07.jpg) is through an optimization
    problem: we minimize a certain sum of squares.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 选择我们的边界线等同于选择系数向量 *w* 和 *w*[0] 项。换句话说，这种情况类似于线性和广义线性模型，在这些模型中，我们选择估计的系数 ![Image](../images/unch08equ07.jpg)。（实际上，*w*
    和 *w*[0] 也是估计值，但为了避免混乱，我们不使用帽记号。）现在回想一下，在线性模型中，我们选择系数 ![Image](../images/unch08equ07.jpg)的方式是通过优化问题：我们最小化某个平方和。
- en: SVM still minimizes a certain sum, but it uses a different loss function than
    squared error. Detailing it would take us too far into some arcane math with little,
    if any, benefit. Fortunately, the math has an easily grasped geometric version,
    which we will now discuss.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 仍然最小化某个和，但它使用的是与平方误差不同的损失函数。详细说明这一点将使我们深入一些晦涩的数学内容，几乎没有什么实际帮助。幸运的是，数学有一个易于理解的几何版本，我们现在将讨论它。
- en: Toward this end, look at [Figure 10-4](ch10.xhtml#ch10fig04). Here we have “roped
    off” our two classes (setosas above and nonsetosas below) into what are called
    *convex hulls*. Again, this is just for illustration purposes; the `qeSVM()` function
    will do the computation for us (and with a different method), and we will not
    compute convex hulls ourselves after this example. We thus omit the code. (One
    can use the function `mvtnorm::chull()`.)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，请查看[图 10-4](ch10.xhtml#ch10fig04)。在这里，我们“圈定”了两个类别（上方是 setosa， 下方是非 setosa），将它们分成了所谓的
    *凸包*。再次说明，这仅仅是为了说明问题；`qeSVM()` 函数会为我们进行计算（并使用不同的方法），而我们在此示例之后将不再自己计算凸包。因此，我们省略了代码。（可以使用
    `mvtnorm::chull()` 函数。）
- en: '![Image](../images/ch10fig04.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10fig04.jpg)'
- en: '*Figure 10-4: SVM convex hulls*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-4：SVM 凸包*'
- en: It can be shown mathematically that the SVM boundary line is “halfway between”
    the two convex hulls. More precisely stated, one first finds the two points in
    the hulls that are closest to each other. Our boundary line is then the perpendicular
    bisector of the line segment between those two points. We draw that in [Figure
    10-5](ch10.xhtml#ch10fig05), along with two related, dashed lines, which define
    the “margin” of an SVM fit.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上可以证明，SVM的边界线恰好位于两个凸包的“中间”。更准确地说，首先找到两个凸包中最接近的两点。我们的边界线就是这两点之间线段的垂直平分线。我们在[图
    10-5](ch10.xhtml#ch10fig05)中绘制了这一点，以及两条相关的虚线，定义了SVM拟合的“边际”。
- en: '![Image](../images/ch10fig05.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10fig05.jpg)'
- en: '*Figure 10-5: SVM margin*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-5：SVM 边际*'
- en: 'Note the following, both here and in general:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意以下几点，无论是此处还是一般情况：
- en: The region between the dashed lines is called the *margin*.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两条虚线之间的区域称为*边际*。
- en: For separable data, there will be no data points *inside* the margin.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于可分数据，边际内不会有数据点。
- en: The points lying *on* the margin are called *support vectors* (the SV in SVM).
    For this dataset, we have three support vectors, one for the setosas at (2.7,1.0)
    and two for the nonsetosas at (2.3,0.3) and (3.5,0.6).
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位于边际上的点称为*支持向量*（SVM中的SV）。对于这个数据集，我们有三个支持向量，一个位于(2.7,1.0)的setosa类，另外两个位于(2.3,0.3)和(3.5,0.6)的非setosa类。
- en: 'In terms of *w* and *w*[0], the value of *w*[0] + *w* • *x* will be:'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就* w *和* w *[0]而言，*w*[0] + *w* • *x* 的值将是：
- en: '**−**     0 for any point *x* on the boundary'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**−**     0 对于边界上的任何点 *x*'
- en: '**−**     +1 for any support vector in the *Y* = +1 class (setosa in this case)'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**−**     +1 对于*Y* = +1类中的任何支持向量（在此案例中为setosa）'
- en: '**−**     −1 for any support vector in the *Y* = −1 class (nonsetosa in this
    case)'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**−**     −1 对于*Y* = −1类中的任何支持向量（在此案例中为非setosa）'
- en: '**−**     > +1 for any nonsupport vector in the *Y* = +1 class'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**−**     > +1 对于*Y* = +1类中的任何非支持向量'
- en: '**−**     < −1 for any nonsupport vector in the *Y* = −1 class'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**−**     < −1 对于*Y* = −1类中的任何非支持向量'
- en: By the way, a look at [Equation 10.13](ch10.xhtml#ch10equ13) shows that the
    values of *w* and *w*[0] are not unique. Multiplying both by, say, 8.8 would still
    result in 0 on the right-hand side of the equation. So, the convention is to choose
    them such that the value of *w*[0] + *w* • *x* is +1 or −1 at the support points.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 顺便提一下，查看[公式 10.13](ch10.xhtml#ch10equ13)可以看到，*w* 和 *w*[0] 的值并不是唯一的。比如将它们都乘以8.8，右侧的结果仍然是0。因此，约定是选择它们，使得在支持点上，*w*[0]
    + *w* • *x* 的值为+1或−1。
- en: To predict *Y* for a new case in which *X* = *x*[new], we guess *Y* to be either
    +1 or −1, depending on whether *w*[0] + *w* • *x*[new] is greater than or less
    than 0\. Note that even though our training data is assumed separable here, new
    data points may fall within the margin.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要预测*Y*对于一个新案例，其中*X* = *x*[new]，我们猜测*Y*为+1或−1，取决于* w *[0] + *w* • *x*[new]是大于还是小于0。请注意，尽管我们假设训练数据在这里是可分的，但新数据点可能会落在边际内。
- en: 'Keep in mind fitting an SVM model amounts to choosing *w* and *w*[0] . The
    above scheme can be shown to be optimal. But wait a minute—what does “optimal”
    even mean? The criterion for optimality generally used in SVM is this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，拟合SVM模型等同于选择* w *和* w *[0]。上面的方案可以证明是最优的。但是等一下——“最优”究竟是什么意思？SVM中通常使用的最优标准是：
- en: We choose *w* and *w*[0] in such a way that the margin will have the largest
    possible width.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择* w *和* w *[0]，使得边际具有最大的可能宽度。
- en: In other words, SVM seeks to not only separate the data points of the two classes
    but also render the two classes as far apart as possible, relative to the boundary.
    It finds a “buffer zone” between the two classes, maximizing the width of the
    zone. As noted, that buffer zone is called the margin.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，SVM不仅仅是试图将两类数据点分开，还要尽可能地将这两类数据点相对于分界面拉得尽可能远。它在两类之间找到一个“缓冲区”，并最大化这个区域的宽度。如前所述，这个缓冲区被称为边际。
- en: The key idea is that a large margin in the training set means that the two classes
    are well separated, which hopefully means that new cases in the future will be
    correctly classified. It turns out that choosing *w* and *w*[0] using convex hulls
    as above (even with *p* > 2) does maximize the margin.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 关键思想是，训练集中的大边际意味着两类数据点被很好地分开，这也意味着未来的新案例可能会被正确分类。事实证明，像上述那样使用凸包选择* w *和* w *[0]（即使*p*
    > 2）确实能够最大化边际。
- en: 10.4.2.1 Significance of the Support Vectors
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 10.4.2.1 支持向量的意义
- en: The support vectors “support” the fit, in the sense that a change in any of
    them will change the fit; a change in the other data points will not change the
    fit (as long as it stays within the hull). Of course, we can also say that *adding*
    new points won’t change the fit either, as long as the new points are within a
    hull.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量通过“支持”拟合，意思是任何一个支持向量的变化都会改变拟合结果；而其他数据点的变化则不会改变拟合结果（只要它们仍然在凸包内）。当然，我们也可以说，*添加*新的数据点也不会改变拟合结果，只要这些新点位于凸包内。
- en: An oft-claimed benefit of SVMs is that (at least in the separable case) they
    tend to produce a sparse fit, in this case, meaning not that most of the components
    of *w* are 0s but rather that the essential dimensionality of the fit is low.
    Viewed another way, the claim is that the more support vectors one has, the greater
    the risk of overfitting. Evidence for this assertion is weak, I believe, though
    it may serve as one of several guides to one’s model-fitting process.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）的一个常见优点是（至少在可分情况中）它们往往会产生稀疏的拟合，在这里，稀疏并不意味着*w*的大多数组件为0，而是拟合的基本维度较低。从另一个角度看，这个说法是：支持向量越多，过拟合的风险就越大。我认为，这个说法的证据较为薄弱，尽管它可以作为模型拟合过程中的几个指南之一。
- en: But that benefit may be illusory. As noted, the source of the sparsity is the
    dependence of *w* on a few data points (that is, the support vectors). But what
    if some of the support vectors are outliers (not representative of the data as
    a whole) or are even downright erroneous? Many real datasets do have some errors.
    Our fit then depends heavily on some questionable data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个好处可能是虚幻的。正如前面所说，稀疏性的来源是*w*依赖于少数数据点（即支持向量）。但如果某些支持向量是异常值（不代表整体数据）或者甚至是明显错误的呢？许多实际数据集确实存在一些错误。那么我们的拟合结果就会高度依赖一些可疑的数据。
- en: Furthermore, this high sensitivity of *w* to just a few data points makes for
    high sampling variability; a different sample likely has a different set of support
    vectors. In other words, *w* has a high variance.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，*w*对少数数据点的高度敏感性导致了较高的抽样变异性；不同的样本可能会有不同的支持向量。换句话说，*w*具有较高的方差。
- en: '10.5 Major Problem: Lack of Linear Separability'
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5 主要问题：缺乏线性可分性
- en: 'The margin, actually known as the *hard margin*, is only defined in situations
    in which some line (or LPH) exists that cleanly separates the data points of the
    two classes. As can be seen in our earlier graphs for the forest cover data, in
    most practical situations, no such line exists. Even with the iris data, no line
    separates the versicolor and virginica in columns 1 and 3:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 边距，实际上被称为*硬边距*，只在存在某条线（或LPH）能干净地分开两类数据点的情况下才定义。正如我们在早期关于森林覆盖数据的图表中看到的那样，在大多数实际情况下，并不存在这样的分隔线。即使是鸢尾花数据，在第一列和第三列中，也没有任何一条线能分开变色鸢尾和维吉尼卡鸢尾：
- en: '[PRE9]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: See [Figure 10-6](ch10.xhtml#ch10fig06); pluses and triangles are not cleanly
    separated.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见[图10-6](ch10.xhtml#ch10fig06)；加号和三角形没有被干净地分开。
- en: 'There are two solutions to that problem: (a) using a *kernel* to transform
    the data into linear separability or (b) creating a *soft margin*, in which we
    allow some points to reside within the margin. Typically a combination of these
    two approaches is used. For instance, after doing a kernel transformation, we
    still may—in fact, probably will—find that no cleanly separating LPH exists, and
    thus we will need to resort to also allowing some exceptional points to lie within
    the margin. However, the fewer the exceptions, the better, so it’s best to use
    both approaches in combination rather than just going directly to a soft margin
    solution.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题有两种方法：(a) 使用*核函数*将数据转换为线性可分，或(b) 创建一个*软边距*，在其中允许一些点位于边距内。通常，这两种方法会结合使用。例如，在进行核转换后，我们仍然可能—实际上，可能会—发现没有能够干净分隔的LPH，因此我们还需要允许一些异常点位于边距内。然而，异常点越少越好，所以最好结合使用这两种方法，而不是直接采用软边距解决方案。
- en: '![Image](../images/ch10fig06.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10fig06.jpg)'
- en: '*Figure 10-6: Iris data; three classes*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-6：鸢尾花数据；三类*'
- en: '***10.5.1 Applying a “Kernel”***'
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***10.5.1 应用“核函数”***'
- en: Here we transform the data, say, by applying a polynomial transformation, and
    then find an LPH separator on the new data. The degree of the polynomial is then
    a hyperparameter.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对数据进行转换，比如通过应用多项式转换，然后在新的数据上找到一个LPH分隔符。多项式的阶数就是一个超参数。
- en: 10.5.1.1 Motivating Illustration
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 10.5.1.1 动机示例
- en: 'To get a feeling as to why kernels can be useful, consider a favorite example
    in ML presentations: “doughnut-shaped” data. Let’s generate some. (The code here
    is rather arcane and can be safely skipped without affecting the sequel.)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么核函数可能有用，我们考虑一个在机器学习展示中常用的例子：“甜甜圈形状”的数据。让我们生成一些数据。（这里的代码相当复杂，可以安全跳过，不影响后续内容。）
- en: '[PRE10]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The data is plotted in [Figure 10-7](ch10.xhtml#ch10fig07). The two classes,
    drawn with pluses and circles, are clearly separable—but by a circle and not a
    straight line.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在[图10-7](ch10.xhtml#ch10fig07)中显示。两类数据，分别用加号和圆圈表示，是明显可分的——但通过一个圆而不是一条直线。
- en: '![Image](../images/ch10fig07.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10fig07.jpg)'
- en: '*Figure 10-7: “Doughnut” data*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-7：“甜甜圈”数据*'
- en: 'But we can fix that by adding a squared term:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可以通过添加一个平方项来解决这个问题：
- en: '[PRE11]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We took our original dataset and transformed it, replacing each data point by
    its square. That square is our new feature, replacing the old one. In the plot
    of the new data, [Figure 10-8](ch10.xhtml#ch10fig08), the pluses and circles are
    easily separated by a straight line.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们取了原始数据集并对其进行了转换，将每个数据点替换为其平方。这个平方就是我们的新特征，替代了旧的特征。在新数据的图表中，[图10-8](ch10.xhtml#ch10fig08)，加号和圆圈很容易被一条直线分开。
- en: '![Image](../images/ch10fig08.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10fig08.jpg)'
- en: '*Figure 10-8: “Doughnut” data, transformed*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-8：“甜甜圈”数据，经过转换*'
- en: 'And that is what SVM users typically do: try to find a transformation of the
    data under which the transformed data will be linearly separable, or at least
    nearly so. Of course, the SVM software does all the work for us; we don’t transform
    the data by hand, as in the above illustration. This is done with a kernel, as
    seen in the next section.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是SVM用户通常做的事情：尝试找到一种数据转换方式，使得转换后的数据能够线性可分，或者至少接近线性可分。当然，SVM软件为我们完成了所有的工作；我们并不是像上面的示例那样手动转换数据。这个过程是通过核函数完成的，如下一节所示。
- en: 10.5.1.2 The Notion of a Kernel
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 10.5.1.2 核函数的概念
- en: We need to cover one more point before turning to an example on real data. What
    is a kernel? A *kernel* is a way of transforming our data, again with the goal
    of making our data separable. But it’s a little more specific than that; it’s
    a function *K*(*u*, *v*) whose inputs are two vectors.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在转向实际数据示例之前，我们还需要讨论一个问题。什么是核函数？*核函数*是将我们的数据进行转换的一种方式，目标是使数据变得可分离。但它比这更具体：它是一个函数*K*(*u*,
    *v*)，其输入是两个向量。
- en: This makes sense because we saw in [Section 10.3.2](ch10.xhtml#ch10lev3sec2)
    that dot products play a key role in SVM, and, in fact, even more so in the internal
    computations, which are not covered in this book. Accordingly, many kernels are
    functions of dot products.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是有道理的，因为我们在[第10.3.2节](ch10.xhtml#ch10lev3sec2)中看到，点积在SVM中起着关键作用，实际上，它在内部计算中起着更加重要的作用，这部分内容在本书中并未涉及。因此，许多核函数是点积的函数。
- en: 'An example is the *polynomial kernel*:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是*多项式核函数*：
- en: '![Image](../images/ch10equ14.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10equ14.jpg)'
- en: The quantities *d* and *γ* are hyperparameters. In the quadratic case *d* =
    2, we achieve essentially the same effect as in the previous section, where we
    squared the *X* values. Here we square dot products.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*d*和*γ*是超参数。在二次情况下，*d* = 2，我们实际上达到了与上一节相同的效果，在上一节中我们将*X*值平方了。在这里，我们对点积进行平方。'
- en: 'Also in common usage is the *radial basis function (RBF)*:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用的函数是*径向基函数（RBF）*：
- en: '![Image](../images/ch10equ15.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch10equ15.jpg)'
- en: Here *γ* is a hyperparameter.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*γ*是一个超参数。
- en: Once again, as with so many ML questions, the answer to the question “Which
    kernel is best?” is “It depends.” The type and size of the dataset, the number
    of features, and so on all make for variation in performance between kernels.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，像许多机器学习问题一样，问题“哪个核函数最好？”的答案是：“这取决于。”数据集的类型和大小、特征的数量等等，都会导致不同核函数在性能上的差异。
- en: '***10.5.2 Soft Margin***'
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***10.5.2 软间隔***'
- en: As noted, it is rather uncommon to have linearly separable data. Nonseparability
    is the typical case. How can we handle it, in the sense of tolerating having a
    few points lying within the margin? Let’s consider two approaches.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，线性可分的数据是比较罕见的。不可分性才是典型情况。我们如何处理这种情况，即容忍一些点位于间隔内？我们可以考虑两种方法。
- en: 10.5.2.1 Geometric View
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 10.5.2.1 几何视角
- en: 'Instead of computing convex hulls, we could work with *reduced convex hulls*.
    Once again, the mathematical formulation is beyond the scope of this book, but
    the essence of the procedure is this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过处理*简化的凸包*来代替计算凸包。再次强调，数学公式超出了本书的范围，但程序的核心是这样的：
- en: We replace the two original convex hulls with shrunken versions. (Not only will
    they be smaller in size, but their shape will tend to be “rounder,” or less oblong.)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The margin will be computed on the basis of the shrunken hulls.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, conduct “business as usual,” even though some of the training set data
    will then fall within the margin.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, the amount of shrinkage is, as always, a hyperparameter to be set
    by the user, possibly via cross-validation.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.2.2 Algebraic View
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The geometric view is intuitive, but the more common approach is via “cost.”
    There is a cost hyperparameter, usually denoted by *C*. Here is how it works.
    Denote data point *i* in our training set by *X**[i]*, the vector of features
    for that data point, and let *Y**[i]* be the class label, either +1 or −1.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: In the separable case, recall that, for data points on the margin boundary,
    *w*[0] + *w* • *X[i]* is equal to either +1 or −1\. For points outside the margin,
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ16.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: 'depending on whether *Y[i]* is +1 or −1\. But there is a neat trick to state
    this requirement more compactly:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ17.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: In the case of a soft margin, we relax that a bit, allowing discrepancies from
    1.0, say,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ18.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: and
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ19.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: Let *d**[i]* denote the discrepancy for data point *i* so that here *d*[3] =
    0.12 and *d*[8] = −1.71\. Set *d**[i]* = 0 if data point *i* has no discrepancy—that
    is, if *Y**[i]* (*w*[0] + *w* • *X**[i]*) ≥ 1.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Note that if 0 < *d**[i]* < 1, then data point *i* is a margin violation, but
    it still is on the correct side of the decision boundary—that is, it will be correctly
    classified. But if *d**[i]* > 1, the point will be on the other side of the boundary
    and thus misclassified.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: We control the total amount of discrepancy by stipulating that
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ20.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: where the hyperparameter *C* is our “discrepancy budget.” Again, the user sets
    *C*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Note that the *d**[i]* are not hyperparameters; they are by-products. The user
    chooses *C*. Each potential value of *w* and *w*[0] then gives rise to the *d**[i]*.
    The smaller the value we set for *C*, the fewer the number of data points within
    the margin—but the narrower the margin. We hope to have a wide margin; the SVM
    algorithm finds the values of *w* and *w*[0] that maximize the width of the margin,
    subject to the constraint ([Equation 10.20](ch10.xhtml#ch10equ20)).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '10.6 Example: Forest Cover Data'
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s give `qeSVM()` a try:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can then predict, say, for a new case similar to case 8 in our data, but
    changing the second feature value to 2,888:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We predict non-type-1 cover, though just slightly more likely than type 1.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: We’re using the default values here, which, among other things, set `kernel`
    to `radial`. If we wish to use just a soft margin (that is, no kernel transformation),
    we set `kernel` to `linear`. The hyperparameter `gamma`’s default value is 1.0\.
    An optional hyperparameter, `cost`, is the value *C* in our earlier discussion
    of soft margins. By the way, `qeML()` wraps `svm` in the famous `e10171` package.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 10.7 And What About That Kernel Trick?
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: No presentation of SVM would be complete without discussing the famous *kernel
    trick*, as it is largely responsible for the success of SVM. As usual, we won’t
    delve into the mathematical details—assuming the reader has no burning desire
    to learn about reproducing kernel Hilbert spaces—but the principle itself has
    major practical implications.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: To motivate this, let’s get an idea of how large our dataset can expand to when
    we transform via polynomials *without* using kernels. Our size measure in the
    transformed data will be the number of columns.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the forest cover data again, together with the `polyreg` package,
    the latter being used in our counting data columns. (The `getPoly()` function
    in `polyreg` is used by the polynomial models in our `qe`-series.)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The original dataset had only 54 features, but in the new form we have over
    1,500 of them! We would have even more if `getPoly()` did not avoid creating duplicates—for
    example, the square of a dummy variable.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: So, the new value of *p* was 1,564, for a dataset with only 500 rows, making
    it impossible to run. With the original data frame of more than 580,000 rows,
    the new size would be 581,012 × 1,564 = 908,702,768 elements. At 8 bytes per element,
    that would mean over 7GB of RAM! And it’s not just space but also time—the code
    would run forever.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: And that was just for degree 2\. Imagine degree-3 polynomials and so on! (Degree
    3 turns out to generate 16,897 columns.) So, some kind of shortcut is badly needed.
    Kernel trick to the rescue!
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The key point is that by using the kernel in [Equation 10.14](ch10.xhtml#ch10equ14),
    we can avoid having to compute and store all those extra columns. In the forest
    cover data, we can stick to those original 54 features—the vectors *u* and *v*
    in that expression are each 54 elements long—rather than computing and storing
    the 1,564\. We get the same calculations mathematically as if we were to take
    *u* and *v* to be 1,564-element vectors.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '10.8 “Warning: Maximum Number of Iterations Reached”'
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with many other ML methods, the computation for SVM is iterative. However,
    unlike those other methods, SVM should not have convergence problems. The search
    space has the *convex* property, which basically says it is bowl-shaped and thus
    easy to find the minimum.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: However, that presumes there is something to minimize. As we’ve seen with using
    soft margins or kernels, a line may not exist to cleanly separate the classes.
    For some particular pair of cost value and kernel (and the latter’s hyperparameters),
    it may be that no solution exists. In that case, we will of course have convergence
    issues, and we will have to try other combinations.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 10.9 Summary
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This chapter has been a bit more mathematical than the others and perhaps a
    bit more abstract as well. But really, the basic principles are simple:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: SVM is primarily for classification problems. Using OVA or AVA pairing, SVM
    can handle any number of classes, but to keep things simple, let’s assume two
    classes here.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we have *p* = 2 features, the basic goal is to find a line separating the
    two classes.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With *p* = 3, we wish to find a separating plane in three-dimensional space.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the cases where *p* > 3, we speak of a separating hyperplane. We cannot
    visualize these, and instead look at dot products with our hyperplane’s *w* vector.
    To classify a new case, we take the dot product of that case’s feature vector,
    add *w*[0], and decide the class based on whether the dot product is greater or
    less than 0.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We associate with the separating LPH a pair of LPHs that are parallel to the
    original one, thus creating the margin. The optimization criterion for choosing
    *w* is to maximize the width of the margin.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Typically the classes overlap, so there is no separating LPH, and we need to
    resort to artificial means to separate the classes. There are two ways to do this,
    usually used in combination:'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**−**     We can posit that a separating LPH does exist but is “curvy” rather
    than straight or flat. We transform the data, using a kernel, in an attempt to
    at least approximate some of that curviness.'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**−**     We can, to various degrees set by the user, allow data points to
    reside within the margin.'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As noted earlier, SVM is a more complicated tool than the ones we’ve seen earlier.
    But it is in wide usage, with many successes to its name, so the extra effort
    in this chapter is quite worthwhile.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
