- en: '**10'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A BOUNDARY APPROACH: SUPPORT VECTOR MACHINES**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Support vector machines (SVMs), together with neural networks (NNs), are arguably
    the two most “purist” of ML methods, motivated originally by artificial intelligence—that
    is, nonstatistical concepts. We’ll cover SVMs in this chapter and NNs in the next.
    SVMs are best known for classification applications. They can be used in regression
    settings as well, but we will focus on classification.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind this chapter will be a tad more mathematical than the others. Staying
    true to the nonmath spirit of the book, though, equations will be kept to the
    absolute minimum. SVM is such a powerful, generally usable method that understanding
    a bit of math here is an excellent investment of time. Even reading the documentation
    of SVM software requires some understanding of the structural underpinnings of
    the method.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Motivation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Everything about SVM involves boundary lines separating one class from another.
    To motivate that, we will first do a boundary analysis using the logistic model
    and then later bring in SVM. It will be important to keep in mind that throughout
    this section, we are simply exploring, to motivate SVM.
  prefs: []
  type: TYPE_NORMAL
- en: '***10.1.1 Example: The Forest Cover Dataset***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s revisit the forest cover data from [Section 5.4](ch05.xhtml#ch05lev4).
    Here we will construct a motivational graph, so we will need to look at only a
    small subset of the data. First, to avoid the “black screen problem,” in which
    we have so many points that the graph becomes an amorphous mess, we will graph
    a random subset of just 500 data points. Second, to keep things to a visualizable
    two dimensions, we will use just two features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `qeML` package includes a dataset `forest500`, consisting of a random 500
    rows of the original data. Now, what about the columns? We could try the Feature
    Ordering by Conditional Independence (FOCI) approach from [Section 4.5.1](ch04.xhtml#ch04lev5sec1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We might run the function again, as there is some randomness involved, but
    let’s go with the above for our example and, as always, get acquainted with the
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As seen above, there are seven cover types, meaning this is a multiclass problem.
    Here we’ll look at a two-class version, in which we wish to predict whether we
    have cover type 1 versus all others. The `regtools::toSubFactor()` function is
    handy for that kind of thing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see what the data looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This code generates the plot shown in [Figure 10-1](ch10.xhtml#ch10fig01).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-1: Forest cover data*'
  prefs: []
  type: TYPE_NORMAL
- en: We want to plot columns 1 and 2, hence the expression `f500[,c(1,2)]`. But we
    want to visually distinguish between the two classes, say, using squares and plus
    signs as symbols. In base-R graphics, plotting symbols are specified via the `pch`
    (point character) argument, and it turns out that the numerical codes were 0 and
    3.^([1](footnote.xhtml#ch10fn1)) The squares are the cover type 1 points, and
    pluses are non−type 1.
  prefs: []
  type: TYPE_NORMAL
- en: There seems to be no sharp tendency in the graph (that is, no trend of separation
    of the two groups). We see squares and pluses all over the graph. However, the
    plus signs seem to fall to the left and more on the upward side, versus more to
    the right for the squares.
  prefs: []
  type: TYPE_NORMAL
- en: We would like to draw a line in [Figure 10-1](ch10.xhtml#ch10fig01), such that
    most pluses are on one side of the line and most squares are on the other side.
    The reader can take a sneak peak at [Figure 10-2](ch10.xhtml#ch10fig02) to see
    where we are headed. But where did the line come from? Actually, one can use a
    logit model here. This should not be too surprising, as you will recall that the
    logit model has a linear form at its core.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how such a line can be drawn. Let’s fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Normally, in prediction contexts we are not interested in the estimated logistic
    model coefficients ![Image](../images/unch08equ07.jpg) . Here, however, we will
    want to draw a separating line in [Figure 10-1](ch10.xhtml#ch10fig01) using these
    coefficients. How do we obtain them from the output object `w`?
  prefs: []
  type: TYPE_NORMAL
- en: Recall from [Section 8.9.3](ch08.xhtml#ch08lev9sec3) that multiclass applications
    of the logit model use either a One vs. All (OVA) or an All vs. All (AVA) approach;
    `qeLogit()` uses OVA. It thus runs one logit model for each class, placing the
    `glm()` outputs in the `glmOuts` component of the `qeLogit()` output.
  prefs: []
  type: TYPE_NORMAL
- en: However, the two-class model is a little different. To avoid essentially running
    the same model twice—for the forest cover data, type 1 versus non−type 1—`qeLogit()`
    runs it just once. In other words, we’ll look at `w$glmOuts[[1]]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain the coefficients, we turn to `coef()`, yet another generic function
    like the ones we’ve seen before, such as `print()` and `plot()`. This function
    extracts the estimated coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now recall again that the logistic model routes the linear model into the logistic
    function, *ℓ*(*t*) = 1/(1 + *e*^(−*t*)); the placeholder *t* is set to the linear
    form. The above output gives the estimated probability of type-1 cover for a location
    having feature values of *v*1 and *v*6 as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Say we guess the location to have type-1 cover or not depending on whether the
    estimated probability in [Equation 10.1](ch10.xhtml#ch10equ01) is greater than
    0.5 or not. Setting that equation to 0.5, things look formidable at first but
    become simple when we note the fact that *e*⁰ = 1\. In other words, if the exponent
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: then the right-hand side of [Equation 10.1](ch10.xhtml#ch10equ01) is equal to
    0.5, which is just what we want, a straight line forming the decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: 'So the line in [Equation 10.2](ch10.xhtml#ch10equ02) forms the boundary between
    predicting type-1 or non-type-1 cover. That is the equation of a straight line,
    which is plotted in [Figure 10-2](ch10.xhtml#ch10fig02). We superimposed that
    line onto [Figure 10-1](ch10.xhtml#ch10fig01) by using R’s `abline()` function,
    which plays exactly the role implied by the name—that is, adding a line to an
    existing plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The result is shown in [Figure 10-2](ch10.xhtml#ch10fig02). It happens to be
    almost vertical, which is not surprising since the coefficient of `V6` is so small,
    but no matter. Data points to the right of the line are predicted to be of type-1
    cover, with a non-type-1 prediction for those to the left.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-2: Forest cover data with a logistic boundary line*'
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, quite a few data points are misclassified—that is, plus signs to the
    right of the line and squares to the left. We probably could reduce the number
    of misclassified points by increasing the number of features we use— we had only
    *p* = 2 features here—but there still would be some misclassified points.
  prefs: []
  type: TYPE_NORMAL
- en: 'This motivates the basic goal of SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: We wish to find a line that separates our classes well and then use that line
    to predict new cases in the future by determining which side of the line they
    fall on. Our line typically will not fully separate our classes, so we will have
    some misclassification errors, just as with any ML method. But hopefully a carefully
    chosen line will serve us well.
  prefs: []
  type: TYPE_NORMAL
- en: With *p* = 3 features, the line becomes a plane in three dimensions, difficult
    to visualize, and if we have more than three features, it is impossible to visualize.
    But by always keeping in mind the two-feature case and its geometric interpretation,
    we will have the intuition to use SVM effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'One more point before getting into the details: Why not simply use the above
    logit scheme to create our boundary line? What might be the advantage of using
    an SVM-produced line? The answer is that logit is very confining. It specifies
    a particular form for the regression function, involving the exponential function
    and so on as in [Equation 10.1](ch10.xhtml#ch10equ01), and though this might be
    a good assumption in some applications, it might not be so in others.'
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, aside from the implicit assumption that the best interclass boundary
    is a straight line rather than some other curve (even this condition can be dropped,
    as we will see later), SVM makes no assumptions, so it is more flexible and may
    produce a better fit (just as is the case for k-NN, random forests, and so on,
    which make even fewer assumptions).
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Lines, Planes, and Hyperplanes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s explore that geometric view a bit further.
  prefs: []
  type: TYPE_NORMAL
- en: The derivation above always holds with a logistic model; prediction of *Y* =
    1 versus *Y* = 0 will always boil down to computing a linear function of the features.
    If we have *p* = 2 (that is, two features, such as *v*1 and *v* 6), the boundary
    between predicting *Y* = 1 and *Y* = 0 is a straight line of the form
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: as we saw in [Equation 10.2](ch10.xhtml#ch10equ02). If *p* = 3, say, adding
    the *v* 8 feature, the boundary takes the form of a plane, with the form
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As noted above, this is hard to visualize. As also noted, for *p* > 3, we can’t
    visualize the setting at all. But we are still working with a linear form in the
    features, whose behavior is like that of a line or plane. Since it is planelike,
    we call it a *hyperplane*. For technical accuracy, we’ll use the acronym LPH (line/plane/hyperplane)
    rather than merely saying “line,” but readers should always think in terms of
    lines in the *p* = 2 case to guide their intuition.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Math Notation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Central to any discussion of SVM—including reading the documentation for SVM
    software—is the “dot product” notation. Though the name and math formula may sound
    intimidating, it’s just a way of stating things more succinctly. We first discuss
    how to change much of the SVM notation to vector form and then introduce dot products.
  prefs: []
  type: TYPE_NORMAL
- en: '***10.3.1 Vector Expressions***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As can be seen from [Equations 10.3](ch10.xhtml#ch10equ03) and [10.4](ch10.xhtml#ch10equ04),
    an LPH can be represented by its coefficient vector—for example, (*c*[1], *c*[2],
    *c*[3], *c*[4]) in [Equation 10.4](ch10.xhtml#ch10equ04). It’s customary in SVM
    to write things in terms of equality to 0, so, for instance, we rewrite [Equation
    10.4](ch10.xhtml#ch10equ04) as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'and set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The vector *w* and the number *w*[0] compose our description of the LPH.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we would summarize [Equation 10.2](ch10.xhtml#ch10equ02) by writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***10.3.2 Dot Products***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The underlying theory of SVM makes heavy use of calculus and linear algebra.
    As noted, such mathematics is far beyond the scope of this book. However, it will
    be productive and easy to use some notation from that subject— just notation and
    nothing conceptual other than a little algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal will be to come up with a simple, compact way to determine on which
    side of the boundary line or LPH a new case falls so that we may easily predict
    its class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *dot product* between two vectors *u* = (*u*[1], . . . , *u**[m]*) and
    *v* = (*v*[1], . . . , *v**[m]*) is simply a sum of products:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, let’s take the dot product of *w* in [Equation 10.7](ch10.xhtml#ch10equ07)
    with the vector (1,−4):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is helpful to recast [Equation 10.1](ch10.xhtml#ch10equ01) in our new dot
    product notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note some algebraic properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '*e*⁰ = 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*e**^t* > 1 for *t* > 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*e*^(−*t*) < 1 for *t* > 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Image](../images/unch10equ01.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: Thus, in [Equation 10.10](ch10.xhtml#ch10equ10),
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, faced with a new case to predict, we simply look at the sign—positive and
    negative—of *w* • (*v*1, *v*6) + *w*[0]. If positive, the new case is more likely
    (probability more than 0.5) to have cover type 1, while in the negative case,
    that probability is less than 0.5\. In other words, we predict cover type 1 for
    the new case if *w* • (*v*1, *v*6) + *w*[0] > 0, and otherwise predict non−type
    1.
  prefs: []
  type: TYPE_NORMAL
- en: 'This again is saying that we guess cover type 1 if our new case to be predicted
    falls to the right of the line or non−type 1 if it is on the left. And our SVM
    boundary is the vector *x* that makes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Again, this is just notation, but in math, it is often the case that convenient
    notation helps clarify things. Drawing lines is visualizable for applications
    with *p* = 2 features, but drawing planes is hard to visualize if *p* = 3, and
    if *p* > 3, visualization is impossible. What’s nice about the dot product notation
    is that we know which way to guess the class of a new case by simply noting whether
    *w* • (*v*1, *v*6) + *w*[0] is positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: By the way, SVM theorists also like to code the two classes as *Y* = +1 and
    *Y* = −1 rather than 1 or 0, as is standard in statistics. We’ll continue to use
    the latter coding in general but will turn to the former in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '***10.3.3 SVM as a Parametric Model***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have referred to linear and logistic models as *parametric* in that the regression
    function is modeled as being determined by a finite number of values *β*[0], *β*[1],
    . . . , *β**[p]* . This is in contrast to, for example, k-NN methods, which make
    no assumptions regarding the form of the regression function.
  prefs: []
  type: TYPE_NORMAL
- en: One implication of [Equation 10.13](ch10.xhtml#ch10equ13) is that SVM too is
    a parametric model. Instead of assuming a parametric form for the regression function,
    here we assume a parametric form for the boundary line between the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: '10.4 SVM: The Basic Ideas—Separable Case'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted earlier, the line we drew in [Figure 10-2](ch10.xhtml#ch10fig02) did
    not cleanly separate the two classes. There were plus signs and squares on both
    sides of the line. This is typical, not only for logit-produced lines but also
    for lines created by SVM—our focus here. However, the SVM method is easier to
    explain if we first consider datasets for which the two classes are cleanly separable,
    so most books begin with that case, as we will here.
  prefs: []
  type: TYPE_NORMAL
- en: For clarity, we will continue to focus on the two-class case, as with the type-1/non-type-1
    cover example above. And again, we will continue looking at the case of *p* =
    2 features, where the LPH is a line.
  prefs: []
  type: TYPE_NORMAL
- en: Note that all our references to “the data” here will be in terms of the training
    data. We find a boundary line for the training data and then predict future cases
    according to that line. Similarly, when we speak of separability of “the data,”
    we mean the training data.
  prefs: []
  type: TYPE_NORMAL
- en: '***10.4.1 Example: The Anderson Iris Dataset***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Edgar Anderson’s data on iris flowers, included in R, has been the subject
    of countless examples in books, websites, and so on. There are three classes:
    *setosa*, *versicolor*, and *virginica*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is included in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that what we do in this section will be mainly for motivating the subsequent
    material. For day-to-day SVM computation, you’ll use `qeSVM()`. So, in the example
    here, in which we do some non-SVM analysis for motivational purposes, we will
    omit some of the code and algebra.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, for this example, we’d like data in which the two classes are
    cleanly separated by a straight line. This is the case if we take our two iris
    classes as setosa and nonsetosa, with the features taken to be the `Sepal.Length`
    and `Petal.Width` columns.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first plot the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This produces the graph in [Figure 10-3](ch10.xhtml#ch10fig03).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-3: Setosa and nonsetosa*'
  prefs: []
  type: TYPE_NORMAL
- en: One can easily draw a line between the two classes—in fact, many lines. But,
    which line is optimal?
  prefs: []
  type: TYPE_NORMAL
- en: '***10.4.2 Optimizing Criterion***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Choosing our boundary line amounts to choosing the coefficient vector *w* and
    the *w*[0] term. In other words, the situation is similar to that of the linear
    and generalized linear models, where we choose the estimated coefficients ![Image](../images/unch08equ07.jpg).
    (Actually, *w* and *w*[0] are estimates as well, but to avoid clutter, we do not
    use the hat notation.) Now recall that with a linear model, the way we choose
    our coefficients ![Image](../images/unch08equ07.jpg) is through an optimization
    problem: we minimize a certain sum of squares.'
  prefs: []
  type: TYPE_NORMAL
- en: SVM still minimizes a certain sum, but it uses a different loss function than
    squared error. Detailing it would take us too far into some arcane math with little,
    if any, benefit. Fortunately, the math has an easily grasped geometric version,
    which we will now discuss.
  prefs: []
  type: TYPE_NORMAL
- en: Toward this end, look at [Figure 10-4](ch10.xhtml#ch10fig04). Here we have “roped
    off” our two classes (setosas above and nonsetosas below) into what are called
    *convex hulls*. Again, this is just for illustration purposes; the `qeSVM()` function
    will do the computation for us (and with a different method), and we will not
    compute convex hulls ourselves after this example. We thus omit the code. (One
    can use the function `mvtnorm::chull()`.)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-4: SVM convex hulls*'
  prefs: []
  type: TYPE_NORMAL
- en: It can be shown mathematically that the SVM boundary line is “halfway between”
    the two convex hulls. More precisely stated, one first finds the two points in
    the hulls that are closest to each other. Our boundary line is then the perpendicular
    bisector of the line segment between those two points. We draw that in [Figure
    10-5](ch10.xhtml#ch10fig05), along with two related, dashed lines, which define
    the “margin” of an SVM fit.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-5: SVM margin*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the following, both here and in general:'
  prefs: []
  type: TYPE_NORMAL
- en: The region between the dashed lines is called the *margin*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For separable data, there will be no data points *inside* the margin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The points lying *on* the margin are called *support vectors* (the SV in SVM).
    For this dataset, we have three support vectors, one for the setosas at (2.7,1.0)
    and two for the nonsetosas at (2.3,0.3) and (3.5,0.6).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In terms of *w* and *w*[0], the value of *w*[0] + *w* • *x* will be:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**−**     0 for any point *x* on the boundary'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**−**     +1 for any support vector in the *Y* = +1 class (setosa in this case)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**−**     −1 for any support vector in the *Y* = −1 class (nonsetosa in this
    case)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**−**     > +1 for any nonsupport vector in the *Y* = +1 class'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**−**     < −1 for any nonsupport vector in the *Y* = −1 class'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By the way, a look at [Equation 10.13](ch10.xhtml#ch10equ13) shows that the
    values of *w* and *w*[0] are not unique. Multiplying both by, say, 8.8 would still
    result in 0 on the right-hand side of the equation. So, the convention is to choose
    them such that the value of *w*[0] + *w* • *x* is +1 or −1 at the support points.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To predict *Y* for a new case in which *X* = *x*[new], we guess *Y* to be either
    +1 or −1, depending on whether *w*[0] + *w* • *x*[new] is greater than or less
    than 0\. Note that even though our training data is assumed separable here, new
    data points may fall within the margin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keep in mind fitting an SVM model amounts to choosing *w* and *w*[0] . The
    above scheme can be shown to be optimal. But wait a minute—what does “optimal”
    even mean? The criterion for optimality generally used in SVM is this:'
  prefs: []
  type: TYPE_NORMAL
- en: We choose *w* and *w*[0] in such a way that the margin will have the largest
    possible width.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, SVM seeks to not only separate the data points of the two classes
    but also render the two classes as far apart as possible, relative to the boundary.
    It finds a “buffer zone” between the two classes, maximizing the width of the
    zone. As noted, that buffer zone is called the margin.
  prefs: []
  type: TYPE_NORMAL
- en: The key idea is that a large margin in the training set means that the two classes
    are well separated, which hopefully means that new cases in the future will be
    correctly classified. It turns out that choosing *w* and *w*[0] using convex hulls
    as above (even with *p* > 2) does maximize the margin.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.2.1 Significance of the Support Vectors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The support vectors “support” the fit, in the sense that a change in any of
    them will change the fit; a change in the other data points will not change the
    fit (as long as it stays within the hull). Of course, we can also say that *adding*
    new points won’t change the fit either, as long as the new points are within a
    hull.
  prefs: []
  type: TYPE_NORMAL
- en: An oft-claimed benefit of SVMs is that (at least in the separable case) they
    tend to produce a sparse fit, in this case, meaning not that most of the components
    of *w* are 0s but rather that the essential dimensionality of the fit is low.
    Viewed another way, the claim is that the more support vectors one has, the greater
    the risk of overfitting. Evidence for this assertion is weak, I believe, though
    it may serve as one of several guides to one’s model-fitting process.
  prefs: []
  type: TYPE_NORMAL
- en: But that benefit may be illusory. As noted, the source of the sparsity is the
    dependence of *w* on a few data points (that is, the support vectors). But what
    if some of the support vectors are outliers (not representative of the data as
    a whole) or are even downright erroneous? Many real datasets do have some errors.
    Our fit then depends heavily on some questionable data.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, this high sensitivity of *w* to just a few data points makes for
    high sampling variability; a different sample likely has a different set of support
    vectors. In other words, *w* has a high variance.
  prefs: []
  type: TYPE_NORMAL
- en: '10.5 Major Problem: Lack of Linear Separability'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The margin, actually known as the *hard margin*, is only defined in situations
    in which some line (or LPH) exists that cleanly separates the data points of the
    two classes. As can be seen in our earlier graphs for the forest cover data, in
    most practical situations, no such line exists. Even with the iris data, no line
    separates the versicolor and virginica in columns 1 and 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: See [Figure 10-6](ch10.xhtml#ch10fig06); pluses and triangles are not cleanly
    separated.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two solutions to that problem: (a) using a *kernel* to transform
    the data into linear separability or (b) creating a *soft margin*, in which we
    allow some points to reside within the margin. Typically a combination of these
    two approaches is used. For instance, after doing a kernel transformation, we
    still may—in fact, probably will—find that no cleanly separating LPH exists, and
    thus we will need to resort to also allowing some exceptional points to lie within
    the margin. However, the fewer the exceptions, the better, so it’s best to use
    both approaches in combination rather than just going directly to a soft margin
    solution.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-6: Iris data; three classes*'
  prefs: []
  type: TYPE_NORMAL
- en: '***10.5.1 Applying a “Kernel”***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here we transform the data, say, by applying a polynomial transformation, and
    then find an LPH separator on the new data. The degree of the polynomial is then
    a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.1.1 Motivating Illustration
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To get a feeling as to why kernels can be useful, consider a favorite example
    in ML presentations: “doughnut-shaped” data. Let’s generate some. (The code here
    is rather arcane and can be safely skipped without affecting the sequel.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The data is plotted in [Figure 10-7](ch10.xhtml#ch10fig07). The two classes,
    drawn with pluses and circles, are clearly separable—but by a circle and not a
    straight line.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-7: “Doughnut” data*'
  prefs: []
  type: TYPE_NORMAL
- en: 'But we can fix that by adding a squared term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We took our original dataset and transformed it, replacing each data point by
    its square. That square is our new feature, replacing the old one. In the plot
    of the new data, [Figure 10-8](ch10.xhtml#ch10fig08), the pluses and circles are
    easily separated by a straight line.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-8: “Doughnut” data, transformed*'
  prefs: []
  type: TYPE_NORMAL
- en: 'And that is what SVM users typically do: try to find a transformation of the
    data under which the transformed data will be linearly separable, or at least
    nearly so. Of course, the SVM software does all the work for us; we don’t transform
    the data by hand, as in the above illustration. This is done with a kernel, as
    seen in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.1.2 The Notion of a Kernel
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We need to cover one more point before turning to an example on real data. What
    is a kernel? A *kernel* is a way of transforming our data, again with the goal
    of making our data separable. But it’s a little more specific than that; it’s
    a function *K*(*u*, *v*) whose inputs are two vectors.
  prefs: []
  type: TYPE_NORMAL
- en: This makes sense because we saw in [Section 10.3.2](ch10.xhtml#ch10lev3sec2)
    that dot products play a key role in SVM, and, in fact, even more so in the internal
    computations, which are not covered in this book. Accordingly, many kernels are
    functions of dot products.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example is the *polynomial kernel*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The quantities *d* and *γ* are hyperparameters. In the quadratic case *d* =
    2, we achieve essentially the same effect as in the previous section, where we
    squared the *X* values. Here we square dot products.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also in common usage is the *radial basis function (RBF)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here *γ* is a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, as with so many ML questions, the answer to the question “Which
    kernel is best?” is “It depends.” The type and size of the dataset, the number
    of features, and so on all make for variation in performance between kernels.
  prefs: []
  type: TYPE_NORMAL
- en: '***10.5.2 Soft Margin***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As noted, it is rather uncommon to have linearly separable data. Nonseparability
    is the typical case. How can we handle it, in the sense of tolerating having a
    few points lying within the margin? Let’s consider two approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.2.1 Geometric View
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Instead of computing convex hulls, we could work with *reduced convex hulls*.
    Once again, the mathematical formulation is beyond the scope of this book, but
    the essence of the procedure is this:'
  prefs: []
  type: TYPE_NORMAL
- en: We replace the two original convex hulls with shrunken versions. (Not only will
    they be smaller in size, but their shape will tend to be “rounder,” or less oblong.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The margin will be computed on the basis of the shrunken hulls.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, conduct “business as usual,” even though some of the training set data
    will then fall within the margin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, the amount of shrinkage is, as always, a hyperparameter to be set
    by the user, possibly via cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.2.2 Algebraic View
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The geometric view is intuitive, but the more common approach is via “cost.”
    There is a cost hyperparameter, usually denoted by *C*. Here is how it works.
    Denote data point *i* in our training set by *X**[i]*, the vector of features
    for that data point, and let *Y**[i]* be the class label, either +1 or −1.
  prefs: []
  type: TYPE_NORMAL
- en: In the separable case, recall that, for data points on the margin boundary,
    *w*[0] + *w* • *X[i]* is equal to either +1 or −1\. For points outside the margin,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'depending on whether *Y[i]* is +1 or −1\. But there is a neat trick to state
    this requirement more compactly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the case of a soft margin, we relax that a bit, allowing discrepancies from
    1.0, say,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let *d**[i]* denote the discrepancy for data point *i* so that here *d*[3] =
    0.12 and *d*[8] = −1.71\. Set *d**[i]* = 0 if data point *i* has no discrepancy—that
    is, if *Y**[i]* (*w*[0] + *w* • *X**[i]*) ≥ 1.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if 0 < *d**[i]* < 1, then data point *i* is a margin violation, but
    it still is on the correct side of the decision boundary—that is, it will be correctly
    classified. But if *d**[i]* > 1, the point will be on the other side of the boundary
    and thus misclassified.
  prefs: []
  type: TYPE_NORMAL
- en: We control the total amount of discrepancy by stipulating that
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch10equ20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the hyperparameter *C* is our “discrepancy budget.” Again, the user sets
    *C*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the *d**[i]* are not hyperparameters; they are by-products. The user
    chooses *C*. Each potential value of *w* and *w*[0] then gives rise to the *d**[i]*.
    The smaller the value we set for *C*, the fewer the number of data points within
    the margin—but the narrower the margin. We hope to have a wide margin; the SVM
    algorithm finds the values of *w* and *w*[0] that maximize the width of the margin,
    subject to the constraint ([Equation 10.20](ch10.xhtml#ch10equ20)).
  prefs: []
  type: TYPE_NORMAL
- en: '10.6 Example: Forest Cover Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s give `qeSVM()` a try:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then predict, say, for a new case similar to case 8 in our data, but
    changing the second feature value to 2,888:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We predict non-type-1 cover, though just slightly more likely than type 1.
  prefs: []
  type: TYPE_NORMAL
- en: We’re using the default values here, which, among other things, set `kernel`
    to `radial`. If we wish to use just a soft margin (that is, no kernel transformation),
    we set `kernel` to `linear`. The hyperparameter `gamma`’s default value is 1.0\.
    An optional hyperparameter, `cost`, is the value *C* in our earlier discussion
    of soft margins. By the way, `qeML()` wraps `svm` in the famous `e10171` package.
  prefs: []
  type: TYPE_NORMAL
- en: 10.7 And What About That Kernel Trick?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: No presentation of SVM would be complete without discussing the famous *kernel
    trick*, as it is largely responsible for the success of SVM. As usual, we won’t
    delve into the mathematical details—assuming the reader has no burning desire
    to learn about reproducing kernel Hilbert spaces—but the principle itself has
    major practical implications.
  prefs: []
  type: TYPE_NORMAL
- en: To motivate this, let’s get an idea of how large our dataset can expand to when
    we transform via polynomials *without* using kernels. Our size measure in the
    transformed data will be the number of columns.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the forest cover data again, together with the `polyreg` package,
    the latter being used in our counting data columns. (The `getPoly()` function
    in `polyreg` is used by the polynomial models in our `qe`-series.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The original dataset had only 54 features, but in the new form we have over
    1,500 of them! We would have even more if `getPoly()` did not avoid creating duplicates—for
    example, the square of a dummy variable.
  prefs: []
  type: TYPE_NORMAL
- en: So, the new value of *p* was 1,564, for a dataset with only 500 rows, making
    it impossible to run. With the original data frame of more than 580,000 rows,
    the new size would be 581,012 × 1,564 = 908,702,768 elements. At 8 bytes per element,
    that would mean over 7GB of RAM! And it’s not just space but also time—the code
    would run forever.
  prefs: []
  type: TYPE_NORMAL
- en: And that was just for degree 2\. Imagine degree-3 polynomials and so on! (Degree
    3 turns out to generate 16,897 columns.) So, some kind of shortcut is badly needed.
    Kernel trick to the rescue!
  prefs: []
  type: TYPE_NORMAL
- en: The key point is that by using the kernel in [Equation 10.14](ch10.xhtml#ch10equ14),
    we can avoid having to compute and store all those extra columns. In the forest
    cover data, we can stick to those original 54 features—the vectors *u* and *v*
    in that expression are each 54 elements long—rather than computing and storing
    the 1,564\. We get the same calculations mathematically as if we were to take
    *u* and *v* to be 1,564-element vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '10.8 “Warning: Maximum Number of Iterations Reached”'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with many other ML methods, the computation for SVM is iterative. However,
    unlike those other methods, SVM should not have convergence problems. The search
    space has the *convex* property, which basically says it is bowl-shaped and thus
    easy to find the minimum.
  prefs: []
  type: TYPE_NORMAL
- en: However, that presumes there is something to minimize. As we’ve seen with using
    soft margins or kernels, a line may not exist to cleanly separate the classes.
    For some particular pair of cost value and kernel (and the latter’s hyperparameters),
    it may be that no solution exists. In that case, we will of course have convergence
    issues, and we will have to try other combinations.
  prefs: []
  type: TYPE_NORMAL
- en: 10.9 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This chapter has been a bit more mathematical than the others and perhaps a
    bit more abstract as well. But really, the basic principles are simple:'
  prefs: []
  type: TYPE_NORMAL
- en: SVM is primarily for classification problems. Using OVA or AVA pairing, SVM
    can handle any number of classes, but to keep things simple, let’s assume two
    classes here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we have *p* = 2 features, the basic goal is to find a line separating the
    two classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With *p* = 3, we wish to find a separating plane in three-dimensional space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the cases where *p* > 3, we speak of a separating hyperplane. We cannot
    visualize these, and instead look at dot products with our hyperplane’s *w* vector.
    To classify a new case, we take the dot product of that case’s feature vector,
    add *w*[0], and decide the class based on whether the dot product is greater or
    less than 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We associate with the separating LPH a pair of LPHs that are parallel to the
    original one, thus creating the margin. The optimization criterion for choosing
    *w* is to maximize the width of the margin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Typically the classes overlap, so there is no separating LPH, and we need to
    resort to artificial means to separate the classes. There are two ways to do this,
    usually used in combination:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**−**     We can posit that a separating LPH does exist but is “curvy” rather
    than straight or flat. We transform the data, using a kernel, in an attempt to
    at least approximate some of that curviness.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**−**     We can, to various degrees set by the user, allow data points to
    reside within the margin.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As noted earlier, SVM is a more complicated tool than the ones we’ve seen earlier.
    But it is in wide usage, with many successes to its name, so the extra effort
    in this chapter is quite worthwhile.
  prefs: []
  type: TYPE_NORMAL
