<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="chn"><span epub:type="pagebreak" id="page_181"/><strong>8</strong></h2>&#13;
<h2 class="cht"><strong>ADVANCED CPU DESIGN</strong></h2>&#13;
<div class="image1"><img src="../images/f0181-01.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="chq">The previous chapter presented a minimal CPU design in digital logic. In this chapter, we’ll look at extending that basic design to increase performance. These extensions include using more registers, using stack architectures that improve subroutine capabilities and speed, adding interrupt requests to enable I/O and operating systems, floating-point hardware, and pipelining and out-of-order execution to enable “superscalar” execution of more than one instruction per clock cycle. At this level of complexity we won’t give full details on how to implement the extensions yourself with digital logic, but you’re welcome to try!</p>&#13;
<h3 class="h3" id="lev158">Number of User Registers</h3>&#13;
<p class="noindent">As we’ve discussed, the Baby is an example of an accumulator architecture, meaning it has only a single user-accessible register: the accumulator. All <span epub:type="pagebreak" id="page_182"/>loads go to the accumulator, all stores are taken from it, and when we do two-element arithmetic, such as subtraction, the first element comes from the accumulator and the second directly from RAM, as in a load.</p>&#13;
<p class="indent">Accumulator architectures are relatively simple to implement, and they give rise to simple instruction sets. The load, store, and arithmetic instructions each need only a single operand. For example, to add the numbers stored at addresses 50A3<sub>16</sub> and 463F<sub>16</sub>, we load the content of the first address into the accumulator, then have an “accumulative add” (<code>AADD</code>) instruction that adds the content of the second address into the accumulator:</p>&#13;
<pre>LOAD $50A3&#13;
AADD $463F</pre>&#13;
<p class="noindent">Once these instructions execute, the accumulator contains the result of the addition.</p>&#13;
<p class="indent">On the other hand, accumulator architectures require any data being used to be moved in and out of the CPU every time the data is needed. This can slow the system down, as RAM is typically slower than the CPU. To avoid this slowdown, it can be helpful to provide additional user registers inside the CPU. These extra registers allow more than one datum to be brought into the CPU at a time, so that multiple calculations can be performed without the need for further RAM access. The 8-bit machines of the 1980s typically had a small number of additional user registers, while modern machines might have tens or even hundreds of user registers.</p>&#13;
<p class="indent">Especially in scientific numerical computing, the ideal for assembly programmers is often to load <em>all</em> relevant data into multiple registers at the start of a computation; this allows huge amounts of heavy number crunching within the CPU without requiring any further memory access. In some ways, having more registers makes assembly programming easier, and it allows faster-running programs to be written.</p>&#13;
<p class="indent">There’s always a trade-off around how many user registers a CPU should have, however, as the additional registers come at a cost: they use up a lot of extra silicon, which adds to the costs of design and manufacturing. They’re also bigger and use more energy. Then there’s the increasing complexity of the instruction set, which in turn requires more silicon in the control unit (CU), with similar additional costs. Likewise, the increasing complexity of the instruction set makes life more complicated for the assembly programmer, whether a human or a compiler. Load, store, and arithmetic instructions now need to have additional operands to say which register or registers are to be used, such as:</p>&#13;
<pre>LOAD  X, $50A3    // load into register X from address 50A3&#13;
STORE $463F, Y    // store to address 463F from register Y&#13;
ADD   Z, $463F, Y // add register Y to data from 463F, store in register Z</pre>&#13;
<p class="indent">Some architectures let us have it both ways: they provide a dedicated accumulator register and accumulative arithmetic instructions, as well as a set of regular user registers. This allows assembly programmers to take <span epub:type="pagebreak" id="page_183"/>advantage of possibly simpler and faster instructions on the accumulator while retaining the flexibility to work with the other registers as well.</p>&#13;
<h3 class="h3" id="lev159">Number of Instructions</h3>&#13;
<p class="noindent">The set of available instructions for a CPU, known as its <em>instruction set architecture (ISA)</em>, defines the interface between what the programmer can see and use, and what needs to be implemented by the CPU designer. As with any interface, designing an ISA always involves trade-offs. In this case, there’s a trade-off between making the assembly language programmer’s (or more likely today, the compiler writer’s) life easy and pleasant, versus making the digital logic implementer’s life easy and bug-free. An ISA that contains instructions in the shapes of human thinking is easier to program and to write compilers for, but it may be hard to implement in digital logic. An ISA that reflects what’s easiest to make in digital logic is easy to build and test, but it may not be easy to program or compile to. Then there are also trade-offs between making human assembly programmers happy versus making compiler writers happy.</p>&#13;
<p class="indent">CISC and RISC are the two historically opposing philosophies of architecture. Most systems actually blur elements of both in various ways, but the CISC versus RISC distinction is still useful to structure our thinking and to consider what aspects of practical designs are more “CISCy” or more “RISCy.”</p>&#13;
<p class="indent"><em>CISC</em>, pronounced “sisc,” stands for <em>complex instruction set computing</em>. CISC emphasizes the creation of lots of instructions in ISAs. These can include adding many variations on basic instructions that each act as new instructions. For example, loading, storing, and adding can be done in different ways by different instructions. CISC style might also create new instructions that perform more complex arithmetic than we’ve seen so far, such as the kinds of instructions found in scientific calculators, and even dedicated instructions for particular operations used in signal processing or cryptography.</p>&#13;
<p class="indent">On the opposing side of the debate is <em>reduced instruction set computing</em>, or <em>RISC</em>, which says hardware is nasty, expensive to develop, and difficult to debug, so we should make the processor as lean and mean as we can, then do all of the more error-prone work in software, as software is much nicer and cheaper to create and debug. RISC style is to keep the instruction set as small as possible, then focus on making it run as fast as possible. Single complex instructions found in CISC can be performed in RISC using longer sequences of more basic instructions, which you try to make go as fast together as the single CISC instruction due to their simplicity.</p>&#13;
<h3 class="h3" id="lev160">Duration of Instructions</h3>&#13;
<p class="noindent">In our Baby implementation, our CU is based on a regularly repeating counter cycle that runs independently of any of the events it triggers. Once you start the counter running, its actions follow a fixed sequence <span epub:type="pagebreak" id="page_184"/>that’s completely predestined and blind to what the rest of the CPU is doing. This is known as an <em>open-loop</em> architecture, as there’s no feedback to the counter about the rest of the CPU’s state. Open-loop architectures are relatively easy to design and to debug because of this independence, which is why we used this style for our Baby. The Analytical Engine also uses this style, via its regularly rotating barrel CU.</p>&#13;
<p class="indent">In a <em>closed-loop</em> architecture, by contrast, the timing of triggers isn’t set by a central counter. Instead, each stage of work is responsible for triggering the next stage when it’s ready to do so. For example, rather than triggering the decode stage from a central counter, it can be triggered by a wire that the fetch stage activates when its own work is done.</p>&#13;
<p class="indent">The advantage of the closed-loop approach is that some instructions may be simpler than others, requiring fewer ticks to complete. These can use only the ticks that are necessary, then trigger the next instruction as soon as possible, rather than sitting around doing nothing. For example, in our Baby implementation some instructions (<code>SUB, LDN</code>) need to do work on tick 4, while others (such as <code>JMP</code>) do nothing during that tick.</p>&#13;
<p class="indent">Open-loop style is usually associated with RISC, due to RISC’s emphasis on making <em>all</em> instructions simple and fast. Closed-loop is associated with CISC, as CISC may want to include single instructions that perform a lot of complex work and take many ticks to complete, as well as short, fast ones.</p>&#13;
<h3 class="h3" id="lev161">Different Addressing Modes</h3>&#13;
<p class="noindent">RISC and CISC present different ideas about how much work should be done by a single instruction, and how many different versions of each instruction should be provided. In particular, multiple variant instructions can be created that combine memory access with arithmetic.</p>&#13;
<p class="indent">RISC aims to reduce the size of the instruction set by maintaining a clean separation between memory access instructions and arithmetic instructions. For example, a program to add two numbers together would use two instructions to load each of the two numbers into registers, a third instruction to add them and put the result in another register, then a fourth to store the result in memory, such as:</p>&#13;
<pre>LOAD  R1 $50A3  // load to register R1 the value from address 50A3&#13;
LOAD  R2 $463F  // load to register R2 the value from address 463F&#13;
ADD   R3 R1 R2  // put into register R3 the result of adding R1 and R2&#13;
STORE $A4B5 R3  // store to address A4B5 the result in register R3</pre>&#13;
<p class="noindent">This separation is often taken as the main defining feature of RISC.</p>&#13;
<p class="indent">CISC, in contrast, aims to provide multiple variations of the <code>ADD</code> instruction to make the programmer’s life easier. In addition to <code>ADD</code>, which adds the contents of two registers, we could create another instruction such as <code>ADDM</code> for “add from memory” that would enable the four-line RISC-style addition program to be written with a single instruction:</p>&#13;
<pre>ADDM $A4B5 $50A3 $463F</pre>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_185"/>We can interpret this as “add the values stored in memory addresses 50A3<sub>16</sub> and 463F<sub>16</sub> and store the result in A4B5<sub>16</sub>.” This makes the assembly programmer’s life easier, but makes the architect’s life harder, as they now need to build extra digital logic in the decoder to decode this extra instruction, as well as additional digital logic in the CU to arrange the sequence of load, arithmetic, and store operations, which were coded explicitly in the RISC version. This design is often taken as the defining feature of CISC.</p>&#13;
<p class="indent">A CISC-style ISA might also include further variations, such as an instruction to add the content of one memory location (50A3<sub>16</sub>) to the content of one register (R1) and then store the result in a register (R3). For example:</p>&#13;
<pre>ADDRMR R3 $50A3 R1</pre>&#13;
<p class="noindent">It likewise might include an instruction to add the contents of one memory location (50A3<sub>16</sub>) to the contents of one register (R1) and then store the result in a memory location (A4B5<sub>16</sub>):</p>&#13;
<pre>ADDRMR $A4B5 $50A3 R1</pre>&#13;
<p class="indent">Another common variant is to add instructions that use <em>indirect addressing</em>, meaning the operand of the instruction contains the <em>address of the address</em> to be used. For example:</p>&#13;
<pre>ADDI  $A4B5 $50A3 $463F</pre>&#13;
<p class="noindent">This means “add the value stored at the address 50A3<sub>16</sub> to the value stored at the address 463F<sub>16</sub> and store the result at A4B5<sub>16</sub>.” This is a quite complex instruction that requires the contents of 50A3<sub>16</sub> and 463F<sub>16</sub> to be loaded into registers, but then these values themselves to be interpreted as addresses, and the values at <em>those</em> addresses loaded into registers before performing the addition and store. This sounds like a fairly obscure thing to want to do, but it is very common and useful when compiling high-level languages, such as C, that have <em>pointers</em>. The indirection operations allow for a fast and efficient hardware implementation of pointer commands.</p>&#13;
<p class="indent">Of course, there are also variations on this indirect form of instruction, such as an instruction to perform indirection on just one argument and add the result to a register’s contents:</p>&#13;
<pre>ADDIR  $A4B5 $50A3 R1</pre>&#13;
<p class="noindent">We could dream up further variations, such as using the contents of registers as memory addresses for the indirection, performing more than two layers of indirection, and so on.</p>&#13;
<p class="indent"><em>Offset addressing</em> (aka <em>index addressing</em>) modes are another popular ISA inclusion. The idea here is that assembly programmers often need to make repeated use of many variables that they tend to store close together in memory. Their life can be made easier if they can first use a new instruction to specify the address of this general region of variable storage, such as A7B2<sub>16</sub>, then refer to each individual variable by the <em>difference</em> between its address <span epub:type="pagebreak" id="page_186"/>and this region’s address, such as 0<sub>16</sub>, 1<sub>16</sub>, 2<sub>16</sub>, 3<sub>16</sub>, and so on, to pick each of the variables in order. Here we’d use new offset instructions such as:</p>&#13;
<pre>SETOFFSET $A7B2&#13;
ADDOOO $2 $0 $1</pre>&#13;
<p class="noindent">This adds the contents of addresses A7B2<sub>16</sub> and A7B3<sub>16</sub> and stores the result in A7B4<sub>16</sub>.</p>&#13;
<p class="indent">Offset addressing was especially nice for assembly programmers in the 1980s, working on machines with 8-bit words but with 16-bit address spaces. This was because they would otherwise need to use two words and two registers every time they wanted to represent a 16-bit address. Using offsets, they could instead divide memory into 256 <em>pages</em> of 256 addresses each. They could then choose to work on a single page at a time, using the page’s start address as the offset. Then they would need only a single word to specify an address within the page. For example, A7B4<sub>16</sub> would be considered to be location B4<sub>16</sub> on page A7<sub>16</sub>.</p>&#13;
<p class="indent">To implement offset addressing, an additional register is usually added to the CPU design and used to store the offset. Its value can then be joined onto new operands to form complete addresses when needed by later instructions.</p>&#13;
<p class="indent">It is easy to see how the size of an instruction set can get very large once all these variations come into play. We’ve only considered variants of a single instruction, <code>ADD</code>. To be consistent, an ISA must typically create the same chosen variations for <em>every</em> type of arithmetic instruction, which can lead to hundreds of new instructions in total.</p>&#13;
<h3 class="h3" id="lev162">Subroutines</h3>&#13;
<p class="noindent">The word <em>subroutine</em> is one of many names—subprograms, procedures, functions, methods—for a very similar concept: a piece of code sitting somewhere in memory that will do something when your main program <em>calls</em> it, and will <em>return</em> to the same line in the main program after it was called. This last bit distinguishes a subroutine from a simple jump like the <code>goto</code> statement, which forgets where it was called from. The invention of the subroutine is generally credited to Maurice Wilkes and his team around 1950.</p>&#13;
<p class="indent">In early high-level languages, only one level of subroutine calling was allowed at a time. You had a main program that could call subroutines, but subroutines could not then call other subroutines. More modern high-level languages rely on the ability for subroutines to hierarchically call other subroutines—including themselves, as in <em>recursion</em>—to encapsulate complexity.</p>&#13;
<p class="indent">The term <em>subroutine</em> is generally used at the level of architecture and assembly language. The other names are used in higher-level languages and have historically had somewhat different meanings that have never been formally defined or used consistently across most languages. The following list is an attempt at definitions that capture what the words <em>would</em> mean if they were ever used consistently by language designers:</p>&#13;
<p class="block"><span epub:type="pagebreak" id="page_187"/><strong>Function</strong> This is the easiest to define formally, as it’s a concept used in the most heavily formalized functional programming languages. A function is (ideally) a mathematical object that takes arguments as inputs and returns a value computed only from these inputs and not from anything else beyond them. The function shouldn’t have any other <em>side effects</em>, meaning it shouldn’t affect anything else.</p>&#13;
<p class="block"><strong>Procedures</strong> This is a name for subroutines in some languages that may or may not take inputs and don’t usually return an output, so they act only via side effects. Some older languages allow only one level of calling, meaning procedures can’t call other procedures.</p>&#13;
<p class="block"><strong>Method</strong> This is a concept that comes up in object-oriented programming to name a subroutine associated with an object. A method may both return a value, like a function, and have side effects, like a procedure.</p>&#13;
<p class="indent">All of these names are heavily abused and confused by practical programming languages; for example, it’s common for languages to have “functions” that produce side effects and don’t return values. Functional programming languages are more likely to enforce the mathematical concept, but even some functional programming languages allow side effects.</p>&#13;
<p class="indent">Now let’s look at how to implement subroutines.</p>&#13;
<h4 class="h4" id="lev163"><em>Stackless Architectures</em></h4>&#13;
<p class="noindent">It’s possible to implement subroutines purely in software, without any additional hardware or instructions. You could do this by writing programs with jump instructions and then having some convention to keep track of the return address. However, this is hard work for the programmer and slow for the computer.</p>&#13;
<p class="indent">Early subroutine-capable architectures such as ENIAC added dedicated CPU instructions to call and return, and simple hardware in the CPU to execute them. One approach used a single return address location in hardware. A special dedicated internal register can be easily built into a CPU to store a return address. This allows the main program to call and return from one subroutine at a time; once you’re inside the subroutine, you can’t call and return from another subroutine, because this would overwrite the single return address.</p>&#13;
<p class="indent">To enable subroutines to call one another (including recursively calling themselves), architectures can add a hardware stack.</p>&#13;
<h4 class="h4" id="lev164"><em>Stack Architectures</em></h4>&#13;
<p class="noindent">A <em>stack</em> is a simple data structure with two operations, push and pop. It behaves like a physical stack of papers on your desk. You can <em>push</em> a new document to the top of the stack when it arrives, and you can <em>pop</em> only the top document on the stack by picking it off and removing it. You can’t take documents from lower down in the stack.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_188"/>Using a stack, you can create a full trail of addresses to return through in the case of nested subroutines. Each time a subroutine is called, its return address is pushed to the stack. When the subroutine returns, this address is popped off the stack and used to set the program counter.</p>&#13;
<p class="indent">Keeping track of subroutines this way can be especially useful in cases of recursion. The stack typically grows very large during recursive execution and (hopefully) is reduced as the program completes and data is read and removed from the top of the stack. A <em>stack overflow</em> error is a failure condition where we run out of stack space; if you use a specific chunk of memory to hold your stack, and you run out of space, the program will create a stack overflow error as you try to write outside the stack boundaries. This usually happens because of something that’s gone wrong in an infinite loop of functions calling themselves or each other. In modern computers, stacks are much less resource-expensive to implement than they used to be, so they’re the standard way to store return addresses.</p>&#13;
<p class="indent"><em>Hardware stacks</em> are found in most modern machines from the 8-bit era onward. They use hardware digital logic implementations of the stack concept in their CUs to enable arbitrary subroutine calling with enhanced speed and security. These stack architectures have an extra, dedicated <em>stack pointer register</em>, an internal register that contains a pointer to the top of the stack. The stack itself may be stored in some area of RAM, with access to this part of RAM often restricted at the hardware level. For example, a stack architecture might have dedicated digital logic to test all load and store instructions from the user, to make sure they aren’t trying to access the stack’s portion of RAM. This prevents malicious programmers from interfering with the stack.</p>&#13;
<p class="indent">Some stack architectures hide their internal workings from the user, and provide only new call- and return-style instructions in the instruction set. When executed, the call instructions will activate digital logic that automatically pushes the program counter to the stack, increments the stack pointer, and jumps to the subroutine. Likewise, the return instructions will pop the program counter, decrement the stack pointer, and jump back to the calling function.</p>&#13;
<p class="indent">Other stack architectures allow full user access to the contents of the stack in addition to or instead of call and return. For example, some designs provide instructions such as <code>PHA</code>, for PusH Accumulator, and <code>POPA</code>, for POP Accumulator. The former pushes whatever is in the accumulator onto the stack and increments the stack pointer, and the latter pops the stack to the accumulator and decrements the stack pointer. This design provides a method to pass arguments to subroutines by storing them on the stack along with the return addresses.</p>&#13;
<div class="sidebar">&#13;
<p class="stitle"><span epub:type="pagebreak" id="page_189"/><strong>CALLING CONVENTIONS</strong></p>&#13;
<p class="stext">Whatever stack or stackless architecture is used to enable subroutines, it will rely on the programmer to maintain a consistent <em>calling convention</em> so the different parts of the program can correctly understand one another as they pass and receive arguments. This is especially important when subroutines are written by a different author from the caller code, as is the case when a general-use library of subroutines is provided. A calling convention includes the following:</p>&#13;
<ul class="bullets">&#13;
<li class="tm">Where arguments, return values, and return addresses are placed: in registers, on the call stack, a mix of both, or in other memory structures</li>&#13;
<li class="tm">The order and format in which arguments are passed</li>&#13;
<li class="tm">How a return value is delivered from the callee back to the caller: in a register, on the stack, or elsewhere in RAM</li>&#13;
<li class="tm">How the task of setting up for and cleaning up after a function call is divided between the caller and the callee</li>&#13;
<li class="tm">Whether and how metadata describing the arguments is passed</li>&#13;
</ul>&#13;
<p class="stext">Calling conventions aren’t part of CPU architecture. Rather, they’re social agreements between programmers. A given architecture can often be used with any one of many different possible calling conventions. In some cases, CPU architects will suggest a convention to try to discourage fragmentation between their users. In other (or sometimes, the same!) cases, programmers create their own conventions and standards wars break out when they need to interface their programs.</p>&#13;
<p class="stext">Calling conventions also define additional features for compatibility between modern high-level languages and compilers. For example, compiled executable code from two languages, such as C and C++, can link to and call one another’s subroutines as long as they obey the same calling convention.</p>&#13;
</div>&#13;
<h3 class="h3" id="lev165">Floating-Point Units</h3>&#13;
<p class="noindent">We saw in <a href="ch02.xhtml">Chapter 2</a> how floating-point numbers are represented with a sign, an exponent, and a mantissa. <em>Floating-point registers</em> are specialized user registers designed to store floating-point data representations for use in floating-point computations.</p>&#13;
<p class="indent">Performing arithmetic on these representations is more complicated than the arithmetic logic unit (ALU) operations on integers seen so far. To multiply two floating points, for example, we need to multiply their mantissas, add their exponents, and multiply their signs. To add two floating-point numbers, we need to shift one of them by the difference in their exponents, then add them, and possibly shift again and update the exponent. Dividing can be error-prone when a large number is divided by a small one, and can also result in special cases defined to yield infinity or NaN (not a number) representations.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_190"/>This can all be done by combining simple ALU-style operations together and using new components of digital logic. The resulting structure is called a <em>floating-point unit (FPU)</em>. FPUs are complex pieces of digital logic and expensive to design; they also take up lots of silicon and are prone to bugs. In 1994, Intel made an error implementing the FPU in their Pentium chip that cost them half a billion dollars in recalls and reputational damage.</p>&#13;
<p class="indent">FPUs appeared in the 1980s, not inside the CPU but as optional additional chips. For example, the Intel 8086 CPU could be paired with an optional extra FPU chip, the lesser-known 8087. Nowadays, FPUs have all moved onto the CPU and behave similarly to their ALU counterparts.</p>&#13;
<p class="indent">If you want to see what the dedicated registers and instructions on a modern FPU look like, they take up most of a full book, volume 3 of the amd64 reference manuals, which you’ll meet in <a href="ch13.xhtml">Chapter 13</a>.</p>&#13;
<h3 class="h3" id="lev166">Pipelining</h3>&#13;
<p class="noindent">Everything we’ve looked at so far involves writing a program in assembly language, compiling that into machine code, and executing the machine code from top to bottom, with some branching and looping. Fundamentally, the instructions are brought in one at a time, and each individual instruction is executed before the next is brought in. Most modern CPUs don’t work like this. Instead, they work on parts of multiple instructions in parallel. We’ll look at many more forms of parallelism in <a href="ch15.xhtml">Chapter 15</a>, but those that operate at this CPU level are known as <em>instruction-level parallelism</em>, and we’ll study them here.</p>&#13;
<p class="indent"><em>Pipelining</em> is a form of instruction-level parallelism that appeared in the 32-bit era. A pipeline is like a production line, where there are multiple workers doing tasks at the same time, as in Henry Ford’s 1913 car factory, shown in <a href="ch08.xhtml#ch08fig1">Figure 8-1</a>.</p>&#13;
<div class="image"><img id="ch08fig1" src="../images/f0190-01.jpg" alt="Image" width="631" height="456"/></div>&#13;
<p class="figcap"><em>Figure 8-1: Ford’s 1913 production line</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_191"/>Ford assigned one specialized task to each worker and positioned them at fixed locations along a conveyor belt. Car parts moved along the conveyor, with each worker in turn doing their work on each car part.</p>&#13;
<p class="indent">Now replace Ford’s car parts with instructions from your machine code program, and imagine them being run down this production line. Instead of human workers, you have parts of the CPU performing tasks such as fetch, decode, and execute. Suppose you’ve written 20 lines of code, and assume there’s no jumping or branching. In the CPU designs we’ve seen so far, a single instruction is placed on the production line and passes by the fetch, decode, and execute workers in turn. Once it gets to the end of the production line, the next instruction is placed at the start of the line. Most of the workers thus end up standing around doing nothing for most of the time when it isn’t their turn to work.</p>&#13;
<p class="indent">We could extract much higher efficiency from our workers by keeping the whole conveyor belt full of instructions the whole time, rather than waiting for one to finish before starting the next one. One worker could be executing one instruction at the same time that a second worker is decoding the next instruction and a third worker is fetching the instruction after that.</p>&#13;
<p class="indent">There are many different ways of dividing up the work of a CPU into such stages, depending on the architecture type. The classic split considers fetch, decode, and execute stages. Our LogiSim Baby used a cycle of five ticks. Modern CPUs can have many more subdivisions—there are around 37 stages in a modern Intel processor’s pipeline!</p>&#13;
<p class="indent">At the digital logic level, pipelines can be implemented by having the CU trigger multiple components at the same time rather than one at a time. It’s common to show pipelines as diagrams like <a href="ch08.xhtml#ch08fig2">Figure 8-2</a>.</p>&#13;
<div class="image"><img id="ch08fig2" src="../images/f0191-01.jpg" alt="Image" width="618" height="488"/></div>&#13;
<p class="figcap"><em>Figure 8-2: A diagram showing how instructions are handled in a basic pipeline</em></p>&#13;
<p class="indent">In <a href="ch08.xhtml#ch08fig2">Figure 8-2</a>, clock cycles go across from left to right, and four instructions (represented by squares) are shown passing through the pipeline. This <span epub:type="pagebreak" id="page_192"/>is a four-stage pipeline, so it can work on up to four instructions at once (as shown on clock cycle 4), each at a different stage of processing.</p>&#13;
<p class="indent">Pipelining is simpler and more efficient for open-loop, RISC architectures, in which all instructions have equal durations so they can progress evenly along the pipeline. It can quickly become complex and less efficient for closed-loop, CISC architectures, where different durations for different instructions must be taken into account, and where some parts of the pipeline get left empty as shorter instructions execute alongside longer ones.</p>&#13;
<h4 class="h4" id="lev167"><em>Hazards</em></h4>&#13;
<p class="noindent">There are several well-known scenarios, called <em>hazards</em>, in which problems occur in pipeline execution. Let’s take a look at the main types of hazards. Then we’ll consider how to fix them.</p>&#13;
<h4 class="h4a"><strong>Branching Hazards</strong></h4>&#13;
<p class="noindent">A <em>branching hazard</em> occurs when, somewhere down the pipeline, an <code>if</code> statement is found and the other, earlier stages were working to complete one outcome of the branch, but you need to go to the other outcome instead. When a conditional branch is reached, you don’t know which condition will be followed until the branch gets executed.</p>&#13;
<h4 class="h4a"><strong>Data Hazards</strong></h4>&#13;
<p class="noindentb">If two of the workers are trying to hit on the same memory location—to fetch and output to, for example—bad things are going to happen. We can split these <em>data hazards</em> into three main categories:</p>&#13;
<p class="block2"><strong>Read after write</strong> This is where you have two instructions, the first trying to write to memory and the second trying to read from the same address. The logic of the program is supposed to be that the value that gets read should be equal to what has just been written. But when pipelining is in play, it may be possible for the RAM access of the read to occur before RAM has been changed by the write.</p>&#13;
<p class="block2"><strong>Write after read</strong> This is the other way around: here two instructions are supposed to first read the old RAM value and then update it with a write. But when they are interleaved by pipelining, it may be possible for the part of the write instruction that actually changes the RAM value to occur before the part of the read that accesses it.</p>&#13;
<p class="block2"><strong>Write after write</strong> This is where two write instructions interfere with one another when trying to write to the same address. The program logic is supposed to be that the first one writes, then the second one, leaving the address containing the second one. But again, pipelining may interleave stages of their executions, in some cases performing the intended first write after the second.</p>&#13;
<h4 class="h4a"><span epub:type="pagebreak" id="page_193"/><strong>Structural Hazards</strong></h4>&#13;
<p class="noindent">The third type of hazard, a <em>structural hazard</em>, is where multiple stages are fighting for resources at the same time. In the production line example, the factory might contain one physical calculator on a shelf behind the workers, for their shared use. There may come a time when two of the workers both want to use this calculator at once. For example, one might need to check if something equals zero, while the other needs to execute an addition operation. The digital CPU analog of this would be two regions of digital logic computing two pipeline states both needing to access the ALU or memory at the same time.</p>&#13;
<h4 class="h4" id="lev168"><em>Hazard Correction</em></h4>&#13;
<p class="noindent">Pipelining tends to work well for all types of signal processing—including audio, video, and radio processing—because there isn’t much branching to handle. The same kind of data is expected to flow through the pipeline in real time and always be processed in the same way. For example, the codecs in your digital TV or laptop used to decode and display movies will reliably chug through frame after frame of incoming video and audio, doing the same operations to decode and display each one, in the same order. They don’t usually have to look at the content of the signals and change their behavior in response to this content.</p>&#13;
<p class="indent">Hazards become more problematic when you’re doing computations that are continually checking the results and changing their flow based on this state. As soon as you have programs with branches—and to a lesser extent jumps and subroutines—you have to think about how to address hazards. Let’s go through a few general strategies.</p>&#13;
<h4 class="h4a"><strong>Programming to Avoid Hazards</strong></h4>&#13;
<p class="noindent">Skilled assembly programmers can write assembly code to avoid many hazards, if they understand the architecture. This often involves considering groups of neighboring instructions and thinking about how they could affect one another in the pipeline, and changing the order of some instructions to make them further apart and less likely to affect one another.</p>&#13;
<p class="indent">Nowadays, most programming is done in compiled languages, so some of the tricks that end-user programmers once employed have moved into compilers. A good compiler can inspect the assembly code it’s produced and look for places likely to lead to hazards. It can tweak this code as a human programmer would to reduce the likelihood of the hazard. For example, the order of instructions that don’t affect each other can be swapped to make two accesses of the same data occur further apart in the execution. Of course, there’s still a human behind these sorts of optimizations: the authors of the compiler, who have likely taken a strong interest in hazards and how to avoid them.</p>&#13;
<p class="indent">Some ISAs provide a <em>null operation (NOP)</em> as an extra instruction that means “do nothing.” NOP instructions still go through the pipeline, taking up time slots, so a human programmer or compiler can insert them between <span epub:type="pagebreak" id="page_194"/>hazard-causing instructions to spread them out and avert the hazard. This typically requires less intelligence than reordering instructions, but will slow down execution as the NOPs are processed through the pipeline.</p>&#13;
<h4 class="h4a"><strong>Stalling</strong></h4>&#13;
<p class="noindent"><em>Stalling</em> (sometimes known as <em>bubbling</em>) simply means putting the result of the pipeline on hold to allow some stage to complete its work. For example, if there’s a structural hazard and two stages want to use the ALU at the same time, we just let one of them use it and tell everyone else to do nothing until the ALU is free. In the production line analogy, this is like the system used in factories where if a worker gets into trouble they can hit a button to stop the conveyor belt to give them time to fix the problem.</p>&#13;
<p class="indent">To allow for stalling, additional digital logic can be added to the CPU to detect the upcoming potential occurrence of hazards—for example, temporally ceasing to trigger stages for the next instructions as soon as a jump or branch is seen to be coming in. This is a heavyweight solution that has a large time cost if it’s used frequently. As with NOPs, the whole pipelining system is effectively disabled around hazards, so if we have to do this all the time then we might as well just use a non-pipelined CPU. But stalling is relatively simple and cheap in terms of silicon and design time to implement.</p>&#13;
<h4 class="h4a"><strong>Redoing Work</strong></h4>&#13;
<p class="noindent"><em>Redoing work</em> means that as a potential hazard instruction is being processed, we allow the following instruction to begin its cycle as normal, with the hope that the hazard won’t actually occur. If we later complete execution of the potential hazard instruction and find that a hazard <em>has</em> actually occurred, then we throw away the work that’s been done on the next instruction and do it again.</p>&#13;
<p class="indent">For example, when a branch instruction arrives, we assume that it won’t be taken, and begin fetching and decoding the following instructions at the same time as testing the branch’s condition. If we then find the branch is not to be taken, the work already done on the next instructions is useful and is kept, progressing to execution. But if we find that the branch is to be taken, we discard the work on the subsequent instructions and start fetching and decoding different ones from the branch target address.</p>&#13;
<p class="indent">This strategy is more efficient than stalling, where a performance hit is taken at every <em>potential</em> hazard, even the ones that don’t end up actually occurring. Say there are 100 branches in your program, only half of which are taken; stalling would delay all 100 of them, whereas redoing work delays only 50.</p>&#13;
<h4 class="h4a"><strong>Eager Execution</strong></h4>&#13;
<p class="noindent"><em>Eager execution</em> means executing <em>both</em> possible branches at the same time for a short period, and then killing the one not taken later on, once we figure out which it should be. For example, a typical use of eager execution occurs in instruction sequences such as (using Baby assembly):</p>&#13;
<span epub:type="pagebreak" id="page_195"/>&#13;
<pre>1: SKN 3&#13;
2: LDN 10&#13;
3: LDN 11</pre>&#13;
<p class="indent">This sequence first asks if a condition is true; depending on the result, it loads from either address 10 or address 11. In eager execution, we begin fetching, decoding, and executing both lines 2 and 3 while we are still executing line 1. Only later, once the result of the line 1 comparison is known, do we decide which of line 2 or 3 is wanted. We keep the work that’s been done on the desired line and throw away the work done on the unwanted one.</p>&#13;
<p class="indent">Implementing eager execution requires doubling up our physical digital logic to perform twice as much computation in parallel during the period of uncertainty. This could involve having two physical copies of the ALU, registers, and execution logic. This can be a good use for the additional silicon that the transistor density form of Moore’s law currently provides, while not allowing faster clocks.</p>&#13;
<h4 class="h4a"><strong>Branch Prediction</strong></h4>&#13;
<p class="noindent"><em>Branch prediction</em> is where we try to predict whether a branch will be taken <em>before</em> actually executing it. Such prediction may initially sound impossible (surely the very meaning of execution is to find out what the branch will do), but we can often make use of prior knowledge to give us at least a better-than-random guess.</p>&#13;
<p class="indent">For branch hazards, the redoing work approach can be viewed as always predicting that branches won’t be taken. It begins to fetch and decode the instruction from the next numerical address while working on execution of the branch instruction. Branch prediction generalizes redoing work by trying to make a more accurate prediction about whether a branch will be taken. Fetch and decode can then begin for whichever branch is predicted, and work redone only in cases where the prediction turns out to be incorrect.</p>&#13;
<p class="indent">Branch prediction remains an active area of research, with several strategies under investigation. One is to assume that all branches <em>are</em> taken—essentially the opposite assumption of the redoing work approach. If users wrote only programs whose branches originated from <code>if</code> statements, then these branches would have a 50/50 chance of being taken, in the absence of any other information. However, many or most of the branches that appear in practical machine code originate from loops rather than <code>if</code> statements, and the usual purpose of a loop is to repeat many, rather than zero or one, times. Therefore, when branches originate from loops, they usually <em>are</em> taken because the user wants to loop a few times.</p>&#13;
<p class="indent">Large-scale statistical studies of real-world machine code found in the wild have confirmed this: their estimates range from 50 to 90 percent of branches being taken.</p>&#13;
<p class="indent">In some cases, another strategy is for the human programmer or compiler to provide hints about which branches will be taken. This could include <span epub:type="pagebreak" id="page_196"/>human programmers adding special comments to their assembly or high-level code, or compilers using code analysis to create their own predictions and annotations. For some compilation tasks, this is easy to do—for example, if a user program says “repeat 100 times,” then we can make a good prediction. Predictions are harder—in some cases uncomputable—to make in the case of <code>while</code> loops.</p>&#13;
<p class="indent">A third, state-of-the-art approach is to use dynamic runtime branch prediction, which involves building statistical or machine learning classifiers into CPU digital logic and using them to make on-the-fly predictions. As with all prediction systems, this requires choosing some features of programs that may be informative about temporally and spatially nearby branch behaviors.</p>&#13;
<p class="indent">Simpler cases include keeping a log of observed frequencies of branch-taking at each branch instruction during execution of the user program, and using these frequencies as probabilistic prediction for which way the branches will go if the same instructions are executed again.</p>&#13;
<p class="indent">More advanced cases now include linear regression and even neural network classifiers built from digital logic and pretrained on large collections of machine code gathered from real-world programs in the wild. These may be trained on all kinds of features of the machine code, such as values of opcodes and operands in many lines before and after a branch instruction.</p>&#13;
<h4 class="h4a"><strong>Operand Forwarding</strong></h4>&#13;
<p class="noindent"><em>Operand forwarding</em> is a technique for avoiding data hazards by adding digital logic to directly route the result of an instruction to become an input to a next or nearby instruction. For example, consider this program:</p>&#13;
<pre>1: ADD R3 R1 R2&#13;
2: ADD R4 R3 R1</pre>&#13;
<p class="indent">This computes R3 = R1 + R2, then R4 = R3 + R1, where all the operands are registers. Here, instruction 2 requires the result of instruction 1 to be placed in R3 before instruction 2 can execute. This will result in a data hazard for most pipelines. However, the value destined for R3 may in fact be available on the output lines of the ALU during execution of instruction 1, but before it appears in its destination register. By connecting a physical wire directly from the ALU output to where the data is required (such as an ALU input), we can bypass the wait for the result to be deposited and reread, and instead start using it immediately.</p>&#13;
<h3 class="h3" id="lev169">Out-of-Order Execution</h3>&#13;
<p class="noindent"><em>Out-of-order execution (OOOE)</em> is a more advanced form of instruction-level parallelism than pipelining. It involves actually swapping around the order of instructions as they come into the CPU, so they’re executed in a different order than they appear in the program. OOOE architectures were first enabled in theory in 1966 by Tomasulo’s algorithm, and appeared commercially in the 1990s.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_197"/>The key to OOOE is recognizing that instructions in serial programs can often be swapped without changing their results. For example, if we have some variables being assigned values, we can make those assignments at any time before the variables are next used without affecting the result; we’ll end up with the same state overall. This gives us freedom to swap instructions around to prevent pipeline hazards from occurring and to maximize efficiency. Whether we have single or multiple hardware copies of CPU substructures available, we can choose orderings that try to make the best use of these resources, keeping them all as busy as possible.</p>&#13;
<p class="indent">To see how OOOE works, consider the following program:</p>&#13;
<pre>1: DIV R1 R4 R7&#13;
2: ADD R8 R1 R2&#13;
3: ADD R5 R5 R9&#13;
4: SUB R6 R6 R3&#13;
5: ADD R4 R5 R6&#13;
6: MUL R7 R8 R4</pre>&#13;
<p class="indent">There are six instructions here. Instruction 1, for example, sets register R1’s contents to the result of dividing the contents of register R4 by the contents of register R7. We’ll assume we have simple machines available for division and multiplication, but that they’re slower than those for addition and subtraction. The left of <a href="ch08.xhtml#ch08fig3">Figure 8-3</a> graphs the dependencies between the instructions.</p>&#13;
<div class="image"><img id="ch08fig3" src="../images/f0197-01.jpg" alt="Image" width="650" height="295"/></div>&#13;
<p class="figcap"><em>Figure 8-3: A dataflow graph (left) and a schedule (right) for the sample program, allowing for OOOE</em></p>&#13;
<p class="indent">As an example of a dependency, consider that instruction 2 can’t begin execution until instruction 1 is complete, because instruction 1 writes to register 1, which is needed as an input to instruction 2. The dependency graph shows that we don’t need to execute the instructions exactly in their original order. As long as any instruction is executed after all of its parents in the graph, the result will be the same. We can reorder the sequence of instructions and/or execute them in parallel as long as the arrows in the graph are respected.</p>&#13;
<p class="indent">The right of <a href="ch08.xhtml#ch08fig3">Figure 8-3</a> shows one possible ordering for execution of the instructions. Here, instructions 1, 3, and 4 are executed in parallel to start. Instruction 5 comes after 3 and 4, but can still occur in parallel with 1 (which takes longer, being a more complex division operation). Instruction 2 can <span epub:type="pagebreak" id="page_198"/>occur once 1 has finished, and 6 (another longer instruction, as a multiply) must come last, since it needs the results of both 2 and 5. Depending on how many ALUs we have available, we could execute this or similar schedules much faster than a single series or even a pipeline of the original program.</p>&#13;
<p class="indent">OOOE is usually performed by digital logic in the CPU, in real time during program execution. Usually only a short window—such as 10 or 20 instructions—around the current instruction in the program is considered for reordering.</p>&#13;
<p class="notes"><strong><span class="nt">NOTE</span></strong></p>&#13;
<p class="noindent"><em>If you extend the idea of OOOE into reordering and parallelizing entire programs, you’ll arrive at GPU dataflows, which you’ll meet in <a href="ch15.xhtml">Chapter 15</a>.</em></p>&#13;
<h3 class="h3" id="lev170">Hyperthreading</h3>&#13;
<p class="noindent">In a basic CPU, only the fetching hardware is active during the fetch stage, only the decoder is active during the decode stage, and only the ALU or CU is active during the execute stage. Pipelining and OOOE are two ways to make better use of the CPU hardware resources that are otherwise idle during the fetch-decode-execute cycle, by having them work on parts of multiple instructions at the same time.</p>&#13;
<p class="indent"><em>Hyperthreading</em> is another way to make use of CPU resources when they would otherwise be sitting idle during the cycle. Rather than work on consecutive instructions from one program, we put them all together to form a second virtual CPU core that operates on a separate set of instructions. Each component of this virtual core runs out of phase with its use in the main CPU core, when it would otherwise be idle. By collecting all the components together, all out of phase, we create a whole extra CPU, keeping all the silicon in constant use at all times.</p>&#13;
<p class="indent">Hyperthreading was conceived in the 1970s and became widespread in commercial CPUs during the 2000s. It effectively doubles the number of apparent cores over the number of physical cores in a device, which is why you often see your computer report having twice the number of cores that were advertised on the hardware you bought.</p>&#13;
<p class="indent">Hyperthreading has the advantage over pipelining that you no longer have to worry about hazards because the two cores can operate completely independently of one other. On the other hand, it doesn’t increase the speed of any one program. It also requires additional digital logic to read, store, and write the states of the two virtual CPUs at the right times, and duplication of some hardware components, so that one doesn’t affect the other. In practice, pipelining and hyperthreading may be used together, especially when pipelines are broken down into many smaller stages. Figuring out how to balance them is advanced work that’s beyond the scope of this book.</p>&#13;
<h3 class="h3" id="lev171"><span epub:type="pagebreak" id="page_199"/>Summary</h3>&#13;
<p class="noindent">Beyond a minimal CPU such as the Baby, architects are faced with many decisions about what trade-offs to make between speed, usability, silicon size, and energy costs. Adding more features to a CPU, such as more registers, ALU and floating-point simple machines, stacks, and different addressing modes, can make life easier and faster for the user programmer or compiler, but at the cost of silicon and energy. Likewise, adding more instructions can make life easier for some programmers and compilers who may ask for them, but harder for others who have to keep up with the extra complexity. Giving all instructions the same fixed duration makes life easier for pipeline and OOOE designers and CPU debuggers, but may be less efficient if a few complex instructions that require long execution times are in use.</p>&#13;
<p class="indent">RISC is a style that generally aims to keep instructions and instruction sets small and simple, while making use of extra silicon to speed up the instructions via more registers, pipelines, and OOOE. CISC is the opposite style: it prefers to make use of extra silicon to add more complex instructions and create larger instruction sets. The two styles tend to fit different applications, as we’ll see in <a href="ch13.xhtml">Chapters 13</a> and <a href="ch14.xhtml">14</a>.</p>&#13;
<p class="indent">Even with the most advanced CPU design, your computing experience would be very limited without input, output, and memory, and the next two chapters will look at how to add these to your computer.</p>&#13;
<h3 class="h3" id="lev172">Exercises</h3>&#13;
<h4 class="h4a"><strong>Confusing a Pipeline</strong></h4>&#13;
<p class="noindent">Design the simplest assembly program needed to confuse a basic pipeline, making it run as slowly as a non-pipelined system. Try to extend this program to further confuse each of the hazard-handling strategies as much as possible. How likely are such programs to occur in practice, and what might be done to avoid them?</p>&#13;
<h4 class="h4a"><strong>Challenging</strong></h4>&#13;
<p class="noindent">Try to build an FPU in LogiSim, based on the floating-point data representation seen in <a href="ch02.xhtml">Chapter 2</a>. Consider how previously seen simple machines can be combined in each of the arithmetic operations of addition, subtraction, multiplication, and division. For example, when two floats are multiplied, their exponents are added.</p>&#13;
<h4 class="h4a"><strong>More Challenging</strong></h4>&#13;
<p class="noindent">Try to extend the previous LogiSim Baby design with some minimal pipelining. For example, you could try to increment the program counter and start performing the next fetch while the current instruction is executing. The hard part is dealing with branching hazards. You may want to assume initially that the branch will be taken, then add logic to clear things out and start again if this turns out to be incorrect.</p>&#13;
<h3 class="h3" id="lev173"><span epub:type="pagebreak" id="page_200"/>Further Reading</h3>&#13;
<ul class="bullet">&#13;
<li class="tm">For the origin of the controversially named “von Neumann” architecture, see John von Neumann, “First Draft of a Report on the EDVAC,” June 30, 1945, <em><a href="https://history-computer.com/Library/edvac.pdf">https://history-computer.com/Library/edvac.pdf</a></em>.</li>&#13;
<li class="tm">For the invention of subroutines, see Maurice Wilkes, David Wheeler, and Stanley Gill, <em>The Preparation of Programs for an Electronic Digital Computer: With Special Reference to the EDSAC and the Use of a Library of Subroutines</em> (Cambridge, MA: Addison-Wesley, 1951).</li>&#13;
<li class="tm"><em>Human Resource Machine</em>, <em>Shenzen I/O</em>, and <em>TIS-100</em> are educational video games that present CPU-like environments with different instruction sets and goals for you to explore.</li>&#13;
</ul>&#13;
</div>
</div>
</body></html>