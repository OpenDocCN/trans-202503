- en: '**11'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**11'
- en: COMPUTER SCIENCE ALGORITHMS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学算法**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/common.jpg)'
- en: While everything we’ve looked at so far can be called a “randomized algorithm,”
    in computer science the phrase refers to two broad categories of algorithms—the
    subject of this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们到目前为止所研究的所有内容都可以称为“随机化算法”，但在计算机科学中，这个术语指的是两大类算法——本章的主题。
- en: A *randomized algorithm* employs randomness as part of its operation. The algorithm
    succeeds in accomplishing its goal, either by producing the correct answer quickly,
    but sometimes not, or by running rapidly with some probability of returning a
    false or nonoptimal result.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机化算法*在其操作过程中使用随机性。该算法通过快速产生正确答案，但有时不产生，或者通过快速运行并以某种概率返回错误或非最优结果来完成其目标。'
- en: We’ll begin by defining the two broad categories of randomized algorithms with
    examples. Next, we’ll learn about estimating the number of animals in a population.
    Following that, we’ll learn how to demonstrate that a number is a prime to any
    desired level of certainty while avoiding the brute force approach of searching
    for all possible divisors. We’ll end with randomized Quicksort, the textbook example
    of a randomized algorithm.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先定义这两大类随机化算法并举例说明。接下来，我们将学习如何估算一个种群中的动物数量。之后，我们将学习如何证明一个数字是素数，且达到任何所需的可信度，同时避免通过穷举所有可能的除数来进行暴力搜索。最后，我们将介绍随机化快速排序，这是一种经典的随机化算法示例。
- en: '**Las Vegas and Monte Carlo**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**拉斯维加斯与蒙特卡洛**'
- en: Las Vegas and Monte Carlo are locations famously associated with gambling, that
    is, with games of chance dependent on randomness and probability. However, when
    computer scientists refer to Las Vegas and Monte Carlo, they are (usually) referring
    to the two main types of randomized algorithms.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 拉斯维加斯和蒙特卡洛是与赌博密切相关的地方，也就是依赖于随机性和概率的机会游戏。然而，当计算机科学家提到拉斯维加斯和蒙特卡洛时，他们通常是指两种主要类型的随机化算法。
- en: A *Las Vegas algorithm* always returns a correct result for its input in a random
    amount of time; that is, how long the algorithm takes to execute isn’t deterministic,
    but the output is correct.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*拉斯维加斯算法*总是能在随机的时间内返回正确的结果；也就是说，算法执行所需的时间不是确定性的，但输出是正确的。'
- en: On the other hand, a *Monte Carlo algorithm* offers no assurance that its output
    is correct, but the runtime is deterministic. There is a nonzero probability that
    the output isn’t correct, but for a practical Monte Carlo algorithm, this probability
    is small. Most algorithms we’ve encountered, including swarm intelligence and
    evolutionary algorithms, are Monte Carlo algorithms. Las Vegas algorithms can
    be transformed into Monte Carlo algorithms by allowing them to exit before locating
    the correct output.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*蒙特卡洛算法*不能保证输出是正确的，但其运行时间是确定性的。输出不正确的概率是非零的，但对于一个实际的蒙特卡洛算法，这个概率是很小的。我们遇到的大多数算法，包括群体智能和进化算法，都是蒙特卡洛算法。通过允许算法在找到正确输出之前退出，拉斯维加斯算法可以转变为蒙特卡洛算法。
- en: The first example we’ll investigate is a sorting algorithm that is, at our discretion,
    a Las Vegas or Monte Carlo algorithm. The second is a Monte Carlo algorithm for
    verifying matrix multiplication.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究的第一个例子是一个排序算法，它是一个 Las Vegas 或 Monte Carlo 算法，具体取决于我们的选择。第二个是一个用于验证矩阵乘法的
    Monte Carlo 算法。
- en: '***Permutation Sort***'
  id: totrans-11
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***排列排序***'
- en: 'A *permutation* is a possible arrangement of a set of items. If there are *n*
    items in the set, there are *n*! = *n*(*n* – 1)(*n* – 2) . . . 1 possible permutations.
    For example, if the set consists of three things, say *A* = {1, 2, 3}, then there
    are six possible permutations:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*排列*是对一组项目的可能排列。如果集合中有 *n* 个项目，则有 *n*! = *n*(*n* - 1)(*n* - 2) …… 1 种可能的排列。例如，如果集合包含三个元素，假设
    *A* = {1, 2, 3}，那么就有六种可能的排列：'
- en: '{1, 2, 3}, {1, 3, 2}, {2, 1, 3}, {2, 3, 1}, {3, 1, 2}, {3, 2, 1}'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '{1, 2, 3}, {1, 3, 2}, {2, 1, 3}, {2, 3, 1}, {3, 1, 2}, {3, 2, 1}'
- en: Notice that one permutation sorts the items from smallest to largest. Therefore,
    if given a vector of unsorted numbers, we might use a sort algorithm that generates
    permutations until finding the one that sorts the items. While we can implement
    this deterministically, we can also use random permutations with the hope that
    we might stumble across the correct ordering before testing too many candidate
    permutations. The *permutation sort* algorithm (also known as *bogosort* or *stupid
    sort*) implements this idea.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一种排列将项目从小到大排序。因此，如果给定一个无序的数字向量，我们可能会使用一种排序算法，生成排列直到找到能排序项目的那个。虽然我们可以实现确定性排序，但我们也可以使用随机排列，希望在测试过多候选排列之前能偶然找到正确的顺序。*permutation
    sort*算法（也叫*bogosort*或*stupid sort*）实现了这个想法。
- en: 'Run the file *permutation_sort.py* with no arguments:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 运行文件*permutation_sort.py*时不带任何参数：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The code generates a random vector of integers in [0, 99] and sorts it by trying
    random permutations up to `limit`. To score each permutation, the code returns
    the fraction of pairs of elements that are out of order, where *a* > *b* for *a*
    at index *i* and *b* at index *i* + 1\. If the array is sorted, the score is zero.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成一个整数随机向量，范围在[0, 99]之间，并通过尝试随机排列最多达到`limit`来对其进行排序。为了对每个排列进行评分，代码返回一对对元素乱序的比例，其中*a*
    > *b*表示*a*位于索引*i*，*b*位于索引*i* + 1。如果数组已经排序，得分为零。
- en: If `limit` is zero, the algorithm runs until it finds the correct permutation,
    which depends on the number of possible permutations. As the number of permutations
    increases (*n*!), the runtime rapidly increases if we insist on trying until we
    succeed. In this way, a `limit` of 0 turns *permutation_sort.py* into a Las Vegas
    algorithm.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`limit`为零，算法将一直运行直到找到正确的排列，这取决于可能的排列数量。随着排列数的增加（*n*!），如果我们坚持尝试直到成功，运行时间会迅速增加。通过这种方式，`limit`为0会将*permutation_sort.py*变成一个拉斯维加斯算法。
- en: 'For example, to run *permutation_sort.py* as a Las Vegas algorithm, use:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要将*permutation_sort.py*作为拉斯维加斯算法运行，使用：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The code found the proper order of elements after testing 268 of the possible
    6! = 720 permutations. Changing the randomness source from `minstd` to `pcg64`
    sorts in 59 iterations while `mt19937` uses 20\. We set the limit to 0 to make
    the code run until success, but the number of permutations tested was often far
    less than the maximum.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 代码在测试了268个可能的6! = 720个排列后找到了正确的元素顺序。将随机源从`minstd`更改为`pcg64`时，排序需要59次迭代，而`mt19937`则使用了20次。我们将限制设置为0，使代码一直运行直到成功，但测试的排列数通常远小于最大值。
- en: 'If we switch to Monte Carlo:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们切换到蒙特卡罗算法：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'we get a partially sorted array with a score > 0\. Because of the fixed randomness
    source and seed, we know we need 268 iterations to sort the array:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个部分排序的数组，得分大于0。由于固定的随机源和种子，我们知道需要268次迭代才能排序数组：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Listing 11-1](ch011.xhtml#ch011list01) shows the main loop in *permutation_sort.py*.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 11-1](ch011.xhtml#ch011list01)显示了*permutation_sort.py*中的主循环。'
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Listing 11-1: The main loop in* permutation_sort.py'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 11-1：*permutation_sort.py中的主循环'
- en: We create the vector (`v`), along with an initial `score`. The main loop, `while`,
    runs until the score is zero or the `limit` is exceeded. If Las Vegas, we set
    `limit` to a huge number to play the odds that we’ll find the true ordering long
    before that many trials.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建向量（`v`），并初始化`score`。主循环`while`一直运行，直到得分为零或`limit`超出。如果是拉斯维加斯算法，我们将`limit`设置为一个非常大的数字，以增加在尝试这么多次之前找到真实排序的概率。
- en: The body of the `while` loop creates a random ordering of `v` and calculates
    the score. Whenever it finds a lower score, the code reorders `v` to return at
    least a partially ordered vector should the limit be reached; however, this is
    not strictly necessary.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`while`循环的主体创建了`v`的随机排序并计算得分。每当找到更低的得分时，代码会重新排序`v`，以便在达到限制时至少返回一个部分排序的向量；然而，这并不是严格必要的。'
- en: The remainder of the file displays the results or calculates the score ([Listing
    11-2](ch011.xhtml#ch011list02)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的其余部分显示结果或计算得分（[列表 11-2](ch011.xhtml#ch011list02)）。
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Listing 11-2: Scoring a permutation*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 11-2：对排列进行评分*'
- en: Let’s plot the mean number of permutations tested as a function of the number
    of items to sort, *permutation_sort_plot.py*, which plots the mean and standard
    error for 10 calls to *permutation_sort.py* for *n* in [2, 10]. The result is
    [Figure 11-1](ch011.xhtml#ch011fig01).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制平均排列次数与要排序项目数量的关系，*permutation_sort_plot.py*，它为[n]范围在[2, 10]之间的10次调用*permutation_sort.py*绘制了均值和标准误差。结果见[图
    11-1](ch011.xhtml#ch011fig01)。
- en: '![Image](../images/11fig01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/11fig01.jpg)'
- en: '*Figure 11-1: The mean number of permutations (in millions) tested as a function
    of the number of items*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-1：作为项目数量的函数，测试的排列平均数量（以百万计）*'
- en: '[Figure 11-1](ch011.xhtml#ch011fig01) illustrates *combinatorial explosion*,
    which is the rapid growth in a problem’s runtime or resource use as a function
    of the size of its input. Permutation sort works decently when sorting lists of
    up to nine items; any more and the complexity explodes.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-1](ch011.xhtml#ch011fig01)展示了*组合爆炸*，即问题的运行时间或资源使用量随着输入大小的增加而迅速增长的现象。当排序最多九个项目的列表时，排列排序工作还算不错；但如果超过九个，复杂度就会爆炸。'
- en: 'We also see this effect in *permutation_sort_plot.py*’s output:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*permutation_sort_plot.py*的输出中也看到了这种效果：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The output shows, as a function of *n*, the mean (± SE) time in seconds to sort
    a vector of that size. After seven elements, runtimes quickly increase.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了作为*n*的函数，排序该大小向量所需的平均时间（±标准误差）。七个元素后，运行时间迅速增加。
- en: Combinatorial explosion is the bane of many otherwise useful algorithms, often
    reaching a point where many lifetimes of the universe are insufficient to find
    the correct output.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 组合爆炸是许多原本有用的算法的诅咒，它通常会达到一个点，宇宙的多个生命周期都不足以找到正确的输出。
- en: 'Permutation sort is tied closely to the factorial, which is why we’re getting
    these results:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 排列排序与阶乘密切相关，这就是我们得到这些结果的原因：
- en: '| 2! = 2 | 5! = 120 | 8! = 40,320 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 2! = 2 | 5! = 120 | 8! = 40,320 |'
- en: '| 3! = 6 | 6! = 720 | 9! = 362,880 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 3! = 6 | 6! = 720 | 9! = 362,880 |'
- en: '| 4! = 24 | 7! = 5,040 | 10! = 3,628,800 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 4! = 24 | 7! = 5,040 | 10! = 3,628,800 |'
- en: The factorial grows at a tremendous rate. If we want to sort 20 items, we have
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 阶乘以惊人的速度增长。如果我们要对20个项目进行排序，我们需要
- en: 20! = 2,432,902,008,176,640,000
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 20! = 2,432,902,008,176,640,000
- en: permutations to check. At 1 millisecond per permutation, we’d need over 77 million
    years of computing time to check them all. This doesn’t mean permutation sort
    couldn’t, by pure chance, sort 20 items in less than a second, but the probability
    is exceedingly low. This is the paradox of randomized algorithms.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 需要检查的排列数。如果每个排列需要 1 毫秒的时间，我们就需要超过 7700 万年的计算时间才能检查完所有排列。这并不意味着排列排序不能偶然在不到一秒的时间内排序
    20 个项目，但这种概率极低。这就是随机算法的悖论。
- en: '***Matrix Multiplication***'
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***矩阵乘法***'
- en: I have three matrices, ***A***, ***B***, and ***C***. We’ll use bold, uppercase
    letters to represent matrices. Does ***AB*** = ***C***? How can we know?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我有三个矩阵，***A***、***B***和***C***。我们将用粗体大写字母表示矩阵。***AB*** = ***C***吗？我们怎么知道？
- en: '**Introducing Matrix Multiplication**'
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**引入矩阵乘法**'
- en: 'First, let’s ensure we’re on the same page regarding matrix multiplication.
    A *matrix* is a 2D array of numbers, so the matrices here might be:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们确保我们对矩阵乘法有相同的理解。*矩阵*是一个二维数字数组，因此这里的矩阵可能是：
- en: '![Image](../images/f0299-03.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0299-03.jpg)'
- en: These are 2×2 matrices with two rows and two columns. If the number of rows
    equals the number of columns, we’re working with *square matrices*. However, the
    number of rows and columns need not match; for example, we might have a matrix
    of 3×5 or 1,000×13\. The latter case is typical in machine learning, where rows
    represent observations and columns represent features associated with those observations.
    An *n*×1 matrix is a *column vector*, while a 1×*n* matrix is a *row vector*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是 2×2 矩阵，有两行和两列。如果行数等于列数，我们就正在处理*方阵*。然而，行数和列数不必相等；例如，我们可能有一个 3×5 或 1,000×13
    的矩阵。后者在机器学习中很常见，行代表观察值，列代表与这些观察值相关的特征。*n*×1 矩阵是*列向量*，而 1×*n* 矩阵是*行向量*。
- en: Asking whether ***AB*** = ***C*** implies that we know how to find ***AB***.
    In NumPy, in order to multiply two 2D arrays, we multiply each corresponding element
    ([Listing 11-3](ch011.xhtml#ch011list03)).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，是否***AB*** = ***C***意味着我们知道如何找到***AB***。在 NumPy 中，为了乘以两个二维数组，我们将每个对应的元素相乘（[清单
    11-3](ch011.xhtml#ch011list03)）。
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Listing 11-3: Multiplying element-wise in NumPy*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 11-3：在 NumPy 中按元素相乘*'
- en: Unfortunately, multiplying matrices is more involved. We begin by checking that
    the number of columns of the first matrix equals the number of rows of the second.
    If not, then we can’t multiply the matrices. Therefore, to multiply an *n*×*m*
    matrix by a *u*×*v* matrix requires *m* = *u*. If this is true, we can multiply
    to produce an *n*×*v* result. The square matrices in this section automatically
    satisfy this requirement.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，矩阵乘法更为复杂。我们首先要检查第一个矩阵的列数是否等于第二个矩阵的行数。如果不相等，则不能进行矩阵乘法。因此，要将一个*n*×*m*矩阵乘以一个*u*×*v*矩阵，需要*m*
    = *u*。如果这一条件成立，我们就可以进行乘法运算，得到一个*n*×*v*的结果。本节中的方阵自动满足这一要求。
- en: 'The matrix multiplication process requires multiplying each column of the second
    matrix by the rows of the first matrix, where the elements of the column multiply
    the corresponding elements of the row. We then sum these products to produce a
    single output value. For example, in symbols, multiplying two 2×2 matrices returns
    a new 2×2 matrix:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法过程需要将第二个矩阵的每一列与第一个矩阵的行相乘，其中列的元素与行的对应元素相乘。然后我们将这些乘积求和以产生单一的输出值。例如，用符号表示，乘法两个
    2×2 矩阵返回一个新的 2×2 矩阵：
- en: '![Image](../images/f0300-01.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0300-01.jpg)'
- en: We’re indexing matrices from 0, as we would NumPy arrays. However, many math
    books index from 1 so that the first element of the first row of matrix ***A***
    is denoted *a*[11], not *a*[00].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 0 开始索引矩阵，就像我们处理 NumPy 数组一样。然而，许多数学书籍是从 1 开始索引的，所以矩阵 ***A*** 第一行的第一个元素记作
    *a*[11]，而不是 *a*[00]。
- en: 'Mathematically, if ***A*** is an *n* × *m* matrix and ***B*** is an *m* × *p*
    matrix, then the elements of ***C*** = ***AB***, an *n* × *p* matrix, are:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，如果 ***A*** 是一个 *n* × *m* 矩阵，***B*** 是一个 *m* × *p* 矩阵，那么 ***C*** = ***AB***
    的元素是一个 *n* × *p* 矩阵：
- en: '![Image](../images/f0300-02.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0300-02.jpg)'
- en: 'Remember that matrix multiplication does not commute; in general, ***AB***
    ≠ ***BA***. The sum over *k* in [Equation 11.1](ch011.xhtml#ch11equ01) illustrates
    why: the single index accesses by row for ***A*** and by column for ***B*** so
    that swapping the order of ***A*** and ***B*** means different elements of the
    matrices are mixed.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 记住矩阵乘法不满足交换律；一般来说，***AB*** ≠ ***BA***。在[方程 11.1](ch011.xhtml#ch11equ01)中的求和展示了这一点：单一的索引按行访问
    ***A***，按列访问 ***B***，因此交换 ***A*** 和 ***B*** 的顺序会导致矩阵中的不同元素被混合。
- en: 'The sum in [Equation 11.1](ch011.xhtml#ch11equ01) uses index variable *k* with
    two more implied sums over all values of *i* and *j* to fill in the output matrix,
    ***C***. These observations point toward an implementation: matrix multiplication
    becomes a triple loop indexing elements of 2D arrays.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 11.1](ch011.xhtml#ch11equ01)中的求和使用了索引变量 *k*，并且对 *i* 和 *j* 的所有值进行了两次隐含的求和，以填充输出矩阵
    ***C***。这些观察结果指向一个实现：矩阵乘法变成了一个三重循环，索引 2D 数组的元素。'
- en: '[Listing 11-4](ch011.xhtml#ch011list04) translates the loops of [Equation 11.1](ch011.xhtml#ch11equ01)
    into code.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 11-4](ch011.xhtml#ch011list04)将[方程 11.1](ch011.xhtml#ch11equ01)中的循环转换为代码。'
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Listing 11-4: Naive matrix multiplication*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 11-4：朴素矩阵乘法*'
- en: We’ll use this implementation even though NumPy supports matrix multiplication
    natively in several ways, for example, via the `@` operator. To understand why,
    we’ll learn how computer scientists measure algorithm performance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个实现，即使 NumPy 原生支持通过几种方式进行矩阵乘法，例如通过 `@` 运算符。为了理解原因，我们将学习计算机科学家如何衡量算法的性能。
- en: '**Introducing Big O Notation**'
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**引入大 O 符号**'
- en: Computer scientists characterize the resource use of an algorithm by comparing
    the algorithm’s performance as input size increases to a similar function that
    captures how the algorithm’s resource use changes as the input grows. Here, resource
    refers to either memory or time. For example, an ![Image](../images/c0301-01.jpg)(*n*)
    algorithm linearly increases the memory used as the size of the input, *n*, increases.
    A linear function can be written as *y* = *mx* + *b* for input *x*, but in big
    O notation, we ignore multiplicative and constant factors so that *y* = *x* is
    functionally the same as *x* gets very large.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学家通过将算法的性能与输入大小增加时类似的函数进行比较，来描述算法的资源使用情况，该函数能够捕捉算法的资源使用如何随着输入的增长而变化。这里的资源指的是内存或时间。例如，一个![Image](../images/c0301-01.jpg)的(*n*)算法会随着输入大小*n*的增加而线性增加所使用的内存。线性函数可以写作
    *y* = *mx* + *b*，其中 *x* 是输入，但在大 O 符号中，我们忽略乘法和常数因素，所以 *y* = *x* 在 *x* 非常大时与 *x*
    是功能上相同的。
- en: The matrix multiplication code in [Listing 11-4](ch011.xhtml#ch011list04) contains
    a triply nested loop. If the input matrices are square (*n*×*n*), then `I` = `J`
    = `K` = *n*. Each loop runs *n* times, making the innermost loop run *n* times
    for every increment of the next outer loop, which must run *n* times to increment
    the outermost loop. Therefore, the total number of operations required to multiply
    two *n*×*n* matrices scales as *n*³. The time needed to create the output matrix,
    `C`, and evaluate the first two lines of the function doesn’t alter the essential
    character of the function—namely that it takes *n*³ passes through the three loops.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 11-4](ch011.xhtml#ch011list04)中的矩阵乘法代码包含一个三重嵌套的循环。如果输入矩阵是方阵（*n*×*n*），则`I`
    = `J` = `K` = *n*。每个循环执行 *n* 次，使得最内层的循环每次外层循环递增时执行 *n* 次，外层循环也必须执行 *n* 次来递增最外层循环。因此，乘法两个
    *n*×*n* 矩阵所需的操作总数按 *n*³ 规模增长。创建输出矩阵 `C` 和计算函数前两行所需的时间不会改变该函数的本质特征——即它需要通过三个循环进行
    *n*³ 次迭代。'
- en: A computer scientist would therefore write that [Listing 11-4](ch011.xhtml#ch011list04)
    is an ![Image](../images/c0301-01.jpg)(*n*³) algorithm and an “*n* cubed” implementation.
    In general, we want algorithms that scale as ![Image](../images/c0301-01.jpg)(*n*)
    or better. As *n* increases, the work required by the algorithm scales linearly
    or, better still, sublinearly like ![Image](../images/c0301-01.jpg)(log *n*) or
    ![Image](../images/c0301-01.jpg)(*n* log *n*). In other words, a plot of the work
    as a function of *n* is a straight line. Ideally, we want ![Image](../images/c0301-01.jpg)(1)
    algorithms that run in constant time regardless of the size of their input, but
    that isn’t always possible. An algorithm that runs in ![Image](../images/c0301-01.jpg)(*n*²)
    is often tolerable, but ![Image](../images/c0301-01.jpg)(*n*³) is suitable only
    for small values of *n*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，计算机科学家会写下 [清单 11-4](ch011.xhtml#ch011list04) 是一个 ![Image](../images/c0301-01.jpg)(*n*³)
    算法，并且是一个“*n* 立方”的实现。通常，我们希望算法的增长率为 ![Image](../images/c0301-01.jpg)(*n*) 或更优。随着
    *n* 的增加，算法所需的工作量按线性增长，或者更好的是，按子线性增长，如 ![Image](../images/c0301-01.jpg)(log *n*)
    或 ![Image](../images/c0301-01.jpg)(*n* log *n*)。换句话说，工作量与 *n* 的关系是一个直线图。理想情况下，我们希望
    ![Image](../images/c0301-01.jpg)(1) 算法，它们在常数时间内运行，无论输入的大小如何，但这并非总是可能的。一个在 ![Image](../images/c0301-01.jpg)(*n*²)
    时间内运行的算法通常是可以容忍的，但 ![Image](../images/c0301-01.jpg)(*n*³) 只适用于小的 *n* 值。
- en: Note that ![Image](../images/c0301-01.jpg)(*n*), ![Image](../images/c0301-01.jpg)(*n*²),
    and ![Image](../images/c0301-01.jpg)(*n*³) are all powers of *n*. Such algorithms
    are known as *polynomial time* algorithms. We never want algorithms that run in
    *superpolynomial time*, with a runtime (or resource use) such that no polynomial
    tracks it. For example, an algorithm running in ![Image](../images/c0301-01.jpg)(2*^n*)
    time is an *exponential time* algorithm, and its resource use grows dramatically
    with the size of the input at a rate no polynomial can match. Worse still is the
    permutation sort we experimented with previously; it’s an ![Image](../images/c0301-01.jpg)(*n*!)
    algorithm that runs in *factorial time*. To see how factorial time is worse than
    exponential time, make a plot comparing 2*^n* and *n*! for *n* = [1, 8].
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，![Image](../images/c0301-01.jpg)(*n*)、![Image](../images/c0301-01.jpg)(*n*²)
    和 ![Image](../images/c0301-01.jpg)(*n*³) 都是 *n* 的幂次。这类算法被称为 *多项式时间* 算法。我们永远不希望有在
    *超多项式时间* 内运行的算法，这类算法的运行时间（或资源使用）无法通过任何多项式来追踪。例如，一个运行在 ![Image](../images/c0301-01.jpg)(2*^n*)
    时间内的算法就是一个 *指数时间* 算法，它的资源使用随输入规模的增大而剧烈增长，增长速度远超任何多项式。更糟糕的是我们之前实验过的排列排序；它是一个 ![Image](../images/c0301-01.jpg)(*n*!)
    算法，运行在 *阶乘时间* 内。为了理解阶乘时间比指数时间更糟糕，可以绘制一个图，比较 *2*^*n* 和 *n*! 在 *n* = [1, 8] 时的增长情况。
- en: Matrix multiplication as in [Listing 11-4](ch011.xhtml#ch011list04) is ![Image](../images/c0301-01.jpg)(*n*³).
    Our goal is to quickly check whether ***AB*** = ***C*** when given three matrices.
    We first multiply ***A*** and ***B*** and then check whether each element of the
    result matches the corresponding element in ***C***. The multiplication is ![Image](../images/c0301-01.jpg)(*n*³)
    and the check runs in ![Image](../images/c0301-01.jpg)(*n*²) time because we have
    to examine each element. As the cube grows much faster than the square, the overall
    naive algorithm runs in essentially ![Image](../images/c0301-01.jpg)(*n*³) time.
    Let’s see if we can do better.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如[Listing 11-4](ch011.xhtml#ch011list04)中的矩阵乘法是![Image](../images/c0301-01.jpg)(*n*³)。我们的目标是快速检查在给定三个矩阵的情况下，是否***AB***
    = ***C***。我们首先将***A***和***B***相乘，然后检查结果的每个元素是否与***C***中对应的元素匹配。乘法是![Image](../images/c0301-01.jpg)(*n*³)，检查的时间是![Image](../images/c0301-01.jpg)(*n*²)，因为我们需要检查每个元素。由于立方体增长速度远快于平方，整体的朴素算法运行时间基本为![Image](../images/c0301-01.jpg)(*n*³)。让我们看看是否可以做得更好。
- en: '**Introducing Freivalds’ Algorithm**'
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**引入Freivalds算法**'
- en: In 1977, Latvian computer scientist Rūsiņš Freivalds invented a randomized algorithm
    that correctly answers the question of whether ***AB*** = ***C*** with high probability,
    yet runs in ![Image](../images/c0301-01.jpg)(*n*²) time.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 1977年，拉脱维亚计算机科学家Rūsiņš Freivalds发明了一种随机算法，能够以高概率正确回答***AB*** = ***C***的问题，并且运行时间为![Image](../images/c0301-01.jpg)(*n*²)。
- en: For the following, we’ll assume that ***A***, ***B***, and ***C*** are *n*×*n*
    matrices. The algorithm works for non-square matrices, but this restriction makes
    things easier to follow.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下内容，我们假设***A***、***B***和***C***是*n*×*n*的矩阵。该算法也适用于非方阵，但此限制使得理解过程更为简单。
- en: 'The algorithm itself is straightforward:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 算法本身是直接的：
- en: Pick a random *n*-element vector, ***r*** = {0, 1}*^n*, that is, a random vector
    of 0s and 1s.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个随机的*n*维向量，***r*** = {0, 1}*^n*，即一个由0和1组成的随机向量。
- en: 'Calculate ***D*** = ***A***(***Br***) – ***Cr***. (Note: the parentheses matter.)'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算***D*** = ***A***(***Br***) – ***Cr***。（注意：括号很重要。）
- en: If all elements of ***D*** are zero, claim “yes,” ***AB*** = ***C***; otherwise,
    claim “no.”
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果***D***的所有元素都是零，则声明“是”，***AB*** = ***C***；否则，声明“否”。
- en: At first glance, Freivalds’ algorithm doesn’t look like it will help. However,
    recall how matrix multiplication works. The expression ***Br*** is multiplying
    an *n*×*n* matrix by an *n*×1 vector, which returns an *n*×1 vector. The next
    multiplication by ***A*** returns another *n*×1 vector. Likewise, ***Cr*** is
    also an *n*×1 vector. At no point is a full *n*×*n* matrix multiplication happening.
    As *n* grows, the savings in the number of calculations grows all the faster.
    Freivalds’ algorithm runs in ![Image](../images/c0301-01.jpg)(*n*²) time—a considerable
    improvement over the ![Image](../images/c0301-01.jpg)(*n*³) runtime of the naive
    algorithm.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，Freivalds算法似乎不会有帮助。然而，回想一下矩阵乘法是如何工作的。表达式***Br***是将一个*n*×*n*的矩阵与一个*n*×1的向量相乘，返回一个*n*×1的向量。接下来的***A***乘法返回另一个*n*×1的向量。同样，***Cr***也是一个*n*×1的向量。此时并没有进行完整的*n*×*n*矩阵乘法。随着*n*的增大，计算节省的速度会更快。Freivalds的算法运行在![Image](../images/c0301-01.jpg)(*n*²)时间内，比起朴素算法的![Image](../images/c0301-01.jpg)(*n*³)运行时间，这是一个相当大的改进。
- en: 'Multiplying ***B*** by ***r*** is the equivalent of selecting a random subset
    of ***B***’s columns and adding their value across the rows. For example:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 将***B***乘以***r***相当于选择***B***的列的随机子集，并将它们的值在行中相加。例如：
- en: '![Image](../images/f0302-01.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0302-01.jpg)'
- en: The algorithm is betting that examining random elements of the three matrices
    will, if they are equal, result in ***D*** being a vector of all zeros more often
    than ***D*** being all zeros by chance. An analysis of the probabilities involved,
    which we won’t cover, demonstrates that the probability of ***D*** being all zeros
    given ***AB*** ≠ ***C*** is less than or equal to 1/2.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的假设是，检查三者矩阵的随机元素时，如果它们相等，结果将更频繁地使***D***成为全零向量，而不是***D***偶然变为全零。对所涉及概率的分析（我们不会在此讨论）表明，给定***AB***
    ≠ ***C***的情况下，***D***为全零的概率小于或等于1/2。
- en: If the probability of one calculation involving a randomly selected ***r***
    returning the wrong answer is at most 1/2, then two random vectors (if we run
    the algorithm twice) have a probability of returning the wrong answer of at most
    (1/2)(1/2) = 1/4\. Here the wrong answer is an output of “yes” when in fact ***AB***
    ≠ ***C***.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个计算涉及随机选择的***r***返回错误答案的概率至多为1/2，那么两次运行算法的随机向量（如果我们运行算法两次）返回错误答案的概率至多为(1/2)(1/2)
    = 1/4。这里的错误答案是输出“是”，但实际上***AB*** ≠ ***C***。
- en: Each application of the algorithm is independent of any previous application.
    For independent events, like the flip of a fair coin, probabilities multiply,
    so *k* runs of Freivalds’ algorithm implies that the probability of a false “yes”
    result is 1/2*^k* or less. This means we can be as confident of the result as
    we like by running the algorithm multiple times.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 每次应用算法都是独立于任何先前应用的。对于独立事件，如公平硬币的抛掷，概率是相乘的，因此*k*次运行Freivalds算法意味着错误“是”结果的概率为1/2*^k*或更小。这意味着通过多次运行算法，我们可以提高对结果的信心。
- en: The algorithm will always return “yes” when ***AB*** = ***C***, meaning it is
    *one sided*—an error in the output happens only if ***AB*** ≠ ***C***. In a *two-sided*
    error, the algorithm could be wrong in either case, with some probability.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当***AB*** = ***C***时，算法总是返回“是”，这意味着它是*单边的*——只有在***AB*** ≠ ***C***时，输出才会出错。在*双边的*错误中，算法可能在任何情况下都出错，具有一定的概率。
- en: '**Testing Freivalds’ Algorithm**'
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**测试Freivalds算法**'
- en: Let’s give the algorithm a try using *freivalds.py*, which generates 1,000 random
    triplets of *n*×*n* matrices, with *n* given on the command line. In all cases,
    ***AB*** ≠ ***C***, so we report failures as a fraction of 1,000.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用*freivalds.py*算法，它生成1,000个随机的*n*×*n*矩阵三元组，*n*由命令行提供。在所有情况下，***AB*** ≠
    ***C***，因此我们将失败的次数作为1,000次中的一个比例报告。
- en: 'Run *freivalds.py* like so:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式运行*freivalds.py*：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first argument is the dimensionality of the matrices. The second decides
    whether to use the naive algorithm that calculates ***AB*** – ***C*** or Freivalds’.
    The third is the number of times to repeat the test with random ***r*** vectors.
    We’ll use this option shortly to track the error rate. As usual, the other arguments
    enable any randomness source and a seed to repeat the same sequence of random
    matrices.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是矩阵的维度。第二个参数决定是否使用计算***AB*** - ***C***的朴素算法或Freivalds算法。第三个参数是重复测试的次数，使用随机的***r***向量。我们稍后会使用这个选项来跟踪错误率。像往常一样，其他参数启用任何随机源和一个种子，以重复相同的随机矩阵序列。
- en: 'For example:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: tells us that testing 1,000 3×3 matrices using Freivalds’ algorithm once each
    took some 0.09 seconds and failed 13.2 percent of the 1,000 tests.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉我们，使用Freivalds算法对1,000个3×3矩阵进行单次测试，每次测试大约需要0.09秒，并且在1,000次测试中失败了13.2%。
- en: 'To use the naive algorithm, change the 0 to 1 on the command line:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用朴素算法，只需在命令行上将0改为1：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As expected, there are no failures because the complete calculation always catches
    when ***AB*** ≠ ***C***. While the naive algorithm seems to run faster than Freivalds’,
    this is an illusion; as *n* increases, the two diverge.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，没有失败，因为完整的计算总是能捕捉到当***AB*** ≠ ***C***时的情况。虽然朴素算法似乎比Freivalds算法运行得更快，但这只是一个错觉；随着*n*的增加，两者的差距会逐渐增大。
- en: 'Failing 13 percent of the time when checking 3×3 matrices isn’t too inspiring.
    Let’s repeat the test, but check twice instead of once:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查3×3矩阵时失败13%的时候并不是很令人振奋。让我们重新测试，但检查两次而不是一次：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we fail only 1.6 percent of the tests at the expense of nearly doubling
    the running time. Let’s try four tests instead of two:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们仅失败了1.6%的测试，但几乎加倍了运行时间。让我们尝试进行四次测试而不是两次：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With four tests, Freivalds’ algorithm is 1,000 out of 1,000.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 经过四次测试，Freivalds算法成功率为1,000/1,000。
- en: Freivalds’ algorithm is probabilistic. The likelihood of error decreases quickly
    as matrix size increases. To see this effect, alter the matrix size while fixing
    the repetitions at 1\. By the time *n* = 11, the error is generally below 0.1
    percent.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Freivalds算法是概率性的。随着矩阵大小的增大，错误的可能性迅速减少。为了看到这个效果，在固定重复次数为1的情况下改变矩阵大小。当*n* = 11时，错误率通常低于0.1%。
- en: It makes sense that the error rate goes down with matrix size. The probability
    that a random selection of values sum by accident to two equal values (***A***(***Br***)
    and ***Cr***) should decrease as the number of values summed increases.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 错误率随着矩阵大小的增大而下降是有道理的。随机选择的数值之和恰好等于两个相等的值（***A***(***Br***) 和 ***Cr***）的概率应随着求和的数值增加而减少。
- en: Let’s explore running time as a function of *n*. Run *freivalds_plots.py* to
    produce the graphs in [Figure 11-2](ch011.xhtml#ch011fig02).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一下运行时间与*n*的关系。运行*freivalds_plots.py*来生成[图11-2](ch011.xhtml#ch011fig02)中的图表。
- en: '![Image](../images/11fig02.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/11fig02.jpg)'
- en: '*Figure 11-2: Comparing Freivalds’ running time to the naive algorithm as a
    function of matrix size (left) and plotting Freivalds’ running time alone to show
    the* ![Image](../images/c0301-01.jpg)*(*n^(*2*)*) complexity—note the* y*-axis
    range (right)*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-2：将Freivalds算法的运行时间与朴素算法的运行时间对比，作为矩阵大小的函数（左），并单独绘制Freivalds算法的运行时间，以展示其*
    ![Image](../images/c0301-01.jpg)*(*n^(*2*)*)复杂度——请注意* y*轴范围（右）*'
- en: On the left of [Figure 11-2](ch011.xhtml#ch011fig02), we see the growth in running
    time between Freivalds’ and the naive algorithm as the size of the matrices increases.
    The naive algorithm, ![Image](../images/c0301-01.jpg)(*n*³), grows substantially
    faster than Freivalds’ ![Image](../images/c0301-01.jpg)(*n*²), shown by itself
    on the right.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 11-2](ch011.xhtml#ch011fig02) 的左侧，我们可以看到随着矩阵大小的增加，Freivalds 算法与朴素算法之间运行时间的增长。朴素算法的复杂度为
    ![Image](../images/c0301-01.jpg)(*n*³)，比 Freivalds 算法的 ![Image](../images/c0301-01.jpg)(*n*²)
    增长得要快得多，后者单独显示在右侧。
- en: The combination of performance gain and decreasing likelihood of error as *n*
    increases makes Freivalds’ algorithm particularly nice. Yes, it’s probabilistic,
    but in the places where it’s most desirable (large *n*), it’s also most likely
    to be correct.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 性能提升与错误发生概率随 *n* 增加而减少的结合，使得 Freivalds 算法特别有用。是的，它是概率性的，但在最需要它的地方（大 *n*），它也最有可能是正确的。
- en: Before examining *freivalds.py*, I have a confession. We can do matrix multiplication
    better than ![Image](../images/c0301-01.jpg)(*n*³), especially for matrices with
    *n* > 100\. Volker Strassen’s 1969 matrix multiplication algorithm has a runtime
    of about ![Image](../images/c0301-01.jpg)(*n*^(log[2] 7)) *≈* ![Image](../images/c0301-01.jpg)(*n*^(2.807)),
    which is slightly better than the naive algorithm. NumPy, based on the BLAS library,
    makes use of Strassen’s algorithm, which is why we didn’t use NumPy in this section.
    However, ![Image](../images/c0301-01.jpg)(*n*²) is better than ![Image](../images/c0301-01.jpg)(*n*^(2.807)),
    so Freivalds’ algorithm is still useful, even with Strassen matrix multiplication.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看 *freivalds.py* 之前，我得做一个自白。我们可以做得比 ![Image](../images/c0301-01.jpg)(*n*³)
    更好，特别是对于 *n* > 100 的矩阵。Volker Strassen 的 1969 年矩阵乘法算法具有约为 ![Image](../images/c0301-01.jpg)(*n*^(log[2]
    7)) *≈* ![Image](../images/c0301-01.jpg)(*n*^(2.807)) 的运行时间，略好于朴素算法。基于 BLAS 库的
    NumPy 利用了 Strassen 算法，这也是为什么我们在这一节中没有使用 NumPy。然而， ![Image](../images/c0301-01.jpg)(*n*²)
    优于 ![Image](../images/c0301-01.jpg)(*n*^(2.807))，因此即使有 Strassen 矩阵乘法，Freivalds
    算法仍然是有用的。
- en: '**GALACTIC ALGORITHMS**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**银河算法**'
- en: There are matrix multiplication algorithms with even better asymptotic behavior
    than Strassen’s algorithm. The current best have complexity ![Image](../images/c0301-01.jpg)(*n*^(2.373))
    or so. However, these algorithms are, in practice, completely useless. The seeming
    contradiction has to do with Big O notation, which shows the overall behavior
    but ignores multiplicative factors and constants. This means that an algorithm
    running in 10*n*³ time is the same as one running in 10,000*n*³ + 10,000 time.
    Both scale as ![Image](../images/c0301-01.jpg)(*n*³), but in practice, the first
    is more likely to be helpful.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 还有比 Strassen 算法更具渐近行为的矩阵乘法算法。目前最好的算法具有复杂度 ![Image](../images/c0301-01.jpg)(*n*^(2.373))
    左右。然而，这些算法在实践中完全无用。这个看似矛盾的现象与大 O 表示法有关，大 O 表示法展示的是总体行为，但忽略了乘法因子和常数。这意味着，运行时间为
    10*n*³ 的算法与运行时间为 10,000*n*³ + 10,000 的算法是一样的。两者的规模都是 ![Image](../images/c0301-01.jpg)(*n*³)，但在实践中，第一个更可能是有用的。
- en: Matrix multiplication algorithms that beat Strassen’s algorithm in overall complexity,
    like the *Coppersmith–Winograd* algorithm, have constants so large that the algorithm
    becomes practical only once *n* is some number far larger than anything computers
    can currently handle, if ever.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 比 Strassen 算法在整体复杂度上更优的矩阵乘法算法，比如 *Coppersmith–Winograd* 算法，其常数非常大，只有当 *n* 大到远超过当前计算机能够处理的范围时（如果有可能的话），该算法才有实际意义。
- en: Such algorithms have been christened *galactic algorithms* by Kenneth W. Regan.
    We cannot effectively use galactic algorithms in practice even if they are “the
    best” in terms of asymptotic behavior. While these algorithms are of theoretical
    importance, they won’t show up in our toolkits any time soon.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法被 Kenneth W. Regan 称为 *银河算法*。即使这些算法在渐近行为上是“最优的”，我们在实践中也无法有效使用它们。尽管这些算法在理论上很重要，但它们不会很快出现在我们的工具箱中。
- en: '**Exploring the Code**'
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**代码解析**'
- en: '[Listing 11-5](ch011.xhtml#ch011list05) contains the code implementing Freivalds’
    algorithm. The `mmult` function is in [Listing 11-4](ch011.xhtml#ch011list04).
    The `array_equal` function asks whether the absolute maximum of the difference
    between ***A***(***Br***) and ***Cr*** is below `eps`, and returns `True` if so.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 11-5](ch011.xhtml#ch011list05)包含了实现 Freivalds 算法的代码。`mmult` 函数在 [列表 11-4](ch011.xhtml#ch011list04)
    中。`array_equal` 函数会判断 ***A***(***Br***) 与 ***Cr*** 之间差值的绝对最大值是否低于 `eps`，如果是，返回
    `True`。'
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '*Listing 11-5: Freivalds’ algorithm*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 11-5：Freivalds 算法*'
- en: The outer `for` loop executes 1,000 trials using a randomly selected set of
    matrices each time. ***C*** is such that it never equals ***AB***, so every call
    to `array_equal` should return `False`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 外部`for`循环执行1000次试验，每次使用随机选择的矩阵集。***C***是这样设置的，它永远不等于***AB***，因此每次调用`array_equal`都应返回`False`。
- en: The body of the outer `for` loop either multiplies ***A*** and ***B*** directly
    (`mode==1`), or uses Freivalds’ algorithm by generating a random binary vector,
    `r`. Note that `r` is reshaped to be an *n*×1 column vector, as required for matrix
    multiplication.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 外部`for`循环的主体要么直接相乘***A***和***B***（`mode==1`），要么通过生成一个随机二进制向量`r`来使用Freivalds算法。注意，`r`被重新形状为*n*×1的列向量，这是矩阵乘法所要求的。
- en: The inner `for` loop applies Freivalds’ repeatedly (`reps`) each time, AND-ing
    the result with `t`. The AND operation means that after `reps` tests with different
    `r` vectors each time, the only way `t` is still true is if all tests give a wrong
    result. Each test should see `array_equal` return `False` because ***AB*** ≠ ***C***
    by design. Once `t` becomes `False`, it remains `False` for all remaining tests,
    so even one correct output from `array_equal` causes `t` to have the expected
    value.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 内部的`for`循环每次应用Freivalds算法重复(`reps`)，并将结果与`t`进行与运算。与运算意味着，在进行`reps`次测试，每次使用不同的`r`向量后，只有当所有测试都给出错误结果时，`t`才会保持为真。每次测试都应该看到`array_equal`返回`False`，因为按设计***AB***
    ≠ ***C***。一旦`t`变为`False`，它将在剩余的所有测试中保持`False`，因此即使`array_equal`返回正确结果，也会导致`t`具有预期的值。
- en: If `t` is still `True` after the inner loop, then the trial failed and we increment
    `k`. After all trials, we print the total runtime and the fraction of the 1,000
    trials that failed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在内部循环后`t`仍然为`True`，则说明试验失败，我们会增加`k`的值。所有试验完成后，我们将打印总运行时间以及1000次试验中失败的比例。
- en: Freivalds’ algorithm is a Monte Carlo algorithm because it might, with a probability
    we can minimize, produce a false output and claim ***AB*** = ***C*** when it isn’t
    true.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Freivalds算法是一种蒙特卡洛算法，因为它可能会产生一个错误输出并声称***AB*** = ***C***，尽管实际情况并非如此，而且这种错误输出的概率是可以最小化的。
- en: 'Let’s turn to a different type of question for the next section: to get an
    estimate of the number of things in a collection, is it necessary to count them
    individually?'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转向另一类问题：要估算一个集合中物品的数量，是否有必要逐个计数？
- en: '**Counting Animals**'
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**动物计数**'
- en: Ecologists often want to know how many animals of a particular species live
    in an area, though counting each one is often impossible. Enter *mark and recapture*,
    a strategy for estimating population size from a small sample.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 生态学家通常希望知道某个特定物种在某一地区的数量，尽管逐一统计每只动物通常是不可能的。这时就需要使用*标记和再捕*方法，这是一种通过小样本来估算种群数量的策略。
- en: In mark and recapture, the ecologist first goes into the field and captures
    *n* specimens, which they then mark and release. A short while later, they revisit
    the field and capture animals again until they get at least one that is marked.
    If they capture *K* animals to get *k* that are marked, they now have everything
    necessary to estimate the full population size, *N*. They do this by using ratios.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记和再捕中，生态学家首先进入现场捕捉*n*只样本，然后进行标记并释放。过一段时间后，他们重新进入现场，再次捕捉动物，直到捕获到至少一只已标记的动物。如果他们捕获了*K*只动物，其中有*k*只被标记，那么他们现在就拥有了估算总体数量*N*所需的所有信息。他们通过使用比率来完成这个过程。
- en: 'Initially, the ecologist marked *n* of the *N* animals, meaning the fraction
    of the total population marked is *n*/*N*. The recapture phase netted *k* marked
    animals out of *K*. Assuming no births, deaths, or migrations, the two ratios
    should be approximately equal, so solving for *N* gives:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，生态学家标记了*N*动物中的*n*只，这意味着标记的动物占总体种群的比例为*n*/*N*。再捕阶段捕获了*K*只动物，其中有*k*只被标记。假设没有出生、死亡或迁徙，这两个比例应该大致相等，因此通过解这个方程得到*N*：
- en: '![Image](../images/f0306-01.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0306-01.jpg)'
- en: This equation results in the *Lincoln-Petersen population estimate*, hence *N[L]*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程得出了*Lincoln-Petersen种群估算*，因此是*N[L]*。
- en: 'A slightly less biased estimate of the population (or so it’s claimed) comes
    from the *Chapman population estimate*:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一种稍微少偏的种群估算（或如此声称）来自*Chapman种群估算*：
- en: '![Image](../images/f0306-02.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0306-02.jpg)'
- en: 'Finally, we have a Bayesian approach to mark and recapture:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们采用贝叶斯方法来进行标记和再捕：
- en: '![Image](../images/f0306-03.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0306-03.jpg)'
- en: This approach requires at least three marked animals in the recapture group
    to avoid dividing by zero.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法要求再捕组中至少有三只标记的动物，以避免除以零的情况。
- en: 'Let’s compare these three different estimates for the same population size
    with *mark_recapture.py*:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过*mark_recapture.py*比较这三种对相同种群大小的不同估算方法：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The code simulates marking and recapturing by randomly marking a specified
    number of animals before recapturing a fraction of the population to count how
    many are marked. Let’s run the code a few times to get a feel for the output.
    We’ll fix the true population size at 1,000 and initially mark 100, or 10 percent.
    Setting the repetitions to 1 takes a single sampling, which is similar to what
    an ecologist might do in practice. We get:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The estimates vary widely from run to run, as we might expect from a randomized
    algorithm. While the Lincoln-Petersen and Chapman estimates are generally low,
    the Bayesian estimates are closer to or even exceed the population size.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a single repetition is akin to attempting to generalize from a single
    collected data point, so let’s increase the repetitions:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The output now reflects the mean and standard errors for 25 repetitions, providing
    a better idea of how the estimates behave. The Lincoln-Petersen and Chapman results
    are closer to the actual population size, while the Bayesian estimate is consistently
    too high. The standard errors are illustrative as well, with the Bayesian standard
    error being larger than the others, indicating more trial-to-trial variation.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Try experimenting with different combinations of population size and number
    of animals initially marked.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-3](ch011.xhtml#ch011fig03) presents three somewhat crowded graphs.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/11fig03.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-3: The three mark and recapture estimators as a function of the
    true population size and the fraction of that size initially marked. The plots
    show the signed deviation from the true population size: Lincoln-Peterson (top
    left), Chapman (top right), and Bayesian (bottom).*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: In the top-left graph in [Figure 11-3](ch011.xhtml#ch011fig03), each of the
    seven plotted lines represents a different true population size from 100 to 10,000\.
    The *x*-axis indicates the fraction of the true population marked by the ecologist
    on their first trip to the field. The value plotted is the median signed difference
    between the Lincoln-Petersen estimate for that combination of population size
    and fraction initially marked and the true population size. If the curve is above
    zero, the estimate is too low; below zero, it’s too high. In other words, the
    graph shows *N*[true] – *N*[est], so underestimating is a positive difference
    and overestimating is negative. The remaining two graphs show the same information
    for the Chapman (top-right) and Bayesian (bottom) estimators.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: For populations above 1,000, the Lincoln-Petersen estimator is generally useful
    when initially marking more than 10 percent of the population, which may not be
    feasible in practice. However, for small populations, the estimator requires some
    20 percent of the population to be marked to achieve reliability. One might use
    a simulation to generate a correction function for the Lincoln-Petersen estimator
    based on the suspected population size and the number of animals initially marked.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The Chapman estimator consistently underestimates the true population size to
    the point where one questions its utility compared to the Lincoln-Petersen estimate.
    However, the underestimate is relatively consistent for populations above 1,000,
    so again, a fudge factor might be derived from a simulation.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The Bayesian estimator’s performance is quite different. It consistently overestimates
    the actual population, converging to the true population value only when the population
    becomes large and the percent initially marked is also significant (at least 15
    percent). In practice, these conditions are unlikely to be met.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-3](ch011.xhtml#ch011fig03) is the output of *mark_recapture_range.py*,
    which can be understood by examining the relevant parts of *mark_recapture.py*
    in [Listing 11-6](ch011.xhtml#ch011list06).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*Listing 11-6: Simulating mark and recapture estimates*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: The outer `for` loop over `nreps` handles the trials. For each trial, we create
    a population (`pop`) vector where `npop` is the population size from the command
    line. The vector is initially zero as we haven’t marked any animals yet.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: The next two lines represent the ecologist’s first field trip. The `argsort`
    trick, coupled with keeping only the first `nmark` elements of the sort order,
    sets `idx` to the indices of `pop` that the ecologist has initially captured and
    marked (`pop[idx]=1`).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The second code paragraph represents the recapture phase in which the ecologist
    returns to the field and captures as many animals as were initially marked (`K`).
    We represent the captured animals by the `K` indices in `idx` as assigned in the
    inner `while` loop.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Marks are binary, so the sum of the selected elements of `pop` is the number
    of marked animals, `k`. If `k` is 3 or greater, break out of the `while` loop.
    Otherwise, increase `K` by five and try again. The final code paragraph calculates
    the three estimates of the true population size for this trial.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: When the outer `for` loop exits, we have three vectors of estimates for the
    given population size and number initially marked. The remainder of *mark_recapture.py*
    displays the results. Given the simulation results, my money’s on the Lincoln-Petersen
    estimator.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on from counting to the mathematically important task of primality
    testing.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '**Testing Primality**'
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prime numbers—integers evenly divisible by only themselves and one—are greatly
    beloved by number theorists. Primes have fascinated humanity since antiquity,
    and significant computing power is currently devoted to locating *Mersenne primes*
    of the form 2*^p* – 1, where *p* is a prime number.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'The largest known primes are Mersenne primes. As of this writing, the largest
    known Mersenne prime, discovered in 2018, is:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '*M*[82,589,933] = 2^(82,589,933) − 1'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '*M*[82,589,933] is a 24,862,048-digit number. Mersenne primes are sometimes
    denoted by their number and not their exponent. Therefore, *M*[82,589,933], the
    51st Mersenne prime, might be given as *M*[51].'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '*To contribute in locating Mersenne primes, visit* [https://www.mersenne.org](https://www.mersenne.org)
    *and sign up for the Great Internet Mersenne Prime Search.*'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we know if *n* is a prime number? The definition gives us a natural
    starting point for a primality testing algorithm: if the only numbers that evenly
    divide *n* (resulting in no remainder) are 1 and *n*, then *n* is a prime.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Let’s turn this definition into an algorithm. The brute force approach is to
    test every number that could be a factor of *n*. In practice, this means testing
    every integer up to ![Image](../images/f0311-01.jpg) because any factor of *n*
    larger than ![Image](../images/f0311-01.jpg) will necessarily be multiplied by
    some number less than ![Image](../images/f0311-01.jpg), and will be caught before
    reaching ![Image](../images/f0311-01.jpg).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: When contemplating numbers comprising nearly 25 million digits, the amount of
    work involved increases dramatically. And if *n* is prime, must we test *every*
    integer up to ![Image](../images/f0311-01.jpg)?
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: The *Miller-Rabin test* is a fast, randomized algorithm to decide whether a
    positive integer, *n*, is prime. However, to understand the Miller-Rabin test,
    we need to know a bit about modular arithmetic.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '***Modular Arithmetic***'
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We’re used to the set of integers, denoted ℤ from the German for number. Integers
    are unbounded and extend infinitely in both directions from zero. If we restrict
    the range to the set {0, 1, 2, 3}, we can define arithmetic operations over this
    set by wrapping around as needed. Adding works as expected if the sum is less
    than 4: 1 + 1 = 2 and 2 + 1 = 3\. However, if the sum exceeds 4, we wrap around.
    For example, 2+3 = 1 because 2+3 = 5, and we subtract 4 from 5 to get 1\. Another
    way to view these operations is to apply the modulo operator after each addition
    to return the remainder after dividing by 4\. For example, 5 mod 4 = 1.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Pierre Fermat, a 17th-century French mathematician, realized that if *n* is
    a prime number, then
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '*a*^(*n* – 1) ≡ 1 (mod *n*), 0 < *a* < *n*'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'where ≡ means:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '*a*^(*n* – 1) mod *n* = 1 mod *n* = 1'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Great! We have a primality test: pick an integer 0 < *a* < *n*, raise it to
    the *n* – 1 power, divide by *n*, and see if the remainder is 1\. If it is, then
    *n* is a prime number, so the algorithm works and identifies *n* as a prime. However,
    some composite numbers also pass the test for many values of *a*, meaning this
    alone isn’t sufficient to prove *n* is a prime. If this test fails, then *n* is
    definitely not a prime.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The Miller-Rabin test combines Fermat’s test with another fact—if *n* is prime,
    the following is likely also true
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0311-03.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: for some *r* in [0, *s*) where *n* = 2*^sd* + 1 and *d* is odd. It’s likely
    true because there are sometimes *a* values satisfying the congruence even if
    *n* is composite. We’ll discuss these non-witness numbers shortly.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: The first condition, Fermat’s test, is straightforward enough, but let’s unpack
    this second condition. We need to express *n* as 2*^sd* + 1 or, equivalently,
    as *n*–1 = 2*^sd*. For suitable choices of *s* and *d*, 2*^sd* is another way
    of writing the exponent in the Fermat condition.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: All of the math in ≡ –1 (mod *n*) is modulo *n*, meaning the numbers are in
    the set {0, 1, 2, . . . , *n* – 1}, usually denoted as ℤ*[n]*. We view a negative
    number as counting backward, so –1 ≡ *n* – 1.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'The second condition checks to see if *x*² ≡ –1 (mod *n*) for some *x*. The
    Miller-Rabin test uses a sequence of such values of *x*, looking for one that,
    when squared modulo *n*, gives –1 (that is, *n* – 1). The sequence begins with
    *r* = 0 and *d* as the exponent. The next check uses the square, which is the
    same as *r* = 1:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0312-01.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: This is all modulo *n*. The next squaring returns *r* = 2, and so on.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: If any of the sequence of such expressions is equivalent to *n* – 1, then *n*
    has a reasonably high probability of being a prime number. Otherwise, *n* is definitely
    *not* prime, and *a* is a *witness* to this fact.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '***The Miller-Rabin Test***'
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s put Miller-Rabin into code, as in [Listing 11-7](ch011.xhtml#ch011list07).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Listing 11-7: Miller-Rabin in code*'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: The function `MillerRabin` accepts *n* and `rounds` with a default value of
    5\. The first code paragraph captures trivial cases. As half of all numbers are
    even, testing directly for 2 and evenness saves time.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The second code paragraph locates *s* and *d* so that *n* = 2*^sd* + 1\. It’s
    always possible to find an *s* and *d* decomposition for any *n* (positive integer).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: For now, we’ll focus on the body of the outer `for` loop in the third paragraph,
    which implements a pass through the Miller-Rabin test for a randomly selected
    *a* and initial *x* value, *a^d* (mod *n*).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: The built-in Python function, `pow`, computes exponents and accepts an optional
    third argument so that `pow(a,d,n)` efficiently implements *a^d* (mod *n*).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The following `if` checks for 1 or –1\. If that’s the case, the Fermat test
    has passed, so this pass through the outer `for` loop is over.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, the inner `for` loop initiates the sequence of successive squarings
    of *x* = *a^d* while looking for one equivalent to –1\. If such a squaring is
    found, the inner loop breaks and the outer loop cycles; otherwise, *n* is composite
    and `MillerRabin` returns `False`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: When all rounds (the loop over `k`) are complete, and every test supports the
    notion that *n* is a prime, `MillerRabin` returns `True`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: The outer `for` loop applies the Miller-Rabin test repeated for randomly selected
    *a* values. Since an *a* value demonstrating *n* to be a composite number is a
    witness number, an *a* value that leads to a claim of prime when *n* is not prime
    is a *non-witness* number. It is never the case that all possible *a* values for
    a given *n* are non-witness numbers, so repeated applications of the outer loop
    body minimize the probability that a non-prime input will return `True`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll find `MillerRabin` in the file *miller_rabin.py*. It expects a number
    to test, the number of rounds (*a* values to try), and the randomness source:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For example:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output must be correct for these cases as 73,939,133 is a prime, and the
    closest two primes can be to each other is 2 away:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Both 8,675,309 and 8,675,311 are twin primes, so the test is correct.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Miller-Rabin always labels a prime a prime. Let’s explore when Miller-Rabin
    fails.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '#### ***Non-witness Numbers***'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, a witness number, *a*, testifies to the fact that *n* isn’t prime.
    Also, there are composite numbers for which the Miller-Rabin test fails if it
    selects a non-witness number as *a*.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: We’ll force the Miller-Rabin algorithm to fail, probabilistically, using a composite
    number with a known set of non-witness numbers to see if we can detect the expected
    number of failures.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Our target is *n* = 65\. As a multiple of 5, 65 is composite. There are 64 potential
    witness numbers, from 1 through 64\. Of these potential witness numbers, it’s
    known that 8, 18, 47, 57, and 64 are non-witness numbers. If the Miller-Rabin
    test runs for one round and selects a non-witness number for *a*, it fails and
    claims that 65 is prime.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Because there are five non-witness numbers out of 64 possible, the Miller-Rabin
    test for a single round should fail about 5/64 = 7.8 percent of the time. I checked
    this by running *miller_rabin.py* 1,000 times and counting the number of times
    the output indicated a prime, which it did precisely 78 times, implying a failure
    rate of 78/1, 000 = 7.8 percent.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: At worst, the Miller-Rabin single-round failure probability for arbitrary *n*
    is 1/4\. Since each round is independent of the previous, running the test for
    *k* rounds means the worst possible failure probability is (1/4)*^k* = 4^(–*k*).
    However, for most *n* values, the actual failure probability is far less than
    this.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Let’s stick with 65\. Knowing that its single-round failure rate is about 7.8
    percent, running two rounds should fail (5/64)² ≈ 0.61 percent of the time. Running
    *miller_rabin.py* 20,000 times produced 131 failures, giving a failure rate of
    131/20, 000 = 0.655 percent. Three rounds puts the failure rate at about 0.05
    percent. We can achieve any desired precision by setting *k* high enough.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '***Miller-Rabin Performance***'
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s compare Miller-Rabin’s runtime performance to the brute force approach
    implemented in *brute_primes.py*. The code in *prime_tests.py* runs both Miller-Rabin
    and the brute force algorithm for the largest 1, 2, 3, . . . , 15-digit prime
    numbers. Recall, the brute force algorithm runs the longest when the input is
    a prime.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: The largest single-digit prime is 7, while the largest 15-digit prime is 999,999,999,999,989\.
    In [Figure 11-4](ch011.xhtml#ch011fig04), we plot the mean of five runs of *miller_rabin.py*
    and *brute_primes.py* for each prime to show how the runtime changed as the inputs
    grew.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/11fig04.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-4: Comparing Miller-Rabin to the brute force primality test*'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: The runtime complexity of the brute force algorithm is ![Image](../images/f0315-01.jpg)
    while that of Miller-Rabin is ![Image](../images/c0301-01.jpg)(log³ *n*). The
    brute force algorithm quickly becomes unmanageable, even though it’s sublinear,
    because ![Image](../images/f0315-02.jpg) and 1/2 < 1.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Miller-Rabin is a Monte Carlo algorithm because it claims *n* is a prime when
    there’s a nonzero probability that it isn’t. If *n* truly is a prime, Miller-Rabin
    always correctly labels it, but it also calls some composite numbers prime regardless
    of the number of rounds. Therefore, Miller-Rabin’s false positive rate is nonzero,
    but its false negative rate is identically zero. In practice, however, increasing
    the number of rounds can make the false-positive rate as low as desired.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: We have one more randomized algorithm to contemplate.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '**Working with Quicksort**'
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quicksort was developed by British computer scientist Tony Hoare in 1959 and
    is probably still the most widely used sorting algorithm. It’s NumPy’s default,
    for example.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'If you take an undergraduate course in algorithms, you’ll almost assuredly
    run across Quicksort, as it’s easy to implement and understand, even if it’s recursive.
    While most courses focus on characterizing its runtime complexity, we’ll discuss
    the algorithm at a high level instead, and then run experiments on two versions:
    the standard nonrandom version and a randomized version.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Quicksort is a recursive, *divide-and-conquer* algorithm, meaning it calls itself
    on smaller and smaller versions of the problem until it encounters a base condition,
    at which point the implementation pieces everything back together to produce a
    sorted output.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm is as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: If the input array is empty or has only one element, return it.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick a *pivot* element, either the first in the array or at random.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Separate the array into three subsets: those elements less than the pivot,
    those equal to the pivot, and those greater than the pivot.'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the concatenation of Quicksort called on the lower elements, the elements
    matching the pivot, and Quicksort called on the higher elements.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step 1 is the base condition. If the array is empty or contains a single element,
    it’s sorted. step 2 picks a pivot value, an array element we use in Step 3 to
    split the array into three parts: those less than, equal to, and greater than
    the pivot. Step 2 is where randomness comes into play. Non-random Quicksort always
    picks a specific element of the array, as it’s already assumed to be in random
    order. Randomized Quicksort, however, selects its pivot element at random. We’ll
    experiment with the subtle difference between nonrandom and random Quicksort.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 is the recursive part. The array is sorted if we merge the sorted lower
    partition with the same partition followed by the sorted higher partition. We
    sort the lower and higher partitions by using the sorting routine, that is, by
    calling Quicksort again. Each call on a portion of the array will, we assume,
    work with a smaller number of elements until we have single elements, the base
    condition of step 1.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Naive sorting methods, like bubble sort or gnome sort, run in ![Image](../images/c0301-01.jpg)(*n*²)
    time where *n* is the number of elements to sort. As we’ve learned, ![Image](../images/c0301-01.jpg)(*n*²)
    algorithms are acceptable for small *n* values, but quickly become unmanageable
    as *n* grows. Quicksort’s average runtime complexity is ![Image](../images/c0301-01.jpg)(*n*
    log *n*), which grows at a much slower rate. This is why Quicksort is still widely
    used over 50 years after its introduction.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: While Quicksort’s *average* complexity is ![Image](../images/c0301-01.jpg)(*n*
    log *n*), if the array passed to Quicksort is already mostly or completely sorted,
    the complexity becomes ![Image](../images/c0301-01.jpg)(*n*²), which is no better
    than bubble sort. This happens if the array is in order or reverse order. Let’s
    find out whether randomized Quicksort can help us here.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '***Running Quicksort in Python***'
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The file *Quicksort.py* implements Quicksort twice. The first implementation
    uses a random pivot (`QuicksortRandom`), and the second implementation always
    uses the first element of the array as the pivot (`Quicksort`). The functions
    are in [Listing 11-8](ch011.xhtml#ch011list08).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '*Listing 11-8: Randomized and nonrandom Quicksort*'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we use NumPy instead of our `RE` class because it’s already
    loaded, which minimizes the overhead when calling `QuicksortRandom`. The implementations
    differ only in how they assign `pivot`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Both implementations follow the Quicksort algorithm step-by-step. First, we
    check for the base condition where `arr` is already sorted. We then split into
    `low`, `same`, and `high` based on the selected `pivot`. Finally, NumPy’s `hstack`
    function concatenates the vectors returned by the recursive calls to `Quicksort`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: A high-performance implementation wouldn’t call `where` three times, as each
    makes a full pass over `arr`, but we’re interested only in relative performance
    differences as the input size changes.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '***Experimenting with Quicksort***'
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The file *quicksort_tests.py* generates two graphs. The first, on the left in
    [Figure 11-5](ch011.xhtml#ch011fig05), compares randomized Quicksort and nonrandom
    Quicksort as the input array size increases. In all cases, the input arrays are
    in random order. Therefore, the left side of [Figure 11-5](ch011.xhtml#ch011fig05)
    represents the average case runtime. The points plotted are the mean over five
    runs. The dashed line represents *y* = *n* log *n*.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/11fig05.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-5: Randomized and nonrandom Quicksort on random inputs (left) and
    the same algorithms on pathological inputs (right)*'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: The rightmost graph in [Figure 11-5](ch011.xhtml#ch011fig05) shows the runtime
    for the case with already sorted input. This situation forces deterministic Quicksort
    into becoming an ![Image](../images/c0301-01.jpg)(*n*²) algorithm, which is why
    it tracks the curved plot, *y* = *n*². Randomized Quicksort, on the other hand,
    is unaffected by the order of the input and runs as before.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Correctly interpreting [Figure 11-5](ch011.xhtml#ch011fig05) requires an explanation.
    Asymptotic runtime performance of algorithms ignores multiplicative factors and
    constants as they don’t alter the overall form of the function as *n* increases.
    The randomized Quicksort function takes slightly longer to run than the non-random
    Quicksort because of the extra step of selecting a random index into the array.
    Therefore, plotting both runtimes together would make it somewhat difficult to
    see that the overall functional form is the same between `QuicksortRandom` and
    `Quicksort`. Moreover, plotting *y* = *n* log *n* follows an entirely different
    scale in terms of *y*-axis values, but again, the form of the function is the
    same. Therefore, to plot all three together, [Figure 11-5](ch011.xhtml#ch011fig05)
    divides each *y* value by the maximum *y* value to map the output to [0, 1] regardless
    of the actual range. This clarifies that randomized Quicksort and nonrandom Quicksort
    scale in the same way, and are following ![Image](../images/c0301-01.jpg)(*n*
    log *n*)—all curves lie essentially on top of each other.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Now reconsider the right side of [Figure 11-5](ch011.xhtml#ch011fig05) showing
    the case where the input array is already sorted. Again, the dashed line shows
    *y* = *n* log *n*, and randomized Quicksort still follows that form. However,
    nonrandom Quicksort, which selects the first element of the array as its pivot,
    does not. Instead, it follows the dotted line, *y* = *n*², meaning the pathological
    input case alters nonrandom Quicksort, turning it into an ![Image](../images/c0301-01.jpg)(*n*²)
    algorithm.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Randomized Quicksort is a Las Vegas algorithm because it always returns the
    proper output—a sorted array. While the randomness involved doesn’t make the implementation
    easier, it protects against a pathological case that’s more frequent in practice
    than we might initially suspect. Therefore, I recommend always using randomized
    Quicksort.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: To understand why nonrandom Quicksort behaves so poorly with sorted input, consider
    what happens during a pass when the pivot is the smallest value in the array;
    for example, when picking the first element as the pivot and the input array is
    already sorted. When this happens, the low vector is empty and, ignoring duplicates
    of the pivot, all the remaining values in the array end up in the high vector.
    This happens every time the function recurses, turning the recursion into a list
    of function calls *n* deep. Add the ![Image](../images/c0301-01.jpg)(*n*) pass
    through the array on each recursive call (implicit in our implementation via `where`),
    and we arrive at an ![Image](../images/c0301-01.jpg)(*n*²) algorithm, which is
    no better than a bubble sort.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a random pivot at each level ensures that this situation won’t happen
    in the long run, as it would amount to a string of *n* rolls of an *n-*sided fair
    die each landing on 1—an increasingly unlikely event as *n* grows.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '### **Exercises**'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following exercises to further explore randomized algorithms:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Write a Las Vegas algorithm to locate positive integers, *a*, *b*, and *c*,
    that satisfy *a*² + *b*² = *c*². Your code will be a Las Vegas algorithm because
    there are an infinite number of solutions, namely all the Pythagorean triples.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can you write a successful Las Vegas program to find positive integers *a*,
    *b*, and *c* such that *a^n* + *b^n* = *c^n* for some *n* > 2? If not, how about
    a Monte Carlo algorithm? What might your stopping criteria be? I recommend searching
    for “Fermat’s last theorem.”
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extend the permutation sort runtime plot for *n* = 11, 12, or even 13\. How
    long do you have to wait?
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a plot of the mean number of trials of Freivalds’ algorithm to get a failure
    case as a function of *n*, the size of the square matrices.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The file *test_mmult.py* generates output suitable for *curves.py* from [Chapter
    4](ch04.xhtml). Use that output and *curves.py* to generate fits. Is the fit exponent
    what you expect for the naive algorithm? What about NumPy, with the knowledge
    that it uses Strassen’s algorithm?
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have a bag full of marbles. I want to estimate how many are in the bag. Therefore,
    I pick one randomly, mark it, and put it back in the bag. I then pick another
    marble randomly, mark it, and put it back in the bag. I repeat this process, counting
    the number of marbles selected, until I pick a marble I’ve already marked. If
    the number of marbles picked and marked is *k*, then the number of marbles in
    the bag is approximately![Image](../images/f0319-01.jpg)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'where the combination of floor (⌊) on the left and ceiling (⌉) on the right
    means “round to the nearest integer.” I encountered this algorithm via a brief
    description of the process, but the description had no derivation for the formula
    and no references. Nonetheless, it sort of works. After experimenting some, I
    realized that the estimate is better if the formula is tweaked slightly to become:'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Image](../images/f0319-04.jpg)'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Implement this algorithm and explore how well it works on average. Then examine
    *count.py*, which runs the algorithm for many iterations, averages the results,
    and produces plots. For example
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: estimates slightly more than 1 billion marbles in the bag. The correct number
    is exactly 1 billion. It uses 40 iterations of the algorithm for a total of 1.7
    million marbles marked. [Figure 11-6](ch011.xhtml#ch011fig06) is the resulting
    plot, *count_plot.png*, which shows each of the 40 estimates, the true value (solid
    line), and overall estimate (dashed).
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Image](../images/11fig06.jpg)'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '*Figure 11-6: Forty estimates of marbles*'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Please contact me if you know a reference for this algorithm or how to derive
    the estimate formula.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Can you think of a “fudge factor” for the Lincoln-Petersen population estimate
    for the case where the population is believed to be small?
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does the runtime performance of nonrandom Quicksort vary as the array becomes
    more disordered? To figure this out, fix the array size (*n*) but change the degree
    of disorder in the array. For example, begin with a sorted array, then swap two
    elements, then three, and so on. Is the transition from ![Image](../images/c0301-01.jpg)(*n*²)
    to ![Image](../images/c0301-01.jpg)(*n* log *n*) linear with the number of elements
    swapped? Or does it seem more rapid?
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### **Summary**'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we explored randomized algorithms, differentiating between
    Las Vegas and Monte Carlo. The former always produced correct output, eventually,
    while the latter may produce incorrect output. We considered permutation sort
    and Freivalds’ algorithm for testing matrix multiplication. We learned that we
    can turn permutation sort from a Las Vegas algorithm into a Monte Carlo algorithm
    by imposing a limit on the number of candidate permutations considered. In general,
    we can transform Las Vegas algorithms into Monte Carlo algorithms, but not vice
    versa.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed the mark and recapture algorithm that ecologists use to estimate
    animal populations. We estimate the number of animals in a population by marking
    a known number and then recapturing animals and looking at the number marked.
    With sufficient numbers, the ratio of marked animals to animals recaptured should
    match the ratio of animals originally marked to the population size. We explored
    three estimators associated with this process and saw how they behave.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: The Miller-Rabin algorithm quickly decides whether a positive integer is a prime.
    However, as a randomized algorithm, there’s a certain probability that it’ll falsely
    claim a composite number is prime. We learned how to decrease the likelihood of
    a false positive by repeated applications.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: We concluded the chapter by comparing nonrandom and randomized Quicksort implementations.
    Randomized Quicksort adds little to the runtime while protecting against pathological
    inputs that are already (or mostly) sorted.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: In our final chapter, we’ll consider randomness as it relates to sampling from
    probability distributions.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
