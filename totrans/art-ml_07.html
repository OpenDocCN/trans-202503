<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h1" id="ch05"><span epub:type="pagebreak" id="page_81" class="calibre2"/><strong class="calibre3"><span class="big">5</span><br class="calibre18"/>A STEP BEYOND K-NN: DECISION TREES</strong></h2>
<div class="imagec"><img alt="Image" src="../images/common.jpg" class="calibre14"/></div>
<p class="noindent">In k-NN, we looked at the neighborhood of the data point to be predicted. Here again we will look at neighborhoods, but in a more sophisticated way. This approach will be easy to implement and explain, lends itself to nice pictures, and has more available hyperparameters with which to fine-tune it.</p>
<p class="indent">Here we will introduce <em class="calibre13">decision trees (DTs)</em>, one of the mainstays in the ML field. Besides being used directly, DTs are also the basis for <em class="calibre13">random forests</em> and <em class="calibre13">gradient boosting</em>, which we will cover in later chapters.</p>
<h3 class="h2" id="ch05lev1">5.1 Basics of Decision Trees</h3>
<p class="noindent">Though some ideas had been proposed earlier, the DT approach became widely used due to the work of statisticians Leo Breiman, Jerry Friedman, Richard Olshen, and Chuck Stone. They called their method <em class="calibre13">classification and regression trees (CART)</em> and described it in their book <em class="calibre13">Classification and Regression Trees</em> (Wadsworth, 1984).</p>
<p class="indent"><span epub:type="pagebreak" id="page_82"/>A DT method basically sets up the prediction process as a flow chart, hence the name <em class="calibre13">decision tree</em>. For instance, look at <a href="ch05.xhtml#ch05fig01" class="calibre12">Figure 5-1</a> in <a href="ch05.xhtml#ch05lev2sec1" class="calibre12">Section 5.2.1</a>. There we are predicting ozone level from features such as temperature and wind speed. In predicting a new case, we start at the top of the tree and follow some path to the bottom, making decisions along the way as to whether to turn left or right. At the bottom of the tree, we make our prediction.</p>
<p class="indent">We produce a tree using our training set data. The top of the tree (the <em class="calibre13">root node</em>) contains all of that data. We then split the data into two parts according to whether some feature is smaller or larger than a given value. This creates two new nodes, below and to the left or right of the root node. Then we split each of <em class="calibre13">those</em> parts into two further parts and so on. Thus an alternative name for the process is <em class="calibre13">recursive partitioning</em>.</p>
<p class="indent">At each step, we have the option of stopping—that is, making no further splits along that particular path or branch within the tree. In that case, the non-split node is called a <em class="calibre13">leaf</em> or <em class="calibre13">terminal node</em> of the tree. Any given branch of the tree will end at some leaf node.</p>
<p class="indent">In the end, to predict a new case, we start at the root of the tree and work our way down to a leaf. Our predicted <em class="calibre13">Y</em> value then depends on the type of application. In numeric- <em class="calibre13">Y</em> cases, our predicted <em class="calibre13">Y</em> is then the average of all the <em class="calibre13">Y</em> values in that node. For classification applications, our predicted <em class="calibre13">Y</em> value is the class that is most numerous in the given leaf node. Or equivalently, express <em class="calibre13">Y</em> as dummy variables and take the average of each dummy. This gives us the probabilities of the various classes, and we set the predicted class to be the one of largest probability.</p>
<p class="indent">It is in this sense that DTs are analogous to k-NN. A leaf node serves as analogous to the neighborhood concept of k-NN.</p>
<p class="indent">Various schemes have been devised to decide (a) <em class="calibre13">whether</em> to split a given node in the tree, and (b) if so, <em class="calibre13">how</em> to do the split. More on this shortly.</p>
<h3 class="h2" id="ch05lev2">5.2 The qeDT() Function</h3>
<p class="noindent">R’s CRAN repository has several DT packages, but two I like especially are <span class="literal">partykit</span> and its earlier version, <span class="literal">party</span>. (These names are a pun on the term <em class="calibre13">recursive partitioning</em>.) Our <span class="literal">qe*</span>-series function <span class="literal">qeDT()</span> wraps <span class="literal">party::ctree()</span>. To illustrate, let’s run an example from the package.</p>
<p class="indent">The dataset here, <span class="literal">airquality</span>, is built into R and looks like this:</p>
<pre class="calibre16">&gt; <span class="codestrong">head(airquality)</span>
  Ozone Solar.R Wind Temp Month Day
1    41     190  7.4   67     5   1
2    36     118  8.0   72     5   2
3    12     149 12.6   74     5   3
4    18     313 11.5   62     5   4
5    NA      NA 14.3   56     5   5
6    28      NA 14.9   66     5   6
7    23     299  8.6   65     5   7</pre>
<p class="indent"><span epub:type="pagebreak" id="page_83"/>Our goal is to predict ozone level from the other features:</p>
<pre class="calibre16">&gt; <span class="codestrong">airq &lt;- subset(airquality, !is.na(Ozone)) # remove rows with Y NAs</span>
&gt; <span class="codestrong">dim(airq)</span>
[1] 116   6
&gt; <span class="codestrong">dtout &lt;- qeDT(airq,'Ozone',holdout=NULL)</span></pre>
<p class="noindent">Since this is such a small dataset, we decided against having a holdout set.</p>
<p class="indent">We predict new data points as usual (after all, the <span class="literal">qe*</span>-series is supposed to give a uniform interface to the various functions they wrap). Say we have a new day to predict, with values the same as in <span class="literal">airq[1,]</span> but with wind at 8.8 miles per hour. What value would we predict for the ozone level?</p>
<pre class="calibre16">&gt; <span class="codestrong">w &lt;- airq[1,-1]</span>
&gt; <span class="codestrong">w[2] &lt;- 8.8</span>
&gt; <span class="codestrong">w</span>
  Solar.R Wind Temp Month Day
1     190  8.8   67     5   1
&gt; <span class="codestrong">predict(dtout,w)</span>
        Ozone
[1,] 18.47917</pre>
<p class="noindent">We would predict ozone at about 18.5 parts per million.</p>
<p class="indent">As you know, <span class="literal">qe*</span>-series functions are wrappers, and their return objects usually include a component containing the return object from the wrapped function. This is the case here for <span class="literal">qeDT()</span>:</p>
<pre class="calibre16">&gt; <span class="codestrong">names(dtout)</span>
[1] "ctout"     "classif"   "trainRow1"</pre>
<p class="noindent">Here <span class="literal">ctout</span> is the object returned by <span class="literal">ctree()</span> when the latter is invoked from <span class="literal">qeDT()</span>. By the way, <span class="literal">ctout</span> is of class <span class="literal">'party'</span>.</p>
<p class="indent">We are using default hyperparameters here and might get better predictions with a better set of them. More on this in <a href="ch05.xhtml#ch05lev6" class="calibre12">Section 5.6</a>, but let’s focus now on how the tree process works by plotting the flow chart.</p>
<h4 class="h3" id="ch05lev2sec1"><em class="calibre22"><strong class="calibre3">5.2.1 Looking at the Plot</strong></em></h4>
<p class="noindent">Most DT packages allow you to plot the tree, which sometimes can provide useful insights for the analyst. In our setting here, though, we will use the plot to gain a better understanding of how the DT process works.</p>
<p class="indent">The call is simple:</p>
<pre class="calibre16">&gt; <span class="codestrong">plot(dtout)</span></pre>
<p class="noindent">As mentioned before, <span class="literal">plot()</span> is an R <em class="calibre13">generic function</em> (that is, a placeholder). The above call is dispatched to <span class="literal">plot.qeDT(dtout)</span>. And since the latter has been written to call <span class="literal">plot()</span> on the <span class="literal">ctout</span> component, in the end, that <span class="literal">plot()</span> call above will eventually be dispatched to <span class="literal">plot.party()</span>.<span epub:type="pagebreak" id="page_84"/></p>
<p class="indent"><a href="ch05.xhtml#ch05fig01" class="calibre12">Figure 5-1</a> shows the plot. As we are just getting an overview now, don’t try to grasp the entire picture in a single glance.</p>
<div class="image"><img alt="Image" id="ch05fig01" src="../images/ch05fig01.jpg" class="calibre34"/></div>
<p class="figcap"><em class="calibre13">Figure 5-1: Sample plot from</em></p>
<p class="indent">A DT indeed takes the form of a flow chart. For a day with given levels of <span class="literal">Solar.R</span>, <span class="literal">Wind</span>, and so on, what value should we predict for <span class="literal">Ozone</span>? The graph shows our prediction procedure.</p>
<p class="indent">Now let’s see what happens when we predict a new point, say, <span class="literal">w</span> from above. We start at the root, Node 1, and look at <span class="literal">Temp</span>. Since the value of the latter for <span class="literal">w</span> is 67, which is smaller than 82 degrees, we go left, to Node 2. There we ask whether <span class="literal">Wind</span> is less than or equal to 6.9 miles per hour. It’s 8.8, so we go right, to Node 4, where we are told to compare <span class="literal">Temp</span> to 77. Again, the value in <span class="literal">w</span> is 67, so we go left, to Node 5.</p>
<p class="indent">We saw earlier that our predicted value was 18.47917. How did the tree produce this from Node 5?</p>
<p class="indent">Our predicted value will be the mean <em class="calibre13">Y</em> value for all training set data points in Node 5. There is information in <span class="literal">dtout</span> as to which data points are in that node. Specifically, the <span class="literal">termNodeMembers</span> component of <span class="literal">qeDT()</span> output is an R list, with one element for each tree node. To gain an understanding of the workings of that function, let’s check Node 5 “by hand”:<span epub:type="pagebreak" id="page_85"/></p>
<pre class="calibre16">&gt; <span class="codestrong">dtout$termNodeMembers[['5']]</span>
 [1]   1   2   3   4   5   6   7   8  10  11  12
[12]  13  14  15  16  17  18  19  20  21  22  23
[23]  26  31  32  33  34  35  45  74  76  79  80
[34]  96  97  99 101 102 104 105 106 108 109 111
[45] 112 114 115 116</pre>
<p class="noindent">We see that 48 data points of <span class="literal">airq</span> ended up in Node 5, specifically the points <span class="literal">airq[1,]</span>, <span class="literal">airq[2,]</span>, and so on. DT then computes the mean <em class="calibre13">Y</em> for these points:</p>
<pre class="calibre16">&gt; <span class="codestrong">node5indices &lt;- dtout$termNodeMembers[['5']]</span>
&gt; <span class="codestrong">mean(airq$Ozone[node5indices])</span>
[1] 18.47917</pre>
<p class="indent">This matches the value we obtained from <span class="literal">predict()</span>.</p>
<h3 class="h2" id="ch05lev3">5.3 Example: New York City Taxi Data</h3>
<p class="noindent">Let’s try all this on a larger dataset. Fortunately for us data analysts, the New York City Taxi and Limousine Commmission makes available voluminous data on taxi trips in the city.<sup class="calibre11"><a id="ch5fn1" class="calibre12"/><a href="footnote.xhtml#ch5fn1b" class="calibre12">1</a></sup> A small portion of that data is available as <span class="literal">yell10k</span> in the <span class="literal">regtools</span> package.</p>
<p class="indent">That dataset consists of 10,000 random records from the January 2019 dataset. It retains only 7 of the original 18 features, and some date conversion has been done.</p>
<p class="indent">It would be nice if taxi operators had an app to predict travel time, which many passengers may wish to know. This will be our goal here.</p>
<p class="indent">Here’s the data:</p>
<pre class="calibre16">&gt; <span class="codestrong">data(yell10k)</span>
&gt; <span class="codestrong">head(yell10k)</span>
        passenger_count trip_distance PULocationID DOLocationID PUweekday
2969561               1          1.37          236           43         1
7301968               2          0.71          238          238         4
3556729               1          2.80          100          263         3
7309631               2          2.62          161          249         4
3893911               1          1.20          236          163         5
4108506               5          2.40          161          164         5
        tripTime
2969561      598
7301968      224
3556729      761
7309631      888
3893911      648
4108506      977</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_86"/>Here <span class="literal">PU</span> and <span class="literal">DO</span> mean “pickup” and “dropoff.” Trip distance is in miles, and trip time is in seconds.</p>
<p class="indent">On the other hand, trip distance is not enough; the pickup and dropoff locations are important, as some parts of the city may be slower to navigate than others. The original data also had time of day, which is important but not used here for simplicity.</p>
<h4 class="h3" id="ch05lev3sec1"><em class="calibre22"><strong class="calibre3">5.3.1 Pitfall: Too Many Combinations of Factor Levels</strong></em></h4>
<p class="noindent">Now, note the location IDs:</p>
<pre class="calibre16">&gt; <span class="codestrong">length(unique(yell10k$PULocationID))</span>
[1] 143
&gt; <span class="codestrong">length(unique(yell10k$DOLocationID))</span>
[1] 205
&gt; <span class="codestrong">143*205</span>
[1] 29315</pre>
<p class="noindent">There are potentially 29,315 pickup and dropoff combinations! Since we have only <em class="calibre13">n</em> = 10000 data points, we risk serious overfitting problems. And at the very least, having so many potential tree nodes will affect run time on the training set.</p>
<p class="indent">Furthermore, when I tried this with the <span class="literal">partykit</span> package rather than <span class="literal">party</span>, I encountered an error message: “Too many levels.” The documentation recommends using <span class="literal">party</span> in such cases, but even then we would likely run into trouble with larger datasets of this type.</p>
<p class="indent">This suggests possible use of consolidation or embedding (see <a href="ch04.xhtml#ch04lev3sec1" class="calibre12">Section 4.3.1</a>). We may, for instance, wish to form groups of contiguous locations. Or we could try an embedding—that is, replacing location IDs by latitude and longitude. But let’s see what happens without taking such measures.</p>
<h4 class="h3" id="ch05lev3sec2"><em class="calibre22"><strong class="calibre3">5.3.2 Tree-Based Analysis</strong></em></h4>
<p class="noindent">As noted, this dataset may present challenges, especially regarding possible overfitting issues. Let’s give it a try:</p>
<pre class="calibre16">&gt; <span class="codestrong">dtout &lt;- qeDT(yell10k,'tripTime')</span>
holdout set has  1000 rows
&gt; <span class="codestrong">dtout$testAccAA</span>
[1] 211.7106
&gt; <span class="codestrong">dtout$baseAcc</span>
[1] 433.8724</pre>
<p class="noindent">Not bad; we cut MAPE in half by using the features here. Again, we might do considerably better with nondefault hyperparameter combinations, as well as by adding some of the features in the original dataset that are not in <span class="literal">yell10k</span>.</p>
<p class="indent"><span epub:type="pagebreak" id="page_87"/>The dataset, even in the scaled-down form we are using here, is far too complex for plotting its tree. We can still display it in printed form:</p>
<pre class="calibre16">&gt; <span class="codestrong">dtout</span>
<br class="calibre1"/>
     Conditional inference tree with 40 terminal nodes
<br class="calibre1"/>
Response:  tripTime
Inputs:  passenger_count, trip_distance, PULocationID, DOLocationID, PUweekday
Number of observations:  9000
<br class="calibre1"/>
1) trip_distance &lt;= 3.08; criterion = 1, statistic = 5713.065
  2) trip_distance &lt;= 1.39; criterion = 1, statistic = 3517.53
    3) trip_distance &lt;= 0.79; criterion = 1, statistic = 1216.608
      4) trip_distance &lt;= 0.49; criterion = 1, statistic = 404.09
        5) trip_distance &lt;= 0.16; criterion = 1, statistic = 138.913
          6)*  weights = 107
        5) trip_distance &gt; 0.16
          7) trip_distance &lt;= 0.27; criterion = 0.998, statistic = 83.348
            8)*  weights = 51
          7) trip_distance &gt; 0.27
            9) DOLocationID == {13, 68, 75, 87, 100, 107, 125, 137, 142, 148,
               161, 162, 209, 230, 233, 234, 237, 264}; criterion = 0.982,
               statistic = 83.545
              10)*  weights = 138
...</pre>
<p class="noindent">Though the display is quite complex even in printed form, forcing only a partial listing here, and though it contains some quantities we have not yet described, one may still glean some interesting information. First we see that there were 40 terminal nodes, as opposed to 5 in our previous example, reflecting the greater complexity of this dataset. (There are 79 nodes in the entire tree, as can be seen by typing <span class="literal">dtout$nNodes</span>.)</p>
<p class="indent">Second, we see in part how that reduction was accomplished: DT was able to form its own groups of pickup and dropoff locations, such as in Node 9:</p>
<pre class="calibre16">            9) DOLocationID == {13, 68, 75, 87, 100, 107, 125, 137, 142, 148,
               161, 162, 209, 230, 233, 234, 237, 264}; criterion = 0.982,
               statistic = 83.545
              10)*  weights = 138</pre>
<p class="noindent">We go left if <span class="literal">DOLocationID</span> is one of 13, 68, and so on, and otherwise go right. This addresses our concerns in <a href="ch05.xhtml#ch05lev3sec1" class="calibre12">Section 5.3.1</a>. The DT grouped the locations for us! No wonder DTs are so popular!<span epub:type="pagebreak" id="page_88"/></p>
<div class="note">
<p class="notet"><strong class="calibre3"><span class="notes">NOTE</span></strong></p>
<p class="notep"><em class="calibre13">If we type an expression when we are in R interactive mode, R prints that expression. Here we typed</em> <span class="codeitalic1">dtout</span><em class="calibre13">, so it’s equivalent to typing</em> <span class="codeitalic1">print(dtout)</span><em class="calibre13">. But</em> <span class="codeitalic1">print()</span> <em class="calibre13">is yet another R generic function, and we will thus have a similar chain of calls as for</em> <span class="codeitalic1">plot()</span> <em class="calibre13">above, ending with</em> <span class="codeitalic1">print.party(dtout$ctout)</span><em class="calibre13">.</em></p>
</div>
<p class="indent">One thing worth checking in DT analysis is the numbers of data points in the various leaf nodes. Say some node has rather few data points. That’s analogous to having too few points in a k-NN neighborhood. Just as we can try different values of <em class="calibre13">k</em> in the latter case, here we may wish to tweak some DT hyperparameters.</p>
<p class="indent">We’ll look at hyperparameters in <a href="ch05.xhtml#ch05lev6" class="calibre12">Section 5.6</a>, but for now, let’s see how to check for leaf nodes with rather few data points:</p>
<pre class="calibre16">&gt; <span class="codestrong">dtout$termNodeCounts</span>
  6   8  10  12  13  16  17  19  20  24  25  27  28  31  32  34  36  37  42  43
107  51 138  71 148 262 245 190 423 492 216  18 361 370 309 266 304  17 496 101
 44  47  48  49  52  54  55  57  58  63  64  66  67  69  71  72  74  76  78  79
317 110 395 286 127 145 287 552 177 266 240  95 245 211 378 105 165 233  10  71</pre>
<p class="noindent">There are a few small nodes, notably Node 78 with only 10 data points. This is a possible reason to tweak the hyperparameters.</p>
<h3 class="h2" id="ch05lev4">5.4 Example: Forest Cover Data</h3>
<p class="noindent">Another UCI dataset, Covertype, aims to “[predict] forest cover type from cartographic variables only.”<sup class="calibre11"><a id="ch5fn2b" class="calibre12"/><a href="footnote.xhtml#ch5fn2" class="calibre12">2</a></sup> The idea is that one might use remote sensing to determine what kinds of grasses there are in difficult-to-access regions. There are 581,012 data points, with 54 features, such as elevation, hillside shade at noon, and distance to the nearest surface water. There are seven different cover types, which are stored in column 55.</p>
<p class="indent">This example is useful for a number of reasons. Here we’ll see DT in action in a classification problem, with multiple classes, and of a size larger than what we’ve seen so far. And besides, what could be better in a chapter on trees than data on forests!</p>
<p class="indent">Input the data, say, with <span class="literal">data.table::fread()</span> for speed:</p>
<pre class="calibre16">&gt; <span class="codestrong">library(data.table)</span>
&gt; <span class="codestrong">cvr &lt;- fread('covtype.data.gz')</span>
&gt; <span class="codestrong">cvr[1,]</span>
     V1 V2 V3  V4 V5  V6  V7  V8  V9  V10 V11 V12
1: 2596 51  3 258  0 510 221 232 148 6279   1   0
   V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23
1:   0   0   0   0   0   0   0   0   0   0   0
   V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34
1:   0   0   0   0   0   0   0   0   0   0   0
   V35 V36 V37 V38 V39 V40 V41 V42 V43 V44 V45
1:   0   0   0   0   0   0   0   0   1   0   0
   V46 V47 V48 V49 V50 V51 V52 V53 V54 V55
1:   0   0   0   0   0   0   0   0   0   5</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_89"/>The class, in column <span class="literal">V55</span>, was read in as an integer, whereas <span class="literal">qe*</span>-series functions need <em class="calibre13">Y</em> to be R factors in classification problems. We could have used <span class="literal">fread()</span>’s <span class="literal">colClasses</span> argument, but let’s just fix it directly:</p>
<pre class="calibre16">&gt; <span class="codestrong">cvr$V55 &lt;- as.factor(cvr$V55)</span></pre>
<p class="indent">There are seven classes, but some are much more common than others:</p>
<pre class="calibre16">&gt; <span class="codestrong">table(cvr$V55)</span>
<br class="calibre1"/>
     1      2      3      4      5      6      7
211840 283301  35754   2747   9493  17367  20510</pre>
<p class="noindent">Cover types 1 and 2 are the most numerous.</p>
<p class="indent">Since both <em class="calibre13">n</em> and <em class="calibre13">p</em> are large, let’s run on a random subset of 50,000 records to more conveniently illustrate the ideas. This approach is also common in data analysis: do a preliminary analysis on a subset of the data, again for convenience, but then do a more thorough analysis on the full data.</p>
<pre class="calibre16">&gt; <span class="codestrong">cvr50k &lt;- cvr[sample(1:nrow(cvr),50000),]</span>
&gt; <span class="codestrong">dto &lt;- qeDT(cvr50k,'V55')</span>
holdout set has  1000 rows
&gt; <span class="codestrong">dto$testAcc</span>
[1] 0.249
&gt; <span class="codestrong">dto$baseAcc</span>
[1] 0.5125714</pre>
<p class="indent">Again, we are doing much better with the features (25 percent error rate) than without them (51 percent).</p>
<p class="indent">We might also look at the confusion matrix:</p>
<pre class="calibre16">&gt; <span class="codestrong">dto$confusion</span>
      pred
actual   1   2   3   4   5   6   7
     1 241 103   0   0   0   0   3
     2  65 402   3   0   3   8   1
     3   0   4  54   0   1  12   0
     4   0   0   3   3   0   0   0
     5   1  10   0   0   8   1   0
     6   0   3  18   0   0  20   0
     7  10   0   0   0   0   0  23</pre>
<p class="noindent">Class 1 is often mispredicted as Class 2, and vice versa.<span epub:type="pagebreak" id="page_90"/></p>
<p class="indent">With the larger sample size <em class="calibre13">n</em> and number of features <em class="calibre13">p</em> here, a really large tree might be generated. In fact, it is much larger than in our previous examples:</p>
<pre class="calibre16">&gt; <span class="codestrong">dto$nNodes</span>
[1] 1065
&gt; <span class="codestrong">dto$nTermNodes</span>
[1] 533</pre>
<p class="noindent">The tree has 1,000 nodes, and about half of those are terminal nodes!</p>
<h3 class="h2" id="ch05lev5">5.5 Decision Tree Hyperparameters: How to Split?</h3>
<p class="noindent">DT packages differ from one another in terms of the details of their node-splitting actions. In most cases, the process is quite complex and thus beyond the scope of this book. The splitting process in <span class="literal">party</span> is no exception, but we need to have at least a rough overview of the process. We will focus on a major splitting criterion in <span class="literal">party</span> known as the <em class="calibre13">p-value</em>.</p>
<p class="indent">Look again at <a href="ch05.xhtml#ch05fig01" class="calibre12">Figure 5-1</a>. The oval contents show that the feature used to split is <span class="literal">Wind</span>, with a p-value of 0.002 and with a split point of 6.9. But originally, as the tree was being built, that oval was empty, with no lines emanating out of the bottom. How, then, was this node built to what we see in the figure?</p>
<p class="indent">Node 2 inherited data points from the left branch out of Node 1. Then the following algorithm was run:</p>
<pre class="calibre16">pv = null vector
for feature f in Solar.R, Wind, Temp, Month, Day do:
   for split_point in the values of f do:
      compute a p-value pval
      append pval to pv</pre>
<p class="noindent">We do the following on the output of the above pseudocode:</p>
<ul class="calibre15">
<li class="noindent3">If the smallest p-value is below a user-specified criterion, split the node using whichever feature and split point yielded the smallest p-value (in this case, <span class="literal">Wind</span> and 6.9).</li>
<li class="noindent3">If, on the other hand, the smallest p-value was not smaller than the user-specified criterion, do not split the node.</li>
</ul>
<p class="indent">We see, for instance, that for Node 2 and the potential (and later, actual) splitting feature <span class="literal">Wind</span>, there are many candidates for a potential split point:<span epub:type="pagebreak" id="page_91"/></p>
<pre class="calibre16">&gt; <span class="codestrong">sort(airq$Wind)</span>
  [1]  2.3  2.8  3.4  4.0  4.1  4.6  4.6  4.6  5.1  5.1  5.1  5.7  5.7  6.3  6.3
 [16]  6.3  6.3  6.3  6.3  6.9  6.9  6.9  6.9  6.9  6.9  6.9  6.9  7.4  7.4  7.4
 [31]  7.4  7.4  7.4  7.4  7.4  7.4  7.4  8.0  8.0  8.0  8.0  8.0  8.0  8.0  8.6
 [46]  8.6  8.6  9.2  9.2  9.2  9.2  9.2  9.2  9.7  9.7  9.7  9.7  9.7  9.7  9.7
 [61]  9.7  9.7 10.3 10.3 10.3 10.3 10.3 10.3 10.3 10.3 10.3 10.3 10.9 10.9 10.9
 [76] 10.9 10.9 10.9 11.5 11.5 11.5 11.5 11.5 11.5 11.5 11.5 11.5 11.5 12.0 12.0
 [91] 12.0 12.0 12.6 12.6 13.2 13.8 13.8 13.8 13.8 14.3 14.3 14.3 14.3 14.9 14.9
[106] 14.9 14.9 14.9 15.5 15.5 15.5 16.6 16.6 18.4 20.1 20.7</pre>
<p class="noindent">Any of the values 2.8, 3.4, . . . , 20.1 could be used. The algorithm takes each one into consideration.</p>
<p class="indent">Intuitively, we would like the split to produce two approximately balanced subsets, say, with a split at 9.7. But a more urgent requirement is that the two subsets differ a lot in their mean values of <em class="calibre13">Y</em>. If mean <em class="calibre13">Y</em> is fairly similar in the two candidate subsets, the node is deemed homogeneous and not split—at least for that feature.</p>
<p class="indent">Well, then, what constitutes differing by “a lot”? This is decided by a formal statistical significance test. This book does not assume a background in statistics, and, for our purposes here, we just state that a test is summarized by a number known as a p-value.</p>
<p class="indent">Testing has come under much criticism in recent years, and for good reason, in my opinion (see the file <em class="calibre13">NoPVals.md</em> in <span class="literal">regtools</span>). However, for the node-splitting purpose here, the p-value threshold is just another hyperparameter, named <span class="literal">alpha</span> in <span class="literal">qeDT()</span>. This default value is 0.05.</p>
<p class="indent">If the p-value is less than <span class="literal">alpha</span> for some candidate feature and candidate split point pair, then the node is deemed worth splitting. The feature and split point chosen are the pair with the smallest p-value. We see in <a href="ch05.xhtml#ch05fig01" class="calibre12">Figure 5-1</a> that the minimum p-value happened to be 0.002, which was associated with the <span class="literal">Wind</span> feature and a split point of 6.9. Since 0.002 &lt; 0.05, the node was split accordingly.</p>
<p class="indent">If no split point meets the above criterion, the node is not split. That happened in Node 3, so it became a terminal node.</p>
<h3 class="h2" id="ch05lev6">5.6 Hyperparameters in the qeDT() Function</h3>
<p class="noindent">As noted, DTs may be viewed as an extension of the k-NN idea. Each leaf node forms a kind of neighborhood whose data points have similar values of certain features. Recall from <a href="ch03.xhtml" class="calibre12">Chapter 3</a> that small neighborhoods lead to larger variance in a predicted <em class="calibre13">Y</em> value—just too small a sample to work from— while large neighborhoods may have bias problems (that is, points in the same neighborhood may be quite different from each other and thus not representative).</p>
<p class="indent">In a DT context, then, we should look at the leaf nodes to consider the Bias-Variance Trade-off. If there are too many small terminal nodes, we risk a variance problem, while too many large terminal nodes may mean a bias issue.<span epub:type="pagebreak" id="page_92"/></p>
<p class="indent">Here is where hyperparameters come into play. They control the tree configuration in various ways, and we can use cross-validation to choose the tree configuration with the best predictive ability.</p>
<p class="indent">The general call form is:</p>
<pre class="calibre16">&gt; <span class="codestrong">args(qeDT)</span>
function (data, yName, alpha = 0.05, minsplit = 20, minbucket = 7,
    maxdepth = 0, mtry = 0, holdout = floor(min(1000, 0.1 * nrow(data))))</pre>
<p class="indent">The <span class="literal">data</span>, <span class="literal">yName</span>, and <span class="literal">holdout</span> arguments are common to all the <span class="literal">qe*</span>-series functions. The remainder, <span class="literal">alpha</span>, <span class="literal">minsplit</span>, <span class="literal">minbucket</span>, <span class="literal">maxdepth</span>, and <span class="literal">mtry</span>, all deal with splitting criteria. Here are their roles:</p>
<p class="block"><span class="codestrong1">alpha</span>   As explained above.</p>
<p class="block"><span class="codestrong1">minsplit</span>   Here we can specify the minimum size for any node. The default of 20 means that we will not allow any node splitting to result in a node with fewer than 20 data points.</p>
<p class="block"><span class="codestrong1">minbucket</span>   Like <span class="literal">minsplit</span>, but specifically for terminal nodes.</p>
<p class="block"><span class="codestrong1">maxdepth</span>   Maximum number of levels or rows of the tree. In <a href="ch05.xhtml#ch05fig01" class="calibre12">Figure 5-1</a>, we have 4 levels, with the root in Level 1 and the leaf nodes in Level 4.</p>
<p class="block"><span class="codestrong1">mtry</span>   If this is nonzero, it is the number of features to try at each node; see below.</p>
<p class="indent">If <span class="literal">mtry</span> is nonzero, our splitting algorithm changes a bit:</p>
<pre class="calibre16">pv = null vector
randomly choose mtry features among Solar.R, Wind, Temp, Month, Day
for f in chosen feature set do
   for split_point in the values of f do:
      compute a p-value pval
      append pval to pv</pre>
<p class="noindent">This adds some randomness to the tree construction process, a step toward the ML method of <em class="calibre13">random forests</em>. We will see in the next chapter why this may be useful, but for the strict DT method, it is usually not used.</p>
<p class="indent">Consider each of the above hyperparameters in terms of the Bias-Variance Trade-off. Say we wish to make the leaf nodes smaller. All else being equal, we could accomplish this by making <span class="literal">alpha</span> larger, <span class="literal">minsplit</span> smaller, <span class="literal">minbucket</span> smaller, <span class="literal">maxdepth</span> larger, and <span class="literal">mtry</span> larger (or 0).</p>
<p class="indent">For instance, with a larger <span class="literal">alpha</span>, more p-values will be below this high threshold, so it is more likely that a node will split. As we go further down a tree, fewer data points remain, so if we encourage splits, when we reach a node that can’t be split, it won’t have many points left in it.</p>
<p class="indent">These hyperparameters don’t work independently of each other, so setting too many of them probably becomes redundant.<span epub:type="pagebreak" id="page_93"/></p>
<h3 class="h2" id="ch05lev7">5.7 Conclusions</h3>
<p class="noindent">Decision trees play a fundamental role in ML, and we will see them again in our material on bagging and boosting. As with any ML algorithm, we must deal with various hyperparameters, another topic to be viewed in depth later in the book.<span epub:type="pagebreak" id="page_94"/></p>
</div></body></html>